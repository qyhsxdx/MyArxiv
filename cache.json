{"2025-02-10T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.06781v1","updated":"2025-02-10T18:57:29Z","published":"2025-02-10T18:57:29Z","title":"Exploring the Limit of Outcome Reward for Learning Mathematical\n  Reasoning","summary":"  Reasoning abilities, especially those for solving complex math problems, are\ncrucial components of general intelligence. Recent advances by proprietary\ncompanies, such as o-series models of OpenAI, have made remarkable progress on\nreasoning tasks. However, the complete technical details remain unrevealed, and\nthe techniques that are believed certainly to be adopted are only reinforcement\nlearning (RL) and the long chain of thoughts. This paper proposes a new RL\nframework, termed OREAL, to pursue the performance limit that can be achieved\nthrough \\textbf{O}utcome \\textbf{RE}w\\textbf{A}rd-based reinforcement\n\\textbf{L}earning for mathematical reasoning tasks, where only binary outcome\nrewards are easily accessible. We theoretically prove that behavior cloning on\npositive trajectories from best-of-N (BoN) sampling is sufficient to learn the\nKL-regularized optimal policy in binary feedback environments. This formulation\nfurther implies that the rewards of negative samples should be reshaped to\nensure the gradient consistency between positive and negative samples. To\nalleviate the long-existing difficulties brought by sparse rewards in RL, which\nare even exacerbated by the partial correctness of the long chain of thought\nfor reasoning tasks, we further apply a token-level reward model to sample\nimportant tokens in reasoning trajectories for learning. With OREAL, for the\nfirst time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL,\nbeing on par with 32B models. OREAL-32B also surpasses previous 32B models\ntrained by distillation with 95.0 pass@1 accuracy on MATH-500. Our\ninvestigation also indicates the importance of initial policy models and\ntraining queries for RL. Code, models, and data will be released to benefit\nfuture research\\footnote{https://github.com/InternLM/OREAL}.\n","authors":["Chengqi Lyu","Songyang Gao","Yuzhe Gu","Wenwei Zhang","Jianfei Gao","Kuikun Liu","Ziyi Wang","Shuaibin Li","Qian Zhao","Haian Huang","Weihan Cao","Jiangning Liu","Hongwei Liu","Junnan Liu","Songyang Zhang","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06781v1.pdf","comment":"We released our code, data, and model on\n  https://github.com/InternLM/OREAL"},{"id":"http://arxiv.org/abs/2501.08413v2","updated":"2025-02-10T18:57:27Z","published":"2025-01-14T20:08:16Z","title":"Ensemble of Large Language Models for Curated Labeling and Rating of\n  Free-text Data","summary":"  Free-text responses are commonly collected in psychological studies,\nproviding rich qualitative insights that quantitative measures may not capture.\nLabeling curated topics of research interest in free-text data by multiple\ntrained human coders is typically labor-intensive and time-consuming. Though\nlarge language models (LLMs) excel in language processing, LLM-assisted\nlabeling techniques relying on closed-source LLMs cannot be directly applied to\nfree-text data, without explicit consent for external use.\n  In this study, we propose a framework of assembling locally-deployable LLMs\nto enhance the labeling of predetermined topics in free-text data under privacy\nconstraints. Analogous to annotation by multiple human raters, this framework\nleverages the heterogeneity of diverse open-source LLMs. The ensemble approach\nseeks a balance between the agreement and disagreement across LLMs, guided by a\nrelevancy scoring methodology that utilizes embedding distances between topic\ndescriptions and LLMs' reasoning. We evaluated the ensemble approach using both\npublicly accessible Reddit data from eating disorder related forums, and\nfree-text responses from eating disorder patients, both complemented by human\nannotations.\n  We found that: (1) there is heterogeneity in the performance of labeling\namong same-sized LLMs, with some showing low sensitivity but high precision,\nwhile others exhibit high sensitivity but low precision. (2) Compared to\nindividual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal\nprecision-sensitivity trade-off in predicting human annotations. (3) The\nrelevancy scores across LLMs showed greater agreement than dichotomous labels,\nindicating that the relevancy scoring method effectively mitigates the\nheterogeneity in LLMs' labeling.\n","authors":["Jiaxing Qiu","Dongliang Guo","Natalie Papini","Noelle Peace","Cheri A. Levinson","Teague R. Henry"],"pdf_url":"https://arxiv.org/pdf/2501.08413v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06773v1","updated":"2025-02-10T18:52:04Z","published":"2025-02-10T18:52:04Z","title":"On the Emergence of Thinking in LLMs I: Searching for the Right\n  Intuition","summary":"  Recent AI advancements, such as OpenAI's new models, are transforming LLMs\ninto LRMs (Large Reasoning Models) that perform reasoning during inference,\ntaking extra time and compute for higher-quality outputs. We aim to uncover the\nalgorithmic framework for training LRMs. Methods like self-consistency, PRM,\nand AlphaZero suggest reasoning as guided search. We ask: what is the simplest,\nmost scalable way to enable search in LLMs?\n  We propose a post-training framework called Reinforcement Learning via\nSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with\nhuman or synthetic demonstrations of the reasoning process, (2) using an\nexploration reward signal to encourage diverse and efficient reasoning\nbehaviors, and (3) RL training with an outcome verifier to ensure correctness\nwhile preventing reward hacking. Our key innovation is to decouple exploration\nand correctness signals during PPO training, carefully balancing them to\nimprove performance and efficiency.\n  Empirical studies in the math domain show that RLSP improves reasoning. On\nthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500\ntest set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due\nto RLSP. However, a more important finding of this work is that the models\ntrained using RLSP, even with the simplest exploration reward that encourages\nthe model to take more intermediate steps, showed several emergent behaviors\nsuch as backtracking, exploration of ideas, and verification. These findings\ndemonstrate that RLSP framework might be enough to enable emergence of complex\nreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why\nRLSP search strategy is more suitable for LLMs inspired by a remarkable result\nthat says CoT provably increases computational power of LLMs, which grows as\nthe number of steps in CoT \\cite{li2024chain,merrill2023expresssive}.\n","authors":["Guanghao Ye","Khiem Duc Pham","Xinzhi Zhang","Sivakanth Gopi","Baolin Peng","Beibin Li","Janardhan Kulkarni","Huseyin A. Inan"],"pdf_url":"https://arxiv.org/pdf/2502.06773v1.pdf","comment":"Abstract shortened for arXiv"},{"id":"http://arxiv.org/abs/2502.06772v1","updated":"2025-02-10T18:51:47Z","published":"2025-02-10T18:51:47Z","title":"ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates","summary":"  We present that hierarchical LLM reasoning via scaling thought templates can\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train our ReasonFlux-32B model with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-level thought templates capable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence of thought templates instead of long CoTs, optimizing a\nbase LLM to plan out an optimal template trajectory for gradually handling\ncomplex problems; (iii) a brand new inference scaling system that enables\nhierarchical LLM reasoning by adaptively scaling thought templates at inference\ntime. With a template trajectory containing sequential thought templates, our\nReasonFlux-32B significantly advances math reasoning capabilities to\nstate-of-the-art levels. Notably, on the MATH benchmark, it achieves an\naccuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad\n(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,\nsurpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:\nhttps://github.com/Gen-Verse/ReasonFlux\n","authors":["Ling Yang","Zhaochen Yu","Bin Cui","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06772v1.pdf","comment":"Code: https://github.com/Gen-Verse/ReasonFlux"},{"id":"http://arxiv.org/abs/2502.06766v1","updated":"2025-02-10T18:47:04Z","published":"2025-02-10T18:47:04Z","title":"Exploiting Sparsity for Long Context Inference: Million Token Contexts\n  on Commodity GPUs","summary":"  There is growing demand for performing inference with hundreds of thousands\nof input tokens on trained transformer models. Inference at this extreme scale\ndemands significant computational resources, hindering the application of\ntransformers at long contexts on commodity (i.e not data center scale)\nhardware. To address the inference time costs associated with running\nself-attention based transformer language models on long contexts and enable\ntheir adoption on widely available hardware, we propose a tunable mechanism\nthat reduces the cost of the forward pass by attending to only the most\nrelevant tokens at every generation step using a top-k selection mechanism. We\nshowcase the efficiency gains afforded by our method by performing inference on\ncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Our\nexperiments reveal that models are capable of handling the sparsity induced by\nthe reduced number of keys and values. By attending to less than 2% of input\ntokens, we achieve over 95% of model performance on common long context\nbenchmarks (LM-Eval, AlpacaEval, and RULER).\n","authors":["Ryan Synk","Monte Hoover","John Kirchenbauer","Neel Jain","Alex Stein","Manli Shu","Josue Melendez Sanchez","Ramani Duraiswami","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2502.06766v1.pdf","comment":"8 pages, 8 figures, 2 tables in main body"},{"id":"http://arxiv.org/abs/2410.10626v2","updated":"2025-02-10T18:43:26Z","published":"2024-10-14T15:31:54Z","title":"Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of\n  Language Family Experts","summary":"  Adapting medical Large Language Models to local languages can reduce barriers\nto accessing healthcare services, but data scarcity remains a significant\nchallenge, particularly for low-resource languages. To address this, we first\nconstruct a high-quality medical dataset and conduct analysis to ensure its\nquality. In order to leverage the generalization capability of multilingual\nLLMs to efficiently scale to more resource-constrained languages, we explore\nthe internal information flow of LLMs from a multilingual perspective using\nMixture of Experts (MoE) modularity. Technically, we propose a novel MoE\nrouting method that employs language-specific experts and cross-lingual\nrouting. Inspired by circuit theory, our routing analysis revealed a Spread Out\nin the End information flow mechanism: while earlier layers concentrate\ncross-lingual information flow, the later layers exhibit language-specific\ndivergence. This insight directly led to the development of the Post-MoE\narchitecture, which applies sparse routing only in the later layers while\nmaintaining dense others. Experimental results demonstrate that this approach\nenhances the generalization of multilingual models to other languages while\npreserving interpretability. Finally, to efficiently scale the model to 50\nlanguages, we introduce the concept of language family experts, drawing on\nlinguistic priors, which enables scaling the number of languages without adding\nadditional parameters.\n","authors":["Guorui Zheng","Xidong Wang","Juhao Liang","Nuo Chen","Yuping Zheng","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01718v3","updated":"2025-02-10T18:40:00Z","published":"2025-02-03T18:46:04Z","title":"ACECODER: Acing Coder RL via Automated Test-Case Synthesis","summary":"  Most progress in recent coder models has been driven by supervised\nfine-tuning (SFT), while the potential of reinforcement learning (RL) remains\nlargely unexplored, primarily due to the lack of reliable reward data/model in\nthe code domain. In this paper, we address this challenge by leveraging\nautomated large-scale test-case synthesis to enhance code model training.\nSpecifically, we design a pipeline that generates extensive (question,\ntest-cases) pairs from existing code data. Using these test cases, we construct\npreference pairs based on pass rates over sampled programs to train reward\nmodels with Bradley-Terry loss. It shows an average of 10-point improvement for\nLlama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through\nbest-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5.\nFurthermore, we conduct reinforcement learning with both reward models and\ntest-case pass rewards, leading to consistent improvements across HumanEval,\nMBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style\ntraining to start from Qwen2.5-Coder-base directly and show that our RL\ntraining can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\%\nfor merely 80 optimization steps. We believe our results highlight the huge\npotential of reinforcement learning in coder models.\n","authors":["Huaye Zeng","Dongfu Jiang","Haozhe Wang","Ping Nie","Xiaotong Chen","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.01718v3.pdf","comment":"9 pages, 1 figure, 8 tables"},{"id":"http://arxiv.org/abs/2409.17755v2","updated":"2025-02-10T18:39:13Z","published":"2024-09-26T11:40:07Z","title":"SECURE: Semantics-aware Embodied Conversation under Unawareness for\n  Lifelong Robot Learning","summary":"  This paper addresses a challenging interactive task learning scenario we call\nrearrangement under unawareness: to manipulate a rigid-body environment in a\ncontext where the agent is unaware of a concept that is key to solving the\ninstructed task. We propose SECURE, an interactive task learning framework\ndesigned to solve such problems. It uses embodied conversation to fix its\ndeficient domain model -- through dialogue, the agent discovers and then learns\nto exploit unforeseen possibilities. In particular, SECURE learns from the\nuser's embodied corrective feedback when it makes a mistake, and it makes\nstrategic dialogue decisions to reveal useful evidence about novel concepts for\nsolving the instructed task. Together, these abilities allow the agent to\ngeneralise to subsequent tasks using newly acquired knowledge. We demonstrate\nthat learning to solve rearrangement under unawareness is more data efficient\nwhen the agent is semantics-aware -- that is, during both learning and\ninference it augments the evidence from the user's embodied conversation with\nits logical consequences, stemming from semantic analysis.\n","authors":["Rimvydas Rubavicius","Peter David Fagan","Alex Lascarides","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2409.17755v2.pdf","comment":"22 pages,4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.06759v1","updated":"2025-02-10T18:38:57Z","published":"2025-02-10T18:38:57Z","title":"Rationalization Models for Text-to-SQL","summary":"  We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.\n","authors":["Gaetano Rossiello","Nhan Pham","Michael Glass","Junkyu Lee","Shankar Subramanian"],"pdf_url":"https://arxiv.org/pdf/2502.06759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06621v2","updated":"2025-02-10T18:35:29Z","published":"2024-06-07T15:28:31Z","title":"LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph\n  Question-Answering","summary":"  We present LinkQ, a system that leverages a large language model (LLM) to\nfacilitate knowledge graph (KG) query construction through natural language\nquestion-answering. Traditional approaches often require detailed knowledge of\na graph querying language, limiting the ability for users -- even experts -- to\nacquire valuable insights from KGs. LinkQ simplifies this process by\nimplementing a multistep protocol in which the LLM interprets a user's\nquestion, then systematically converts it into a well-formed query. LinkQ helps\nusers iteratively refine any open-ended questions into precise ones, supporting\nboth targeted and exploratory analysis. Further, LinkQ guards against the LLM\nhallucinating outputs by ensuring users' questions are only ever answered from\nground truth KG data. We demonstrate the efficacy of LinkQ through a\nqualitative study with five KG practitioners. Our results indicate that\npractitioners find LinkQ effective for KG question-answering, and desire future\nLLM-assisted exploratory data analysis systems.\n","authors":["Harry Li","Gabriel Appleby","Ashley Suh"],"pdf_url":"https://arxiv.org/pdf/2406.06621v2.pdf","comment":"Open-source code: https://github.com/mit-ll/linkq"},{"id":"http://arxiv.org/abs/2408.00761v4","updated":"2025-02-10T18:26:14Z","published":"2024-08-01T17:59:12Z","title":"Tamper-Resistant Safeguards for Open-Weight LLMs","summary":"  Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after hundreds of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that progress on tamper-resistance is\npossible, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.\n","authors":["Rishub Tamirisa","Bhrugu Bharathi","Long Phan","Andy Zhou","Alice Gatti","Tarun Suresh","Maxwell Lin","Justin Wang","Rowan Wang","Ron Arel","Andy Zou","Dawn Song","Bo Li","Dan Hendrycks","Mantas Mazeika"],"pdf_url":"https://arxiv.org/pdf/2408.00761v4.pdf","comment":"Website: https://www.tamper-resistant-safeguards.com"},{"id":"http://arxiv.org/abs/2501.18101v3","updated":"2025-02-10T18:22:52Z","published":"2025-01-30T02:47:41Z","title":"Diverse Preference Optimization","summary":"  Post-training of language models, either through reinforcement learning,\npreference optimization or supervised finetuning, tends to sharpen the output\nprobability distribution and reduce the diversity of generated responses. This\nis particularly a problem for creative generative tasks where varied responses\nare desired. In this work we introduce Diverse Preference Optimization (DivPO),\nan optimization method which learns to generate much more diverse responses\nthan standard pipelines, while maintaining the quality of the generations. In\nDivPO, preference pairs are selected by first considering a pool of responses,\nand a measure of diversity among them, and selecting chosen examples as being\nmore rare but high quality, while rejected examples are more common, but low\nquality. DivPO results in generating 45.6% more diverse persona attributes, and\nan 74.6% increase in story diversity, while maintaining similar win rates as\nstandard baselines.\n","authors":["Jack Lanchantin","Angelica Chen","Shehzaad Dhuliawala","Ping Yu","Jason Weston","Sainbayar Sukhbaatar","Ilia Kulikov"],"pdf_url":"https://arxiv.org/pdf/2501.18101v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06703v1","updated":"2025-02-10T17:30:23Z","published":"2025-02-10T17:30:23Z","title":"Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling","summary":"  Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.\n","authors":["Runze Liu","Junqi Gao","Jian Zhao","Kaiyan Zhang","Xiu Li","Biqing Qi","Wanli Ouyang","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.06703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11619v2","updated":"2025-02-10T17:26:40Z","published":"2024-10-15T13:56:34Z","title":"MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video\n  Retrieval","summary":"  Efficiently retrieving and synthesizing information from large-scale\nmultimodal collections has become a critical challenge. However, existing video\nretrieval datasets suffer from scope limitations, primarily focusing on\nmatching descriptive but vague queries with small collections of professionally\nedited, English-centric videos. To address this gap, we introduce\n$\\textbf{MultiVENT 2.0}$, a large-scale, multilingual event-centric video\nretrieval benchmark featuring a collection of more than 218,000 news videos and\n3,906 queries targeting specific world events. These queries specifically\ntarget information found in the visual content, audio, embedded text, and text\nmetadata of the videos, requiring systems leverage all these sources to succeed\nat the task. Preliminary results show that state-of-the-art vision-language\nmodels struggle significantly with this task, and while alternative approaches\nshow promise, they are still insufficient to adequately address this problem.\nThese findings underscore the need for more robust multimodal retrieval\nsystems, as effective video retrieval is a crucial step towards multimodal\ncontent understanding and generation.\n","authors":["Reno Kriz","Kate Sanders","David Etter","Kenton Murray","Cameron Carpenter","Kelly Van Ochten","Hannah Recknor","Jimena Guallar-Blasco","Alexander Martin","Ronald Colaianni","Nolan King","Eugene Yang","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2410.11619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13629v2","updated":"2025-02-10T17:19:21Z","published":"2025-01-23T12:58:14Z","title":"Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models","summary":"  We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.\n","authors":["Zhenghao Lin","Zihao Tang","Xiao Liu","Yeyun Gong","Yi Cheng","Qi Chen","Hang Li","Ying Xin","Ziyue Yang","Kailai Yang","Yu Yan","Xiao Liang","Shuai Lu","Yiming Huang","Zheheng Luo","Lei Qu","Xuan Feng","Yaoxiang Wang","Yuqing Xia","Feiyang Chen","Yuting Jiang","Yasen Hu","Hao Ni","Binyang Li","Guoshuai Zhao","Jui-Hao Chiang","Zhongxin Guo","Chen Lin","Kun Kuang","Wenjie Li","Yelong Shen","Jian Jiao","Peng Cheng","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2501.13629v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06692v1","updated":"2025-02-10T17:16:55Z","published":"2025-02-10T17:16:55Z","title":"Multi-label Scandinavian Language Identification (SLIDE)","summary":"  Identifying closely related languages at sentence level is difficult, in\nparticular because it is often impossible to assign a sentence to a single\nlanguage. In this paper, we focus on multi-label sentence-level Scandinavian\nlanguage identification (LID) for Danish, Norwegian Bokm\\r{a}l, Norwegian\nNynorsk, and Swedish. We present the Scandinavian Language Identification and\nEvaluation, SLIDE, a manually curated multi-label evaluation dataset and a\nsuite of LID models with varying speed-accuracy tradeoffs. We demonstrate that\nthe ability to identify multiple languages simultaneously is necessary for any\naccurate LID method, and present a novel approach to training such multi-label\nLID models.\n","authors":["Mariia Fedorova","Jonas Sebulon Frydenberg","Victoria Handford","Victoria Ovedie Chruickshank Langø","Solveig Helene Willoch","Marthe Løken Midtgaard","Yves Scherrer","Petter Mæhlum","David Samuel"],"pdf_url":"https://arxiv.org/pdf/2502.06692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06669v1","updated":"2025-02-10T16:54:03Z","published":"2025-02-10T16:54:03Z","title":"Boosting Self-Efficacy and Performance of Large Language Models via\n  Verbal Efficacy Stimulations","summary":"  Significant improvements have been observed in the zero-shot capabilities of\nthe Large Language Models (LLMs). Due to their high sensitivity to input,\nresearch has increasingly focused on enhancing LLMs' performance via direct and\nsimple prompt engineering rather than intricate domain adaptation. Studies\nsuggest that LLMs exhibit emotional intelligence, and both positive and\nnegative emotions can potentially enhance task performances. However, prior\ninteraction prompts have predominantly concentrated on a single stimulus type,\nneglecting to compare different stimulus effects, examine the influence of\nvarying task difficulties, or explore underlying mechanisms. This paper,\ninspired by the positive correlation between self-efficacy and task performance\nwithin the social cognitive theory, introduces Verbal Efficacy Stimulations\n(VES). Our VES comprises three types of verbal prompts: encouraging,\nprovocative, and critical, addressing six aspects such as helpfulness and\ncompetence. And we further categorize task difficulty, aiming to extensively\ninvestigate how distinct VES influence the self-efficacy and task achievements\nof language models at varied levels of difficulty. The experimental results\nshow that the three types of VES improve the performance of LLMs on most tasks,\nand the most effective VES varies for different models. In extensive\nexperiments, we have obtained some findings consistent with psychological\ntheories, providing novel insights for future research.\n","authors":["Rui Chen","Tailai Peng","Xinran Xie","Dekun Lin","Zhe Cui","Zheng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06669v1.pdf","comment":"to be published in ICONIP 2024"},{"id":"http://arxiv.org/abs/2502.06666v1","updated":"2025-02-10T16:52:39Z","published":"2025-02-10T16:52:39Z","title":"Automatic Evaluation of Healthcare LLMs Beyond Question-Answering","summary":"  Current Large Language Models (LLMs) benchmarks are often based on open-ended\nor close-ended QA evaluations, avoiding the requirement of human labor.\nClose-ended measurements evaluate the factuality of responses but lack\nexpressiveness. Open-ended capture the model's capacity to produce discourse\nresponses but are harder to assess for correctness. These two approaches are\ncommonly used, either independently or together, though their relationship\nremains poorly understood. This work is focused on the healthcare domain, where\nboth factuality and discourse matter greatly. It introduces a comprehensive,\nmulti-axis suite for healthcare LLM evaluation, exploring correlations between\nopen and close benchmarks and metrics. Findings include blind spots and\noverlaps in current methodologies. As an updated sanity check, we release a new\nmedical benchmark--CareQA--, with both open and closed variants. Finally, we\npropose a novel metric for open-ended evaluations --Relaxed Perplexity-- to\nmitigate the identified limitations.\n","authors":["Anna Arias-Duart","Pablo Agustin Martin-Torres","Daniel Hinjos","Pablo Bernabeu-Perez","Lucia Urcelay Ganzabal","Marta Gonzalez Mallo","Ashwin Kumar Gururajan","Enrique Lopez-Cuena","Sergio Alvarez-Napagao","Dario Garcia-Gasulla"],"pdf_url":"https://arxiv.org/pdf/2502.06666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06659v1","updated":"2025-02-10T16:48:56Z","published":"2025-02-10T16:48:56Z","title":"Who Taught You That? Tracing Teachers in Model Distillation","summary":"  Model distillation -- using outputs from a large teacher model to teach a\nsmall student model -- is a practical means of creating efficient models for a\nparticular task. We ask: Can we identify a students' teacher based on its\noutputs? Such \"footprints\" left by teacher LLMs would be interesting artifacts.\nBeyond this, reliable teacher inference may have practical implications as\nactors seek to distill specific capabilities of massive proprietary LLMs into\ndeployed smaller LMs, potentially violating terms of service. We consider\npractical task distillation targets including summarization, question\nanswering, and instruction-following. We assume a finite set of candidate\nteacher models, which we treat as blackboxes. We design discriminative models\nthat operate over lexical features. We find that $n$-gram similarity alone is\nunreliable for identifying teachers, but part-of-speech (PoS) templates\npreferred by student models mimic those of their teachers.\n","authors":["Somin Wadhwa","Chantal Shaib","Silvio Amir","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2502.06659v1.pdf","comment":"Preprint; under review"},{"id":"http://arxiv.org/abs/2502.06653v1","updated":"2025-02-10T16:43:32Z","published":"2025-02-10T16:43:32Z","title":"In-Context Learning (and Unlearning) of Length Biases","summary":"  Large language models have demonstrated strong capabilities to learn\nin-context, where exemplar input-output pairings are appended to the prompt for\ndemonstration. However, existing work has demonstrated the ability of models to\nlearn lexical and label biases in-context, which negatively impacts both\nperformance and robustness of models. The impact of other statistical data\nbiases remains under-explored, which this work aims to address. We specifically\ninvestigate the impact of length biases on in-context learning. We demonstrate\nthat models do learn length biases in the context window for their predictions,\nand further empirically analyze the factors that modulate the level of bias\nexhibited by the model. In addition, we show that learning length information\nin-context can be used to counter the length bias that has been encoded in\nmodels (e.g., via fine-tuning). This reveals the power of in-context learning\nin debiasing model prediction behaviors without the need for costly parameter\nupdates.\n","authors":["Stephanie Schoch","Yangfeng Ji"],"pdf_url":"https://arxiv.org/pdf/2502.06653v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2409.19020v3","updated":"2025-02-10T16:42:23Z","published":"2024-09-25T07:03:31Z","title":"DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications","summary":"  The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research.\n","authors":["Sathya Krishnan Suresh","Wu Mengjun","Tushar Pranav","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2409.19020v3.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.06652v1","updated":"2025-02-10T16:42:00Z","published":"2025-02-10T16:42:00Z","title":"Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A","summary":"  The transparency principle of the General Data Protection Regulation (GDPR)\nrequires data processing information to be clear, precise, and accessible.\nWhile language models show promise in this context, their probabilistic nature\ncomplicates truthfulness and comprehensibility.\n  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)\nsystems enhanced with alignment techniques to fulfill GDPR obligations. We\nevaluate RAG systems incorporating an alignment module like Rewindable\nAuto-regressive Inference (RAIN) and our proposed multidimensional extension,\nMultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness\nand comprehensibility and are assessed through 21 metrics, including\ndeterministic and large language model-based evaluations.\n  Our results show that RAG systems with an alignment module outperform\nbaseline RAG systems on most metrics, though none fully match human answers.\nPrincipal component analysis of the results reveals complex interactions\nbetween metrics, highlighting the need to refine metrics. This study provides a\nfoundation for integrating advanced natural language processing systems into\nlegal compliance frameworks.\n","authors":["Anna Leschanowsky","Zahra Kolagar","Erion Çano","Ivan Habernal","Dara Hallinan","Emanuël A. P. Habets","Birgit Popp"],"pdf_url":"https://arxiv.org/pdf/2502.06652v1.pdf","comment":"Submitted to ARR"},{"id":"http://arxiv.org/abs/2502.06648v1","updated":"2025-02-10T16:38:03Z","published":"2025-02-10T16:38:03Z","title":"The 2021 Tokyo Olympics Multilingual News Article Dataset","summary":"  In this paper, we introduce a dataset of multilingual news articles covering\nthe 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from\n1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and\npublished between July 1, 2021, and August 14, 2021. These articles are written\nin nine languages from different language families and in different scripts. To\ncreate the dataset, the raw news articles were first retrieved via a service\nthat collects and analyzes news articles. Then, the articles were grouped using\nan online clustering algorithm, with each group containing articles reporting\non the same sub-event. Finally, the groups were manually annotated and\nevaluated. The development of this dataset aims to provide a resource for\nevaluating the performance of multilingual news clustering algorithms, for\nwhich limited datasets are available. It can also be used to analyze the\ndynamics and events of the 2021 Tokyo Olympics from different perspectives. The\ndataset is available in CSV format and can be accessed from the CLARIN.SI\nrepository.\n","authors":["Erik Novak","Erik Calcina","Dunja Mladenić","Marko Grobelnik"],"pdf_url":"https://arxiv.org/pdf/2502.06648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04419v2","updated":"2025-02-10T16:34:03Z","published":"2025-02-06T15:20:58Z","title":"Understanding and Mitigating the Bias Inheritance in LLM-based Data\n  Augmentation on Downstream Tasks","summary":"  Generating synthetic datasets via large language models (LLMs) themselves has\nemerged as a promising approach to improve LLM performance. However, LLMs\ninherently reflect biases present in their training data, leading to a critical\nchallenge: when these models generate synthetic data for training, they may\npropagate and amplify their inherent biases that can significantly impact model\nfairness and robustness on downstream tasks--a phenomenon we term bias\ninheritance. This work presents the first systematic investigation in\nunderstanding, analyzing, and mitigating bias inheritance. We study this\nproblem by fine-tuning LLMs with a combined dataset consisting of original and\nLLM-augmented data, where bias ratio represents the proportion of augmented\ndata. Through systematic experiments across 10 classification and generation\ntasks, we analyze how 6 different types of biases manifest at varying bias\nratios. Our results reveal that bias inheritance has nuanced effects on\ndownstream tasks, influencing both classification tasks and generation tasks\ndifferently. Then, our analysis identifies three key misalignment factors:\nmisalignment of values, group data, and data distributions. Based on these\ninsights, we propose three mitigation strategies: token-based, mask-based, and\nloss-based approaches. Experiments demonstrate that these strategies also work\ndifferently on various tasks and bias, indicating the substantial challenges to\nfully mitigate bias inheritance. We hope this work can provide valuable\ninsights to the research of LLM data augmentation.\n","authors":["Miaomiao Li","Hao Chen","Yang Wang","Tingyuan Zhu","Weijia Zhang","Kaijie Zhu","Kam-Fai Wong","Jindong Wang"],"pdf_url":"https://arxiv.org/pdf/2502.04419v2.pdf","comment":"Technical report; 31 pages"},{"id":"http://arxiv.org/abs/2502.06635v1","updated":"2025-02-10T16:31:37Z","published":"2025-02-10T16:31:37Z","title":"Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM","summary":"  Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM.\n","authors":["Qingshui Gu","Shu Li","Tianyu Zheng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04295v2","updated":"2025-02-10T16:25:03Z","published":"2025-02-06T18:36:44Z","title":"Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization","summary":"  Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code is available at\nhttps://github.com/HenryLau7/CFPO.\n","authors":["Yuanye Liu","Jiahang Xu","Li Lyna Zhang","Qi Chen","Xuan Feng","Yang Chen","Zhongxin Guo","Yuqing Yang","Peng Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.04295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05232v2","updated":"2025-02-10T16:22:08Z","published":"2024-12-06T18:02:59Z","title":"LIAR: Leveraging Inference Time Alignment (Best-of-N) to Jailbreak LLMs\n  in Seconds","summary":"  Traditional jailbreaks have successfully exposed vulnerabilities in LLMs,\nprimarily relying on discrete combinatorial optimization, while more recent\nmethods focus on training LLMs to generate adversarial prompts. However, both\napproaches are computationally expensive and slow, often requiring significant\nresources to generate a single successful attack. We hypothesize that the\ninefficiency of these methods arises from an inadequate characterization of the\njailbreak problem itself. To address this gap, we approach the jailbreak\nproblem as an alignment problem, leading us to propose LIAR (Leveraging\nInference time Alignment to jailbReak), a fast and efficient best-of-N approach\ntailored for jailbreak attacks. LIAR offers several key advantages: it\neliminates the need for additional training, operates in a fully black-box\nsetting, significantly reduces computational overhead, and produces more\nhuman-readable adversarial prompts while maintaining competitive attack success\nrates. Our results demonstrate that a best-of-N approach is a simple yet highly\neffective strategy for evaluating the robustness of aligned LLMs, achieving\nattack success rates (ASR) comparable to state-of-the-art methods while\noffering a 10x improvement in perplexity and a significant speedup in\nTime-to-Attack, reducing execution time from tens of hours to seconds.\nAdditionally, We also provide sub-optimality guarantees for the proposed LIAR.\nOur work highlights the potential of efficient, alignment-based jailbreak\nstrategies for assessing and stress-testing AI safety measures.\n","authors":["James Beetham","Souradip Chakraborty","Mengdi Wang","Furong Huang","Amrit Singh Bedi","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2412.05232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06617v1","updated":"2025-02-10T16:15:08Z","published":"2025-02-10T16:15:08Z","title":"Scaling Multi-Document Event Summarization: Evaluating Compression vs.\n  Full-Text Approaches","summary":"  Automatically summarizing large text collections is a valuable tool for\ndocument research, with applications in journalism, academic research, legal\nwork, and many other fields. In this work, we contrast two classes of systems\nfor large-scale multi-document summarization (MDS): compression and full-text.\nCompression-based methods use a multi-stage pipeline and often lead to lossy\nsummaries. Full-text methods promise a lossless summary by relying on recent\nadvances in long-context reasoning. To understand their utility on large-scale\nMDS, we evaluated them on three datasets, each containing approximately one\nhundred documents per summary. Our experiments cover a diverse set of\nlong-context transformers (Llama-3.1, Command-R, Jamba-1.5-Mini) and\ncompression methods (retrieval-augmented, hierarchical, incremental). Overall,\nwe find that full-text and retrieval methods perform the best in most settings.\nWith further analysis into the salient information retention patterns, we show\nthat compression-based methods show strong promise at intermediate stages, even\noutperforming full-context. However, they suffer information loss due to their\nmulti-stage pipeline and lack of global context. Our results highlight the need\nto develop hybrid approaches that combine compression and full-text approaches\nfor optimal performance on large-scale multi-document summarization.\n","authors":["Adithya Pratapa","Teruko Mitamura"],"pdf_url":"https://arxiv.org/pdf/2502.06617v1.pdf","comment":"NAACL 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2502.06604v1","updated":"2025-02-10T16:01:55Z","published":"2025-02-10T16:01:55Z","title":"Do we really have to filter out random noise in pre-training data for\n  language models?","summary":"  Web-scale pre-training datasets are the cornerstone of LLMs' success.\nHowever, text data curated from the internet inevitably contains random noise\ncaused by decoding errors or unregulated web content. In contrast to previous\nworks that focus on low quality or synthetic data, our study \\textbf{provides\nthe first systematic investigation into such random noise through a cohesive\n``What-Why-How'' framework.} Surprisingly, we observed that the resulting\nincrease in next-token prediction (NTP) loss was significantly lower than the\nproportion of random noise. We provide a theoretical justification for this\nphenomenon, which also elucidates the success of multilingual models. On the\nother hand, experiments show that the model's performance in downstream tasks\nis not based solely on the NTP loss, which means that random noise may result\nin degraded downstream performance. To address the potential adverse effects,\nwe introduce a novel plug-and-play Local Gradient Matching loss, which\nexplicitly enhances the denoising capability of the downstream task head by\naligning the gradient of normal and perturbed features without requiring\nknowledge of the model's parameters. Additional experiments on 8 language and\n14 vision benchmarks further validate its effectiveness.\n","authors":["Jinghan Ru","Yuxin Xie","Xianwei Zhuang","Yuguo Yin","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2502.06604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06600v1","updated":"2025-02-10T16:00:00Z","published":"2025-02-10T16:00:00Z","title":"Evaluation of Multilingual Image Captioning: How far can we get with\n  CLIP models?","summary":"  The evaluation of image captions, looking at both linguistic fluency and\nsemantic correspondence to visual contents, has witnessed a significant effort.\nStill, despite advancements such as the CLIPScore metric, multilingual\ncaptioning evaluation has remained relatively unexplored. This work presents\nseveral strategies, and extensive experiments, related to evaluating CLIPScore\nvariants in multilingual settings. To address the lack of multilingual test\ndata, we consider two different strategies: (1) using quality aware\nmachine-translated datasets with human judgements, and (2) re-purposing\nmultilingual datasets that target semantic inference and reasoning. Our results\nhighlight the potential of finetuned multilingual models to generalize across\nlanguages and to handle complex linguistic challenges. Tests with\nmachine-translated data show that multilingual CLIPScore models can maintain a\nhigh correlation with human judgements across different languages, and\nadditional tests with natively multilingual and multicultural data further\nattest to the high-quality assessments.\n","authors":["Gonçalo Gomes","Chrysoula Zerva","Bruno Martins"],"pdf_url":"https://arxiv.org/pdf/2502.06600v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.06589v1","updated":"2025-02-10T15:54:34Z","published":"2025-02-10T15:54:34Z","title":"Hephaestus: Improving Fundamental Agent Capabilities of Large Language\n  Models through Continual Pre-Training","summary":"  Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments.\n","authors":["Yuchen Zhuang","Jingfeng Yang","Haoming Jiang","Xin Liu","Kewei Cheng","Sanket Lokegaonkar","Yifan Gao","Qing Ping","Tianyi Liu","Binxuan Huang","Zheng Li","Zhengyang Wang","Pei Chen","Ruijie Wang","Rongzhi Zhang","Nasser Zalmout","Priyanka Nigam","Bing Yin","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06589v1.pdf","comment":"Accepted to NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2408.05212v2","updated":"2025-02-10T15:42:08Z","published":"2024-08-10T05:41:19Z","title":"Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions","summary":"  Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.\n","authors":["Michele Miranda","Elena Sofia Ruzzetti","Andrea Santilli","Fabio Massimo Zanzotto","Sébastien Bratières","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2408.05212v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)\n  https://openreview.net/forum?id=Ss9MTTN7OL"},{"id":"http://arxiv.org/abs/2501.10868v2","updated":"2025-02-10T15:41:37Z","published":"2025-01-18T20:26:00Z","title":"Generating Structured Outputs from Language Models: Benchmark and\n  Studies","summary":"  Reliably generating structured outputs has become a critical capability for\nmodern language model (LM) applications. Constrained decoding has emerged as\nthe dominant technology across sectors for enforcing structured outputs during\ngeneration. Despite its growing adoption, little has been done with the\nsystematic evaluation of the behaviors and performance of constrained decoding.\nConstrained decoding frameworks have standardized around JSON Schema as a\nstructured data format, with most uses guaranteeing constraint compliance given\na schema. However, there is poor understanding of the effectiveness of the\nmethods in practice. We present an evaluation framework to assess constrained\ndecoding approaches across three critical dimensions: efficiency in generating\nconstraint-compliant outputs, coverage of diverse constraint types, and quality\nof the generated outputs. To facilitate this evaluation, we introduce\nJSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world\nJSON schemas that encompass a wide range of constraints with varying\ncomplexity. We pair the benchmark with the existing official JSON Schema Test\nSuite and evaluate six state-of-the-art constrained decoding frameworks,\nincluding Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through\nextensive experiments, we gain insights into the capabilities and limitations\nof constrained decoding on structured generation with real-world JSON schemas.\nOur work provides actionable insights for improving constrained decoding\nframeworks and structured generation tasks, setting a new standard for\nevaluating constrained decoding and structured generation. We release\nJSONSchemaBench at https://github.com/guidance-ai/jsonschemabench\n","authors":["Saibo Geng","Hudson Cooper","Michał Moskal","Samuel Jenkins","Julian Berman","Nathan Ranchin","Robert West","Eric Horvitz","Harsha Nori"],"pdf_url":"https://arxiv.org/pdf/2501.10868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06572v1","updated":"2025-02-10T15:40:35Z","published":"2025-02-10T15:40:35Z","title":"LawGPT: Knowledge-Guided Data Generation and Its Application to Legal\n  LLM","summary":"  Large language models (LLMs), both proprietary and open-source, have\ndemonstrated remarkable capabilities across various natural language processing\ntasks. However, they face significant limitations in legal reasoning tasks.\nProprietary models introduce data privacy risks and high inference costs, while\nopen-source models underperform due to insufficient legal domain training data.\nTo address these limitations, we study data generation for legal reasoning to\nimprove the legal reasoning performance of open-source LLMs with the help of\nproprietary LLMs. This is challenging due to the lack of legal knowledge in\nproprietary LLMs and the difficulty in verifying the generated data. We propose\nKgDG, a knowledge-guided data generation framework for legal reasoning. Our\nframework enables leveraging legal knowledge to enhance generation diversity\nand introduces a refinement and verification process to ensure the quality of\ngenerated data. Moreover, we expand the generated dataset to further enhance\nthe LLM reasoning capabilities. Using KgDG, we create a synthetic legal\nreasoning dataset containing 50K high-quality examples. Our trained model\nLawGPT outperforms existing legal-specific LLMs and achieves performance\ncomparable to proprietary LLMs, demonstrating the effectiveness of KgDG and\nLawGPT. Our code and resources is publicly available at\nhttps://anonymous.4open.science/r/KgDG-45F5 .\n","authors":["Zhi Zhou","Kun-Yang Yu","Shi-Yu Tian","Jiang-Xin Shi","Xiao-Wen Yang","Pengxiao Song","Yi-Xuan Jin","Lan-Zhe Guo","Yu-Feng Li"],"pdf_url":"https://arxiv.org/pdf/2502.06572v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.06563v1","updated":"2025-02-10T15:31:54Z","published":"2025-02-10T15:31:54Z","title":"Large Language Models Meet Symbolic Provers for Logical Reasoning\n  Evaluation","summary":"  First-order logic (FOL) reasoning, which involves sequential deduction, is\npivotal for intelligent systems and serves as a valuable task for evaluating\nreasoning capabilities, particularly in chain-of-thought (CoT) contexts.\nExisting benchmarks often rely on extensive human annotation or handcrafted\ntemplates, making it difficult to achieve the necessary complexity,\nscalability, and diversity for robust evaluation. To address these limitations,\nwe propose a novel framework called ProverGen that synergizes the generative\nstrengths of Large Language Models (LLMs) with the rigor and precision of\nsymbolic provers, enabling the creation of a scalable, diverse, and\nhigh-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by\nits inclusion of accessible and logically coherent intermediate reasoning steps\nfor each problem. Our evaluation shows that state-of-the-art LLMs struggle to\nsolve ProverQA problems, even with CoT prompting, highlighting the dataset's\nchallenging nature. We also finetune Llama3.1-8B-Instruct on a separate\ntraining set generated by our framework. The finetuned model demonstrates\nconsistent improvements on both in-distribution and out-of-distribution test\nsets, suggesting the value of our proposed data generation framework. Code\navailable at: https://github.com/opendatalab/ProverGen\n","authors":["Chengwen Qi","Ren Ma","Bowen Li","He Du","Binyuan Hui","Jinwang Wu","Yuanjun Laili","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2502.06563v1.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2501.18280v2","updated":"2025-02-10T15:27:04Z","published":"2025-01-30T11:37:40Z","title":"Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models","summary":"  The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner.\n","authors":["Haoyu Liang","Youran Sun","Yunfeng Cai","Jun Zhu","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.18280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06560v1","updated":"2025-02-10T15:25:11Z","published":"2025-02-10T15:25:11Z","title":"Position: It's Time to Act on the Risk of Efficient Personalized Text\n  Generation","summary":"  The recent surge in high-quality open-sourced Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, has opened\nthe possibility of creating high-quality personalized models, i.e., models\ngenerating text attuned to a specific individual's needs and capable of\ncredibly imitating their writing style by leveraging that person's own data to\nrefine an open-source model. The technology to create such models is accessible\nto private individuals, and training and running such models can be done\ncheaply on consumer-grade hardware. These advancements are a huge gain for\nusability and privacy. This position paper argues, however, that these\nadvancements also introduce new safety risks by making it practically feasible\nfor malicious actors to impersonate specific individuals at scale, for instance\nfor the purpose of phishing emails, based on small amounts of publicly\navailable text. We further argue that these risks are complementary to - and\ndistinct from - the much-discussed risks of other impersonation attacks such as\nimage, voice, or video deepfakes, and are not adequately addressed by the\nlarger research community, or the current generation of open - and\nclosed-source models.\n","authors":["Eugenia Iofinova","Andrej Jovanovic","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2502.06560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06556v1","updated":"2025-02-10T15:24:30Z","published":"2025-02-10T15:24:30Z","title":"ProjectTest: A Project-level Unit Test Generation Benchmark and Impact\n  of Error Fixing Mechanisms","summary":"  Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant simple errors,\nincluding compilation and cascade errors. Motivated by this observation, we\nfurther evaluate all frontier LLMs under manual error-fixing and\nself-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms.\n","authors":["Yibo Wang","Congying Xia","Wenting Zhao","Jiangshu Du","Chunyu Miao","Zhongfen Deng","Philip S. Yu","Chen Xing"],"pdf_url":"https://arxiv.org/pdf/2502.06556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06551v1","updated":"2025-02-10T15:19:22Z","published":"2025-02-10T15:19:22Z","title":"Efficient Scientific Full Text Classification: The Case of EICAT Impact\n  Assessments","summary":"  This study explores strategies for efficiently classifying scientific full\ntexts using both small, BERT-based models and local large language models like\nLlama-3.1 8B. We focus on developing methods for selecting subsets of input\nsentences to reduce input size while simultaneously enhancing classification\nperformance. To this end, we compile a novel dataset consisting of full-text\nscientific papers from the field of invasion biology, specifically addressing\nthe impacts of invasive species. These papers are aligned with publicly\navailable impact assessments created by researchers for the International Union\nfor Conservation of Nature (IUCN). Through extensive experimentation, we\ndemonstrate that various sources like human evidence annotations, LLM-generated\nannotations or explainability scores can be used to train sentence selection\nmodels that improve the performance of both encoder- and decoder-based language\nmodels while optimizing efficiency through the reduction in input length,\nleading to improved results even if compared to models like ModernBERT that are\nable to handle the complete text as input. Additionally, we find that repeated\nsampling of shorter inputs proves to be a very effective strategy that, at a\nslightly increased cost, can further improve classification performance.\n","authors":["Marc Felix Brinner","Sina Zarrieß"],"pdf_url":"https://arxiv.org/pdf/2502.06551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07507v2","updated":"2025-02-10T15:09:12Z","published":"2024-10-10T00:47:59Z","title":"Thought2Text: Text Generation from EEG Signal using Large Language\n  Models (LLMs)","summary":"  Decoding and expressing brain activity in a comprehensible form is a\nchallenging frontier in AI. This paper presents Thought2Text, which uses\ninstruction-tuned Large Language Models (LLMs) fine-tuned with EEG data to\nachieve this goal. The approach involves three stages: (1) training an EEG\nencoder for visual feature extraction, (2) fine-tuning LLMs on image and text\ndata, enabling multimodal description generation, and (3) further fine-tuning\non EEG embeddings to generate text directly from EEG during inference.\nExperiments on a public EEG dataset collected for six subjects with image\nstimuli and text captions demonstrate the efficacy of multimodal LLMs\n(LLaMA-v3, Mistral-v0.3, Qwen2.5), validated using traditional language\ngeneration evaluation metrics, as well as fluency and adequacy measures. This\napproach marks a significant advancement towards portable, low-cost\n\"thoughts-to-text\" technology with potential applications in both neuroscience\nand natural language processing.\n","authors":["Abhijit Mishra","Shreya Shukla","Jose Torres","Jacek Gwizdka","Shounak Roychowdhury"],"pdf_url":"https://arxiv.org/pdf/2410.07507v2.pdf","comment":"Accepted to Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2404.19442v4","updated":"2025-02-10T15:08:20Z","published":"2024-04-30T10:45:40Z","title":"Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs","summary":"  Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian\nPidgin spoken by approximately 120M speakers and it is a mixed language (e.g.,\nEnglish, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a\nspoken language until recently, there are some online platforms (e.g.,\nWikipedia), publishing in written Naija as well. West African Pidgin English\n(WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the\ninternet to a wider audience not only in Nigeria but also in other West African\ncountries (e.g., Cameroon and Ghana). Through statistical analyses and Machine\nTranslation experiments, our paper shows that these two pidgin varieties do not\nrepresent each other (i.e., there are linguistic differences in word order and\nvocabulary) and Generative AI operates only based on WAPE. In other words,\nNaija is underrepresented in Generative AI, and it is hard to teach LLMs with\nfew examples. In addition to the statistical analyses, we also provide\nhistorical information on both pidgins as well as insights from the interviews\nconducted with volunteer Wikipedia contributors in Naija.\n","authors":["David Ifeoluwa Adelani","A. Seza Doğruöz","Iyanuoluwa Shode","Anuoluwapo Aremu"],"pdf_url":"https://arxiv.org/pdf/2404.19442v4.pdf","comment":"Accepted to NAACL 2025 (findings)"},{"id":"http://arxiv.org/abs/2407.10994v4","updated":"2025-02-10T15:08:07Z","published":"2024-06-24T12:09:34Z","title":"Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant","summary":"  The availability of powerful open-source large language models (LLMs) opens\nexciting use-cases, such as using personal data to fine-tune these models to\nimitate a user's unique writing style. Two key requirements for such assistants\nare personalization - in the sense that the assistant should recognizably\nreflect the user's own writing style - and privacy - users may justifiably be\nwary of uploading extremely personal data, such as their email archive, to a\nthird-party service. In this paper, we present a new design and evaluation for\nsuch an automated assistant, for the specific use case of email generation,\nwhich we call Panza. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to reflect a user's writing\nstyle using limited data, while executing on extremely limited resources, e.g.\non a free Google Colab instance. Our key methodological contribution is the\nfirst detailed study of evaluation metrics for this personalized writing task,\nand of how different choices of system components--the use of RAG and of\ndifferent fine-tuning approaches-impact the system's performance. Additionally,\nwe demonstrate that very little data - under 100 email samples - are sufficient\nto create models that convincingly imitate humans. This finding showcases a\npreviously-unknown attack vector in language models - that access to a small\nnumber of writing samples can allow a bad actor to cheaply create generative\nmodels that imitate a target's writing style. We are releasing the full Panza\ncode as well as three new email datasets licensed for research use at\nhttps://github.com/IST-DASLab/PanzaMail.\n","authors":["Armand Nicolicioiu","Eugenia Iofinova","Andrej Jovanovic","Eldar Kurtic","Mahdi Nikdan","Andrei Panferov","Ilia Markov","Nir Shavit","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2407.10994v4.pdf","comment":"Panza is available at https://github.com/IST-DASLab/PanzaMail"},{"id":"http://arxiv.org/abs/2405.20973v2","updated":"2025-02-10T15:04:53Z","published":"2024-05-31T16:21:05Z","title":"LCQ: Low-Rank Codebook based Quantization for Large Language Models","summary":"  Large language models~(LLMs) have recently demonstrated promising performance\nin many tasks. However, the high storage and computational cost of LLMs has\nbecome a challenge for deploying LLMs. Weight quantization has been widely used\nfor model compression, which can reduce both storage and computational cost.\nMost existing weight quantization methods for LLMs use a rank-one codebook for\nquantization, which results in substantial accuracy loss when the compression\nratio is high. In this paper, we propose a novel weight quantization method,\ncalled low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a\nlow-rank codebook, the rank of which can be larger than one, for quantization.\nExperiments show that LCQ can achieve better accuracy than existing methods\nwith a negligibly extra storage cost.\n","authors":["Wen-Pu Cai","Ming-Yang Li","Wu-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2405.20973v2.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.06533v1","updated":"2025-02-10T14:56:25Z","published":"2025-02-10T14:56:25Z","title":"Ignore the KL Penalty! Boosting Exploration on Critical Tokens to\n  Enhance RL Fine-Tuning","summary":"  The ability to achieve long-term goals is a key challenge in the current\ndevelopment of large language models (LLMs). To address this, pre-trained LLMs\ncan be fine-tuned with reinforcement learning (RL) to explore solutions that\noptimize a given goal. However, exploration with LLMs is difficult, as a\nbalance has to be struck between discovering new solutions and staying close\nenough to the pre-trained model, so as not to degrade basic capabilities. This\nis typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we\ninvestigate the exploration dynamics of a small language model on a simple\narithmetic task. We show how varying degrees of pre-training influence\nexploration and demonstrate the importance of \"critical tokens\" which have a\ndramatic impact on the final outcome. Consequently, we introduce a simple\nmodification to the KL penalty that favors exploration on critical tokens,\nincreasing the efficiency of the RL fine-tuning stage.\n","authors":["Jean Vassoyan","Nathanaël Beau","Roman Plaud"],"pdf_url":"https://arxiv.org/pdf/2502.06533v1.pdf","comment":"11 pages, 6 figures, 5 tables. Accepted for publication in the\n  Findings of the North American Chapter of the Association for Computational\n  Linguistics (NAACL) 2025"},{"id":"http://arxiv.org/abs/2410.14399v2","updated":"2025-02-10T14:11:58Z","published":"2024-10-18T12:02:41Z","title":"SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning","summary":"  Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications.\n","authors":["Magdalena Wysocka","Danilo Carvalho","Oskar Wysocki","Marco Valentino","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2410.14399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06494v1","updated":"2025-02-10T14:11:32Z","published":"2025-02-10T14:11:32Z","title":"GuideLLM: Exploring LLM-Guided Conversation with Applications in\n  Autobiography Interviewing","summary":"  Although Large Language Models (LLMs) succeed in human-guided conversations\nsuch as instruction following and question answering, the potential of\nLLM-guided conversations-where LLMs direct the discourse and steer the\nconversation's objectives-remains under-explored. In this study, we first\ncharacterize LLM-guided conversation into three fundamental components: (i)\nGoal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and\npropose GuideLLM as an installation. We then implement an interviewing\nenvironment for the evaluation of LLM-guided conversation. Specifically,\nvarious topics are involved in this environment for comprehensive interviewing\nevaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over\n200 events mentioned during the interviewing for each chatbot evaluation. We\ncompare GuideLLM with 6 state-of-the-art LLMs such as GPT-4o and\nLlama-3-70b-Instruct, from the perspective of interviewing quality, and\nautobiography generation quality. For automatic evaluation, we derive user\nproxies from multiple autobiographies and employ LLM-as-a-judge to score LLM\nbehaviors. We further conduct a human-involved experiment by employing 45 human\nparticipants to chat with GuideLLM and baselines. We then collect human\nfeedback, preferences, and ratings regarding the qualities of conversation and\nautobiography. Experimental results indicate that GuideLLM significantly\noutperforms baseline LLMs in automatic evaluation and achieves consistent\nleading performances in human ratings.\n","authors":["Jinhao Duan","Xinyu Zhao","Zhuoxuan Zhang","Eunhye Ko","Lily Boddy","Chenan Wang","Tianhao Li","Alexander Rasgon","Junyuan Hong","Min Kyung Lee","Chenxi Yuan","Qi Long","Ying Ding","Tianlong Chen","Kaidi Xu"],"pdf_url":"https://arxiv.org/pdf/2502.06494v1.pdf","comment":"31 pages; the first three authors contributed equally"},{"id":"http://arxiv.org/abs/2410.14596v2","updated":"2025-02-10T14:09:46Z","published":"2024-10-18T16:49:36Z","title":"Teaching Models to Balance Resisting and Accepting Persuasion","summary":"  Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Training (or PBT), which\nleverages multi-agent recursive dialogue trees to create data and trains models\nvia preference optimization to accept persuasion when appropriate. PBT allows\nus to use data generated from dialogues between smaller 7-8B models for\ntraining much larger 70B models. Moreover, PBT consistently improves resistance\nto misinformation and resilience to being challenged while also resulting in\nthe best overall performance on holistic data containing both positive and\nnegative persuasion. Crucially, we show that PBT models are better teammates in\nmulti-agent debates across two domains (trivia and commonsense QA). We find\nthat without PBT, pairs of stronger and weaker models have unstable\nperformance, with the order in which the models present their answers\ndetermining whether the team obtains the stronger or weaker model's\nperformance. PBT leads to better and more stable results and less order\ndependence, with the stronger model consistently pulling the weaker one up.\n","authors":["Elias Stengel-Eskin","Peter Hase","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.14596v2.pdf","comment":"NAACL Camera-Ready. Code:\n  https://github.com/esteng/persuasion_balanced_training"},{"id":"http://arxiv.org/abs/2502.03793v2","updated":"2025-02-10T14:08:19Z","published":"2025-02-06T05:47:37Z","title":"It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like\n  Masked Language Models As Generative Classifiers","summary":"  While encoder-only models such as BERT and ModernBERT are ubiquitous in\nreal-world NLP applications, their conventional reliance on task-specific\nclassification heads can limit their applicability compared to decoder-based\nlarge language models (LLMs). In this work, we introduce\nModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its\nmasked language modelling (MLM) head for generative classification. Our\napproach employs an intentionally simple training loop and inference mechanism\nthat requires no heavy pre-processing, heavily engineered prompting, or\narchitectural modifications. ModernBERT-Large-Instruct exhibits strong\nzero-shot performance on both classification and knowledge-based tasks,\noutperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's\nMMLU performance with 60% less parameters. We also demonstrate that, when\nfine-tuned, the generative approach using the MLM head matches or even\nsurpasses traditional classification-head methods across diverse NLU tasks.This\ncapability emerges specifically in models trained on contemporary, diverse data\nmixes, with models trained on lower volume, less-diverse data yielding\nconsiderably weaker performance. Although preliminary, these results\ndemonstrate the potential of using the original generative masked language\nmodelling head over traditional task-specific heads for downstream tasks. Our\nwork suggests that further exploration into this area is warranted,\nhighlighting many avenues for future improvements.\n","authors":["Benjamin Clavié","Nathan Cooper","Benjamin Warner"],"pdf_url":"https://arxiv.org/pdf/2502.03793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06487v1","updated":"2025-02-10T14:06:19Z","published":"2025-02-10T14:06:19Z","title":"Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection","summary":"  Recent advances on instruction fine-tuning have led to the development of\nvarious prompting techniques for large language models, such as explicit\nreasoning steps. However, the success of techniques depends on various\nparameters, such as the task, language model, and context provided. Finding an\neffective prompt is, therefore, often a trial-and-error process. Most existing\napproaches to automatic prompting aim to optimize individual techniques instead\nof compositions of techniques and their dependence on the input. To fill this\ngap, we propose an adaptive prompting approach that predicts the optimal prompt\ncomposition ad-hoc for a given input. We apply our approach to social bias\ndetection, a highly context-dependent task that requires semantic\nunderstanding. We evaluate it with three large language models on three\ndatasets, comparing compositions to individual techniques and other baselines.\nThe results underline the importance of finding an effective prompt\ncomposition. Our approach robustly ensures high detection performance, and is\nbest in several settings. Moreover, first experiments on other tasks support\nits generalizability.\n","authors":["Maximilian Spliethöver","Tim Knebler","Fabian Fumagalli","Maximilian Muschalik","Barbara Hammer","Eyke Hüllermeier","Henning Wachsmuth"],"pdf_url":"https://arxiv.org/pdf/2502.06487v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.06472v1","updated":"2025-02-10T13:51:36Z","published":"2025-02-10T13:51:36Z","title":"KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph\n  Enrichment","summary":"  Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical\nfor modern AI systems, but manual curation struggles to scale with the rapid\ngrowth of scientific literature. This paper presents KARMA, a novel framework\nemploying multi-agent large language models (LLMs) to automate KG enrichment\nthrough structured analysis of unstructured text. Our approach employs nine\ncollaborative agents, spanning entity discovery, relation extraction, schema\nalignment, and conflict resolution that iteratively parse documents, verify\nextracted knowledge, and integrate it into existing graph structures while\nadhering to domain-specific schema. Experiments on 1,200 PubMed articles from\nthree different domains demonstrate the effectiveness of KARMA in knowledge\ngraph enrichment, with the identification of up to 38,230 new entities while\nachieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\%\nthrough multi-layer assessments.\n","authors":["Yuxing Lu","Jinzhuo Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06472v1.pdf","comment":"24 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2502.06470v1","updated":"2025-02-10T13:50:25Z","published":"2025-02-10T13:50:25Z","title":"A Survey of Theory of Mind in Large Language Models: Evaluations,\n  Representations, and Safety Risks","summary":"  Theory of Mind (ToM), the ability to attribute mental states to others and\npredict their behaviour, is fundamental to social intelligence. In this paper,\nwe survey studies evaluating behavioural and representational ToM in Large\nLanguage Models (LLMs), identify important safety risks from advanced LLM ToM\ncapabilities, and suggest several research directions for effective evaluation\nand mitigation of these risks.\n","authors":["Hieu Minh \"Jord\" Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.06470v1.pdf","comment":"Advancing Artificial Intelligence through Theory of Mind Workshop,\n  AAAI 2025"},{"id":"http://arxiv.org/abs/2502.06468v1","updated":"2025-02-10T13:50:12Z","published":"2025-02-10T13:50:12Z","title":"Beyond Literal Token Overlap: Token Alignability for Multilinguality","summary":"  Previous work has considered token overlap, or even similarity of token\ndistributions, as predictors for multilinguality and cross-lingual knowledge\ntransfer in language models. However, these very literal metrics assign large\ndistances to language pairs with different scripts, which can nevertheless show\ngood cross-linguality. This limits the explanatory strength of token overlap\nfor knowledge transfer between language pairs that use distinct scripts or\nfollow different orthographic conventions. In this paper, we propose subword\ntoken alignability as a new way to understand the impact and quality of\nmultilingual tokenisation. In particular, this metric predicts multilinguality\nmuch better when scripts are disparate and the overlap of literal tokens is\nlow. We analyse this metric in the context of both encoder and decoder models,\nlook at data size as a potential distractor, and discuss how this insight may\nbe applied to multilingual tokenisation in future work. We recommend our\nsubword token alignability metric for identifying optimal language pairs for\ncross-lingual transfer, as well as to guide the construction of better\nmultilingual tokenisers in the future. We publish our code and reproducibility\ndetails.\n","authors":["Katharina Hämmerl","Tomasz Limisiewicz","Jindřich Libovický","Alexander Fraser"],"pdf_url":"https://arxiv.org/pdf/2502.06468v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.06453v1","updated":"2025-02-10T13:31:46Z","published":"2025-02-10T13:31:46Z","title":"MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard\n  Perturbations","summary":"  Large language models have demonstrated impressive performance on challenging\nmathematical reasoning tasks, which has triggered the discussion of whether the\nperformance is achieved by true reasoning capability or memorization. To\ninvestigate this question, prior work has constructed mathematical benchmarks\nwhen questions undergo simple perturbations -- modifications that still\npreserve the underlying reasoning patterns of the solutions. However, no work\nhas explored hard perturbations, which fundamentally change the nature of the\nproblem so that the original solution steps do not apply. To bridge the gap, we\nconstruct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard\nperturbation, respectively. Each consists of 279 perturbed math problems\nderived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.\nal., 2021). We observe significant performance drops on MATH-P-Hard across\nvarious models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking\n(-12.9%). We also raise concerns about a novel form of memorization where\nmodels blindly apply learned problem-solving skills without assessing their\napplicability to modified contexts. This issue is amplified when using original\nproblems for in-context learning. We call for research efforts to address this\nchallenge, which is critical for developing more robust and reliable reasoning\nmodels.\n","authors":["Kaixuan Huang","Jiacheng Guo","Zihao Li","Xiang Ji","Jiawei Ge","Wenzhe Li","Yingqing Guo","Tianle Cai","Hui Yuan","Runzhe Wang","Yue Wu","Ming Yin","Shange Tang","Yangsibo Huang","Chi Jin","Xinyun Chen","Chiyuan Zhang","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17428v2","updated":"2025-02-10T13:28:01Z","published":"2024-03-26T06:50:04Z","title":"Aligning Large Language Models for Enhancing Psychiatric Interviews\n  Through Symptom Delineation and Summarization: Pilot Study","summary":"  Background: Advancements in large language models (LLMs) have opened new\npossibilities in psychiatric interviews, an underexplored area where LLMs could\nbe valuable. This study focuses on enhancing psychiatric interviews by\nanalyzing counseling data from North Korean defectors who have experienced\ntrauma and mental health issues.\n  Objective: The study investigates whether LLMs can (1) identify parts of\nconversations that suggest psychiatric symptoms and recognize those symptoms,\nand (2) summarize stressors and symptoms based on interview transcripts.\n  Methods: LLMs are tasked with (1) extracting stressors from transcripts, (2)\nidentifying symptoms and their corresponding sections, and (3) generating\ninterview summaries using the extracted data. The transcripts were labeled by\nmental health experts for training and evaluation.\n  Results: In the zero-shot inference setting using GPT-4 Turbo, 73 out of 102\nsegments demonstrated a recall mid-token distance d < 20 in identifying\nsymptom-related sections. For recognizing specific symptoms, fine-tuning\noutperformed zero-shot inference, achieving an accuracy, precision, recall, and\nF1-score of 0.82. For the generative summarization task, LLMs using symptom and\nstressor information scored highly on G-Eval metrics: coherence (4.66),\nconsistency (4.73), fluency (2.16), and relevance (4.67). Retrieval-augmented\ngeneration showed no notable performance improvement.\n  Conclusions: LLMs, with fine-tuning or appropriate prompting, demonstrated\nstrong accuracy (over 0.8) for symptom delineation and achieved high coherence\n(4.6+) in summarization. This study highlights their potential to assist mental\nhealth practitioners in analyzing psychiatric interviews.\n","authors":["Jae-hee So","Joonhwan Chang","Eunji Kim","Junho Na","JiYeon Choi","Jy-yong Sohn","Byung-Hoon Kim","Sang Hui Chu"],"pdf_url":"https://arxiv.org/pdf/2403.17428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19345v3","updated":"2025-02-10T13:18:25Z","published":"2024-07-27T21:56:23Z","title":"Inference-Time Selective Debiasing to Enhance Fairness in Text\n  Classification Models","summary":"  We propose selective debiasing -- an inference-time safety mechanism designed\nto enhance the overall model quality in terms of prediction performance and\nfairness, especially in scenarios where retraining the model is impractical.\nThe method draws inspiration from selective classification, where at inference\ntime, predictions with low quality, as indicated by their uncertainty scores,\nare discarded. In our approach, we identify the potentially biased model\npredictions and, instead of discarding them, we remove bias from these\npredictions using LEACE -- a post-processing debiasing method. To select\nproblematic predictions, we propose a bias quantification approach based on KL\ndivergence, which achieves better results than standard uncertainty\nquantification methods. Experiments on text classification datasets with\nencoder-based classification models demonstrate that selective debiasing helps\nto reduce the performance gap between post-processing methods and debiasing\ntechniques from the at-training and pre-processing categories.\n","authors":["Gleb Kuzmin","Neemesh Yadav","Ivan Smirnov","Timothy Baldwin","Artem Shelmanov"],"pdf_url":"https://arxiv.org/pdf/2407.19345v3.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.06430v1","updated":"2025-02-10T13:06:25Z","published":"2025-02-10T13:06:25Z","title":"Content-Driven Local Response: Supporting Sentence-Level and\n  Message-Level Mobile Email Replies With and Without AI","summary":"  Mobile emailing demands efficiency in diverse situations, which motivates the\nuse of AI. However, generated text does not always reflect how people want to\nrespond. This challenges users with AI involvement tradeoffs not yet considered\nin email UIs. We address this with a new UI concept called Content-Driven Local\nResponse (CDLR), inspired by microtasking. This allows users to insert\nresponses into the email by selecting sentences, which additionally serves to\nguide AI suggestions. The concept supports combining AI for local suggestions\nand message-level improvements. Our user study (N=126) compared CDLR with\nmanual typing and full reply generation. We found that CDLR supports flexible\nworkflows with varying degrees of AI involvement, while retaining the benefits\nof reduced typing and errors. This work contributes a new approach to\nintegrating AI capabilities: By redesigning the UI for workflows with and\nwithout AI, we can empower users to dynamically adjust AI involvement.\n","authors":["Tim Zindulka","Sven Goller","Florian Lehmann","Daniel Buschek"],"pdf_url":"https://arxiv.org/pdf/2502.06430v1.pdf","comment":"23 pages, 14 figures, 2 tables, ACM CHI 2025"},{"id":"http://arxiv.org/abs/2502.06415v1","updated":"2025-02-10T12:54:17Z","published":"2025-02-10T12:54:17Z","title":"Systematic Outliers in Large Language Models","summary":"  Outliers have been widely observed in Large Language Models (LLMs),\nsignificantly impacting model performance and posing challenges for model\ncompression. Understanding the functionality and formation mechanisms of these\noutliers is critically important. Existing works, however, largely focus on\nreducing the impact of outliers from an algorithmic perspective, lacking an\nin-depth investigation into their causes and roles. In this work, we provide a\ndetailed analysis of the formation process, underlying causes, and functions of\noutliers in LLMs. We define and categorize three types of outliers-activation\noutliers, weight outliers, and attention outliers-and analyze their\ndistributions across different dimensions, uncovering inherent connections\nbetween their occurrences and their ultimate influence on the attention\nmechanism. Based on these observations, we hypothesize and explore the\nmechanisms by which these outliers arise and function, demonstrating through\ntheoretical derivations and experiments that they emerge due to the\nself-attention mechanism's softmax operation. These outliers act as implicit\ncontext-aware scaling factors within the attention mechanism. As these outliers\nstem from systematic influences, we term them systematic outliers. Our study\nnot only enhances the understanding of Transformer-based LLMs but also shows\nthat structurally eliminating outliers can accelerate convergence and improve\nmodel compression. The code is avilable at\nhttps://github.com/an-yongqi/systematic-outliers.\n","authors":["Yongqi An","Xu Zhao","Tao Yu","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06415v1.pdf","comment":"Accepted at ICLR 2025. Project Page:\n  https://github.com/an-yongqi/systematic-outliers"},{"id":"http://arxiv.org/abs/2405.02765v3","updated":"2025-02-10T12:35:49Z","published":"2024-05-04T22:02:24Z","title":"Has this Fact been Edited? Detecting Knowledge Edits in Language Models","summary":"  Knowledge editing methods (KEs) can update language models' obsolete or\ninaccurate knowledge learned from pre-training. However, KEs can be used for\nmalicious applications, e.g., inserting misinformation and toxic content.\nKnowing whether a generated output is based on edited knowledge or first-hand\nknowledge from pre-training can increase users' trust in generative models and\nprovide more transparency. Driven by this, we propose a novel task: detecting\nedited knowledge in language models. Given an edited model and a fact retrieved\nby a prompt from an edited model, the objective is to classify the knowledge as\neither unedited (based on the pre-training), or edited (based on subsequent\nediting). We instantiate the task with four KEs, two LLMs, and two datasets.\nAdditionally, we propose using the hidden state representations and the\nprobability distributions as features for the detection. Our results reveal\nthat, using these features as inputs to a simple AdaBoost classifiers\nestablishes a strong baseline. This classifier requires only a limited amount\nof data and maintains its performance even in cross-domain settings. Last, we\nfind it more challenging to distinguish edited knowledge from unedited but\nrelated knowledge, highlighting the need for further research. Our work lays\nthe groundwork for addressing malicious model editing, which is a critical\nchallenge associated with the strong generative capabilities of LLMs.\n","authors":["Paul Youssef","Zhixue Zhao","Christin Seifert","Jörg Schlötterer"],"pdf_url":"https://arxiv.org/pdf/2405.02765v3.pdf","comment":"Accepted at NAACL Main 2025"},{"id":"http://arxiv.org/abs/2501.11613v5","updated":"2025-02-10T12:35:22Z","published":"2025-01-20T17:19:02Z","title":"Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems","summary":"  This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.\n","authors":["Giorgio Robino"],"pdf_url":"https://arxiv.org/pdf/2501.11613v5.pdf","comment":"Section 1.2 (Harnessing Multi-Agent Potential in CAS with OpenAI\n  SWARM) Renamed. Section 5.3 (Future Directions) Updated. Minor typo\n  corrections"},{"id":"http://arxiv.org/abs/2502.06394v1","updated":"2025-02-10T12:30:25Z","published":"2025-02-10T12:30:25Z","title":"SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators","summary":"  Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.\n","authors":["Daniil Moskovskiy","Nikita Sushko","Sergey Pletenev","Elena Tutubalina","Alexander Panchenko"],"pdf_url":"https://arxiv.org/pdf/2502.06394v1.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2412.17498v3","updated":"2025-02-10T11:35:28Z","published":"2024-12-23T11:55:33Z","title":"DRT: Deep Reasoning Translation via Long Chain-of-Thought","summary":"  Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to quantify the translation quality in each round. In this way,\nwe collect tens of thousands of long-thought MT data, which is used to train\nour DRT. Using Qwen2.5 and LLama-3.1 as the backbones, DRT models can learn the\nthought process during machine translation, and outperform vanilla LLMs as well\nas LLMs which are simply fine-tuning on the paired sentences without long\nthought, showing its effectiveness.\n","authors":["Jiaan Wang","Fandong Meng","Yunlong Liang","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.17498v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12586v2","updated":"2025-02-10T11:04:12Z","published":"2024-10-16T14:04:26Z","title":"How to Make LLMs Forget: On Reversing In-Context Knowledge Edits","summary":"  In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness.\n","authors":["Paul Youssef","Zhixue Zhao","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2410.12586v2.pdf","comment":"Accepted at NAACL Main 2025"},{"id":"http://arxiv.org/abs/2502.06342v1","updated":"2025-02-10T10:45:00Z","published":"2025-02-10T10:45:00Z","title":"The exponential distribution of the orders of demonstrative, numeral,\n  adjective and noun","summary":"  The frequency of the preferred order for a noun phrase formed by\ndemonstrative, numeral, adjective and noun has received significant attention\nover the last two decades. We investigate the actual distribution of the\npreferred 24 possible orders. There is no consensus on whether it can be\nwell-fitted by an exponential or a power law distribution. We find that an\nexponential distribution is a much better model. This finding and other\ncircumstances where an exponential-like distribution is found challenge the\nview that power-law distributions, e.g., Zipf's law for word frequencies, are\ninevitable. We also investigate which of two exponential distributions gives a\nbetter fit: an exponential model where the 24 orders have non-zero probability\nor an exponential model where the number of orders that can have non-zero\nprobability is variable. When parsimony and generalizability are prioritized,\nwe find strong support for the exponential model where all 24 orders have\nnon-zero probability. This finding suggests that there is no hard constraint on\nword order variation and then unattested orders merely result from\nundersampling, consistently with Cysouw's view.\n","authors":["Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2502.06342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06329v1","updated":"2025-02-10T10:29:28Z","published":"2025-02-10T10:29:28Z","title":"Expect the Unexpected: FailSafe Long Context QA for Finance","summary":"  We propose a new long-context financial benchmark, FailSafeQA, designed to\ntest the robustness and context-awareness of LLMs against six variations in\nhuman-interface interactions in LLM-based query-answer systems within finance.\nWe concentrate on two case studies: Query Failure and Context Failure. In the\nQuery Failure scenario, we perturb the original query to vary in domain\nexpertise, completeness, and linguistic accuracy. In the Context Failure case,\nwe simulate the uploads of degraded, irrelevant, and empty documents. We employ\nthe LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained\nrating criteria to define and calculate Robustness, Context Grounding, and\nCompliance scores for 24 off-the-shelf models. The results suggest that\nalthough some models excel at mitigating input perturbations, they must balance\nrobust answering with the ability to refrain from hallucinating. Notably,\nPalmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained\nstrong baseline performance but encountered challenges in sustaining robust\npredictions in 17% of test cases. On the other hand, the most robust model,\nOpenAI o3-mini, fabricated information in 41% of tested cases. The results\ndemonstrate that even high-performing models have significant room for\nimprovement and highlight the role of FailSafeQA as a tool for developing LLMs\noptimized for dependability in financial applications. The dataset is available\nat: https://huggingface.co/datasets/Writer/FailSafeQA\n","authors":["Kiran Kamble","Melisa Russak","Dmytro Mozolevskyi","Muayad Ali","Mateusz Russak","Waseem AlShikh"],"pdf_url":"https://arxiv.org/pdf/2502.06329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08599v3","updated":"2025-02-10T10:28:55Z","published":"2024-11-13T13:30:21Z","title":"A Preview of XiYan-SQL: A Multi-Generator Ensemble Framework for\n  Text-to-SQL","summary":"  To tackle the challenges of large language model performance in natural\nlanguage to SQL tasks, we introduce XiYan-SQL, an innovative framework that\nemploys a multi-generator ensemble strategy to improve candidate generation. We\nintroduce M-Schema, a semi-structured schema representation method designed to\nenhance the understanding of database structures. To enhance the quality and\ndiversity of generated candidate SQL queries, XiYan-SQL integrates the\nsignificant potential of in-context learning (ICL) with the precise control of\nsupervised fine-tuning. On one hand, we propose a series of training strategies\nto fine-tune models to generate high-quality candidates with diverse\npreferences. On the other hand, we implement the ICL approach with an example\nselection method based on named entity recognition to prevent overemphasis on\nentities. The refiner optimizes each candidate by correcting logical or\nsyntactical errors. To address the challenge of identifying the best candidate,\nwe fine-tune a selection model to distinguish nuances of candidate SQL queries.\nThe experimental results on multiple dialect datasets demonstrate the\nrobustness of XiYan-SQL in addressing challenges across different scenarios.\nOverall, our proposed XiYan-SQL achieves the state-of-the-art execution\naccuracy of 75.63% on Bird benchmark, 89.65% on the Spider test set, 69.86% on\nSQL-Eval, 41.20% on NL2GQL. The proposed framework not only enhances the\nquality and diversity of SQL queries but also outperforms previous methods.\n","authors":["Yingqi Gao","Yifu Liu","Xiaoxia Li","Xiaorong Shi","Yin Zhu","Yiming Wang","Shiqi Li","Wei Li","Yuntao Hong","Zhiling Luo","Jinyang Gao","Liyu Mou","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2411.08599v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01185v7","updated":"2025-02-10T10:14:44Z","published":"2023-12-02T17:24:17Z","title":"A ripple in time: a discontinuity in American history","summary":"  In this technical note we suggest a novel approach to discover temporal\n(related and unrelated to language dilation) and personality (authorship\nattribution) aspects in historical datasets. We exemplify our approach on the\nState of the Union addresses given by the past 42 US presidents: this dataset\nis known for its relatively small amount of data, and high variability of the\nsize and style of texts. Nevertheless, we manage to achieve about 95\\% accuracy\non the authorship attribution task, and pin down the date of writing to a\nsingle presidential term.\n","authors":["Alexander Kolpakov","Igor Rivin"],"pdf_url":"https://arxiv.org/pdf/2312.01185v7.pdf","comment":"6 pages, 8 figures ; GitHub repository\n  https://github.com/sashakolpakov/ripple_in_time ; to appear in 8th NLPIR\n  Okayama, Japan | December 13-15, 2024 as \"Discovering temporal and\n  personality aspects in meager and highly variable text samples\""},{"id":"http://arxiv.org/abs/2502.06316v1","updated":"2025-02-10T10:09:29Z","published":"2025-02-10T10:09:29Z","title":"Can AI Examine Novelty of Patents?: Novelty Evaluation Based on the\n  Correspondence between Patent Claim and Prior Art","summary":"  Assessing the novelty of patent claims is a critical yet challenging task\ntraditionally performed by patent examiners. While advancements in NLP have\nenabled progress in various patent-related tasks, novelty assessment remains\nunexplored. This paper introduces a novel challenge by evaluating the ability\nof large language models (LLMs) to assess patent novelty by comparing claims\nwith cited prior art documents, following the process similar to that of patent\nexaminers done. We present the first dataset specifically designed for novelty\nevaluation, derived from real patent examination cases, and analyze the\ncapabilities of LLMs to address this task. Our study reveals that while\nclassification models struggle to effectively assess novelty, generative models\nmake predictions with a reasonable level of accuracy, and their explanations\nare accurate enough to understand the relationship between the target patent\nand prior art. These findings demonstrate the potential of LLMs to assist in\npatent evaluation, reducing the workload for both examiners and applicants. Our\ncontributions highlight the limitations of current models and provide a\nfoundation for improving AI-driven patent analysis through advanced models and\nrefined datasets.\n","authors":["Hayato Ikoma","Teruko Mitamura"],"pdf_url":"https://arxiv.org/pdf/2502.06316v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.06302v1","updated":"2025-02-10T09:46:33Z","published":"2025-02-10T09:46:33Z","title":"Latent Convergence Modulation in Large Language Models: A Novel Approach\n  to Iterative Contextual Realignment","summary":"  Token prediction stability remains a challenge in autoregressive generative\nmodels, where minor variations in early inference steps often lead to\nsignificant semantic drift over extended sequences. A structured modulation\nmechanism was introduced to regulate hidden state transitions, ensuring that\nlatent representation trajectories remain aligned with prior contextual\ndependencies while preserving generative flexibility. The modulation framework\nwas designed to function within transformer-based architectures, dynamically\nconstraining representation evolution without imposing external memory\ndependencies or extensive architectural modifications. Empirical evaluations\ndemonstrated that structured latent adjustments contributed to reductions in\nperplexity fluctuations, entropy variance, and lexical instability, improving\ncoherence in long-form text generation. Gradient propagation stability was\nfurther analyzed, revealing that the modulation process led to smoother\noptimization pathways, mitigating erratic fluctuations in weight updates across\nsuccessive inference steps. The computational efficiency of the modulation\nprocess was assessed, showing that its integration within transformer-based\narchitectures introduced only marginal overhead while maintaining compatibility\nwith existing optimization frameworks. The structured modulation constraints\nalso influenced syntactic variation, preventing excessive repetition while\nmaintaining balanced sentence length distributions. Comparative evaluations\nagainst baseline models reinforced the role of controlled latent state\nevolution in improving pronoun resolution, logical consistency, and contextual\nalignment across autoregressive text generation tasks.\n","authors":["Patricia Porretta","Sylvester Pakenham","Huxley Ainsworth","Gregory Chatten","Godfrey Allerton","Simon Hollingsworth","Vance Periwinkle"],"pdf_url":"https://arxiv.org/pdf/2502.06302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06298v1","updated":"2025-02-10T09:40:25Z","published":"2025-02-10T09:40:25Z","title":"SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual\n  Questions in Southeast Asia","summary":"  This study introduces two novel benchmarks, SeaExam and SeaBench, designed to\nevaluate the capabilities of Large Language Models (LLMs) in Southeast Asian\n(SEA) application scenarios. Unlike existing multilingual datasets primarily\nderived from English translations, these benchmarks are constructed based on\nreal-world scenarios from SEA regions. SeaExam draws from regional educational\nexams to form a comprehensive dataset that encompasses subjects such as local\nhistory and literature. In contrast, SeaBench is crafted around multi-turn,\nopen-ended tasks that reflect daily interactions within SEA communities. Our\nevaluations demonstrate that SeaExam and SeaBench more effectively discern LLM\nperformance on SEA language tasks compared to their translated benchmarks. This\nhighlights the importance of using real-world queries to assess the\nmultilingual capabilities of LLMs.\n","authors":["Chaoqun Liu","Wenxuan Zhang","Jiahao Ying","Mahani Aljunied","Anh Tuan Luu","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2502.06298v1.pdf","comment":"Accepted to Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2409.19737v2","updated":"2025-02-10T09:39:24Z","published":"2024-09-29T15:30:59Z","title":"A Systematic Review of NLP for Dementia -- Tasks, Datasets and\n  Opportunities","summary":"  The close link between cognitive decline and language has fostered\nlong-standing collaboration between the NLP and medical communities in dementia\nresearch. To examine this, we reviewed over 240 papers applying NLP to\ndementia-related efforts, drawing from medical, technological, and NLP-focused\nliterature. We identify key research areas, including dementia detection,\nlinguistic biomarker extraction, caregiver support, and patient assistance,\nshowing that half of all papers focus solely on dementia detection using\nclinical data. Yet, many directions remain unexplored -- artificially degraded\nlanguage models, synthetic data, digital twins, and more. We highlight gaps and\nopportunities around trust, scientific rigor, applicability and cross-community\ncollaboration. We raise ethical dilemmas in the field, and highlight the\ndiverse datasets encountered throughout our review -- recorded, written,\nstructured, spontaneous, synthetic, clinical, social media-based, and more.\nThis review aims to inspire more creative, impactful, and rigorous research on\nNLP for dementia.\n","authors":["Lotem Peled-Cohen","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2409.19737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06282v1","updated":"2025-02-10T09:24:06Z","published":"2025-02-10T09:24:06Z","title":"Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE","summary":"  Speculative decoding (SD) accelerates large language model inference by using\na smaller draft model to predict multiple tokens, which are then verified in\nparallel by the larger target model. However, the limited capacity of the draft\nmodel often necessitates tree-based sampling to improve prediction accuracy,\nwhere multiple candidates are generated at each step. We identify a key\nlimitation in this approach: the candidates at the same step are derived from\nthe same representation, limiting diversity and reducing overall effectiveness.\nTo address this, we propose Jakiro, leveraging Mixture of Experts (MoE), where\nindependent experts generate diverse predictions, effectively decoupling\ncorrelations among candidates. Furthermore, we introduce a hybrid inference\nstrategy, combining autoregressive decoding for initial tokens with parallel\ndecoding for subsequent stages, and enhance the latter with contrastive\nmechanism in features to improve accuracy. Our method significantly boosts\nprediction accuracy and achieves higher inference speedups. Extensive\nexperiments across diverse models validate the effectiveness and robustness of\nour approach, establishing a new SOTA in speculative decoding. Our codes are\navailable at https://github.com/haiduo/Jakiro.\n","authors":["Haiduo Huang","Fuwei Yang","Zhenhua Liu","Yixing Xu","Jinze Li","Yang Liu","Xuanwu Yin","Dong Li","Pengju Ren","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2502.06282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06279v1","updated":"2025-02-10T09:23:03Z","published":"2025-02-10T09:23:03Z","title":"DebateBench: A Challenging Long Context Reasoning Benchmark For Large\n  Language Models","summary":"  We introduce DebateBench, a novel dataset consisting of an extensive\ncollection of transcripts and metadata from some of the world's most\nprestigious competitive debates. The dataset consists of British Parliamentary\ndebates from prestigious debating tournaments on diverse topics, annotated with\ndetailed speech-level scores and house rankings sourced from official\nadjudication data. We curate 256 speeches across 32 debates with each debate\nbeing over 1 hour long with each input being an average of 32,000 tokens.\nDesigned to capture long-context, large-scale reasoning tasks, DebateBench\nprovides a benchmark for evaluating modern large language models (LLMs) on\ntheir ability to engage in argumentation, deliberation, and alignment with\nhuman experts. To do well on DebateBench, the LLMs must perform in-context\nlearning to understand the rules and evaluation criteria of the debates, then\nanalyze 8 seven minute long speeches and reason about the arguments presented\nby all speakers to give the final results. Our preliminary evaluation using GPT\no1, GPT-4o, and Claude Haiku, shows that LLMs struggle to perform well on\nDebateBench, highlighting the need to develop more sophisticated techniques for\nimproving their performance.\n","authors":["Utkarsh Tiwari","Aryan Seth","Adi Mukherjee","Kaavya Mer"," Kavish","Dhruv Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.06279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06258v1","updated":"2025-02-10T08:48:10Z","published":"2025-02-10T08:48:10Z","title":"Emergent Response Planning in LLM","summary":"  In this work, we argue that large language models (LLMs), though trained to\npredict only the next token, exhibit emergent planning behaviors:\n$\\textbf{their hidden representations encode future outputs beyond the next\ntoken}$. Through simple probing, we demonstrate that LLM prompt representations\nencode global attributes of their entire responses, including\n$\\textit{structural attributes}$ (response length, reasoning steps),\n$\\textit{content attributes}$ (character choices in storywriting,\nmultiple-choice answers at the end of response), and $\\textit{behavioral\nattributes}$ (answer confidence, factual consistency). In addition to\nidentifying response planning, we explore how it scales with model size across\ntasks and how it evolves during generation. The findings that LLMs plan ahead\nfor the future in their hidden representations suggests potential applications\nfor improving transparency and generation control.\n","authors":["Zhichen Dong","Zhanhui Zhou","Zhixuan Liu","Chao Yang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2502.06258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06257v1","updated":"2025-02-10T08:45:56Z","published":"2025-02-10T08:45:56Z","title":"K-ON: Stacking Knowledge On the Head Layer of Large Language Model","summary":"  Recent advancements in large language models (LLMs) have significantly\nimproved various natural language processing (NLP) tasks. Typically, LLMs are\ntrained to predict the next token, aligning well with many NLP tasks. However,\nin knowledge graph (KG) scenarios, entities are the fundamental units and\nidentifying an entity requires at least several tokens. This leads to a\ngranularity mismatch between KGs and natural languages. To address this issue,\nwe propose K-ON, which integrates KG knowledge into the LLM by employing\nmultiple head layers for next k-step prediction. K-ON can not only generate\nentity-level results in one step, but also enables contrastive loss against\nentities, which is the most powerful tool in KG representation learning.\nExperimental results show that K-ON outperforms state-of-the-art methods that\nincorporate text and even the other modalities.\n","authors":["Lingbing Guo","Yichi Zhang","Zhongpu Bo","Zhuo Chen","Mengshu Sun","Zhiqiang Zhang","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06257v1.pdf","comment":"AAAI 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.06252v1","updated":"2025-02-10T08:33:47Z","published":"2025-02-10T08:33:47Z","title":"Evaluating Entity Retrieval in Electronic Health Records: a Semantic Gap\n  Perspective","summary":"  Entity retrieval plays a crucial role in the utilization of Electronic Health\nRecords (EHRs) and is applied across a wide range of clinical practices.\nHowever, a comprehensive evaluation of this task is lacking due to the absence\nof a public benchmark. In this paper, we propose the development and release of\na novel benchmark for evaluating entity retrieval in EHRs, with a particular\nfocus on the semantic gap issue. Using discharge summaries from the MIMIC-III\ndataset, we incorporate ICD codes and prescription labels associated with the\nnotes as queries, and annotate relevance judgments using GPT-4. In total, we\nuse 1,000 patient notes, generate 1,246 queries, and provide over 77,000\nrelevance annotations. To offer the first assessment of the semantic gap, we\nintroduce a novel classification system for relevance matches. Leveraging\nGPT-4, we categorize each relevant pair into one of five categories: string,\nsynonym, abbreviation, hyponym, and implication. Using the proposed benchmark,\nwe evaluate several retrieval methods, including BM25, query expansion, and\nstate-of-the-art dense retrievers. Our findings show that BM25 provides a\nstrong baseline but struggles with semantic matches. Query expansion\nsignificantly improves performance, though it slightly reduces string match\ncapabilities. Dense retrievers outperform traditional methods, particularly for\nsemantic matches, and general-domain dense retrievers often surpass those\ntrained specifically in the biomedical domain.\n","authors":["Zhengyun Zhao","Hongyi Yuan","Jingjing Liu","Haichao Chen","Huaiyuan Ying","Songchi Zhou","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2502.06252v1.pdf","comment":"Under review, and the dataset will be made public upon reception of\n  our paper"},{"id":"http://arxiv.org/abs/2502.06233v1","updated":"2025-02-10T08:10:29Z","published":"2025-02-10T08:10:29Z","title":"Confidence Improves Self-Consistency in LLMs","summary":"  Self-consistency decoding enhances LLMs' performance on reasoning tasks by\nsampling diverse reasoning paths and selecting the most frequent answer.\nHowever, it is computationally expensive, as sampling many of these (lengthy)\npaths is required to increase the chances that the correct answer emerges as\nthe most frequent one. To address this, we introduce Confidence-Informed\nSelf-Consistency (CISC). CISC performs a weighted majority vote based on\nconfidence scores obtained directly from the model. By prioritizing\nhigh-confidence paths, it can identify the correct answer with a significantly\nsmaller sample size. When tested on nine models and four datasets, CISC\noutperforms self-consistency in nearly all configurations, reducing the\nrequired number of reasoning paths by over 40% on average. In addition, we\nintroduce the notion of within-question confidence evaluation, after showing\nthat standard evaluation methods are poor predictors of success in\ndistinguishing correct and incorrect answers to the same question. In fact, the\nmost calibrated confidence method proved to be the least effective for CISC.\nLastly, beyond these practical implications, our results and analyses show that\nLLMs can effectively judge the correctness of their own outputs, contributing\nto the ongoing debate on this topic.\n","authors":["Amir Taubenfeld","Tom Sheffer","Eran Ofek","Amir Feder","Ariel Goldstein","Zorik Gekhman","Gal Yona"],"pdf_url":"https://arxiv.org/pdf/2502.06233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06217v1","updated":"2025-02-10T07:49:35Z","published":"2025-02-10T07:49:35Z","title":"Examining False Positives under Inference Scaling for Mathematical\n  Reasoning","summary":"  Recent advancements in language models have led to significant improvements\nin mathematical reasoning across various benchmarks. However, most of these\nbenchmarks rely on automatic evaluation methods that only compare final answers\nusing heuristics, without verifying the underlying reasoning steps. This\nlimitation results in false positive solutions, where models may produce\ncorrect final answers but with flawed deduction paths. In this paper, we\nsystematically examine the prevalence of false positive solutions in\nmathematical problem solving for language models. We analyze the\ncharacteristics and extent of this issue across different open-source models,\ndatasets of varying difficulty levels, and decoding strategies. Specifically,\nwe explore how false positives influence the inference time scaling behavior of\nlanguage models. Our experimental results reveal that: (1) false positive\nsolutions persist across different models, datasets, and decoding methods, (2)\nsampling-based inference time scaling methods do not alleviate the problem, and\n(3) the pass@N evaluation metric is more susceptible to false positives,\nsuggesting a significantly lower scaling ceiling than what automatic\nevaluations indicate. Additionally, we analyze specific instances of false\npositives and discuss potential limitations in self-improvement techniques and\nsynthetic data generation under such conditions.\n","authors":["Yu Wang","Nan Yang","Liang Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.06217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18457v2","updated":"2025-02-10T07:46:06Z","published":"2025-01-30T16:15:38Z","title":"CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language\n  Model Question Answering","summary":"  Large Language Models (LLMs) are pretrained on extensive multilingual corpora\nto acquire both language-specific cultural knowledge and general knowledge.\nIdeally, while LLMs should provide consistent responses to culture-independent\nquestions across languages, we observe significant performance disparities. To\naddress this, we explore the Cross-Lingual Self-Aligning ability of Language\nModels (CALM) to align knowledge across languages. Specifically, for a given\nquestion, we sample multiple responses across different languages and select\nthe most self-consistent response as the target, leaving the remaining\nresponses as negative examples. We then employ direct preference optimization\n(DPO) to align the model's knowledge across different languages. Evaluations on\nthe MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing\ncross-lingual knowledge question answering, both in zero-shot and\nretrieval-augmented settings. We also found that increasing the number of\nlanguages involved in CALM training leads to higher accuracy and consistency.\nWe offer a qualitative analysis of how cross-lingual consistency can enhance\nknowledge alignment and explore the method's generalizability.\n","authors":["Yumeng Wang","Zhiyuan Fan","Qingyun Wang","May Fung","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2501.18457v2.pdf","comment":"Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2502.06215v1","updated":"2025-02-10T07:33:49Z","published":"2025-02-10T07:33:49Z","title":"LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83\n  Software Engineering Benchmarks","summary":"  Large Language Models (LLMs) are widely utilized in software engineering (SE)\ntasks, such as code generation and automated program repair. However, their\nreliance on extensive and often undisclosed pre-training datasets raises\nsignificant concerns about data leakage, where the evaluation benchmark data is\nunintentionally ``seen'' by LLMs during the model's construction phase. The\ndata leakage issue could largely undermine the validity of LLM-based research\nand evaluations. Despite the increasing use of LLMs in the SE community, there\nis no comprehensive study that assesses the extent of data leakage in SE\nbenchmarks for LLMs yet. To address this gap, this paper presents the first\nlarge-scale analysis of data leakage in 83 SE benchmarks concerning LLMs. Our\nresults show that in general, data leakage in SE benchmarks is minimal, with\naverage leakage ratios of only 4.8\\%, 2.8\\%, and 0.7\\% for Python, Java, and\nC/C++ benchmarks, respectively. However, some benchmarks exhibit relatively\nhigher leakage ratios, which raises concerns about their bias in evaluation.\nFor instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\\% and\n55.7\\%, respectively. Furthermore, we observe that data leakage has a\nsubstantial impact on LLM evaluation. We also identify key causes of high data\nleakage, such as the direct inclusion of benchmark data in pre-training\ndatasets and the use of coding platforms like LeetCode for benchmark\nconstruction. To address the data leakage, we introduce\n\\textbf{LessLeak-Bench}, a new benchmark that removes leaked samples from the\n83 SE benchmarks, enabling more reliable LLM evaluations in future research.\nOur study enhances the understanding of data leakage in SE benchmarks and\nprovides valuable insights for future research involving LLMs in SE.\n","authors":["Xin Zhou","Martin Weyssow","Ratnadira Widyasari","Ting Zhang","Junda He","Yunbo Lyu","Jianming Chang","Beiqi Zhang","Dan Huang","David Lo"],"pdf_url":"https://arxiv.org/pdf/2502.06215v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2409.11887v2","updated":"2025-02-10T07:31:35Z","published":"2024-09-18T11:34:28Z","title":"DocMamba: Efficient Document Pre-training with State Space Model","summary":"  In recent years, visually-rich document understanding has attracted\nincreasing attention. Transformer-based pre-trained models have become the\nmainstream approach, yielding significant performance gains in this field.\nHowever, the self-attention mechanism's quadratic computational complexity\nhinders their efficiency and ability to process long documents. In this paper,\nwe present DocMamba, a novel framework based on the state space model. It is\ndesigned to reduce computational complexity to linear while preserving global\nmodeling capabilities. To further enhance its effectiveness in document\nprocessing, we introduce the Segment-First Bidirectional Scan (SFBS) to capture\ncontiguous semantic information. Experimental results demonstrate that DocMamba\nachieves new state-of-the-art results on downstream datasets such as FUNSD,\nCORD, and SORIE, while significantly improving speed and reducing memory usage.\nNotably, experiments on the HRDoc confirm DocMamba's potential for length\nextrapolation.\n","authors":["Pengfei Hu","Zhenrong Zhang","Jiefeng Ma","Shuhang Liu","Jun Du","Jianshu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.11887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06207v1","updated":"2025-02-10T07:14:26Z","published":"2025-02-10T07:14:26Z","title":"Unveiling the Capabilities of Large Language Models in Detecting\n  Offensive Language with Annotation Disagreement","summary":"  LLMs are widely used for offensive language detection due to their advanced\ncapability. However, the challenges posed by human annotation disagreement in\nreal-world datasets remain underexplored. These disagreement samples are\ndifficult to detect due to their ambiguous nature. Additionally, the confidence\nof LLMs in processing disagreement samples can provide valuable insights into\ntheir alignment with human annotators. To address this gap, we systematically\nevaluate the ability of LLMs to detect offensive language with annotation\ndisagreement. We compare the binary accuracy of multiple LLMs across varying\nannotation agreement levels and analyze the relationship between LLM confidence\nand annotation agreement. Furthermore, we investigate the impact of\ndisagreement samples on LLM decision-making during few-shot learning and\ninstruction fine-tuning. Our findings highlight the challenges posed by\ndisagreement samples and offer guidance for improving LLM-based offensive\nlanguage detection.\n","authors":["Junyu Lu","Kai Ma","Kaichun Wang","Kelaiti Xiao","Roy Ka-Wei Lee","Bo Xu","Liang Yang","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2502.06207v1.pdf","comment":"17 pages, submitted to the ACL 2025"},{"id":"http://arxiv.org/abs/2502.06205v1","updated":"2025-02-10T07:04:32Z","published":"2025-02-10T07:04:32Z","title":"C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like\n  Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) systems face a fundamental challenge in\naligning independently developed retrievers and large language models (LLMs).\nExisting approaches typically involve modifying either component or introducing\nsimple intermediate modules, resulting in practical limitations and sub-optimal\nperformance. Inspired by human search behavior -- typically involving a\nback-and-forth process of proposing search queries and reviewing documents, we\npropose C-3PO, a proxy-centric framework that facilitates communication between\nretrievers and LLMs through a lightweight multi-agent system. Our framework\nimplements three specialized agents that collaboratively optimize the entire\nRAG pipeline without altering the retriever and LLMs. These agents work\ntogether to assess the need for retrieval, generate effective queries, and\nselect information suitable for the LLMs. To enable effective multi-agent\ncoordination, we develop a tree-structured rollout approach for reward credit\nassignment in reinforcement learning. Extensive experiments in both in-domain\nand out-of-distribution scenarios demonstrate that C-3PO significantly enhances\nRAG performance while maintaining plug-and-play flexibility and superior\ngeneralization capabilities.\n","authors":["Guoxin Chen","Minpeng Liao","Peiying Yu","Dingmin Wang","Zile Qiao","Chao Yang","Xin Zhao","Kai Fan"],"pdf_url":"https://arxiv.org/pdf/2502.06205v1.pdf","comment":"Ongong work"},{"id":"http://arxiv.org/abs/2502.06204v1","updated":"2025-02-10T07:03:00Z","published":"2025-02-10T07:03:00Z","title":"Non-literal Understanding of Number Words by Language Models","summary":"  Humans naturally interpret numbers non-literally, effortlessly combining\ncontext, world knowledge, and speaker intent. We investigate whether large\nlanguage models (LLMs) interpret numbers similarly, focusing on hyperbole and\npragmatic halo effects. Through systematic comparison with human data and\ncomputational models of pragmatic reasoning, we find that LLMs diverge from\nhuman interpretation in striking ways. By decomposing pragmatic reasoning into\ntestable components, grounded in the Rational Speech Act framework, we pinpoint\nwhere LLM processing diverges from human cognition -- not in prior knowledge,\nbut in reasoning with it. This insight leads us to develop a targeted solution\n-- chain-of-thought prompting inspired by an RSA model makes LLMs'\ninterpretations more human-like. Our work demonstrates how computational\ncognitive models can both diagnose AI-human differences and guide development\nof more human-like language understanding capabilities.\n","authors":["Polina Tsvilodub","Kanishk Gandhi","Haoran Zhao","Jan-Philipp Fränken","Michael Franke","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2502.06204v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2311.01030v2","updated":"2025-02-10T07:01:30Z","published":"2023-11-02T06:43:50Z","title":"Joint Learning of Local and Global Features for Aspect-based Sentiment\n  Classification","summary":"  Aspect-based sentiment classification (ASC) aims to judge the sentiment\npolarity conveyed by the given aspect term in a sentence. The sentiment\npolarity is not only determined by the local context but also related to the\nwords far away from the given aspect term. Most recent efforts related to the\nattention-based models can not sufficiently distinguish which words they should\npay more attention to in some cases. Meanwhile, graph-based models are coming\ninto ASC to encode syntactic dependency tree information. But these models do\nnot fully leverage syntactic dependency trees as they neglect to incorporate\ndependency relation tag information into representation learning effectively.\nIn this paper, we address these problems by effectively modeling the local and\nglobal features. Firstly, we design a local encoder containing: a Gaussian mask\nlayer and a covariance self-attention layer. The Gaussian mask layer tends to\nadjust the receptive field around aspect terms adaptively to deemphasize the\neffects of unrelated words and pay more attention to local information. The\ncovariance self-attention layer can distinguish the attention weights of\ndifferent words more obviously. Furthermore, we propose a dual-level graph\nattention network as a global encoder by fully employing dependency tag\ninformation to capture long-distance information effectively. Our model\nachieves state-of-the-art performance on both SemEval 2014 and Twitter\ndatasets.\n","authors":["Hao Niu","Yun Xiong","Xiaosu Wang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2311.01030v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2502.04643v2","updated":"2025-02-10T06:46:48Z","published":"2025-02-07T04:07:36Z","title":"Confidence Elicitation: A New Attack Vector for Large Language Models","summary":"  A fundamental issue in deep learning has been adversarial robustness. As\nthese systems have scaled, such issues have persisted. Currently, large\nlanguage models (LLMs) with billions of parameters suffer from adversarial\nattacks just like their earlier, smaller counterparts. However, the threat\nmodels have changed. Previously, having gray-box access, where input embeddings\nor output logits/probabilities were visible to the user, might have been\nreasonable. However, with the introduction of closed-source models, no\ninformation about the model is available apart from the generated output. This\nmeans that current black-box attacks can only utilize the final prediction to\ndetect if an attack is successful. In this work, we investigate and demonstrate\nthe potential of attack guidance, akin to using output probabilities, while\nhaving only black-box access in a classification setting. This is achieved\nthrough the ability to elicit confidence from the model. We empirically show\nthat the elicited confidence is calibrated and not hallucinated for current\nLLMs. By minimizing the elicited confidence, we can therefore increase the\nlikelihood of misclassification. Our new proposed paradigm demonstrates\npromising state-of-the-art results on three datasets across two models\n(LLaMA-3-8B-Instruct and Mistral-7B-Instruct-V0.3) when comparing our technique\nto existing hard-label black-box attack methods that introduce word-level\nsubstitutions.\n","authors":["Brian Formento","Chuan Sheng Foo","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2502.04643v2.pdf","comment":"Published in ICLR 2025. The code is publicly available at\n  https://github.com/Aniloid2/Confidence_Elicitation_Attacks"},{"id":"http://arxiv.org/abs/2502.06185v1","updated":"2025-02-10T06:30:15Z","published":"2025-02-10T06:30:15Z","title":"Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long\n  Document Summarization","summary":"  Detecting factual inconsistency for long document summarization remains\nchallenging, given the complex structure of the source article and long summary\nlength. In this work, we study factual inconsistency errors and connect them\nwith a line of discourse analysis. We find that errors are more common in\ncomplex sentences and are associated with several discourse features. We\npropose a framework that decomposes long texts into discourse-inspired chunks\nand utilizes discourse information to better aggregate sentence-level scores\npredicted by natural language inference models. Our approach shows improved\nperformance on top of different model baselines over several evaluation\nbenchmarks, covering rich domains of texts, focusing on long document\nsummarization. This underscores the significance of incorporating discourse\nfeatures in developing models for scoring summaries for long document factual\ninconsistency.\n","authors":["Yang Zhong","Diane Litman"],"pdf_url":"https://arxiv.org/pdf/2502.06185v1.pdf","comment":"NAACL 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2407.01257v4","updated":"2025-02-10T06:27:01Z","published":"2024-07-01T13:07:01Z","title":"uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in\n  Low-Data Regimes","summary":"  Recent work on distilling Whisper's knowledge into small models using\npseudo-labels shows promising performance while reducing the size by up to 50%.\nThis results in small, efficient, and dedicated models. However, a critical\nstep of distillation using pseudo-labels involves filtering high-quality\npredictions and using only those during training. This step requires ground\ntruth labels to compare with and filter low-quality examples, making the\nprocess dependent on human labels. Additionally, the distillation process\nrequires a large amount of data thereby limiting its applicability in\nlow-resource settings. To address this, we propose a distillation framework\nthat does not require any labeled data. Through experimentation, we show that\nour best-distilled models outperform the teacher model by 5-7 WER points and\nare on par with or outperform similar supervised data filtering setups. When\nscaling the data, our models significantly outperform all zero-shot and\nsupervised models. Our models are also 25-50% more compute- and\nmemory-efficient while maintaining performance equal to or better than that of\nthe teacher model. For more details about our models, dataset, and other\nresources, please visit our GitHub page:\nhttps://github.com/UBC-NLP/uDistilWhisper.\n","authors":["Abdul Waheed","Karima Kadaoui","Bhiksha Raj","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2407.01257v4.pdf","comment":"Accepted to NAACL'25 main conference"},{"id":"http://arxiv.org/abs/2501.15754v3","updated":"2025-02-10T06:26:57Z","published":"2025-01-27T03:45:29Z","title":"Weight-based Analysis of Detokenization in Language Models:\n  Understanding the First Stage of Inference Without Inference","summary":"  According to the stages-of-inference hypothesis, early layers of language\nmodels map their subword-tokenized input, which does not necessarily correspond\nto a linguistically meaningful segmentation, to more meaningful representations\nthat form the model's \"inner vocabulary\". Prior analysis of this detokenization\nstage has predominantly relied on probing and interventions such as path\npatching, which involve selecting particular inputs, choosing a subset of\ncomponents that will be patched, and then observing changes in model behavior.\nHere, we show that several important aspects of the detokenization stage can be\nunderstood purely by analyzing model weights, without performing any model\ninference steps. Specifically, we introduce an analytical decomposition of\nfirst-layer attention in GPT-2. Our decomposition yields interpretable terms\nthat quantify the relative contributions of position-related, token-related,\nand mixed effects. By focusing on terms in this decomposition, we discover\nweight-based explanations of attention bias toward close tokens and attention\nfor detokenization.\n","authors":["Go Kamoda","Benjamin Heinzerling","Tatsuro Inaba","Keito Kudo","Keisuke Sakaguchi","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2501.15754v3.pdf","comment":"22 pages, 14 figures, to appear in NAACL Findings 2025"},{"id":"http://arxiv.org/abs/2502.06180v1","updated":"2025-02-10T06:18:07Z","published":"2025-02-10T06:18:07Z","title":"RideKE: Leveraging Low-Resource, User-Generated Twitter Content for\n  Sentiment and Emotion Detection in Kenyan Code-Switched Dataset","summary":"  Social media has become a crucial open-access platform for individuals to\nexpress opinions and share experiences. However, leveraging low-resource\nlanguage data from Twitter is challenging due to scarce, poor-quality content\nand the major variations in language use, such as slang and code-switching.\nIdentifying tweets in these languages can be difficult as Twitter primarily\nsupports high-resource languages. We analyze Kenyan code-switched data and\nevaluate four state-of-the-art (SOTA) transformer-based pretrained models for\nsentiment and emotion classification, using supervised and semi-supervised\nmethods. We detail the methodology behind data collection and annotation, and\nthe challenges encountered during the data curation phase. Our results show\nthat XLM-R outperforms other models; for sentiment analysis, XLM-R supervised\nmodel achieves the highest accuracy (69.2\\%) and F1 score (66.1\\%), XLM-R\nsemi-supervised (67.2\\% accuracy, 64.1\\% F1 score). In emotion analysis,\nDistilBERT supervised leads in accuracy (59.8\\%) and F1 score (31\\%), mBERT\nsemi-supervised (accuracy (59\\% and F1 score 26.5\\%). AfriBERTa models show the\nlowest accuracy and F1 scores. All models tend to predict neutral sentiment,\nwith Afri-BERT showing the highest bias and unique sensitivity to empathy\nemotion. https://github.com/NEtori21/Ride_hailing\n","authors":["Naome A. Etori","Maria L. Gini"],"pdf_url":"https://arxiv.org/pdf/2502.06180v1.pdf","comment":"Accepted in WASSA 2024"},{"id":"http://arxiv.org/abs/2502.06173v1","updated":"2025-02-10T05:54:36Z","published":"2025-02-10T05:54:36Z","title":"Uncertainty-Aware Adaptation of Large Language Models for\n  Protein-Protein Interaction Analysis","summary":"  Identification of protein-protein interactions (PPIs) helps derive cellular\nmechanistic understanding, particularly in the context of complex conditions\nsuch as neurodegenerative disorders, metabolic syndromes, and cancer. Large\nLanguage Models (LLMs) have demonstrated remarkable potential in predicting\nprotein structures and interactions via automated mining of vast biomedical\nliterature; yet their inherent uncertainty remains a key challenge for deriving\nreproducible findings, critical for biomedical applications. In this study, we\npresent an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging\nfine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we\nintegrate LoRA ensembles and Bayesian LoRA models for uncertainty\nquantification (UQ), ensuring confidence-calibrated insights into protein\nbehavior. Our approach achieves competitive performance in PPI identification\nacross diverse disease contexts while addressing model uncertainty, thereby\nenhancing trustworthiness and reproducibility in computational biology. These\nfindings underscore the potential of uncertainty-aware LLM adaptation for\nadvancing precision medicine and biomedical research.\n","authors":["Sanket Jantre","Tianle Wang","Gilchan Park","Kriti Chopra","Nicholas Jeon","Xiaoning Qian","Nathan M. Urban","Byung-Jun Yoon"],"pdf_url":"https://arxiv.org/pdf/2502.06173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13191v4","updated":"2025-02-10T05:52:10Z","published":"2024-10-17T03:38:29Z","title":"MCQG-SRefine: Multiple Choice Question Generation and Evaluation with\n  Iterative Self-Critique, Correction, and Comparison Feedback","summary":"  Automatic question generation (QG) is essential for AI and NLP, particularly\nin intelligent tutoring, dialogue systems, and fact verification. Generating\nmultiple-choice questions (MCQG) for professional exams, like the United States\nMedical Licensing Examination (USMLE), is particularly challenging, requiring\ndomain expertise and complex multi-hop reasoning for high-quality questions.\nHowever, current large language models (LLMs) like GPT-4 struggle with\nprofessional MCQG due to outdated knowledge, hallucination issues, and prompt\nsensitivity, resulting in unsatisfactory quality and difficulty. To address\nthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique\nand Correction) framework for converting medical cases into high-quality\nUSMLE-style questions. By integrating expert-driven prompt engineering with\niterative self-critique and self-correction feedback, MCQG-SRefine\nsignificantly enhances human expert satisfaction regarding both the quality and\ndifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based\nautomatic metric to replace the complex and costly expert evaluation process,\nensuring reliable and expert-aligned assessments.\n","authors":["Zonghai Yao","Aditya Parashar","Huixue Zhou","Won Seok Jang","Feiyun Ouyang","Zhichao Yang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.13191v4.pdf","comment":"Equal contribution for the first two authors. To appear in\n  proceedings of the Main Conference on 2025 Annual Conference of the Nations\n  of the Americas Chapter of the Association for Computational Linguistics\n  (NAACL). Keywords: Question Generation, USMLE, Self-Refine, Self-Critique,\n  and Self-Correction, LLM-as-Judge, AI for Medical Education"},{"id":"http://arxiv.org/abs/2410.06625v2","updated":"2025-02-10T05:47:27Z","published":"2024-10-09T07:21:43Z","title":"ETA: Evaluating Then Aligning Safety of Vision Language Models at\n  Inference Time","summary":"  Vision Language Models (VLMs) have become essential backbones for multimodal\nintelligence, yet significant safety challenges limit their real-world\napplication. While textual inputs are often effectively safeguarded,\nadversarial visual inputs can easily bypass VLM defense mechanisms. Existing\ndefense methods are either resource-intensive, requiring substantial data and\ncompute, or fail to simultaneously ensure safety and usefulness in responses.\nTo address these limitations, we propose a novel two-phase inference-time\nalignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual\ncontents and output responses to establish a robust safety awareness in\nmultimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep\nlevels by conditioning the VLMs' generative distribution with an interference\nprefix and performing sentence-level best-of-N to search the most harmless and\nhelpful generation paths. Extensive experiments show that ETA outperforms\nbaseline methods in terms of harmlessness, helpfulness, and efficiency,\nreducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%\nwin-ties in GPT-4 helpfulness evaluation. The code is publicly available at\nhttps://github.com/DripNowhy/ETA.\n","authors":["Yi Ding","Bolian Li","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06625v2.pdf","comment":"29pages"},{"id":"http://arxiv.org/abs/2502.06167v1","updated":"2025-02-10T05:36:30Z","published":"2025-02-10T05:36:30Z","title":"Universal Approximation of Visual Autoregressive Transformers","summary":"  We investigate the fundamental limits of transformer-based foundation models,\nextending our analysis to include Visual Autoregressive (VAR) transformers. VAR\nrepresents a big step toward generating images using a novel, scalable,\ncoarse-to-fine ``next-scale prediction'' framework. These models set a new\nquality bar, outperforming all previous methods, including Diffusion\nTransformers, while having state-of-the-art performance for image synthesis\ntasks. Our primary contributions establish that, for single-head VAR\ntransformers with a single self-attention layer and single interpolation layer,\nthe VAR Transformer is universal. From the statistical perspective, we prove\nthat such simple VAR transformers are universal approximators for any\nimage-to-image Lipschitz functions. Furthermore, we demonstrate that flow-based\nautoregressive transformers inherit similar approximation capabilities. Our\nresults provide important design principles for effective and computationally\nefficient VAR Transformer strategies that can be used to extend their utility\nto more sophisticated VAR models in image generation and other related areas.\n","authors":["Yifang Chen","Xiaoyu Li","Yingyu Liang","Zhenmei Shi","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2502.06167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15797v4","updated":"2025-02-10T05:31:43Z","published":"2025-01-27T05:46:06Z","title":"LemmaHead: RAG Assisted Proof Generation Using Large Language Models","summary":"  Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.\n","authors":["Tianbo Yang","Mingqi Yan","Hongyi Zhao","Tianshuo Yang"],"pdf_url":"https://arxiv.org/pdf/2501.15797v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04757v2","updated":"2025-02-10T04:39:28Z","published":"2025-02-07T08:43:15Z","title":"ELITE: Enhanced Language-Image Toxicity Evaluation for Safety","summary":"  Current Vision Language Models (VLMs) remain vulnerable to malicious prompts\nthat induce harmful outputs. Existing safety benchmarks for VLMs primarily rely\non automated evaluation methods, but these methods struggle to detect implicit\nharmful content or produce inaccurate evaluations. Therefore, we found that\nexisting benchmarks have low levels of harmfulness, ambiguous data, and limited\ndiversity in image-text pair combinations. To address these issues, we propose\nthe ELITE benchmark, a high-quality safety evaluation benchmark for VLMs,\nunderpinned by our enhanced evaluation method, the ELITE evaluator. The ELITE\nevaluator explicitly incorporates a toxicity score to accurately assess\nharmfulness in multimodal contexts, where VLMs often provide specific,\nconvincing, but unharmful descriptions of images. We filter out ambiguous and\nlow-quality image-text pairs from existing benchmarks using the ELITE evaluator\nand generate diverse combinations of safe and unsafe image-text pairs. Our\nexperiments demonstrate that the ELITE evaluator achieves superior alignment\nwith human evaluations compared to prior automated methods, and the ELITE\nbenchmark offers enhanced benchmark quality and diversity. By introducing\nELITE, we pave the way for safer, more robust VLMs, contributing essential\ntools for evaluating and mitigating safety risks in real-world applications.\n","authors":["Wonjun Lee","Doehyeon Lee","Eugene Choi","Sangyoon Yu","Ashkan Yousefpour","Haon Park","Bumsub Ham","Suhyun Kim"],"pdf_url":"https://arxiv.org/pdf/2502.04757v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06150v1","updated":"2025-02-10T04:33:27Z","published":"2025-02-10T04:33:27Z","title":"Scaling Public Health Text Annotation: Zero-Shot Learning vs.\n  Crowdsourcing for Improved Efficiency and Labeling Accuracy","summary":"  Public health researchers are increasingly interested in using social media\ndata to study health-related behaviors, but manually labeling this data can be\nlabor-intensive and costly. This study explores whether zero-shot labeling\nusing large language models (LLMs) can match or surpass conventional\ncrowd-sourced annotation for Twitter posts related to sleep disorders, physical\nactivity, and sedentary behavior. Multiple annotation pipelines were designed\nto compare labels produced by domain experts, crowd workers, and LLM-driven\napproaches under varied prompt-engineering strategies. Our findings indicate\nthat LLMs can rival human performance in straightforward classification tasks\nand significantly reduce labeling time, yet their accuracy diminishes for tasks\nrequiring more nuanced domain knowledge. These results clarify the trade-offs\nbetween automated scalability and human expertise, demonstrating conditions\nunder which LLM-based labeling can be efficiently integrated into public health\nresearch without undermining label quality.\n","authors":["Kamyar Kazari","Yong Chen","Zahra Shakeri"],"pdf_url":"https://arxiv.org/pdf/2502.06150v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.06148v1","updated":"2025-02-10T04:29:36Z","published":"2025-02-10T04:29:36Z","title":"Optimizing Knowledge Integration in Retrieval-Augmented Generation with\n  Self-Selection","summary":"  Retrieval-Augmented Generation (RAG), which integrates external knowledge\ninto Large Language Models (LLMs), has proven effective in enabling LLMs to\nproduce more accurate and reliable responses. However, it remains a significant\nchallenge how to effectively integrate external retrieved knowledge with\ninternal parametric knowledge in LLMs. In this work, we propose a novel\nSelf-Selection RAG framework, where the LLM is made to select from pairwise\nresponses generated with internal parametric knowledge solely and with external\nretrieved knowledge together to achieve enhanced accuracy. To this end, we\ndevise a Self-Selection-RGP method to enhance the capabilities of the LLM in\nboth generating and selecting the correct answer, by training the LLM with\nDirect Preference Optimization (DPO) over a curated Retrieval Generation\nPreference (RGP) dataset. Experimental results with two open-source LLMs (i.e.,\nLlama2-13B-Chat and Mistral-7B) well demonstrate the superiority of our\napproach over other baseline methods on Natural Questions (NQ) and TrivialQA\ndatasets.\n","authors":["Yan Weng","Fengbin Zhu","Tong Ye","Haoyan Liu","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.06148v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.01624v2","updated":"2025-02-10T04:28:46Z","published":"2024-12-02T15:43:10Z","title":"Headline-Guided Extractive Summarization for Thai News Articles","summary":"  Text summarization is a process of condensing lengthy texts while preserving\ntheir essential information. Previous studies have predominantly focused on\nhigh-resource languages, while low-resource languages like Thai have received\nless attention. Furthermore, earlier extractive summarization models for Thai\ntexts have primarily relied on the article's body, without considering the\nheadline. This omission can result in the exclusion of key sentences from the\nsummary. To address these limitations, we propose CHIMA, an extractive\nsummarization model that incorporates the contextual information of the\nheadline for Thai news articles. Our model utilizes a pre-trained language\nmodel to capture complex language semantics and assigns a probability to each\nsentence to be included in the summary. By leveraging the headline to guide\nsentence selection, CHIMA enhances the model's ability to recover important\nsentences and discount irrelevant ones. Additionally, we introduce two\nstrategies for aggregating headline-body similarities, simple average and\nharmonic mean, providing flexibility in sentence selection to accommodate\nvarying writing styles. Experiments on publicly available Thai news datasets\ndemonstrate that CHIMA outperforms baseline models across ROUGE, BLEU, and F1\nscores. These results highlight the effectiveness of incorporating the\nheadline-body similarities as model guidance. The results also indicate an\nenhancement in the model's ability to recall critical sentences, even those\nscattered throughout the middle or end of the article. With this potential,\nheadline-guided extractive summarization offers a promising approach to improve\nthe quality and relevance of summaries for Thai news articles.\n","authors":["Pimpitchaya Kositcharoensuk","Nakarin Sritrakool","Ploy N. Pratanwanich"],"pdf_url":"https://arxiv.org/pdf/2412.01624v2.pdf","comment":"Accepted for publication in IEEE Access"},{"id":"http://arxiv.org/abs/2502.06147v1","updated":"2025-02-10T04:25:05Z","published":"2025-02-10T04:25:05Z","title":"LegalViz: Legal Text Visualization by Text To Diagram Generation","summary":"  Legal documents including judgments and court orders require highly\nsophisticated legal knowledge for understanding. To disclose expert knowledge\nfor non-experts, we explore the problem of visualizing legal texts with\neasy-to-understand diagrams and propose a novel dataset of LegalViz with 23\nlanguages and 7,010 cases of legal document and visualization pairs, using the\nDOT graph description language of Graphviz. LegalViz provides a simple diagram\nfrom a complicated legal corpus identifying legal entities, transactions, legal\nsources, and statements at a glance, that are essential in each judgment. In\naddition, we provide new evaluation metrics for the legal diagram visualization\nby considering graph structures, textual similarities, and legal contents. We\nconducted empirical studies on few-shot and finetuning large language models\nfor generating legal diagrams and evaluated them with these metrics, including\nlegal content-based evaluation within 23 languages. Models trained with\nLegalViz outperform existing models including GPTs, confirming the\neffectiveness of our dataset.\n","authors":["Eri Onami","Taiki Miyanishi","Koki Maeda","Shuhei Kurita"],"pdf_url":"https://arxiv.org/pdf/2502.06147v1.pdf","comment":"NAACL2025"},{"id":"http://arxiv.org/abs/2501.17905v2","updated":"2025-02-10T04:07:04Z","published":"2025-01-29T14:28:11Z","title":"DReSS: Data-driven Regularized Structured Streamlining for Large\n  Language Models","summary":"  Large language models (LLMs) have achieved significant progress across\nvarious domains, but their increasing scale results in high computational and\nmemory costs. Recent studies have revealed that LLMs exhibit sparsity,\nproviding the potential to reduce model size through pruning techniques.\nHowever, existing pruning methods typically follow a prune-then-finetune\nparadigm. Since the pruned components still contain valuable information, their\ndirect removal often leads to irreversible performance degradation, imposing a\nsubstantial computational burden to recover performance during finetuning. In\nthis paper, we propose a novel paradigm that first applies regularization, then\nprunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a\nsimple and effective Data-driven Regularized Structured Streamlining method for\nLLMs. By leveraging a small amount of data to regularize the components to be\npruned, DReSS explicitly transfers the important information to the remaining\nparts of the model in advance. Compared to direct pruning, this can reduce the\ninformation loss caused by parameter removal, thereby enhancing its language\nmodeling capabilities. Experimental results demonstrate that DReSS\nsignificantly outperforms existing pruning methods even under extreme pruning\nratios, significantly reducing latency and increasing throughput.\n","authors":["Mingkuan Feng","Jinyang Wu","Shuai Zhang","Pengpeng Shao","Ruihan Jin","Zhengqi Wen","Jianhua Tao","Feihu Che"],"pdf_url":"https://arxiv.org/pdf/2501.17905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06139v1","updated":"2025-02-10T04:02:18Z","published":"2025-02-10T04:02:18Z","title":"LCIRC: A Recurrent Compression Approach for Efficient Long-form Context\n  and Query Dependent Modeling in LLMs","summary":"  While large language models (LLMs) excel in generating coherent and\ncontextually rich outputs, their capacity to efficiently handle long-form\ncontexts is limited by fixed-length position embeddings. Additionally, the\ncomputational cost of processing long sequences increases quadratically, making\nit challenging to extend context length. To address these challenges, we\npropose Long-form Context Injection with Recurrent Compression (LCIRC), a\nmethod that enables the efficient processing long-form sequences beyond the\nmodel's length limit through recurrent compression without retraining the\nentire model. We further introduce query dependent context modeling, which\nselectively compresses query-relevant information, ensuring that the model\nretains the most pertinent content. Our empirical results demonstrate that\nQuery Dependent LCIRC (QD-LCIRC) significantly improves LLM's ability to manage\nextended contexts, making it well-suited for tasks that require both\ncomprehensive context understanding and query relevance.\n","authors":["Sumin An","Junyoung Sung","Wonpyo Park","Chanjun Park","Paul Hongsuck Seo"],"pdf_url":"https://arxiv.org/pdf/2502.06139v1.pdf","comment":"Accepted to NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2502.06132v1","updated":"2025-02-10T03:46:39Z","published":"2025-02-10T03:46:39Z","title":"Enhancing Document Key Information Localization Through Data\n  Augmentation","summary":"  The Visually Rich Form Document Intelligence and Understanding (VRDIU) Track\nB focuses on the localization of key information in document images. The goal\nis to develop a method capable of localizing objects in both digital and\nhandwritten documents, using only digital documents for training. This paper\npresents a simple yet effective approach that includes a document augmentation\nphase and an object detection phase. Specifically, we augment the training set\nof digital documents by mimicking the appearance of handwritten documents. Our\nexperiments demonstrate that this pipeline enhances the models' generalization\nability and achieves high performance in the competition.\n","authors":["Yue Dai"],"pdf_url":"https://arxiv.org/pdf/2502.06132v1.pdf","comment":"Accepted as a workshop paper in DOCUI-AAAI2025"},{"id":"http://arxiv.org/abs/2502.06130v1","updated":"2025-02-10T03:43:55Z","published":"2025-02-10T03:43:55Z","title":"Self-Correcting Decoding with Generative Feedback for Mitigating\n  Hallucinations in Large Vision-Language Models","summary":"  While recent Large Vision-Language Models (LVLMs) have shown remarkable\nperformance in multi-modal tasks, they are prone to generating hallucinatory\ntext responses that do not align with the given visual input, which restricts\ntheir practical applicability in real-world scenarios. In this work, inspired\nby the observation that the text-to-image generation process is the inverse of\nimage-conditioned response generation in LVLMs, we explore the potential of\nleveraging text-to-image generative models to assist in mitigating\nhallucinations in LVLMs. We discover that generative models can offer valuable\nself-feedback for mitigating hallucinations at both the response and token\nlevels. Building on this insight, we introduce self-correcting Decoding with\nGenerative Feedback (DeGF), a novel training-free algorithm that incorporates\nfeedback from text-to-image generative models into the decoding process to\neffectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an\nimage from the initial response produced by LVLMs, which acts as an auxiliary\nvisual reference and provides self-feedback to verify and correct the initial\nresponse through complementary or contrastive decoding. Extensive experimental\nresults validate the effectiveness of our approach in mitigating diverse types\nof hallucinations, consistently surpassing state-of-the-art methods across six\nbenchmarks. Code is available at https://github.com/zhangce01/DeGF.\n","authors":["Ce Zhang","Zifu Wan","Zhehan Kan","Martin Q. Ma","Simon Stepputtis","Deva Ramanan","Russ Salakhutdinov","Louis-Philippe Morency","Katia Sycara","Yaqi Xie"],"pdf_url":"https://arxiv.org/pdf/2502.06130v1.pdf","comment":"Accepted by ICLR 2025. Project page:https://zhangce01.github.io/DeGF/"},{"id":"http://arxiv.org/abs/2407.10805v7","updated":"2025-02-10T03:16:09Z","published":"2024-07-15T15:20:40Z","title":"Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation","summary":"  Retrieval-augmented generation (RAG) has improved large language models\n(LLMs) by using knowledge retrieval to overcome knowledge deficiencies.\nHowever, current RAG methods often fall short of ensuring the depth and\ncompleteness of retrieved information, which is necessary for complex reasoning\ntasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG\nframework that iteratively retrieves information from both unstructured and\nstructured knowledge sources in a tight-coupling manner. Specifically, ToG-2\nleverages knowledge graphs (KGs) to link documents via entities, facilitating\ndeep and knowledge-guided context retrieval. Simultaneously, it utilizes\ndocuments as entity contexts to achieve precise and efficient graph retrieval.\nToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate answers. We\nconduct a series of well-designed experiments to highlight the following\nadvantages of ToG-2: 1) ToG-2 tightly couples the processes of context\nretrieval and graph retrieval, deepening context retrieval via the KG while\nenabling reliable graph retrieval based on contexts; 2) it achieves deep and\nfaithful reasoning in LLMs through an iterative knowledge retrieval process of\ncollaboration between contexts and the KG; and 3) ToG-2 is training-free and\nplug-and-play compatible with various LLMs. Extensive experiments demonstrate\nthat ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7\nknowledge-intensive datasets with GPT-3.5, and can elevate the performance of\nsmaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.\nThe source code is available on https://github.com/IDEA-FinAI/ToG-2.\n","authors":["Shengjie Ma","Chengjin Xu","Xuhui Jiang","Muzhi Li","Huaren Qu","Cehao Yang","Jiaxin Mao","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.10805v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16672v3","updated":"2025-02-10T03:02:26Z","published":"2024-06-24T14:27:54Z","title":"CAVE: Controllable Authorship Verification Explanations","summary":"  Authorship Verification (AV) (do two documents have the same author?) is\nessential in many real-life applications. AV is often used in privacy-sensitive\ndomains that require an offline proprietary model that is deployed on premises,\nmaking publicly served online models (APIs) a suboptimal choice. Current\noffline AV models however have lower downstream utility due to limited accuracy\n(eg: traditional stylometry AV systems) and lack of accessible post-hoc\nexplanations. In this work, we address the above challenges by developing a\ntrained, offline model CAVE (Controllable Authorship Verification\nExplanations). CAVE generates free-text AV explanations that are controlled to\nbe (1) accessible (uniform structure that can be decomposed into\nsub-explanations grounded to relevant linguistic features), and (2) easily\nverified for explanation-label consistency. We generate silver-standard\ntraining data grounded to the desirable linguistic features by a prompt-based\nmethod Prompt-CAVE. We then filter the data based on rationale-label\nconsistency using a novel metric Cons-R-L. Finally, we fine-tune a small,\noffline model (Llama-3-8B) with this data to create our model CAVE. Results on\nthree difficult AV datasets show that CAVE generates high quality explanations\n(as measured by automatic and human evaluation) as well as competitive task\naccuracy.\n","authors":["Sahana Ramnath","Kartik Pandey","Elizabeth Boschee","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2406.16672v3.pdf","comment":"Accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2502.06115v1","updated":"2025-02-10T02:49:46Z","published":"2025-02-10T02:49:46Z","title":"Task-driven Layerwise Additive Activation Intervention","summary":"  Modern language models (LMs) have significantly advanced generative modeling\nin natural language processing (NLP). Despite their success, LMs often struggle\nwith adaptation to new contexts in real-time applications. A promising approach\nto task adaptation is activation intervention, which steers the LMs' generation\nprocess by identifying and manipulating the activations. However, existing\ninterventions are highly dependent on heuristic rules or require many prompt\ninputs to determine effective interventions. This paper proposes a layer-wise\nadditive activation intervention framework that optimizes the intervention\nprocess, thus enhancing the sample efficiency. We benchmark our framework on\nvarious datasets, demonstrating improvements in the accuracy of pre-trained LMs\nand competing intervention baselines.\n","authors":["Hieu Trung Nguyen","Bao Nguyen","Binh Nguyen","Viet Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.06115v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2407.00936v3","updated":"2025-02-10T02:38:25Z","published":"2024-07-01T03:37:35Z","title":"Large Language Model Enhanced Knowledge Representation Learning: A\n  Survey","summary":"  Knowledge Representation Learning (KRL) is crucial for enabling applications\nof symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by\nprojecting knowledge facts into vector spaces. Despite their effectiveness in\nmodeling KG structural information, KRL methods are suffering from the\nsparseness of KGs. The rise of Large Language Models (LLMs) built on the\nTransformer architecture present promising opportunities for enhancing KRL by\nincorporating textual information to address information sparsity in KGs.\nLLM-enhanced KRL methods, including three key approaches, encoder-based methods\nthat leverage detailed contextual information, encoder-decoder-based methods\nthat utilize a unified seq2seq model for comprehensive encoding and decoding,\nand decoder-based methods that utilize extensive knowledge from large corpora,\nhas significantly advanced the effectiveness and generalization of KRL in\naddressing a wide range of downstream tasks. This work provides a broad\noverview of downstream tasks while simultaneously identifying emerging research\ndirections in these evolving domains.\n","authors":["Xin Wang","Zirui Chen","Haofen Wang","Leong Hou U","Zhao Li","Wenbin Guo"],"pdf_url":"https://arxiv.org/pdf/2407.00936v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06106v1","updated":"2025-02-10T02:35:53Z","published":"2025-02-10T02:35:53Z","title":"Circuit-tuning: A Mechanistic Approach for Identifying Parameter\n  Redundancy and Fine-tuning Neural Networks","summary":"  The study of mechanistic interpretability aims to reverse-engineer a model to\nexplain its behaviors. While recent studies have focused on the static\nmechanism of a certain behavior, the training dynamics inside a model remain to\nbe explored. In this work, we develop an interpretable method for fine-tuning\nand reveal the mechanism behind learning. We first propose the concept of node\nredundancy as an extension of intrinsic dimension and explain the idea behind\ncircuit discovery from a fresh view. Based on the theory, we propose\ncircuit-tuning, a two-stage algorithm that iteratively performs circuit\ndiscovery to mask out irrelevant edges and updates the remaining parameters\nresponsible for a specific task. Experiments show that our method not only\nimproves performance on a wide range of tasks but is also scalable while\npreserving general capabilities. We visualize and analyze the circuits before,\nduring, and after fine-tuning, providing new insights into the\nself-organization mechanism of a neural network in the learning process.\n","authors":["Yueyan Li","Caixia Yuan","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04528v2","updated":"2025-02-10T02:22:15Z","published":"2025-02-06T21:58:48Z","title":"Group-Adaptive Threshold Optimization for Robust AI-Generated Text\n  Detection","summary":"  The advancement of large language models (LLMs) has made it difficult to\ndifferentiate human-written text from AI-generated text. Several AI-text\ndetectors have been developed in response, which typically utilize a fixed\nglobal threshold (e.g., {\\theta} = 0.5) to classify machine-generated text.\nHowever, we find that one universal threshold can fail to account for\nsubgroup-specific distributional variations. For example, when using a fixed\nthreshold, detectors make more false positive errors on shorter human-written\ntext than longer, and more positive classifications on neurotic writing styles\nthan open among long text. These discrepancies can lead to misclassification\nthat disproportionately affects certain groups. We address this critical\nlimitation by introducing FairOPT, an algorithm for group-specific threshold\noptimization in AI-generated content classifiers. Our approach partitions data\ninto subgroups based on attributes (e.g., text length and writing style) and\nlearns decision thresholds for each group, which enables careful balancing of\nperformance and fairness metrics within each subgroup. In experiments with four\nAI text classifiers on three datasets, FairOPT enhances overall F1 score and\ndecreases balanced error rate (BER) discrepancy across subgroups. Our framework\npaves the way for more robust and fair classification criteria in AI-generated\noutput detection.\n","authors":["Minseok Jung","Cynthia Fuertes Panizo","Liam Dugan","Yi R."," Fung","Pin-Yu Chen","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2502.04528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06101v1","updated":"2025-02-10T02:15:12Z","published":"2025-02-10T02:15:12Z","title":"RALLRec: Improving Retrieval Augmented Large Language Model\n  Recommendation with Representation Learning","summary":"  Large Language Models (LLMs) have been integrated into recommendation systems\nto enhance user behavior comprehension. The Retrieval Augmented Generation\n(RAG) technique is further incorporated into these systems to retrieve more\nrelevant items and improve system performance. However, existing RAG methods\nrely primarily on textual semantics and often fail to incorporate the most\nrelevant items, limiting the effectiveness of the systems.\n  In this paper, we propose Representation learning for retrieval-Augmented\nLarge Language model Recommendation (RALLRec). Specifically, we enhance textual\nsemantics by prompting LLMs to generate more detailed item descriptions,\nfollowed by joint representation learning of textual and collaborative\nsemantics, which are extracted by the LLM and recommendation models,\nrespectively. Considering the potential time-varying characteristics of user\ninterest, a simple yet effective reranking method is further introduced to\ncapture the dynamics of user preference. We conducted extensive experiments on\nthree real-world datasets, and the evaluation results validated the\neffectiveness of our method. Code is made public at\nhttps://github.com/JianXu95/RALLRec.\n","authors":["Jian Xu","Sichun Luo","Xiangyu Chen","Haoming Huang","Hanxu Hou","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2502.06101v1.pdf","comment":"Accepted by TheWebConf'25 (WWW'25) as a Short Paper"},{"id":"http://arxiv.org/abs/2410.13155v2","updated":"2025-02-10T02:02:11Z","published":"2024-10-17T02:16:37Z","title":"SLM-Mod: Small Language Models Surpass LLMs at Content Moderation","summary":"  Large language models (LLMs) have shown promise in many natural language\nunderstanding tasks, including content moderation. However, these models can be\nexpensive to query in real-time and do not allow for a community-specific\napproach to content moderation. To address these challenges, we explore the use\nof open-source small language models (SLMs) for community-specific content\nmoderation tasks. We fine-tune and evaluate SLMs (less than 15B parameters) by\ncomparing their performance against much larger open- and closed-sourced models\nin both a zero-shot and few-shot setting. Using 150K comments from 15 popular\nReddit communities, we find that SLMs outperform zero-shot LLMs at content\nmoderation -- 11.5% higher accuracy and 25.7% higher recall on average across\nall communities. Moreover, few-shot in-context learning leads to only a\nmarginal increase in the performance of LLMs, still lacking compared to SLMs.\nWe further show the promise of cross-community content moderation, which has\nimplications for new communities and the development of cross-platform\nmoderation techniques. Finally, we outline directions for future work on\nlanguage model based content moderation. Code and models can be found at\nhttps://github.com/AGoyal0512/SLM-Mod.\n","authors":["Xianyang Zhan","Agam Goyal","Yilun Chen","Eshwar Chandrasekharan","Koustuv Saha"],"pdf_url":"https://arxiv.org/pdf/2410.13155v2.pdf","comment":"NAACL 2025 (Main): 17 pages, 8 figures, 10 tables"},{"id":"http://arxiv.org/abs/2502.06087v1","updated":"2025-02-10T01:04:36Z","published":"2025-02-10T01:04:36Z","title":"ConMeC: A Dataset for Metonymy Resolution with Common Nouns","summary":"  Metonymy plays an important role in our daily communication. People naturally\nthink about things using their most salient properties or commonly related\nconcepts. For example, by saying \"The bus decided to skip our stop today,\" we\nactually mean that the bus driver made the decision, not the bus. Prior work on\nmetonymy resolution has mainly focused on named entities. However, metonymy\ninvolving common nouns (such as desk, baby, and school) is also a frequent and\nchallenging phenomenon. We argue that NLP systems should be capable of\nidentifying the metonymic use of common nouns in context. We create a new\nmetonymy dataset ConMeC, which consists of 6,000 sentences, where each sentence\nis paired with a target common noun and annotated by humans to indicate whether\nthat common noun is used metonymically or not in that context. We also\nintroduce a chain-of-thought based prompting method for detecting metonymy\nusing large language models (LLMs). We evaluate our LLM-based pipeline, as well\nas a supervised BERT model on our dataset and three other metonymy datasets.\nOur experimental results demonstrate that LLMs could achieve performance\ncomparable to the supervised BERT model on well-defined metonymy categories,\nwhile still struggling with instances requiring nuanced semantic understanding.\nOur dataset is publicly available at: https://github.com/SaptGhosh/ConMeC.\n","authors":["Saptarshi Ghosh","Tianyu Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.06087v1.pdf","comment":"17 pages, 12 tables, 3 figures, NAACL 2025"},{"id":"http://arxiv.org/abs/2502.06086v1","updated":"2025-02-10T00:52:17Z","published":"2025-02-10T00:52:17Z","title":"Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual\n  Combination with Property Type","summary":"  Conceptual combination is a cognitive process that merges basic concepts,\nenabling the creation of complex expressions. During this process, the\nproperties of combination (e.g., the whiteness of a peeled apple) can be\ninherited from basic concepts, newly emerge, or be canceled. However, previous\nstudies have evaluated a limited set of properties and have not examined the\ngenerative process. To address this gap, we introduce the Conceptual\nCombination with Property Type dataset (CCPT), which consists of 12.3K\nannotated triplets of noun phrases, properties, and property types. Using CCPT,\nwe establish three types of tasks to evaluate LLMs for conceptual combination\nthoroughly. Our key findings are threefold: (1) Our automatic metric grading\nproperty emergence and cancellation closely corresponds with human judgments.\n(2) LLMs, including OpenAI's o1, struggle to generate noun phrases which\npossess given emergent properties. (3) Our proposed method, inspired by\ncognitive psychology model that explains how relationships between concepts are\nformed, improves performances in all generative tasks. The dataset and\nexperimental code are available at https://github.com/seokwon99/CCPT.git.\n","authors":["Seokwon Song","Taehyun Lee","Jaewoo Ahn","Jae Hyuk Sung","Gunhee Kim"],"pdf_url":"https://arxiv.org/pdf/2502.06086v1.pdf","comment":"NAACL 2025; the dataset and experimental code are available at\n  https://github.com/seokwon99/CCPT.git"},{"id":"http://arxiv.org/abs/2406.11107v2","updated":"2025-02-10T00:10:17Z","published":"2024-06-17T00:17:11Z","title":"Exploring Safety-Utility Trade-Offs in Personalized Language Models","summary":"  As large language models (LLMs) become increasingly integrated into daily\napplications, it is essential to ensure they operate fairly across diverse user\ndemographics. In this work, we show that LLMs suffer from personalization bias,\nwhere their performance is impacted when they are personalized to a user's\nidentity. We quantify personalization bias by evaluating the performance of\nLLMs along two axes - safety and utility. We measure safety by examining how\nbenign LLM responses are to unsafe prompts with and without personalization. We\nmeasure utility by evaluating the LLM's performance on various tasks, including\ngeneral knowledge, mathematical abilities, programming, and reasoning skills.\nWe find that various LLMs, ranging from open-source models like Llama (Touvron\net al., 2023) and Mistral (Jiang et al., 2023) to API-based ones like GPT-3.5\nand GPT-4o (Ouyang et al., 2022), exhibit significant variance in performance\nin terms of safety-utility trade-offs depending on the user's identity.\nFinally, we discuss several strategies to mitigate personalization bias using\npreference tuning and prompt-based defenses.\n","authors":["Anvesh Rao Vijjini","Somnath Basu Roy Chowdhury","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2406.11107v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07131v1","updated":"2025-02-10T23:49:39Z","published":"2025-02-10T23:49:39Z","title":"TWICE: What Advantages Can Low-Resource Domain-Specific Embedding Model\n  Bring? - A Case Study on Korea Financial Texts","summary":"  Domain specificity of embedding models is critical for effective performance.\nHowever, existing benchmarks, such as FinMTEB, are primarily designed for\nhigh-resource languages, leaving low-resource settings, such as Korean,\nunder-explored. Directly translating established English benchmarks often fails\nto capture the linguistic and cultural nuances present in low-resource domains.\nIn this paper, titled TWICE: What Advantages Can Low-Resource Domain-Specific\nEmbedding Models Bring? A Case Study on Korea Financial Texts, we introduce\nKorFinMTEB, a novel benchmark for the Korean financial domain, specifically\ntailored to reflect its unique cultural characteristics in low-resource\nlanguages. Our experimental results reveal that while the models perform\nrobustly on a translated version of FinMTEB, their performance on KorFinMTEB\nuncovers subtle yet critical discrepancies, especially in tasks requiring\ndeeper semantic understanding, that underscore the limitations of direct\ntranslation. This discrepancy highlights the necessity of benchmarks that\nincorporate language-specific idiosyncrasies and cultural nuances. The insights\nfrom our study advocate for the development of domain-specific evaluation\nframeworks that can more accurately assess and drive the progress of embedding\nmodels in low-resource settings.\n","authors":["Yewon Hwang","Sungbum Jung","Hanwool Lee","Sara Yu"],"pdf_url":"https://arxiv.org/pdf/2502.07131v1.pdf","comment":"Submitted to ICLR@Financial AI"},{"id":"http://arxiv.org/abs/2502.07128v1","updated":"2025-02-10T23:47:35Z","published":"2025-02-10T23:47:35Z","title":"Cardiverse: Harnessing LLMs for Novel Card Game Prototyping","summary":"  The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game designs, an LLM-driven system for consistent\ngame code generation validated by gameplay records, and a gameplay AI\nconstructing method that uses an ensemble of LLM-generated action-value\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers.\n","authors":["Danrui Li","Sen Zhang","Sam S. Sohn","Kaidong Hu","Muhammad Usman","Mubbasir Kapadia"],"pdf_url":"https://arxiv.org/pdf/2502.07128v1.pdf","comment":"13 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.07124v1","updated":"2025-02-10T23:37:39Z","published":"2025-02-10T23:37:39Z","title":"Structural Reformation of Large Language Model Neuron Encapsulation for\n  Divergent Information Aggregation","summary":"  Structured neuron encapsulation introduces a modular framework that enables\nmore effective aggregation and specialization of information within deep\nlearning architectures. A model modified through this framework demonstrated\nimproved perplexity scores, greater lexical variability, and enhanced\nconsistency in logical reasoning, suggesting that structured parameter\ndistribution contributes to more efficient language representation. Statistical\nanalyses of generated text highlighted a wider range of sentence structures and\nreduced redundancy in token selection, indicating that encapsulation fosters\nmore adaptable language generation. A detailed evaluation of attention weight\ndistributions revealed that the experimental model exhibited greater divergence\nin cross-layer activations, supporting the hypothesis that encapsulated neurons\nassume specialized processing roles. Logical consistency assessments further\ndemonstrated that modular architectures mitigate contradictory outputs,\nreducing internal conflicts in inferred relationships between linguistic\nconstructs. Computational trade-offs were analyzed, with results showing a\nminor increase in processing overhead, though improvements in parameter\nefficiency and structured decision-making compensated for the additional\ncomplexity. The mathematical formulation of the encapsulation mechanism\nconfirmed that modular aggregation maintains stable convergence properties\nwhile promoting distinct functional roles for different neuron clusters.\n","authors":["Denis Bakushev","Gideon Boultinghouse","Harriet Oppenheimer","Sebastian Gillingwater","Valentina Ashington","Wilfred Stanborough"],"pdf_url":"https://arxiv.org/pdf/2502.07124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06529v4","updated":"2025-02-10T23:28:39Z","published":"2024-02-09T16:40:59Z","title":"Introspective Planning: Aligning Robots' Uncertainty with Inherent Task\n  Ambiguity","summary":"  Large language models (LLMs) exhibit advanced reasoning skills, enabling\nrobots to comprehend natural language instructions and strategically plan\nhigh-level actions through proper grounding. However, LLM hallucination may\nresult in robots confidently executing plans that are misaligned with user\ngoals or even unsafe in critical scenarios. Additionally, inherent ambiguity in\nnatural language instructions can introduce uncertainty into the LLM's\nreasoning and planning processes.We propose introspective planning, a\nsystematic approach that align LLM's uncertainty with the inherent ambiguity of\nthe task. Our approach constructs a knowledge base containing introspective\nreasoning examples as post-hoc rationalizations of human-selected safe and\ncompliant plans, which are retrieved during deployment. Evaluations on three\ntasks, including a newly introduced safe mobile manipulation benchmark,\ndemonstrate that introspection substantially improves both compliance and\nsafety over state-of-the-art LLM-based planning methods. Furthermore, we\nempirically show that introspective planning, in combination with conformal\nprediction, achieves tighter confidence bounds, maintaining statistical success\nguarantees while minimizing unnecessary user clarification requests. The\nwebpage and code are accessible at https://introplan.github.io.\n","authors":["Kaiqu Liang","Zixu Zhang","Jaime Fernández Fisac"],"pdf_url":"https://arxiv.org/pdf/2402.06529v4.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.07131v2","updated":"2025-02-10T23:07:49Z","published":"2024-09-11T09:27:50Z","title":"Reranking Laws for Language Generation: A Communication-Theoretic\n  Perspective","summary":"  To ensure large language models (LLMs) are used safely, one must reduce their\npropensity to hallucinate or to generate unacceptable answers. A simple and\noften used strategy is to first let the LLM generate multiple hypotheses and\nthen employ a reranker to choose the best one. In this paper, we draw a\nparallel between this strategy and the use of redundancy to decrease the error\nrate in noisy communication channels. We conceptualize the generator as a\nsender transmitting multiple descriptions of a message through parallel noisy\nchannels. The receiver decodes the message by ranking the (potentially\ncorrupted) descriptions and selecting the one found to be most reliable. We\nprovide conditions under which this protocol is asymptotically error-free\n(i.e., yields an acceptable answer almost surely) even in scenarios where the\nreranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the\nchannel distributions are statistically dependent. We use our framework to\nobtain reranking laws which we validate empirically on two real-world tasks\nusing LLMs: text-to-code generation with DeepSeek-Coder 7B and machine\ntranslation of medical data with TowerInstruct 13B.\n","authors":["António Farinhas","Haau-Sing Li","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2409.07131v2.pdf","comment":"NeurIPS 2024 (spotlight)"},{"id":"http://arxiv.org/abs/2410.22944v3","updated":"2025-02-10T23:03:19Z","published":"2024-10-30T12:01:48Z","title":"Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification","summary":"  Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments.\n","authors":["Tom A. Lamb","Adam Davies","Alasdair Paren","Philip H. S. Torr","Francesco Pinto"],"pdf_url":"https://arxiv.org/pdf/2410.22944v3.pdf","comment":"32pages, 17 figures"},{"id":"http://arxiv.org/abs/2411.12882v2","updated":"2025-02-10T22:59:18Z","published":"2024-11-19T22:00:01Z","title":"ProSec: Fortifying Code LLMs with Proactive Security Alignment","summary":"  Recent advances in code-specific large language models (LLMs) have greatly\nenhanced code generation and refinement capabilities. However, the safety of\ncode LLMs remains under-explored, posing potential risks as insecure code\ngenerated by these models may introduce vulnerabilities into real-world\nsystems. Previous work proposes to collect security-focused instruction-tuning\ndataset from real-world vulnerabilities. It is constrained by the data sparsity\nof vulnerable code, and has limited applicability in the iterative\npost-training workflows of modern LLMs. In this paper, we propose ProSec, a\nnovel proactive security alignment approach designed to align code LLMs with\nsecure coding practices. ProSec systematically exposes the vulnerabilities in a\ncode LLM by synthesizing error-inducing coding scenarios from Common Weakness\nEnumerations (CWEs), and generates fixes to vulnerable code snippets, allowing\nthe model to learn secure practices through advanced preference learning\nobjectives. The scenarios synthesized by ProSec triggers 25 times more\nvulnerable code than a normal instruction-tuning dataset, resulting in a\nsecurity-focused alignment dataset 7 times larger than the previous work.\nExperiments show that models trained with ProSec are 25.2% to 91.4% more secure\ncompared to previous work without degrading models' utility.\n","authors":["Xiangzhe Xu","Zian Su","Jinyao Guo","Kaiyuan Zhang","Zhenting Wang","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.12882v2.pdf","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2412.10418v2","updated":"2025-02-10T22:51:59Z","published":"2024-12-09T22:29:57Z","title":"Constrained Decoding with Speculative Lookaheads","summary":"  Constrained decoding with lookahead heuristics (CDLH) is a highly effective\nmethod for aligning LLM generations to human preferences. However, the\nextensive lookahead roll-out operations for each generated token makes CDLH\nprohibitively expensive, resulting in low adoption in practice. In contrast,\ncommon decoding strategies such as greedy decoding are extremely efficient, but\nachieve very low constraint satisfaction. We propose constrained decoding with\nspeculative lookaheads (CDSL), a technique that significantly improves upon the\ninference efficiency of CDLH without experiencing the drastic performance\nreduction seen with greedy decoding. CDSL is motivated by the recently proposed\nidea of speculative decoding that uses a much smaller draft LLM for generation\nand a larger target LLM for verification. In CDSL, the draft model is used to\ngenerate lookaheads which is verified by a combination of target LLM and\ntask-specific reward functions. This process accelerates decoding by reducing\nthe computational burden while maintaining strong performance. We evaluate CDSL\nin two constraint decoding tasks with three LLM families and achieve 2.2x to\n12.15x speedup over CDLH without significant performance reduction.\n","authors":["Nishanth Nakshatri","Shamik Roy","Rajarshi Das","Suthee Chaidaroon","Leonid Boytsov","Rashmi Gangadharaiah"],"pdf_url":"https://arxiv.org/pdf/2412.10418v2.pdf","comment":"NAACL 2025 (main) camera-ready"},{"id":"http://arxiv.org/abs/2502.07101v1","updated":"2025-02-10T22:46:57Z","published":"2025-02-10T22:46:57Z","title":"SMAB: MAB based word Sensitivity Estimation Framework and its\n  Applications in Adversarial Text Generation","summary":"  To understand the complexity of sequence classification tasks, Hahn et al.\n(2021) proposed sensitivity as the number of disjoint subsets of the input\nsequence that can each be individually changed to change the output. Though\neffective, calculating sensitivity at scale using this framework is costly\nbecause of exponential time complexity. Therefore, we introduce a\nSensitivity-based Multi-Armed Bandit framework (SMAB), which provides a\nscalable approach for calculating word-level local (sentence-level) and global\n(aggregated) sensitivities concerning an underlying text classifier for any\ndataset. We establish the effectiveness of our approach through various\napplications. We perform a case study on CHECKLIST generated sentiment analysis\ndataset where we show that our algorithm indeed captures intuitively high and\nlow-sensitive words. Through experiments on multiple tasks and languages, we\nshow that sensitivity can serve as a proxy for accuracy in the absence of gold\ndata. Lastly, we show that guiding perturbation prompts using sensitivity\nvalues in adversarial example generation improves attack success rate by\n15.58%, whereas using sensitivity as an additional reward in adversarial\nparaphrase generation gives a 12.00% improvement over SOTA approaches. Warning:\nContains potentially offensive content.\n","authors":["Saurabh Kumar Pandey","Sachin Vashistha","Debrup Das","Somak Aditya","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15773v2","updated":"2025-02-10T22:42:15Z","published":"2025-01-27T04:43:18Z","title":"Is It Navajo? Accurate Language Detection in Endangered Athabaskan\n  Languages","summary":"  Endangered languages, such as Navajo - the most widely spoken Native American\nlanguage - are significantly underrepresented in contemporary language\ntechnologies, exacerbating the challenges of their preservation and\nrevitalization. This study evaluates Google's Language Identification (LangID)\ntool, which does not currently support any Native American languages. To\naddress this, we introduce a random forest classifier trained on Navajo and\ntwenty erroneously suggested languages by LangID. Despite its simplicity, the\nclassifier achieves near-perfect accuracy (97-100%). Additionally, the model\ndemonstrates robustness across other Athabaskan languages - a family of Native\nAmerican languages spoken primarily in Alaska, the Pacific Northwest, and parts\nof the Southwestern United States - suggesting its potential for broader\napplication. Our findings underscore the pressing need for NLP systems that\nprioritize linguistic diversity and adaptability over centralized,\none-size-fits-all solutions, especially in supporting underrepresented\nlanguages in a multicultural world. This work directly contributes to ongoing\nefforts to address cultural biases in language models and advocates for the\ndevelopment of culturally localized NLP tools that serve diverse linguistic\ncommunities.\n","authors":["Ivory Yang","Weicheng Ma","Chunhui Zhang","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2501.15773v2.pdf","comment":"Accepted to NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2502.07082v1","updated":"2025-02-10T22:21:29Z","published":"2025-02-10T22:21:29Z","title":"\"Once Upon a Time...\" Literary Narrative Connectedness Progresses with\n  Grade Level: Potential Impact on Reading Fluency and Literacy Skills","summary":"  Selecting an appropriate book is crucial for fostering reading habits in\nchildren. While children exhibit varying levels of complexity when generating\noral narratives, the question arises: do children's books also differ in\nnarrative complexity? This study explores the narrative dynamics of literary\ntexts used in schools, focusing on how their complexity evolves across\ndifferent grade levels. Using Word-Recurrence Graph Analysis, we examined a\ndataset of 1,627 literary texts spanning 13 years of education. The findings\nreveal significant exponential growth in connectedness, particularly during the\nfirst three years of schooling, mirroring patterns observed in children's oral\nnarratives. These results highlight the potential of literary texts as a tool\nto support the development of literacy skills.\n","authors":["Marina Ribeiro","Bárbara Malcorra","Diego Pintor","Natália Bezerra Mota"],"pdf_url":"https://arxiv.org/pdf/2502.07082v1.pdf","comment":"14 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.08193v3","updated":"2025-02-10T22:20:07Z","published":"2024-10-10T17:58:24Z","title":"GenARM: Reward Guided Generation with Autoregressive Reward Model for\n  Test-time Alignment","summary":"  Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining. Our project page is\navailable at: https://genarm.github.io.\n","authors":["Yuancheng Xu","Udari Madhushani Sehwag","Alec Koppel","Sicheng Zhu","Bang An","Furong Huang","Sumitra Ganesh"],"pdf_url":"https://arxiv.org/pdf/2410.08193v3.pdf","comment":"Published at the Thirteenth International Conference on Learning\n  Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2502.07077v1","updated":"2025-02-10T22:09:57Z","published":"2025-02-10T22:09:57Z","title":"Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language\n  Models","summary":"  The tendency of users to anthropomorphise large language models (LLMs) is of\ngrowing interest to AI developers, researchers, and policy-makers. Here, we\npresent a novel method for empirically evaluating anthropomorphic LLM\nbehaviours in realistic and varied settings. Going beyond single-turn static\nbenchmarks, we contribute three methodological advances in state-of-the-art\n(SOTA) LLM evaluation. First, we develop a multi-turn evaluation of 14\nanthropomorphic behaviours. Second, we present a scalable, automated approach\nby employing simulations of user interactions. Third, we conduct an\ninteractive, large-scale human subject study (N=1101) to validate that the\nmodel behaviours we measure predict real users' anthropomorphic perceptions. We\nfind that all SOTA LLMs evaluated exhibit similar behaviours, characterised by\nrelationship-building (e.g., empathy and validation) and first-person pronoun\nuse, and that the majority of behaviours only first occur after multiple turns.\nOur work lays an empirical foundation for investigating how design choices\ninfluence anthropomorphic model behaviours and for progressing the ethical\ndebate on the desirability of these behaviours. It also showcases the necessity\nof multi-turn evaluations for complex social phenomena in human-AI interaction.\n","authors":["Lujain Ibrahim","Canfer Akbulut","Rasmi Elasmar","Charvi Rastogi","Minsuk Kahng","Meredith Ringel Morris","Kevin R. McKee","Verena Rieser","Murray Shanahan","Laura Weidinger"],"pdf_url":"https://arxiv.org/pdf/2502.07077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07072v1","updated":"2025-02-10T22:07:02Z","published":"2025-02-10T22:07:02Z","title":"IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large\n  Language Models","summary":"  Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair.\n","authors":["Sayem Mohammad Imtiaz","Astha Singh","Fraol Batole","Hridesh Rajan"],"pdf_url":"https://arxiv.org/pdf/2502.07072v1.pdf","comment":"Accepted as full research paper at FSE'2025"},{"id":"http://arxiv.org/abs/2502.07068v1","updated":"2025-02-10T21:59:27Z","published":"2025-02-10T21:59:27Z","title":"Specializing Large Language Models to Simulate Survey Response\n  Distributions for Global Populations","summary":"  Large-scale surveys are essential tools for informing social science research\nand policy, but running surveys is costly and time-intensive. If we could\naccurately simulate group-level survey results, this would therefore be very\nvaluable to social science research. Prior work has explored the use of large\nlanguage models (LLMs) for simulating human behaviors, mostly through\nprompting. In this paper, we are the first to specialize LLMs for the task of\nsimulating survey response distributions. As a testbed, we use country-level\nresults from two global cultural surveys. We devise a fine-tuning method based\non first-token probabilities to minimize divergence between predicted and\nactual response distributions for a given question. Then, we show that this\nmethod substantially outperforms other methods and zero-shot classifiers, even\non unseen questions, countries, and a completely unseen survey. While even our\nbest models struggle with the task, especially on unseen questions, our results\ndemonstrate the benefits of specialization for simulation, which may accelerate\nprogress towards sufficiently accurate simulation in the future.\n","authors":["Yong Cao","Haijiang Liu","Arnav Arora","Isabelle Augenstein","Paul Röttger","Daniel Hershcovich"],"pdf_url":"https://arxiv.org/pdf/2502.07068v1.pdf","comment":"15 pages, 9 figures, accepted to NAACL 2025 main"},{"id":"http://arxiv.org/abs/2502.07058v1","updated":"2025-02-10T21:49:35Z","published":"2025-02-10T21:49:35Z","title":"Using Contextually Aligned Online Reviews to Measure LLMs' Performance\n  Disparities Across Language Varieties","summary":"  A language can have different varieties. These varieties can affect the\nperformance of natural language processing (NLP) models, including large\nlanguage models (LLMs), which are often trained on data from widely spoken\nvarieties. This paper introduces a novel and cost-effective approach to\nbenchmark model performance across language varieties. We argue that\ninternational online review platforms, such as Booking.com, can serve as\neffective data sources for constructing datasets that capture comments in\ndifferent language varieties from similar real-world scenarios, like reviews\nfor the same hotel with the same rating using the same language (e.g., Mandarin\nChinese) but different language varieties (e.g., Taiwan Mandarin, Mainland\nMandarin). To prove this concept, we constructed a contextually aligned dataset\ncomprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs\nin a sentiment analysis task. Our results show that LLMs consistently\nunderperform in Taiwan Mandarin.\n","authors":["Zixin Tang","Chieh-Yang Huang","Tsung-Chi Li","Ho Yim Sam Ng","Hen-Hsen Huang","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07058v1.pdf","comment":"Accepted by 2025 Annual Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics (NAACL), theme track"},{"id":"http://arxiv.org/abs/2502.07057v1","updated":"2025-02-10T21:47:49Z","published":"2025-02-10T21:47:49Z","title":"Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark","summary":"  Tokenization is a fundamental preprocessing step in NLP, directly impacting\nlarge language models' (LLMs) ability to capture syntactic, morphosyntactic,\nand semantic structures. This paper introduces a novel framework for\nsystematically evaluating tokenization strategies, addressing challenges in\nmorphologically rich and low-resource languages. Using a Turkish dataset of\n6,200 multiple-choice questions from the Massive Multitask Language\nUnderstanding (MMLU) benchmark, the framework assesses tokenizers across five\nkey metrics: vocabulary size, token count, processing time, language-specific\ntoken percentages (\\%TR), and token purity. These metrics provide a structured\napproach to evaluating how well tokenizers preserve linguistic structures.\nWhile \\%TR measures the proportion of valid words in the target language,\n\\%Pure assesses the alignment of tokens with meaningful linguistic units, such\nas roots and valid morphemes, minimizing semantic fragmentation. The findings\nreveal that \\%TR, introduced as a critical metric, exhibits a stronger\ncorrelation with downstream performance (e.g., MMLU scores) than token purity,\nemphasizing its role in improving model accuracy. Additionally, larger model\nparameters do not necessarily yield better tokenization quality or enhanced\nresults, highlighting the importance of tailored tokenization strategies that\nprioritize linguistic alignment. This framework sets a new standard for\ndeveloping robust tokenization methods optimized for morphologically complex\nand low-resource languages. Future work will refine morphological analysis,\nexplore domain-specific customizations, and conduct cross-linguistic\nevaluations to further enhance tokenization practices.\n","authors":["M. Ali Bayram","Ali Arda Fincan","Ahmet Semih Gümüş","Sercan Karakaş","Banu Diri","Savaş Yıldırım"],"pdf_url":"https://arxiv.org/pdf/2502.07057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07045v1","updated":"2025-02-10T21:27:06Z","published":"2025-02-10T21:27:06Z","title":"Scalable and Ethical Insider Threat Detection through Data Synthesis and\n  Analysis by LLMs","summary":"  Insider threats wield an outsized influence on organizations,\ndisproportionate to their small numbers. This is due to the internal access\ninsiders have to systems, information, and infrastructure. %One example of this\ninfluence is where anonymous respondents submit web-based job search site\nreviews, an insider threat risk to organizations. Signals for such risks may be\nfound in anonymous submissions to public web-based job search site reviews.\nThis research studies the potential for large language models (LLMs) to analyze\nand detect insider threat sentiment within job site reviews. Addressing ethical\ndata collection concerns, this research utilizes synthetic data generation\nusing LLMs alongside existing job review datasets. A comparative analysis of\nsentiment scores generated by LLMs is benchmarked against expert human scoring.\nFindings reveal that LLMs demonstrate alignment with human evaluations in most\ncases, thus effectively identifying nuanced indicators of threat sentiment. The\nperformance is lower on human-generated data than synthetic data, suggesting\nareas for improvement in evaluating real-world data. Text diversity analysis\nfound differences between human-generated and LLM-generated datasets, with\nsynthetic data exhibiting somewhat lower diversity. Overall, the results\ndemonstrate the applicability of LLMs to insider threat detection, and a\nscalable solution for insider sentiment testing by overcoming ethical and\nlogistical barriers tied to data acquisition.\n","authors":["Haywood Gelman","John D. Hastings"],"pdf_url":"https://arxiv.org/pdf/2502.07045v1.pdf","comment":"6 pages, 0 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.20245v2","updated":"2025-02-10T21:17:54Z","published":"2024-10-26T18:21:44Z","title":"Improving Model Evaluation using SMART Filtering of Benchmark Datasets","summary":"  One of the most challenging problems facing NLP today is evaluation. Some of\nthe most pressing issues pertain to benchmark saturation, data contamination,\nand diversity in the quality of test examples. To address these concerns, we\npropose Selection Methodology for Accurate, Reduced, and Targeted (SMART)\nfiltering, a novel approach to select a high-quality subset of examples from\nexisting benchmark datasets by systematically removing less informative and\nless challenging examples. Our approach applies three filtering criteria,\nremoving (i) easy examples, (ii) data-contaminated examples, and (iii) examples\nthat are similar to each other based on distance in an embedding space. We\ndemonstrate the effectiveness of SMART on three multiple choice QA datasets,\nwhere our methodology increases efficiency by reducing dataset size by 48\\% on\naverage, while increasing Pearson correlation with rankings from ChatBot Arena,\na more open-ended human evaluation setting. Our method enables us to be more\nefficient, whether using SMART to make new benchmarks more challenging or to\nrevitalize older datasets, while still preserving the relative model rankings.\n","authors":["Vipul Gupta","Candace Ross","David Pantoja","Rebecca J. Passonneau","Megan Ung","Adina Williams"],"pdf_url":"https://arxiv.org/pdf/2410.20245v2.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.08617v2","updated":"2025-02-10T21:17:01Z","published":"2025-01-15T06:33:15Z","title":"RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation","summary":"  While Reinforcement Learning from Human Feedback (RLHF) has shown promise in\naligning generative AI, we present empirical evidence that it can also cause\nsevere, systematic misalignment. We hypothesize that this stems from evaluator\nfeedback depending on downstream outcome predictions (foresight) that can be\ninfluenced by the AI's output, inducing Goodhart's law dynamics. Conversely,\nour theoretical analysis shows that conditioning evaluator feedback on\ndownstream observations (hindsight) inhibits this effect by decoupling the\nalignment signal from potentially compromised predictions-crucially, the result\nholds even if the observed outcomes are sampled from the AI's own world model.\nBuilding on this insight, we introduce Reinforcement Learning from Hindsight\nSimulation (RLHS), which presents plausible simulated outcomes to evaluators\nbefore eliciting feedback. We demonstrate RLHS on online (PPO) and offline\n(DPO) large language model fine-tuning, obtaining superior alignment over RLHF\nin controlled consultancy-type experiments and user studies. We evaluate\npost-hoc on the TruthfulQA benchmark and find that, even after single-task\nfine-tuning, both RLHF misalignment and RLHS alignment carry over to\nsubstantially different settings.\n","authors":["Kaiqu Liang","Haimin Hu","Ryan Liu","Thomas L. Griffiths","Jaime Fernández Fisac"],"pdf_url":"https://arxiv.org/pdf/2501.08617v2.pdf","comment":"24 pages, 18 figures"},{"id":"http://arxiv.org/abs/2408.05636v4","updated":"2025-02-10T20:51:18Z","published":"2024-08-10T21:24:25Z","title":"Speculative Diffusion Decoding: Accelerating Language Generation through\n  Diffusion","summary":"  Speculative decoding has emerged as a widely adopted method to accelerate\nlarge language model inference without sacrificing the quality of the model\noutputs. While this technique has facilitated notable speed improvements by\nenabling parallel sequence verification, its efficiency remains inherently\nlimited by the reliance on incremental token generation in existing draft\nmodels. To overcome this limitation, this paper proposes an adaptation of\nspeculative decoding which uses discrete diffusion models to generate draft\nsequences. This allows parallelization of both the drafting and verification\nsteps, providing significant speedups to the inference process. Our proposed\napproach, $\\textit{Speculative Diffusion Decoding (SpecDiff)}$, is validated on\nstandard language generation benchmarks and empirically demonstrated to provide\nup to 7.2x speedups over standard generation processes and up to 1.75x speedups\nover existing speculative decoding approaches.\n","authors":["Jacob K Christopher","Brian R Bartoldson","Tal Ben-Nun","Michael Cardei","Bhavya Kailkhura","Ferdinando Fioretto"],"pdf_url":"https://arxiv.org/pdf/2408.05636v4.pdf","comment":"Published at the 2025 Annual Conference of the Nations of the\n  Americas Chapter of the Association for Computational Linguistics (NAACL\n  2025)"},{"id":"http://arxiv.org/abs/2502.07029v1","updated":"2025-02-10T20:46:42Z","published":"2025-02-10T20:46:42Z","title":"Leveraging Allophony in Self-Supervised Speech Models for Atypical\n  Pronunciation Assessment","summary":"  Allophony refers to the variation in the phonetic realization of a phoneme\nbased on its phonetic environment. Modeling allophones is crucial for atypical\npronunciation assessment, which involves distinguishing atypical from typical\npronunciations. However, recent phoneme classifier-based approaches often\nsimplify this by treating various realizations as a single phoneme, bypassing\nthe complexity of modeling allophonic variation. Motivated by the acoustic\nmodeling capabilities of frozen self-supervised speech model (S3M) features, we\npropose MixGoP, a novel approach that leverages Gaussian mixture models to\nmodel phoneme distributions with multiple subclusters. Our experiments show\nthat MixGoP achieves state-of-the-art performance across four out of five\ndatasets, including dysarthric and non-native speech. Our analysis further\nsuggests that S3M features capture allophonic variation more effectively than\nMFCCs and Mel spectrograms, highlighting the benefits of integrating MixGoP\nwith S3M features.\n","authors":["Kwanghee Choi","Eunjung Yeo","Kalvin Chang","Shinji Watanabe","David Mortensen"],"pdf_url":"https://arxiv.org/pdf/2502.07029v1.pdf","comment":"Accepted to NAACL 2025. Codebase available at\n  https://github.com/juice500ml/acoustic-units-for-ood"},{"id":"http://arxiv.org/abs/2502.07022v1","updated":"2025-02-10T20:30:32Z","published":"2025-02-10T20:30:32Z","title":"AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in\n  Corporate Statements","summary":"  Despite over a decade of legislative efforts to address modern slavery in the\nsupply chains of large corporations, the effectiveness of government oversight\nremains hampered by the challenge of scrutinizing thousands of statements\nannually. While Large Language Models (LLMs) can be considered a well\nestablished solution for the automatic analysis and summarization of documents,\nrecognizing concrete modern slavery countermeasures taken by companies and\ndifferentiating those from vague claims remains a challenging task. To help\nevaluate and fine-tune LLMs for the assessment of corporate statements, we\nintroduce a dataset composed of 5,731 modern slavery statements taken from the\nAustralian Modern Slavery Register and annotated at the sentence level. This\npaper details the construction steps for the dataset that include the careful\ndesign of annotation specifications, the selection and preprocessing of\nstatements, and the creation of high-quality annotation subsets for effective\nmodel evaluations. To demonstrate our dataset's utility, we propose a machine\nlearning methodology for the detection of sentences relevant to mandatory\nreporting requirements set by the Australian Modern Slavery Act. We then follow\nthis methodology to benchmark modern language models under zero-shot and\nsupervised learning settings.\n","authors":["Adriana Eufrosiana Bora","Pierre-Luc St-Charles","Mirko Bronzi","Arsène Fansi Tchango","Bruno Rousseau","Kerrie Mengersen"],"pdf_url":"https://arxiv.org/pdf/2502.07022v1.pdf","comment":"Camera ready. ICLR 2025"},{"id":"http://arxiv.org/abs/2411.14473v3","updated":"2025-02-10T20:30:03Z","published":"2024-11-18T21:28:00Z","title":"Large Language Model for Qualitative Research -- A Systematic Mapping\n  Study","summary":"  The exponential growth of text-based data in domains such as healthcare,\neducation, and social sciences has outpaced the capacity of traditional\nqualitative analysis methods, which are time-intensive and prone to\nsubjectivity. Large Language Models (LLMs), powered by advanced generative AI,\nhave emerged as transformative tools capable of automating and enhancing\nqualitative analysis. This study systematically maps the literature on the use\nof LLMs for qualitative research, exploring their application contexts,\nconfigurations, methodologies, and evaluation metrics. Findings reveal that\nLLMs are utilized across diverse fields, demonstrating the potential to\nautomate processes traditionally requiring extensive human input. However,\nchallenges such as reliance on prompt engineering, occasional inaccuracies, and\ncontextual limitations remain significant barriers. This research highlights\nopportunities for integrating LLMs with human expertise, improving model\nrobustness, and refining evaluation methodologies. By synthesizing trends and\nidentifying research gaps, this study aims to guide future innovations in the\napplication of LLMs for qualitative analysis.\n","authors":["Cauã Ferreira Barros","Bruna Borges Azevedo","Valdemar Vicente Graciano Neto","Mohamad Kassab","Marcos Kalinowski","Hugo Alexandre D. do Nascimento","Michelle C. G. S. P. Bandeira"],"pdf_url":"https://arxiv.org/pdf/2411.14473v3.pdf","comment":"8 pages, includes 1 figures and 3 tables. Submitted and Accepted to\n  the WSESE 2025 ICSE Workshop"},{"id":"http://arxiv.org/abs/2502.07017v1","updated":"2025-02-10T20:22:32Z","published":"2025-02-10T20:22:32Z","title":"Finding Words Associated with DIF: Predicting Differential Item\n  Functioning using LLMs and Explainable AI","summary":"  We fine-tuned and compared several encoder-based Transformer large language\nmodels (LLM) to predict differential item functioning (DIF) from the item text.\nWe then applied explainable artificial intelligence (XAI) methods to these\nmodels to identify specific words associated with DIF. The data included 42,180\nitems designed for English language arts and mathematics summative state\nassessments among students in grades 3 to 11. Prediction $R^2$ ranged from .04\nto .32 among eight focal and reference group pairs. Our findings suggest that\nmany words associated with DIF reflect minor sub-domains included in the test\nblueprint by design, rather than construct-irrelevant item content that should\nbe removed from assessments. This may explain why qualitative reviews of DIF\nitems often yield confusing or inconclusive results. Our approach can be used\nto screen words associated with DIF during the item-writing process for\nimmediate revision, or help review traditional DIF analysis results by\nhighlighting key words in the text. Extensions of this research can enhance the\nfairness of assessment programs, especially those that lack resources to build\nhigh-quality items, and among smaller subpopulations where we do not have\nsufficient sample sizes for traditional DIF analyses.\n","authors":["Hotaka Maeda","Yikai Lu"],"pdf_url":"https://arxiv.org/pdf/2502.07017v1.pdf","comment":"14 pages, 2 figures, 6 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.06788v1","updated":"2025-02-10T18:59:58Z","published":"2025-02-10T18:59:58Z","title":"EVEv2: Improved Baselines for Encoder-Free Vision-Language Models","summary":"  Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE.\n","authors":["Haiwen Diao","Xiaotong Li","Yufeng Cui","Yueze Wang","Haoge Deng","Ting Pan","Wenxuan Wang","Huchuan Lu","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06788v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.06787v1","updated":"2025-02-10T18:59:35Z","published":"2025-02-10T18:59:35Z","title":"Visual Agentic AI for Spatial Reasoning with a Dynamic API","summary":"  Visual reasoning -- the ability to interpret the visual world -- is crucial\nfor embodied agents that operate within three-dimensional scenes. Progress in\nAI has led to vision and language models capable of answering questions from\nimages. However, their performance declines when tasked with 3D spatial\nreasoning. To tackle the complexity of such reasoning problems, we introduce an\nagentic program synthesis approach where LLM agents collaboratively generate a\nPythonic API with new functions to solve common subproblems. Our method\novercomes limitations of prior approaches that rely on a static, human-defined\nAPI, allowing it to handle a wider range of queries. To assess AI capabilities\nfor 3D understanding, we introduce a new benchmark of queries involving\nmultiple steps of grounding and inference. We show that our method outperforms\nprior zero-shot models for visual reasoning in 3D and empirically validate the\neffectiveness of our agentic framework for 3D spatial reasoning tasks. Project\nwebsite: https://glab-caltech.github.io/vadar/\n","authors":["Damiano Marsili","Rohun Agrawal","Yisong Yue","Georgia Gkioxari"],"pdf_url":"https://arxiv.org/pdf/2502.06787v1.pdf","comment":"Project website: https://glab-caltech.github.io/vadar/"},{"id":"http://arxiv.org/abs/2502.06782v1","updated":"2025-02-10T18:58:11Z","published":"2025-02-10T18:58:11Z","title":"Lumina-Video: Efficient and Flexible Video Generation with Multi-scale\n  Next-DiT","summary":"  Recent advancements have established Diffusion Transformers (DiTs) as a\ndominant framework in generative modeling. Building on this success,\nLumina-Next achieves exceptional performance in the generation of\nphotorealistic images with Next-DiT. However, its potential for video\ngeneration remains largely untapped, with significant challenges in modeling\nthe spatiotemporal complexity inherent to video data. To address this, we\nintroduce Lumina-Video, a framework that leverages the strengths of Next-DiT\nwhile introducing tailored solutions for video synthesis. Lumina-Video\nincorporates a Multi-scale Next-DiT architecture, which jointly learns multiple\npatchifications to enhance both efficiency and flexibility. By incorporating\nthe motion score as an explicit condition, Lumina-Video also enables direct\ncontrol of generated videos' dynamic degree. Combined with a progressive\ntraining scheme with increasingly higher resolution and FPS, and a multi-source\ntraining scheme with mixed natural and synthetic data, Lumina-Video achieves\nremarkable aesthetic quality and motion smoothness at high training and\ninference efficiency. We additionally propose Lumina-V2A, a video-to-audio\nmodel based on Next-DiT, to create synchronized sounds for generated videos.\nCodes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.\n","authors":["Dongyang Liu","Shicheng Li","Yutong Liu","Zhen Li","Kai Wang","Xinyue Li","Qi Qin","Yufei Liu","Yi Xin","Zhongyu Li","Bin Fu","Chenyang Si","Yuewen Cao","Conghui He","Ziwei Liu","Yu Qiao","Qibin Hou","Hongsheng Li","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2502.06782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06779v1","updated":"2025-02-10T18:56:14Z","published":"2025-02-10T18:56:14Z","title":"KARST: Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission\n  for Visual Classification","summary":"  Fine-tuning pre-trained vision models for specific tasks is a common practice\nin computer vision. However, this process becomes more expensive as models grow\nlarger. Recently, parameter-efficient fine-tuning (PEFT) methods have emerged\nas a popular solution to improve training efficiency and reduce storage needs\nby tuning additional low-rank modules within pre-trained backbones. Despite\ntheir advantages, they struggle with limited representation capabilities and\nmisalignment with pre-trained intermediate features. To address these issues,\nwe introduce an innovative Multi-Kernel Kronecker Adaptation with Re-Scaling\nTransmission (KARST) for various recognition tasks. Specifically, its\nmulti-kernel design extends Kronecker projections horizontally and separates\nadaptation matrices into multiple complementary spaces, reducing parameter\ndependency and creating more compact subspaces. Besides, it incorporates extra\nlearnable re-scaling factors to better align with pre-trained feature\ndistributions, allowing for more flexible and balanced feature aggregation.\nExtensive experiments validate that our KARST outperforms other PEFT\ncounterparts with a negligible inference cost due to its re-parameterization\ncharacteristics. Code is publicly available at:\nhttps://github.com/Lucenova/KARST.\n","authors":["Yue Zhu","Haiwen Diao","Shang Gao","Long Chen","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2502.06779v1.pdf","comment":"5 pages, 3 figures, Accepted by ICASSP2025"},{"id":"http://arxiv.org/abs/2412.12771v2","updated":"2025-02-10T18:55:08Z","published":"2024-12-17T10:33:34Z","title":"Guided and Variance-Corrected Fusion with One-shot Style Alignment for\n  Large-Content Image Generation","summary":"  Producing large images using small diffusion models is gaining increasing\npopularity, as the cost of training large models could be prohibitive. A common\napproach involves jointly generating a series of overlapped image patches and\nobtaining large images by merging adjacent patches. However, results from\nexisting methods often exhibit noticeable artifacts, e.g., seams and\ninconsistent objects and styles. To address the issues, we proposed Guided\nFusion (GF), which mitigates the negative impact from distant image regions by\napplying a weighted average to the overlapping regions. Moreover, we proposed\nVariance-Corrected Fusion (VCF), which corrects data variance at\npost-averaging, generating more accurate fusion for the Denoising Diffusion\nProbabilistic Model. Furthermore, we proposed a one-shot Style Alignment (SA),\nwhich generates a coherent style for large images by adjusting the initial\ninput noise without adding extra computational burden. Extensive experiments\ndemonstrated that the proposed fusion methods improved the quality of the\ngenerated image significantly. The proposed method can be widely applied as a\nplug-and-play module to enhance other fusion-based methods for large image\ngeneration. Code: https://github.com/TitorX/GVCFDiffusion\n","authors":["Shoukun Sun","Min Xian","Tiankai Yao","Fei Xu","Luca Capriotti"],"pdf_url":"https://arxiv.org/pdf/2412.12771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09194v2","updated":"2025-02-10T18:54:23Z","published":"2025-01-15T22:55:26Z","title":"Grounding Text-to-Image Diffusion Models for Controlled High-Quality\n  Image Generation","summary":"  Text-to-image (T2I) generative diffusion models have demonstrated outstanding\nperformance in synthesizing diverse, high-quality visuals from text captions.\nSeveral layout-to-image models have been developed to control the generation\nprocess by utilizing a wide range of layouts, such as segmentation maps, edges,\nand human keypoints. In this work, we propose ObjectDiffusion, a model that\nconditions T2I diffusion models on semantic and spatial grounding information,\nenabling the precise rendering and placement of desired objects in specific\nlocations defined by bounding boxes. To achieve this, we make substantial\nmodifications to the network architecture introduced in ControlNet to integrate\nit with the grounding method proposed in GLIGEN. We fine-tune ObjectDiffusion\non the COCO2017 training dataset and evaluate it on the COCO2017 validation\ndataset. Our model improves the precision and quality of controllable image\ngeneration, achieving an AP$_{\\text{50}}$ of 46.6, an AR of 44.5, and an FID of\n19.8, outperforming the current SOTA model trained on open-source datasets\nacross all three metrics. ObjectDiffusion demonstrates a distinctive capability\nin synthesizing diverse, high-quality, high-fidelity images that seamlessly\nconform to the semantic and spatial control layout. Evaluated in qualitative\nand quantitative tests, ObjectDiffusion exhibits remarkable grounding\ncapabilities in closed-set and open-set vocabulary settings across a wide\nvariety of contexts. The qualitative assessment verifies the ability of\nObjectDiffusion to generate multiple detailed objects in varying sizes, forms,\nand locations.\n","authors":["Ahmad Süleyman","Göksel Biricik"],"pdf_url":"https://arxiv.org/pdf/2501.09194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06764v1","updated":"2025-02-10T18:44:25Z","published":"2025-02-10T18:44:25Z","title":"History-Guided Video Diffusion","summary":"  Classifier-free guidance (CFG) is a key technique for improving conditional\ngeneration in diffusion models, enabling more accurate control while enhancing\nsample quality. It is natural to extend this technique to video diffusion,\nwhich generates video conditioned on a variable number of context frames,\ncollectively referred to as history. However, we find two key challenges to\nguiding with variable-length history: architectures that only support\nfixed-size conditioning, and the empirical observation that CFG-style history\ndropout performs poorly. To address this, we propose the Diffusion Forcing\nTransformer (DFoT), a video diffusion architecture and theoretically grounded\ntraining objective that jointly enable conditioning on a flexible number of\nhistory frames. We then introduce History Guidance, a family of guidance\nmethods uniquely enabled by DFoT. We show that its simplest form, vanilla\nhistory guidance, already significantly improves video generation quality and\ntemporal consistency. A more advanced method, history guidance across time and\nfrequency further enhances motion dynamics, enables compositional\ngeneralization to out-of-distribution history, and can stably roll out\nextremely long videos. Website: https://boyuan.space/history-guidance\n","authors":["Kiwhan Song","Boyuan Chen","Max Simchowitz","Yilun Du","Russ Tedrake","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2502.06764v1.pdf","comment":"Project Website: https://boyuan.space/history-guidance"},{"id":"http://arxiv.org/abs/2501.13432v2","updated":"2025-02-10T18:37:12Z","published":"2025-01-23T07:35:47Z","title":"Emotion estimation from video footage with LSTM","summary":"  Emotion estimation in general is a field that has been studied for a long\ntime, and several approaches exist using machine learning. in this paper, we\npresent an LSTM model, that processes the blend-shapes produced by the library\nMediaPipe, for a face detected in a live stream of a camera, to estimate the\nmain emotion from the facial expressions, this model is trained on the FER2013\ndataset and delivers a result of 71% accuracy and 62% f1-score which meets the\naccuracy benchmark of the FER2013 dataset, with significantly reduced\ncomputation costs.\nhttps://github.com/Samir-atra/Emotion_estimation_from_video_footage_with_LSTM_ML_algorithm\n","authors":["Samer Attrah"],"pdf_url":"https://arxiv.org/pdf/2501.13432v2.pdf","comment":"12 pages, 5 figures, 34 references, 4 tables, 3 equations"},{"id":"http://arxiv.org/abs/2502.06756v1","updated":"2025-02-10T18:33:15Z","published":"2025-02-10T18:33:15Z","title":"SAMRefiner: Taming Segment Anything Model for Universal Mask Refinement","summary":"  In this paper, we explore a principal way to enhance the quality of widely\npre-existing coarse masks, enabling them to serve as reliable training data for\nsegmentation models to reduce the annotation cost. In contrast to prior\nrefinement techniques that are tailored to specific models or tasks in a\nclose-world manner, we propose SAMRefiner, a universal and efficient approach\nby adapting SAM to the mask refinement task. The core technique of our model is\nthe noise-tolerant prompting scheme. Specifically, we introduce a multi-prompt\nexcavation strategy to mine diverse input prompts for SAM (i.e.,\ndistance-guided points, context-aware elastic bounding boxes, and\nGaussian-style masks) from initial coarse masks. These prompts can collaborate\nwith each other to mitigate the effect of defects in coarse masks. In\nparticular, considering the difficulty of SAM to handle the multi-object case\nin semantic segmentation, we introduce a split-then-merge (STM) pipeline.\nAdditionally, we extend our method to SAMRefiner++ by introducing an additional\nIoU adaption step to further boost the performance of the generic SAMRefiner on\nthe target dataset. This step is self-boosted and requires no additional\nannotation. The proposed framework is versatile and can flexibly cooperate with\nexisting segmentation methods. We evaluate our mask framework on a wide range\nof benchmarks under different settings, demonstrating better accuracy and\nefficiency. SAMRefiner holds significant potential to expedite the evolution of\nrefinement tools. Our code is available at\nhttps://github.com/linyq2117/SAMRefiner.\n","authors":["Yuqi Lin","Hengjia Li","Wenqi Shao","Zheng Yang","Jun Zhao","Xiaofei He","Ping Luo","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06756v1.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.06755v1","updated":"2025-02-10T18:32:41Z","published":"2025-02-10T18:32:41Z","title":"Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision\n  Models","summary":"  To truly understand vision models, we must not only interpret their learned\nfeatures but also validate these interpretations through controlled\nexperiments. Current approaches either provide interpretable features without\nthe ability to test their causal influence, or enable model editing without\ninterpretable controls. We present a unified framework using sparse\nautoencoders (SAEs) that bridges this gap, allowing us to discover\nhuman-interpretable visual features and precisely manipulate them to test\nhypotheses about model behavior. By applying our method to state-of-the-art\nvision models, we reveal key differences in the semantic abstractions learned\nby models with different pre-training objectives. We then demonstrate the\npractical usage of our framework through controlled interventions across\nmultiple vision tasks. We show that SAEs can reliably identify and manipulate\ninterpretable visual features without model re-training, providing a powerful\ntool for understanding and controlling vision model behavior. We provide code,\ndemos and models on our project website: https://osu-nlp-group.github.io/SAE-V.\n","authors":["Samuel Stevens","Wei-Lun Chao","Tanya Berger-Wolf","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2502.06755v1.pdf","comment":"Main text is 11 pages with 7 figures"},{"id":"http://arxiv.org/abs/2502.06750v1","updated":"2025-02-10T18:23:55Z","published":"2025-02-10T18:23:55Z","title":"Accelerating Data Processing and Benchmarking of AI Models for Pathology","summary":"  Advances in foundation modeling have reshaped computational pathology.\nHowever, the increasing number of available models and lack of standardized\nbenchmarks make it increasingly complex to assess their strengths, limitations,\nand potential for further development. To address these challenges, we\nintroduce a new suite of software tools for whole-slide image processing,\nfoundation model benchmarking, and curated publicly available tasks. We\nanticipate that these resources will promote transparency, reproducibility, and\ncontinued progress in the field.\n","authors":["Andrew Zhang","Guillaume Jaume","Anurag Vaidya","Tong Ding","Faisal Mahmood"],"pdf_url":"https://arxiv.org/pdf/2502.06750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06747v1","updated":"2025-02-10T18:16:30Z","published":"2025-02-10T18:16:30Z","title":"Wandering around: A bioinspired approach to visual attention through\n  object motion sensitivity","summary":"  Active vision enables dynamic visual perception, offering an alternative to\nstatic feedforward architectures in computer vision, which rely on large\ndatasets and high computational resources. Biological selective attention\nmechanisms allow agents to focus on salient Regions of Interest (ROIs),\nreducing computational demand while maintaining real-time responsiveness.\nEvent-based cameras, inspired by the mammalian retina, enhance this capability\nby capturing asynchronous scene changes enabling efficient low-latency\nprocessing. To distinguish moving objects while the event-based camera is in\nmotion the agent requires an object motion segmentation mechanism to accurately\ndetect targets and center them in the visual field (fovea). Integrating\nevent-based sensors with neuromorphic algorithms represents a paradigm shift,\nusing Spiking Neural Networks to parallelize computation and adapt to dynamic\nenvironments. This work presents a Spiking Convolutional Neural Network\nbioinspired attention system for selective attention through object motion\nsensitivity. The system generates events via fixational eye movements using a\nDynamic Vision Sensor integrated into the Speck neuromorphic hardware, mounted\non a Pan-Tilt unit, to identify the ROI and saccade toward it. The system,\ncharacterized using ideal gratings and benchmarked against the Event Camera\nMotion Segmentation Dataset, reaches a mean IoU of 82.2% and a mean SSIM of 96%\nin multi-object motion segmentation. The detection of salient objects reaches\n88.8% accuracy in office scenarios and 89.8% in low-light conditions on the\nEvent-Assisted Low-Light Video Object Segmentation Dataset. A real-time\ndemonstrator shows the system's 0.12 s response to dynamic scenes. Its\nlearning-free design ensures robustness across perceptual scenes, making it a\nreliable foundation for real-time robotic applications serving as a basis for\nmore complex architectures.\n","authors":["Giulia D Angelo","Victoria Clerico","Chiara Bartolozzi","Matej Hoffmann","P. Michael Furlong","Alexander Hadjiivanov"],"pdf_url":"https://arxiv.org/pdf/2502.06747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11766v2","updated":"2025-02-10T18:16:18Z","published":"2024-10-15T16:39:50Z","title":"DPD-NeuralEngine: A 22-nm 6.6-TOPS/W/mm$^2$ Recurrent Neural Network\n  Accelerator for Wideband Power Amplifier Digital Pre-Distortion","summary":"  The increasing adoption of Deep Neural Network (DNN)-based Digital\nPre-distortion (DPD) in modern communication systems necessitates efficient\nhardware implementations. This paper presents DPD-NeuralEngine, an ultra-fast,\ntiny-area, and power-efficient DPD accelerator based on a Gated Recurrent Unit\n(GRU) neural network (NN). Leveraging a co-designed software and hardware\napproach, our 22 nm CMOS implementation operates at 2 GHz, capable of\nprocessing I/Q signals up to 250 MSps. Experimental results demonstrate a\nthroughput of 256.5 GOPS and power efficiency of 1.32 TOPS/W with DPD\nlinearization performance measured in Adjacent Channel Power Ratio (ACPR) of\n-45.3 dBc and Error Vector Magnitude (EVM) of -39.8 dB. To our knowledge, this\nwork represents the first AI-based DPD application-specific integrated circuit\n(ASIC) accelerator, achieving a power-area efficiency (PAE) of 6.6\nTOPS/W/mm$^2$.\n","authors":["Ang Li","Haolin Wu","Yizhuo Wu","Qinyu Chen","Leo C. N. de Vreede","Chang Gao"],"pdf_url":"https://arxiv.org/pdf/2410.11766v2.pdf","comment":"This paper has been accepted to be presented at the 2025\n  International Symposium on Circuits and Systems (ISCAS)"},{"id":"http://arxiv.org/abs/2502.06741v1","updated":"2025-02-10T18:09:45Z","published":"2025-02-10T18:09:45Z","title":"ViSIR: Vision Transformer Single Image Reconstruction Method for Earth\n  System Models","summary":"  Purpose: Earth system models (ESMs) integrate the interactions of the\natmosphere, ocean, land, ice, and biosphere to estimate the state of regional\nand global climate under a wide variety of conditions. The ESMs are highly\ncomplex, and thus, deep neural network architectures are used to model the\ncomplexity and store the down-sampled data. In this paper, we propose the\nVision Transformer Sinusoidal Representation Networks (ViSIR) to improve the\nsingle image SR (SR) reconstruction task for the ESM data.\n  Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with\nthe high-frequency detail preservation of the Sinusoidal Representation Network\n(SIREN) to address the spectral bias observed in SR tasks.\n  Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and\nSR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three\ndifferent measurements.\n  Conclusion: The proposed ViSIR is evaluated and compared with\nstate-of-the-art methods. The results show that the proposed algorithm is\noutperforming other methods in terms of Mean Square Error(MSE),\nPeak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index\nMeasure(SSIM).\n","authors":["Ehsan Zeraatkar","Salah Faroughi","Jelena Tesic"],"pdf_url":"https://arxiv.org/pdf/2502.06741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11062v2","updated":"2025-02-10T18:07:54Z","published":"2024-10-14T20:18:03Z","title":"CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel\n  Pruning","summary":"  This paper presents CleanUMamba, a time-domain neural network architecture\ndesigned for real-time causal audio denoising directly applied to raw\nwaveforms. CleanUMamba leverages a U-Net encoder-decoder structure,\nincorporating the Mamba state-space model in the bottleneck layer. By replacing\nconventional self-attention and LSTM mechanisms with Mamba, our architecture\noffers superior denoising performance while maintaining a constant memory\nfootprint, enabling streaming operation. To enhance efficiency, we applied\nstructured channel pruning, achieving an 8X reduction in model size without\ncompromising audio quality. Our model demonstrates strong results in the\nInterspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba\nachieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and\n468M MACs, matching or outperforming larger models in real-time performance.\nCode will be available at: https://github.com/lab-emi/CleanUMamba\n","authors":["Sjoerd Groot","Qinyu Chen","Jan C. van Gemert","Chang Gao"],"pdf_url":"https://arxiv.org/pdf/2410.11062v2.pdf","comment":"This paper has been accepted to be presented at the 2025\n  International Symposium on Circuits and Systems (ISCAS)"},{"id":"http://arxiv.org/abs/2502.06735v1","updated":"2025-02-10T17:58:58Z","published":"2025-02-10T17:58:58Z","title":"Enhancing Pneumonia Diagnosis and Severity Assessment through Deep\n  Learning: A Comprehensive Approach Integrating CNN Classification and\n  Infection Segmentation","summary":"  Lung disease poses a substantial global health challenge, with pneumonia\nbeing a prevalent concern. This research focuses on leveraging deep learning\ntechniques to detect and assess pneumonia, addressing two interconnected\nobjectives. Initially, Convolutional Neural Network (CNN) models are introduced\nfor pneumonia classification, emphasizing the necessity of comprehensive\ndiagnostic assessments considering COVID-19. Subsequently, the study advocates\nfor the utilization of deep learning-based segmentation to determine the\nseverity of infection. This dual-pronged approach offers valuable insights for\nmedical professionals, facilitating a more nuanced understanding and effective\ntreatment of pneumonia. Integrating deep learning aims to elevate the accuracy\nand efficiency of pneumonia detection, thereby contributing to enhanced\nhealthcare outcomes on a global scale.\n","authors":["S Kumar Reddy Mallidi"],"pdf_url":"https://arxiv.org/pdf/2502.06735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06734v1","updated":"2025-02-10T17:58:22Z","published":"2025-02-10T17:58:22Z","title":"Señorita-2M: A High-Quality Instruction-based Dataset for General\n  Video Editing by Video Specialists","summary":"  Recent advancements in video generation have spurred the development of video\nediting techniques, which can be divided into inversion-based and end-to-end\nmethods. However, current video editing methods still suffer from several\nchallenges. Inversion-based methods, though training-free and flexible, are\ntime-consuming during inference, struggle with fine-grained editing\ninstructions, and produce artifacts and jitter. On the other hand, end-to-end\nmethods, which rely on edited video pairs for training, offer faster inference\nspeeds but often produce poor editing results due to a lack of high-quality\ntraining video pairs. In this paper, to close the gap in end-to-end methods, we\nintroduce Se\\~norita-2M, a high-quality video editing dataset. Se\\~norita-2M\nconsists of approximately 2 millions of video editing pairs. It is built by\ncrafting four high-quality, specialized video editing models, each crafted and\ntrained by our team to achieve state-of-the-art editing results. We also\npropose a filtering pipeline to eliminate poorly edited video pairs.\nFurthermore, we explore common video editing architectures to identify the most\neffective structure based on current pre-trained generative model. Extensive\nexperiments show that our dataset can help to yield remarkably high-quality\nvideo editing results. More details are available at\nhttps://senorita.github.io.\n","authors":["Bojia Zi","Penghui Ruan","Marco Chen","Xianbiao Qi","Shaozhe Hao","Shihao Zhao","Youze Huang","Bin Liang","Rong Xiao","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2502.06734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14318v2","updated":"2025-02-10T17:57:42Z","published":"2024-03-21T11:40:51Z","title":"A Lightweight Attention-based Deep Network via Multi-Scale Feature\n  Fusion for Multi-View Facial Expression Recognition","summary":"  Convolutional neural networks (CNNs) and their variations have shown\neffectiveness in facial expression recognition (FER). However, they face\nchallenges when dealing with high computational complexity and multi-view head\nposes in real-world scenarios. We introduce a lightweight attentional network\nincorporating multi-scale feature fusion (LANMSFF) to tackle these issues. For\nthe first challenge, we carefully design a lightweight network. We address the\nsecond challenge by presenting two novel components, namely mass attention\n(MassAtt) and point wise feature selection (PWFS) blocks. The MassAtt block\nsimultaneously generates channel and spatial attention maps to recalibrate\nfeature maps by emphasizing important features while suppressing irrelevant\nones. In addition, the PWFS block employs a feature selection mechanism that\ndiscards less meaningful features prior to the fusion process. This mechanism\ndistinguishes it from previous methods that directly fuse multi-scale features.\nOur proposed approach achieved results comparable to state-of-the-art methods\nin terms of parameter count and robustness to pose variation, with accuracy\nrates of 90.77% on KDEF, 70.44% on FER-2013, and 86.96% on FERPlus datasets.\nThe code for LANMSFF is available at https://github.com/AE-1129/LANMSFF.\n","authors":["Ali Ezati","Mohammadreza Dezyani","Rajib Rana","Roozbeh Rajabi","Ahmad Ayatollahi"],"pdf_url":"https://arxiv.org/pdf/2403.14318v2.pdf","comment":"10 pages, two-column, submitted to journal"},{"id":"http://arxiv.org/abs/2502.06710v1","updated":"2025-02-10T17:41:57Z","published":"2025-02-10T17:41:57Z","title":"Learning Musical Representations for Music Performance Question\n  Answering","summary":"  Music performances are representative scenarios for audio-visual modeling.\nUnlike common scenarios with sparse audio, music performances continuously\ninvolve dense audio signals throughout. While existing multimodal learning\nmethods on the audio-video QA demonstrate impressive capabilities in general\nscenarios, they are incapable of dealing with fundamental problems within the\nmusic performances: they underexplore the interaction between the multimodal\nsignals in performance and fail to consider the distinctive characteristics of\ninstruments and music. Therefore, existing methods tend to answer questions\nregarding musical performances inaccurately. To bridge the above research gaps,\n(i) given the intricate multimodal interconnectivity inherent to music data,\nour primary backbone is designed to incorporate multimodal interactions within\nthe context of music; (ii) to enable the model to learn music characteristics,\nwe annotate and release rhythmic and music sources in the current music\ndatasets; (iii) for time-aware audio-visual modeling, we align the model's\nmusic predictions with the temporal dimension. Our experiments show\nstate-of-the-art effects on the Music AVQA datasets. Our code is available at\nhttps://github.com/xid32/Amuse.\n","authors":["Xingjian Diao","Chunhui Zhang","Tingxuan Wu","Ming Cheng","Zhongyu Ouyang","Weiyi Wu","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2502.06710v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2502.06708v1","updated":"2025-02-10T17:37:34Z","published":"2025-02-10T17:37:34Z","title":"TEMSET-24K: Densely Annotated Dataset for Indexing Multipart Endoscopic\n  Videos using Surgical Timeline Segmentation","summary":"  Indexing endoscopic surgical videos is vital in surgical data science,\nforming the basis for systematic retrospective analysis and clinical\nperformance evaluation. Despite its significance, current video analytics rely\non manual indexing, a time-consuming process. Advances in computer vision,\nparticularly deep learning, offer automation potential, yet progress is limited\nby the lack of publicly available, densely annotated surgical datasets. To\naddress this, we present TEMSET-24K, an open-source dataset comprising 24,306\ntrans-anal endoscopic microsurgery (TEMS) video micro-clips. Each clip is\nmeticulously annotated by clinical experts using a novel hierarchical labeling\ntaxonomy encompassing phase, task, and action triplets, capturing intricate\nsurgical workflows. To validate this dataset, we benchmarked deep learning\nmodels, including transformer-based architectures. Our in silico evaluation\ndemonstrates high accuracy (up to 0.99) and F1 scores (up to 0.99) for key\nphases like Setup and Suturing. The STALNet model, tested with ConvNeXt, ViT,\nand SWIN V2 encoders, consistently segmented well-represented phases.\nTEMSET-24K provides a critical benchmark, propelling state-of-the-art solutions\nin surgical data science.\n","authors":["Muhammad Bilal","Mahmood Alam","Deepa Bapu","Stephan Korsgen","Neeraj Lal","Simon Bach","Amir M Hajivanand","Muhammed Ali","Kamran Soomro","Iqbal Qasim","Paweł Capik","Aslam Khan","Zaheer Khan","Hunaid Vohra","Massimo Caputo","Andrew Beggs","Adnan Qayyum","Junaid Qadir","Shazad Ashraf"],"pdf_url":"https://arxiv.org/pdf/2502.06708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03359v2","updated":"2025-02-10T17:33:29Z","published":"2025-02-05T16:56:14Z","title":"GHOST: Gaussian Hypothesis Open-Set Technique","summary":"  Evaluations of large-scale recognition methods typically focus on overall\nperformance. While this approach is common, it often fails to provide insights\ninto performance across individual classes, which can lead to fairness issues\nand misrepresentation. Addressing these gaps is crucial for accurately\nassessing how well methods handle novel or unseen classes and ensuring a fair\nevaluation. To address fairness in Open-Set Recognition (OSR), we demonstrate\nthat per-class performance can vary dramatically. We introduce Gaussian\nHypothesis Open Set Technique (GHOST), a novel hyperparameter-free algorithm\nthat models deep features using class-wise multivariate Gaussian distributions\nwith diagonal covariance matrices. We apply Z-score normalization to logits to\nmitigate the impact of feature magnitudes that deviate from the model's\nexpectations, thereby reducing the likelihood of the network assigning a high\nscore to an unknown sample. We evaluate GHOST across multiple ImageNet-1K\npre-trained deep networks and test it with four different unknown datasets.\nUsing standard metrics such as AUOSCR, AUROC and FPR95, we achieve\nstatistically significant improvements, advancing the state-of-the-art in\nlarge-scale OSR. Source code is provided online.\n","authors":["Ryan Rabinowitz","Steve Cruz","Manuel Günther","Terrance E. Boult"],"pdf_url":"https://arxiv.org/pdf/2502.03359v2.pdf","comment":"Accepted at AAAI Conference on Artificial Intelligence 2025"},{"id":"http://arxiv.org/abs/2410.11619v2","updated":"2025-02-10T17:26:40Z","published":"2024-10-15T13:56:34Z","title":"MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video\n  Retrieval","summary":"  Efficiently retrieving and synthesizing information from large-scale\nmultimodal collections has become a critical challenge. However, existing video\nretrieval datasets suffer from scope limitations, primarily focusing on\nmatching descriptive but vague queries with small collections of professionally\nedited, English-centric videos. To address this gap, we introduce\n$\\textbf{MultiVENT 2.0}$, a large-scale, multilingual event-centric video\nretrieval benchmark featuring a collection of more than 218,000 news videos and\n3,906 queries targeting specific world events. These queries specifically\ntarget information found in the visual content, audio, embedded text, and text\nmetadata of the videos, requiring systems leverage all these sources to succeed\nat the task. Preliminary results show that state-of-the-art vision-language\nmodels struggle significantly with this task, and while alternative approaches\nshow promise, they are still insufficient to adequately address this problem.\nThese findings underscore the need for more robust multimodal retrieval\nsystems, as effective video retrieval is a crucial step towards multimodal\ncontent understanding and generation.\n","authors":["Reno Kriz","Kate Sanders","David Etter","Kenton Murray","Cameron Carpenter","Kelly Van Ochten","Hannah Recknor","Jimena Guallar-Blasco","Alexander Martin","Ronald Colaianni","Nolan King","Eugene Yang","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2410.11619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12124v3","updated":"2025-02-10T17:19:02Z","published":"2024-09-18T16:46:36Z","title":"Optimal Visual Search with Highly Heuristic Decision Rules","summary":"  Visual search is a fundamental natural task for humans and other animals. We\ninvestigated the decision processes humans use in covert (single-fixation)\nsearch with briefly presented displays having well-separated potential target\nlocations. Performance was compared with the Bayesian-optimal decision process\nunder the assumption that the information from the different potential target\nlocations is statistically independent. Surprisingly, humans performed slightly\nbetter than optimal, despite humans' substantial loss of sensitivity in the\nfovea (foveal neglect), and the implausibility of the human brain replicating\nthe optimal computations. We show that three factors can quantitatively explain\nthese seemingly paradoxical results. Most importantly, simple and fixed\nheuristic decision rules reach near optimal search performance. Secondly,\nfoveal neglect primarily affects only the central potential target location.\nFinally, spatially correlated neural noise can cause search performance to\nexceed that predicted for independent noise. These findings have broad\nimplications for understanding visual search tasks and other identification\ntasks in humans and other animals.\n","authors":["Anqi Zhang","Wilson S. Geisler"],"pdf_url":"https://arxiv.org/pdf/2409.12124v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06682v1","updated":"2025-02-10T17:07:53Z","published":"2025-02-10T17:07:53Z","title":"Transfer Your Perspective: Controllable 3D Generation from Any Viewpoint\n  in a Driving Scene","summary":"  Self-driving cars relying solely on ego-centric perception face limitations\nin sensing, often failing to detect occluded, faraway objects. Collaborative\nautonomous driving (CAV) seems like a promising direction, but collecting data\nfor development is non-trivial. It requires placing multiple sensor-equipped\nagents in a real-world driving scene, simultaneously! As such, existing\ndatasets are limited in locations and agents. We introduce a novel surrogate to\nthe rescue, which is to generate realistic perception from different viewpoints\nin a driving scene, conditioned on a real-world sample - the ego-car's sensory\ndata. This surrogate has huge potential: it could potentially turn any ego-car\ndataset into a collaborative driving one to scale up the development of CAV. We\npresent the very first solution, using a combination of simulated collaborative\ndata and real ego-car data. Our method, Transfer Your Perspective (TYP), learns\na conditioned diffusion model whose output samples are not only realistic but\nalso consistent in both semantics and layouts with the given ego-car data.\nEmpirical results demonstrate TYP's effectiveness in aiding in a CAV setting.\nIn particular, TYP enables us to (pre-)train collaborative perception\nalgorithms like early and late fusion with little or no real-world\ncollaborative data, greatly facilitating downstream CAV applications.\n","authors":["Tai-Yu Pan","Sooyoung Jeon","Mengdi Fan","Jinsu Yoo","Zhenyang Feng","Mark Campbell","Kilian Q. Weinberger","Bharath Hariharan","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2502.06682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06681v1","updated":"2025-02-10T17:07:43Z","published":"2025-02-10T17:07:43Z","title":"CHIRLA: Comprehensive High-resolution Identification and\n  Re-identification for Large-scale Analysis","summary":"  Person re-identification (Re-ID) is a key challenge in computer vision,\nrequiring the matching of individuals across different cameras, locations, and\ntime periods. While most research focuses on short-term scenarios with minimal\nappearance changes, real-world applications demand robust Re-ID systems capable\nof handling long-term scenarios, where persons' appearances can change\nsignificantly due to variations in clothing and physical characteristics. In\nthis paper, we present CHIRLA, Comprehensive High-resolution Identification and\nRe-identification for Large-scale Analysis, a novel dataset specifically\ndesigned for long-term person Re-ID. CHIRLA consists of recordings from\nstrategically placed cameras over a seven-month period, capturing significant\nvariations in both temporal and appearance attributes, including controlled\nchanges in participants' clothing and physical features. The dataset includes\n22 individuals, four connected indoor environments, and seven cameras. We\ncollected more than five hours of video that we semi-automatically labeled to\ngenerate around one million bounding boxes with identity annotations. By\nintroducing this comprehensive benchmark, we aim to facilitate the development\nand evaluation of Re-ID algorithms that can reliably perform in challenging,\nlong-term real-world scenarios.\n","authors":["Bessie Dominguez-Dager","Felix Escalona","Francisco Gomez-Donoso","Miguel Cazorla"],"pdf_url":"https://arxiv.org/pdf/2502.06681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08680v5","updated":"2025-02-10T16:49:25Z","published":"2024-07-11T17:13:15Z","title":"Generalizable Implicit Motion Modeling for Video Frame Interpolation","summary":"  Motion modeling is critical in flow-based Video Frame Interpolation (VFI).\nExisting paradigms either consider linear combinations of bidirectional flows\nor directly predict bilateral flows for given timestamps without exploring\nfavorable motion priors, thus lacking the capability of effectively modeling\nspatiotemporal dynamics in real-world videos. To address this limitation, in\nthis study, we introduce Generalizable Implicit Motion Modeling (GIMM), a novel\nand effective approach to motion modeling for VFI. Specifically, to enable GIMM\nas an effective motion modeling paradigm, we design a motion encoding pipeline\nto model spatiotemporal motion latent from bidirectional flows extracted from\npre-trained flow estimators, effectively representing input-specific motion\npriors. Then, we implicitly predict arbitrary-timestep optical flows within two\nadjacent input frames via an adaptive coordinate-based neural network, with\nspatiotemporal coordinates and motion latent as inputs. Our GIMM can be easily\nintegrated with existing flow-based VFI works by supplying accurately modeled\nmotion. We show that GIMM performs better than the current state of the art on\nstandard VFI benchmarks.\n","authors":["Zujin Guo","Wei Li","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2407.08680v5.pdf","comment":"Project Page: https://gseancdat.github.io/projects/GIMMVFI"},{"id":"http://arxiv.org/abs/2502.06650v1","updated":"2025-02-10T16:40:26Z","published":"2025-02-10T16:40:26Z","title":"Prototype Contrastive Consistency Learning for Semi-Supervised Medical\n  Image Segmentation","summary":"  Medical image segmentation is a crucial task in medical image analysis, but\nit can be very challenging especially when there are less labeled data but with\nlarge unlabeled data. Contrastive learning has proven to be effective for\nmedical image segmentation in semi-supervised learning by constructing\ncontrastive samples from partial pixels. However, although previous contrastive\nlearning methods can mine semantic information from partial pixels within\nimages, they ignore the whole context information of unlabeled images, which is\nvery important to precise segmentation. In order to solve this problem, we\npropose a novel prototype contrastive learning method called Prototype\nContrastive Consistency Segmentation (PCCS) for semi-supervised medical image\nsegmentation. The core idea is to enforce the prototypes of the same semantic\nclass to be closer and push the prototypes in different semantic classes far\naway from each other. Specifically, we construct a signed distance map and an\nuncertainty map from unlabeled images. The signed distance map is used to\nconstruct prototypes for contrastive learning, and then we estimate the\nprototype uncertainty from the uncertainty map as trade-off among prototypes.\nIn order to obtain better prototypes, based on the student-teacher\narchitecture, a new mechanism named prototype updating prototype is designed to\nassist in updating the prototypes for contrastive learning. In addition, we\npropose an uncertainty-consistency loss to mine more reliable information from\nunlabeled data. Extensive experiments on medical image segmentation demonstrate\nthat PCCS achieves better segmentation performance than the state-of-the-art\nmethods. The code is available at https://github.com/comphsh/PCCS.\n","authors":["Shihuan He","Zhihui Lai","Ruxin Wang","Heng Kong"],"pdf_url":"https://arxiv.org/pdf/2502.06650v1.pdf","comment":"17 pages, 10 figures, 7 tables"},{"id":"http://arxiv.org/abs/2501.09038v2","updated":"2025-02-10T16:31:57Z","published":"2025-01-14T20:59:37Z","title":"Do generative video models learn physical principles from watching\n  videos?","summary":"  AI video generation is undergoing a revolution, with quality and realism\nadvancing rapidly. These advances have led to a passionate scientific debate:\nDo video models learn \"world models\" that discover laws of physics -- or,\nalternatively, are they merely sophisticated pixel predictors that achieve\nvisual realism without understanding the physical principles of reality? We\naddress this question by developing Physics-IQ, a comprehensive benchmark\ndataset that can only be solved by acquiring a deep understanding of various\nphysical principles, like fluid dynamics, optics, solid mechanics, magnetism\nand thermodynamics. We find that across a range of current models (Sora,\nRunway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical\nunderstanding is severely limited, and unrelated to visual realism. At the same\ntime, some test cases can already be successfully solved. This indicates that\nacquiring certain physical principles from observation alone may be possible,\nbut significant challenges remain. While we expect rapid advances ahead, our\nwork demonstrates that visual realism does not imply physical understanding.\nOur project page is at https://physics-iq.github.io; code at\nhttps://github.com/google-deepmind/physics-IQ-benchmark.\n","authors":["Saman Motamed","Laura Culp","Kevin Swersky","Priyank Jaini","Robert Geirhos"],"pdf_url":"https://arxiv.org/pdf/2501.09038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06632v1","updated":"2025-02-10T16:28:35Z","published":"2025-02-10T16:28:35Z","title":"Few-Shot Classification and Anatomical Localization of Tissues in SPECT\n  Imaging","summary":"  Accurate classification and anatomical localization are essential for\neffective medical diagnostics and research, which may be efficiently performed\nusing deep learning techniques. However, availability of limited labeled data\nposes a significant challenge. To address this, we adapted Prototypical\nNetworks and the Propagation-Reconstruction Network (PRNet) for few-shot\nclassification and localization, respectively, in Single Photon Emission\nComputed Tomography (SPECT) images. For the proof of concept we used a\n2D-sliced image cropped around heart. The Prototypical Network, with a\npre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver\ntissues with 96.67% training and 93.33% validation accuracy. PRNet, adapted for\n2D imaging with an encoder-decoder architecture and skip connections, achieved\na training loss of 1.395, accurately reconstructing patches and capturing\nspatial relationships. These results highlight the potential of Prototypical\nNetworks for tissue classification with limited labeled data and PRNet for\nanatomical landmark localization, paving the way for improved performance in\ndeep learning frameworks.\n","authors":["Mohammed Abdul Hafeez Khan","Samuel Morries Boddepalli","Siddhartha Bhattacharyya","Debasis Mitra"],"pdf_url":"https://arxiv.org/pdf/2502.06632v1.pdf","comment":"2 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.06631v1","updated":"2025-02-10T16:27:20Z","published":"2025-02-10T16:27:20Z","title":"Conformal Predictions for Human Action Recognition with Vision-Language\n  Models","summary":"  Human-In-The-Loop (HITL) frameworks are integral to many real-world computer\nvision systems, enabling human operators to make informed decisions with AI\nassistance. Conformal Predictions (CP), which provide label sets with rigorous\nguarantees on ground truth inclusion probabilities, have recently gained\ntraction as a valuable tool in HITL settings. One key application area is video\nsurveillance, closely associated with Human Action Recognition (HAR). This\nstudy explores the application of CP on top of state-of-the-art HAR methods\nthat utilize extensively pre-trained Vision-Language Models (VLMs). Our\nfindings reveal that CP can significantly reduce the average number of\ncandidate classes without modifying the underlying VLM. However, these\nreductions often result in distributions with long tails. To address this, we\nintroduce a method based on tuning the temperature parameter of the VLMs to\nminimize these tails without requiring additional calibration data. Our code is\nmade available on GitHub at the address https://github.com/tbary/CP4VLM.\n","authors":["Bary Tim","Fuchs Clément","Macq Benoît"],"pdf_url":"https://arxiv.org/pdf/2502.06631v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.06619v1","updated":"2025-02-10T16:16:34Z","published":"2025-02-10T16:16:34Z","title":"Unleashing the Potential of Pre-Trained Diffusion Models for\n  Generalizable Person Re-Identification","summary":"  Domain-generalizable re-identification (DG Re-ID) aims to train a model on\none or more source domains and evaluate its performance on unseen target\ndomains, a task that has attracted growing attention due to its practical\nrelevance. While numerous methods have been proposed, most rely on\ndiscriminative or contrastive learning frameworks to learn generalizable\nfeature representations. However, these approaches often fail to mitigate\nshortcut learning, leading to suboptimal performance. In this work, we propose\na novel method called diffusion model-assisted representation learning with a\ncorrelation-aware conditioning scheme (DCAC) to enhance DG Re-ID. Our method\nintegrates a discriminative and contrastive Re-ID model with a pre-trained\ndiffusion model through a correlation-aware conditioning scheme. By\nincorporating ID classification probabilities generated from the Re-ID model\nwith a set of learnable ID-wise prompts, the conditioning scheme injects dark\nknowledge that captures ID correlations to guide the diffusion process.\nSimultaneously, feedback from the diffusion model is back-propagated through\nthe conditioning scheme to the Re-ID model, effectively improving the\ngeneralization capability of Re-ID features. Extensive experiments on both\nsingle-source and multi-source DG Re-ID tasks demonstrate that our method\nachieves state-of-the-art performance. Comprehensive ablation studies further\nvalidate the effectiveness of the proposed approach, providing insights into\nits robustness. Codes will be available at https://github.com/RikoLi/DCAC.\n","authors":["Jiachen Li","Xiaojin Gong"],"pdf_url":"https://arxiv.org/pdf/2502.06619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06615v1","updated":"2025-02-10T16:12:46Z","published":"2025-02-10T16:12:46Z","title":"Multi-Scale Feature Fusion with Image-Driven Spatial Integration for\n  Left Atrium Segmentation from Cardiac MRI Images","summary":"  Accurate segmentation of the left atrium (LA) from late gadolinium-enhanced\nmagnetic resonance imaging plays a vital role in visualizing diseased atrial\nstructures, enabling the diagnosis and management of cardiovascular diseases.\nIt is particularly essential for planning treatment with ablation therapy, a\nkey intervention for atrial fibrillation (AF). However, manual segmentation is\ntime-intensive and prone to inter-observer variability, underscoring the need\nfor automated solutions. Class-agnostic foundation models like DINOv2 have\ndemonstrated remarkable feature extraction capabilities in vision tasks.\nHowever, their lack of domain specificity and task-specific adaptation can\nreduce spatial resolution during feature extraction, impacting the capture of\nfine anatomical detail in medical imaging. To address this limitation, we\npropose a segmentation framework that integrates DINOv2 as an encoder with a\nUNet-style decoder, incorporating multi-scale feature fusion and input image\nintegration to enhance segmentation accuracy. The learnable weighting mechanism\ndynamically prioritizes hierarchical features from different encoder blocks of\nthe foundation model, optimizing feature selection for task relevance.\nAdditionally, the input image is reintroduced during the decoding stage to\npreserve high-resolution spatial details, addressing limitations of\ndownsampling in the encoder. We validate our approach on the LAScarQS 2022\ndataset and demonstrate improved performance with a 92.3% Dice and 84.1% IoU\nscore for giant architecture compared to the nnUNet baseline model. These\nfindings emphasize the efficacy of our approach in advancing the field of\nautomated left atrium segmentation from cardiac MRI.\n","authors":["Bipasha Kundu","Zixin Yang","Richard Simon","Cristian Linte"],"pdf_url":"https://arxiv.org/pdf/2502.06615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06608v1","updated":"2025-02-10T16:07:54Z","published":"2025-02-10T16:07:54Z","title":"TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models","summary":"  Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprece- dented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data process- ing, and insufficient exploration of\nadvanced tech- niques in the 3D domain. Current approaches to 3D shape\ngeneration face substantial challenges in terms of output quality,\ngeneralization capa- bility, and alignment with input conditions. We present\nTripoSG, a new streamlined shape diffu- sion paradigm capable of generating\nhigh-fidelity 3D meshes with precise correspondence to input images.\nSpecifically, we propose: 1) A large-scale rectified flow transformer for 3D\nshape generation, achieving state-of-the-art fidelity through training on\nextensive, high-quality data. 2) A hybrid supervised training strategy\ncombining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality\n3D reconstruction performance. 3) A data processing pipeline to generate 2\nmillion high- quality 3D samples, highlighting the crucial rules for data\nquality and quantity in training 3D gen- erative models. Through comprehensive\nexperi- ments, we have validated the effectiveness of each component in our new\nframework. The seamless integration of these parts has enabled TripoSG to\nachieve state-of-the-art performance in 3D shape generation. The resulting 3D\nshapes exhibit en- hanced detail due to high-resolution capabilities and\ndemonstrate exceptional fidelity to input im- ages. Moreover, TripoSG\ndemonstrates improved versatility in generating 3D models from diverse image\nstyles and contents, showcasing strong gen- eralization capabilities. To foster\nprogress and innovation in the field of 3D generation, we will make our model\npublicly available.\n","authors":["Yangguang Li","Zi-Xin Zou","Zexiang Liu","Dehu Wang","Yuan Liang","Zhipeng Yu","Xingchao Liu","Yuan-Chen Guo","Ding Liang","Wanli Ouyang","Yan-Pei Cao"],"pdf_url":"https://arxiv.org/pdf/2502.06608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01751v2","updated":"2025-02-10T16:05:55Z","published":"2023-09-04T18:19:20Z","title":"Multispectral Indices for Wildfire Management","summary":"  The increasing frequency and severity of wildfires requires advanced methods\nfor effective surveillance and management. Traditional ground-based observation\ntechniques often struggle to adapt to rapidly changing fire behavior and\nenvironmental conditions. This paper examines the application of multispectral\naerial and satellite imagery in wildfire management, emphasizing the\nidentification and analysis of key factors influencing wildfire behavior, such\nas combustible vegetation and water features. Through a comprehensive review of\ncurrent literature and the presentation of two practical case studies, we\nassess various multispectral indices and evaluate their effectiveness in\nextracting critical environmental attributes essential for wildfire prevention\nand management. Our case studies highlight several indices as particularly\neffective for segmentation and extraction: NVDI for vegetation, MNDWI for water\nfeatures, and MSR for artificial structures. These indices significantly\nenhance wildfire data processing, thereby supporting improved monitoring and\nresponse strategies.\n","authors":["Afonso Oliveira","João P. Matos-Carvalho","Filipe Moutinho","Nuno Fachada"],"pdf_url":"https://arxiv.org/pdf/2309.01751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06607v1","updated":"2025-02-10T16:04:54Z","published":"2025-02-10T16:04:54Z","title":"Illegal Waste Detection in Remote Sensing Images: A Case Study","summary":"  Environmental crime currently represents the third largest criminal activity\nworldwide while threatening ecosystems as well as human health. Among the\ncrimes related to this activity, improper waste management can nowadays be\ncountered more easily thanks to the increasing availability and decreasing cost\nof Very-High-Resolution Remote Sensing images, which enable semi-automatic\nterritory scanning in search of illegal landfills. This paper proposes a\npipeline, developed in collaboration with professionals from a local\nenvironmental agency, for detecting candidate illegal dumping sites leveraging\na classifier of Remote Sensing images. To identify the best configuration for\nsuch classifier, an extensive set of experiments was conducted and the impact\nof diverse image characteristics and training settings was thoroughly analyzed.\nThe local environmental agency was then involved in an experimental exercise\nwhere outputs from the developed classifier were integrated in the experts'\neveryday work, resulting in time savings with respect to manual\nphoto-interpretation. The classifier was eventually run with valuable results\non a location outside of the training area, highlighting potential for\ncross-border applicability of the proposed pipeline.\n","authors":["Federico Gibellini","Piero Fraternali","Giacomo Boracchi","Luca Morandini","Andrea Diecidue","Simona Malegori"],"pdf_url":"https://arxiv.org/pdf/2502.06607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06606v1","updated":"2025-02-10T16:04:33Z","published":"2025-02-10T16:04:33Z","title":"MaterialFusion: High-Quality, Zero-Shot, and Controllable Material\n  Transfer with Diffusion Models","summary":"  Manipulating the material appearance of objects in images is critical for\napplications like augmented reality, virtual prototyping, and digital content\ncreation. We present MaterialFusion, a novel framework for high-quality\nmaterial transfer that allows users to adjust the degree of material\napplication, achieving an optimal balance between new material properties and\nthe object's original features. MaterialFusion seamlessly integrates the\nmodified object into the scene by maintaining background consistency and\nmitigating boundary artifacts. To thoroughly evaluate our approach, we have\ncompiled a dataset of real-world material transfer examples and conducted\ncomplex comparative analyses. Through comprehensive quantitative evaluations\nand user studies, we demonstrate that MaterialFusion significantly outperforms\nexisting methods in terms of quality, user control, and background\npreservation. Code is available at\nhttps://github.com/kzGarifullin/MaterialFusion.\n","authors":["Kamil Garifullin","Maxim Nikolaev","Andrey Kuznetsov","Aibek Alanov"],"pdf_url":"https://arxiv.org/pdf/2502.06606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06593v1","updated":"2025-02-10T15:56:28Z","published":"2025-02-10T15:56:28Z","title":"A Large-scale AI-generated Image Inpainting Benchmark","summary":"  Recent advances in generative models enable highly realistic image\nmanipulations, creating an urgent need for robust forgery detection methods.\nCurrent datasets for training and evaluating these methods are limited in scale\nand diversity. To address this, we propose a methodology for creating\nhigh-quality inpainting datasets and apply it to create DiQuID, comprising over\n95,000 inpainted images generated from 78,000 original images sourced from\nMS-COCO, RAISE, and OpenImages. Our methodology consists of three components:\n(1) Semantically Aligned Object Replacement (SAOR) that identifies suitable\nobjects through instance segmentation and generates contextually appropriate\nprompts, (2) Multiple Model Image Inpainting (MMII) that employs various\nstate-of-the-art inpainting pipelines primarily based on diffusion models to\ncreate diverse manipulations, and (3) Uncertainty-Guided Deceptiveness\nAssessment (UGDA) that evaluates image realism through comparative analysis\nwith originals. The resulting dataset surpasses existing ones in diversity,\naesthetic quality, and technical quality. We provide comprehensive benchmarking\nresults using state-of-the-art forgery detection methods, demonstrating the\ndataset's effectiveness in evaluating and improving detection algorithms.\nThrough a human study with 42 participants on 1,000 images, we show that while\nhumans struggle with images classified as deceiving by our methodology, models\ntrained on our dataset maintain high performance on these challenging cases.\nCode and dataset are available at https://github.com/mever-team/DiQuID.\n","authors":["Paschalis Giakoumoglou","Dimitrios Karageorgiou","Symeon Papadopoulos","Panagiotis C. Petrantonakis"],"pdf_url":"https://arxiv.org/pdf/2502.06593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06587v1","updated":"2025-02-10T15:53:26Z","published":"2025-02-10T15:53:26Z","title":"evclust: Python library for evidential clustering","summary":"  A recent developing trend in clustering is the advancement of algorithms that\nnot only identify clusters within data, but also express and capture the\nuncertainty of cluster membership. Evidential clustering addresses this by\nusing the Dempster-Shafer theory of belief functions, a framework designed to\nmanage and represent uncertainty. This approach results in a credal partition,\na structured set of mass functions that quantify the uncertain assignment of\neach object to potential groups. The Python framework evclust, presented in\nthis paper, offers a suite of efficient evidence clustering algorithms as well\nas tools for visualizing, evaluating and analyzing credal partitions.\n","authors":["Armel Soubeiga","Violaine Antoine"],"pdf_url":"https://arxiv.org/pdf/2502.06587v1.pdf","comment":"13 pages, 2 figures, Preprint"},{"id":"http://arxiv.org/abs/2502.06583v1","updated":"2025-02-10T15:50:26Z","published":"2025-02-10T15:50:26Z","title":"Adaptive Perception for Unified Visual Multi-modal Object Tracking","summary":"  Recently, many multi-modal trackers prioritize RGB as the dominant modality,\ntreating other modalities as auxiliary, and fine-tuning separately various\nmulti-modal tasks. This imbalance in modality dependence limits the ability of\nmethods to dynamically utilize complementary information from each modality in\ncomplex scenarios, making it challenging to fully perceive the advantages of\nmulti-modal. As a result, a unified parameter model often underperforms in\nvarious multi-modal tracking tasks. To address this issue, we propose APTrack,\na novel unified tracker designed for multi-modal adaptive perception. Unlike\nprevious methods, APTrack explores a unified representation through an equal\nmodeling strategy. This strategy allows the model to dynamically adapt to\nvarious modalities and tasks without requiring additional fine-tuning between\ndifferent tasks. Moreover, our tracker integrates an adaptive modality\ninteraction (AMI) module that efficiently bridges cross-modality interactions\nby generating learnable tokens. Experiments conducted on five diverse\nmulti-modal datasets (RGBT234, LasHeR, VisEvent, DepthTrack, and VOT-RGBD2022)\ndemonstrate that APTrack not only surpasses existing state-of-the-art unified\nmulti-modal trackers but also outperforms trackers designed for specific\nmulti-modal tasks.\n","authors":["Xiantao Hu","Bineng Zhong","Qihua Liang","Zhiyi Mo","Liangtao Shi","Ying Tai","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2502.06583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06581v1","updated":"2025-02-10T15:48:11Z","published":"2025-02-10T15:48:11Z","title":"A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems","summary":"  The explosive growth of video data has driven the development of distributed\nvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enabling\nefficient video processing, real-time inference, and privacy-preserving\nanalysis. Among multiple advantages, CETC systems can distribute video\nprocessing tasks and enable adaptive analytics across cloud, edge, and terminal\ndevices, leading to breakthroughs in video surveillance, autonomous driving,\nand smart cities. In this survey, we first analyze fundamental architectural\ncomponents, including hierarchical, distributed, and hybrid frameworks,\nalongside edge computing platforms and resource management mechanisms. Building\nupon these foundations, edge-centric approaches emphasize on-device processing,\nedge-assisted offloading, and edge intelligence, while cloud-centric methods\nleverage powerful computational capabilities for complex video understanding\nand model training. Our investigation also covers hybrid video analytics\nincorporating adaptive task offloading and resource-aware scheduling techniques\nthat optimize performance across the entire system. Beyond conventional\napproaches, recent advances in large language models and multimodal integration\nreveal both opportunities and challenges in platform scalability, data\nprotection, and system reliability. Future directions also encompass\nexplainable systems, efficient processing mechanisms, and advanced video\nanalytics, offering valuable insights for researchers and practitioners in this\ndynamic field.\n","authors":["Linxiao Gong","Hao Yang","Gaoyun Fang","Bobo Ju","Juncen Guo","Xiaoguang Zhu","Yan Wang","Xiping Hu","Peng Sun","Azzedine Boukerche"],"pdf_url":"https://arxiv.org/pdf/2502.06581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19184v2","updated":"2025-02-10T15:47:32Z","published":"2025-01-31T14:47:09Z","title":"A Survey on Class-Agnostic Counting: Advancements from Reference-Based\n  to Open-World Text-Guided Approaches","summary":"  Visual object counting has recently shifted towards class-agnostic counting\n(CAC), which addresses the challenge of counting objects across arbitrary\ncategories -- a crucial capability for flexible and generalizable counting\nsystems. Unlike humans, who effortlessly identify and count objects from\ndiverse categories without prior knowledge, most existing counting methods are\nrestricted to enumerating instances of known classes, requiring extensive\nlabeled datasets for training and struggling in open-vocabulary settings. In\ncontrast, CAC aims to count objects belonging to classes never seen during\ntraining, operating in a few-shot setting. In this paper, we present the first\ncomprehensive review of CAC methodologies. We propose a taxonomy to categorize\nCAC approaches into three paradigms based on how target object classes can be\nspecified: reference-based, reference-less, and open-world text-guided.\nReference-based approaches achieve state-of-the-art performance by relying on\nexemplar-guided mechanisms. Reference-less methods eliminate exemplar\ndependency by leveraging inherent image patterns. Finally, open-world\ntext-guided methods use vision-language models, enabling object class\ndescriptions via textual prompts, offering a flexible and promising solution.\nBased on this taxonomy, we provide an overview of the architectures of 29 CAC\napproaches and report their results on gold-standard benchmarks. We compare\ntheir performance and discuss their strengths and limitations. Specifically, we\npresent results on the FSC-147 dataset, setting a leaderboard using\ngold-standard metrics, and on the CARPK dataset to assess generalization\ncapabilities. Finally, we offer a critical discussion of persistent challenges,\nsuch as annotation dependency and generalization, alongside future directions.\nWe believe this survey will be a valuable resource, showcasing CAC advancements\nand guiding future research.\n","authors":["Luca Ciampi","Ali Azmoudeh","Elif Ecem Akbaba","Erdi Sarıtaş","Ziya Ata Yazıcı","Hazım Kemal Ekenel","Giuseppe Amato","Fabrizio Falchi"],"pdf_url":"https://arxiv.org/pdf/2501.19184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04640v2","updated":"2025-02-10T15:41:04Z","published":"2025-02-07T03:53:46Z","title":"Building Rome with Convex Optimization","summary":"  Global bundle adjustment is made easy by depth prediction and convex\noptimization. We (i) propose a scaled bundle adjustment (SBA) formulation that\nlifts 2D keypoint measurements to 3D with learned depth, (ii) design an\nempirically tight convex semidfinite program (SDP) relaxation that solves SBA\nto certfiable global optimality, (iii) solve the SDP relaxations at extreme\nscale with Burer-Monteiro factorization and a CUDA-based trust-region\nRiemannian optimizer (dubbed XM), (iv) build a structure from motion (SfM)\npipeline with XM as the optimization engine and show that XM-SfM dominates or\ncompares favorably with existing SfM pipelines in terms of reconstruction\nquality while being faster, more scalable, and initialization-free.\n","authors":["Haoyu Han","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2502.04640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08256v2","updated":"2025-02-10T15:35:58Z","published":"2024-01-16T10:18:57Z","title":"Multitask Learning in Minimally Invasive Surgical Vision: A Review","summary":"  Minimally invasive surgery (MIS) has revolutionized many procedures and led\nto reduced recovery time and risk of patient injury. However, MIS poses\nadditional complexity and burden on surgical teams. Data-driven surgical vision\nalgorithms are thought to be key building blocks in the development of future\nMIS systems with improved autonomy. Recent advancements in machine learning and\ncomputer vision have led to successful applications in analyzing videos\nobtained from MIS with the promise of alleviating challenges in MIS videos.\nSurgical scene and action understanding encompasses multiple related tasks\nthat, when solved individually, can be memory-intensive, inefficient, and fail\nto capture task relationships. Multitask learning (MTL), a learning paradigm\nthat leverages information from multiple related tasks to improve performance\nand aid generalization, is well suited for fine-grained and high-level\nunderstanding of MIS data. This review provides a narrative overview of the\ncurrent state-of-the-art MTL systems that leverage videos obtained from MIS.\nBeyond listing published approaches, we discuss the benefits and limitations of\nthese MTL systems. Moreover, this manuscript presents an analysis of the\nliterature for various application fields of MTL in MIS, including those with\nlarge models, highlighting notable trends, new directions of research, and\ndevelopments.\n","authors":["Oluwatosin Alabi","Tom Vercauteren","Miaojing Shi"],"pdf_url":"https://arxiv.org/pdf/2401.08256v2.pdf","comment":"Published at Medical Image Analysis"},{"id":"http://arxiv.org/abs/2502.06552v1","updated":"2025-02-10T15:20:07Z","published":"2025-02-10T15:20:07Z","title":"Diffusion Models for Computational Neuroimaging: A Survey","summary":"  Computational neuroimaging involves analyzing brain images or signals to\nprovide mechanistic insights and predictive tools for human cognition and\nbehavior. While diffusion models have shown stability and high-quality\ngeneration in natural images, there is increasing interest in adapting them to\nanalyze brain data for various neurological tasks such as data enhancement,\ndisease diagnosis and brain decoding. This survey provides an overview of\nrecent efforts to integrate diffusion models into computational neuroimaging.\nWe begin by introducing the common neuroimaging data modalities, follow with\nthe diffusion formulations and conditioning mechanisms. Then we discuss how the\nvariations of the denoising starting point, condition input and generation\ntarget of diffusion models are developed and enhance specific neuroimaging\ntasks. For a comprehensive overview of the ongoing research, we provide a\npublicly available repository at https://github.com/JoeZhao527/dm4neuro.\n","authors":["Haokai Zhao","Haowei Lou","Lina Yao","Wei Peng","Ehsan Adeli","Kilian M Pohl","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06552v1.pdf","comment":"9 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.15802v2","updated":"2025-02-10T15:12:04Z","published":"2024-08-28T13:53:27Z","title":"Visual Prompt Engineering for Vision Language Models in Radiology","summary":"  Medical image classification plays a crucial role in clinical\ndecision-making, yet most models are constrained to a fixed set of predefined\nclasses, limiting their adaptability to new conditions. Contrastive\nLanguage-Image Pretraining (CLIP) offers a promising solution by enabling\nzero-shot classification through multimodal large-scale pretraining. However,\nwhile CLIP effectively captures global image content, radiology requires a more\nlocalized focus on specific pathology regions to enhance both interpretability\nand diagnostic accuracy. To address this, we explore the potential of\nincorporating visual cues into zero-shot classification, embedding visual\nmarkers $\\unicode{x2013}$ such as arrows, bounding boxes, and circles\n$\\unicode{x2013}$ directly into radiological images to guide model attention.\nEvaluating across four public chest X-ray datasets, we demonstrate that visual\nmarkers improve AUROC by up to 0.185, highlighting their effectiveness in\nenhancing classification performance. Furthermore, attention map analysis\nconfirms that visual cues help models focus on clinically relevant areas,\nleading to more interpretable predictions. To support further research, we use\npublic datasets and will release our code and preprocessing pipeline, providing\na reference point for future work on localized classification in medical\nimaging.\n","authors":["Stefan Denner","Markus Bujotzek","Dimitrios Bounias","David Zimmerer","Raphael Stock","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2408.15802v2.pdf","comment":"Accepted at ECCV 2024 Workshop on Emergent Visual Abilities and\n  Limits of Foundation Models"},{"id":"http://arxiv.org/abs/2502.06544v1","updated":"2025-02-10T15:09:56Z","published":"2025-02-10T15:09:56Z","title":"Sequence Transferability and Task Order Selection in Continual Learning","summary":"  In continual learning, understanding the properties of task sequences and\ntheir relationships to model performance is important for developing advanced\nalgorithms with better accuracy. However, efforts in this direction remain\nunderdeveloped despite encouraging progress in methodology development. In this\nwork, we investigate the impacts of sequence transferability on continual\nlearning and propose two novel measures that capture the total transferability\nof a task sequence, either in the forward or backward direction. Based on the\nempirical properties of these measures, we then develop a new method for the\ntask order selection problem in continual learning. Our method can be shown to\noffer a better performance than the conventional strategy of random task\nselection.\n","authors":["Thinh Nguyen","Cuong N. Nguyen","Quang Pham","Binh T. Nguyen","Savitha Ramasamy","Xiaoli Li","Cuong V. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.06544v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.06543v1","updated":"2025-02-10T15:09:29Z","published":"2025-02-10T15:09:29Z","title":"Unsupervised Learning for Feature Extraction and Temporal Alignment of\n  3D+t Point Clouds of Zebrafish Embryos","summary":"  Zebrafish are widely used in biomedical research and developmental stages of\ntheir embryos often need to be synchronized for further analysis. We present an\nunsupervised approach to extract descriptive features from 3D+t point clouds of\nzebrafish embryos and subsequently use those features to temporally align\ncorresponding developmental stages. An autoencoder architecture is proposed to\nlearn a descriptive representation of the point clouds and we designed a deep\nregression network for their temporal alignment. We achieve a high alignment\naccuracy with an average mismatch of only 3.83 minutes over an experimental\nduration of 5.3 hours. As a fully-unsupervised approach, there is no manual\nlabeling effort required and unlike manual analyses the method easily scales.\nBesides, the alignment without human annotation of the data also avoids any\ninfluence caused by subjective bias.\n","authors":["Zhu Chen","Ina Laube","Johannes Stegmaier"],"pdf_url":"https://arxiv.org/pdf/2502.06543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08840v2","updated":"2025-02-10T15:06:38Z","published":"2024-09-13T13:53:52Z","title":"Direct-CP: Directed Collaborative Perception for Connected and\n  Autonomous Vehicles via Proactive Attention","summary":"  Collaborative perception (CP) leverages visual data from connected and\nautonomous vehicles (CAV) to enhance an ego vehicle's field of view (FoV).\nDespite recent progress, current CP methods expand the ego vehicle's 360-degree\nperceptual range almost equally, which faces two key challenges. Firstly, in\nareas with uneven traffic distribution, focusing on directions with little\ntraffic offers limited benefits. Secondly, under limited communication budgets,\nallocating excessive bandwidth to less critical directions lowers the\nperception accuracy in more vital areas. To address these issues, we propose\nDirect-CP, a proactive and direction-aware CP system aiming at improving CP in\nspecific directions. Our key idea is to enable an ego vehicle to proactively\nsignal its interested directions and readjust its attention to enhance local\ndirectional CP performance. To achieve this, we first propose an RSU-aided\ndirection masking mechanism that assists an ego vehicle in identifying vital\ndirections. Additionally, we design a direction-aware selective attention\nmodule to wisely aggregate pertinent features based on ego vehicle's\ndirectional priorities, communication budget, and the positional data of CAVs.\nMoreover, we introduce a direction-weighted detection loss (DWLoss) to capture\nthe divergence between directional CP outcomes and the ground truth,\nfacilitating effective model training. Extensive experiments on the V2X-Sim 2.0\ndataset demonstrate that our approach achieves 19.8\\% higher local perception\naccuracy in interested directions and 2.5\\% higher overall perception accuracy\nthan the state-of-the-art methods in collaborative 3D object detection tasks.\n","authors":["Yihang Tao","Senkang Hu","Zhengru Fang","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2409.08840v2.pdf","comment":"Accepted by ICRA'25"},{"id":"http://arxiv.org/abs/2502.05066v2","updated":"2025-02-10T14:58:43Z","published":"2025-02-07T16:39:39Z","title":"Beautiful Images, Toxic Words: Understanding and Addressing Offensive\n  Text in Generated Images","summary":"  State-of-the-art visual generation models, such as Diffusion Models (DMs) and\nVision Auto-Regressive Models (VARs), produce highly realistic images. While\nprior work has successfully mitigated Not Safe For Work (NSFW) content in the\nvisual domain, we identify a novel threat: the generation of NSFW text embedded\nwithin images. This includes offensive language, such as insults, racial slurs,\nand sexually explicit terms, posing significant risks to users. We show that\nall state-of-the-art DMs (e.g., SD3, Flux, DeepFloyd IF) and VARs (e.g.,\nInfinity) are vulnerable to this issue. Through extensive experiments, we\ndemonstrate that existing mitigation techniques, effective for visual content,\nfail to prevent harmful text generation while substantially degrading benign\ntext generation. As an initial step toward addressing this threat, we explore\nsafety fine-tuning of the text encoder underlying major DM architectures using\na customized dataset. Thereby, we suppress NSFW generation while preserving\noverall image and text generation quality. Finally, to advance research in this\narea, we introduce ToxicBench, an open-source benchmark for evaluating NSFW\ntext generation in images. ToxicBench provides a curated dataset of harmful\nprompts, new metrics, and an evaluation pipeline assessing both NSFW-ness and\ngeneration quality. Our benchmark aims to guide future efforts in mitigating\nNSFW text generation in text-to-image models.\n","authors":["Aditya Kumar","Tom Blanchard","Adam Dziedzic","Franziska Boenisch"],"pdf_url":"https://arxiv.org/pdf/2502.05066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11802v4","updated":"2025-02-10T14:53:09Z","published":"2024-07-16T14:53:35Z","title":"Discriminative and Consistent Representation Distillation","summary":"  Knowledge Distillation (KD) aims to transfer knowledge from a large teacher\nmodel to a smaller student model. While contrastive learning has shown promise\nin self-supervised learning by creating discriminative representations, its\napplication in knowledge distillation remains limited and focuses primarily on\ndiscrimination, neglecting the structural relationships captured by the teacher\nmodel. To address this limitation, we propose Discriminative and Consistent\nDistillation (DCD), which employs a contrastive loss along with a consistency\nregularization to minimize the discrepancy between the distributions of teacher\nand student representations. Our method introduces learnable temperature and\nbias parameters that adapt during training to balance these complementary\nobjectives, replacing the fixed hyperparameters commonly used in contrastive\nlearning approaches. Through extensive experiments on CIFAR-100 and ImageNet\nILSVRC-2012, we demonstrate that DCD achieves state-of-the-art performance,\nwith the student model sometimes surpassing the teacher's accuracy.\nFurthermore, we show that DCD's learned representations exhibit superior\ncross-dataset generalization when transferred to Tiny ImageNet and STL-10.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2407.11802v4.pdf","comment":"Preprint. Code: https://github.com/giakoumoglou/distillers,\n  Supplementary: https://giakoumoglou.com/src/dcd_suppl.pdf"},{"id":"http://arxiv.org/abs/2502.04847v2","updated":"2025-02-10T14:51:29Z","published":"2025-02-07T11:36:36Z","title":"HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion\n  Video Generation","summary":"  Human motion video generation has advanced significantly, while existing\nmethods still struggle with accurately rendering detailed body parts like hands\nand faces, especially in long sequences and intricate motions. Current\napproaches also rely on fixed resolution and struggle to maintain visual\nconsistency. To address these limitations, we propose HumanDiT, a pose-guided\nDiffusion Transformer (DiT)-based framework trained on a large and wild dataset\ncontaining 14,000 hours of high-quality video to produce high-fidelity videos\nwith fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,\nsupports numerous video resolutions and variable sequence lengths, facilitating\nlearning for long-sequence video generation; (ii) we introduce a prefix-latent\nreference strategy to maintain personalized characteristics across extended\nsequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to\ngenerate subsequent pose sequences, facilitating video continuation from static\nimages or existing videos. It also utilizes a Pose Adapter to enable pose\ntransfer with given sequences. Extensive experiments demonstrate its superior\nperformance in generating long-form, pose-accurate videos across diverse\nscenarios.\n","authors":["Qijun Gan","Yi Ren","Chen Zhang","Zhenhui Ye","Pan Xie","Xiang Yin","Zehuan Yuan","Bingyue Peng","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.04847v2.pdf","comment":"https://agnjason.github.io/HumanDiT-page/"},{"id":"http://arxiv.org/abs/2502.06527v1","updated":"2025-02-10T14:50:32Z","published":"2025-02-10T14:50:32Z","title":"CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for\n  Zero-Shot Customized Video Diffusion Transformers","summary":"  Customized generation has achieved significant progress in image synthesis,\nyet personalized video generation remains challenging due to temporal\ninconsistencies and quality degradation. In this paper, we introduce\nCustomVideoX, an innovative framework leveraging the video diffusion\ntransformer for personalized video generation from a reference image.\nCustomVideoX capitalizes on pre-trained video networks by exclusively training\nthe LoRA parameters to extract reference features, ensuring both efficiency and\nadaptability. To facilitate seamless interaction between the reference image\nand video content, we propose 3D Reference Attention, which enables direct and\nsimultaneous engagement of reference image features with all video frames\nacross spatial and temporal dimensions. To mitigate the excessive influence of\nreference image features and textual guidance on generated video content during\ninference, we implement the Time-Aware Reference Attention Bias (TAB) strategy,\ndynamically modulating reference bias over different time steps. Additionally,\nwe introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly\nactivated regions of key entity tokens with reference feature injection by\nadjusting attention bias. To thoroughly evaluate personalized video generation,\nwe establish a new benchmark, VideoBench, comprising over 50 objects and 100\nprompts for extensive assessment. Experimental results show that CustomVideoX\nsignificantly outperforms existing methods in terms of video consistency and\nquality.\n","authors":["D. She","Mushui Liu","Jingxuan Pang","Jin Wang","Zhen Yang","Wanggui He","Guanghao Zhang","Yi Wang","Qihan Huang","Haobin Tang","Yunlong Yu","Siming Fu"],"pdf_url":"https://arxiv.org/pdf/2502.06527v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.06519v1","updated":"2025-02-10T14:41:38Z","published":"2025-02-10T14:41:38Z","title":"SIREN: Semantic, Initialization-Free Registration of Multi-Robot\n  Gaussian Splatting Maps","summary":"  We present SIREN for registration of multi-robot Gaussian Splatting (GSplat)\nmaps, with zero access to camera poses, images, and inter-map transforms for\ninitialization or fusion of local submaps. To realize these capabilities, SIREN\nharnesses the versatility and robustness of semantics in three critical ways to\nderive a rigorous registration pipeline for multi-robot GSplat maps. First,\nSIREN utilizes semantics to identify feature-rich regions of the local maps\nwhere the registration problem is better posed, eliminating the need for any\ninitialization which is generally required in prior work. Second, SIREN\nidentifies candidate correspondences between Gaussians in the local maps using\nrobust semantic features, constituting the foundation for robust geometric\noptimization, coarsely aligning 3D Gaussian primitives extracted from the local\nmaps. Third, this key step enables subsequent photometric refinement of the\ntransformation between the submaps, where SIREN leverages novel-view synthesis\nin GSplat maps along with a semantics-based image filter to compute a\nhigh-accuracy non-rigid transformation for the generation of a high-fidelity\nfused map. We demonstrate the superior performance of SIREN compared to\ncompeting baselines across a range of real-world datasets, and in particular,\nacross the most widely-used robot hardware platforms, including a manipulator,\ndrone, and quadruped. In our experiments, SIREN achieves about 90x smaller\nrotation errors, 300x smaller translation errors, and 44x smaller scale errors\nin the most challenging scenes, where competing methods struggle. We will\nrelease the code and provide a link to the project page after the review\nprocess.\n","authors":["Ola Shorinwa","Jiankai Sun","Mac Schwager","Anirudha Majumdar"],"pdf_url":"https://arxiv.org/pdf/2502.06519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06516v1","updated":"2025-02-10T14:37:26Z","published":"2025-02-10T14:37:26Z","title":"Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation","summary":"  Minority samples are underrepresented instances located in low-density\nregions of a data manifold, and are valuable in many generative AI\napplications, such as data augmentation, creative content generation, etc.\nUnfortunately, existing diffusion-based minority generators often rely on\ncomputationally expensive guidance dedicated for minority generation. To\naddress this, here we present a simple yet powerful guidance-free approach\ncalled Boost-and-Skip for generating minority samples using diffusion models.\nThe key advantage of our framework requires only two minimal changes to\nstandard generative processes: (i) variance-boosted initialization and (ii)\ntimestep skipping. We highlight that these seemingly-trivial modifications are\nsupported by solid theoretical and empirical evidence, thereby effectively\npromoting emergence of underrepresented minority features. Our comprehensive\nexperiments demonstrate that Boost-and-Skip greatly enhances the capability of\ngenerating minority samples, even rivaling guidance-based state-of-the-art\napproaches while requiring significantly fewer computations.\n","authors":["Soobin Um","Beomsu Kim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2502.06516v1.pdf","comment":"29 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.06501v1","updated":"2025-02-10T14:20:01Z","published":"2025-02-10T14:20:01Z","title":"Learning Clustering-based Prototypes for Compositional Zero-shot\n  Learning","summary":"  Learning primitive (i.e., attribute and object) concepts from seen\ncompositions is the primary challenge of Compositional Zero-Shot Learning\n(CZSL). Existing CZSL solutions typically rely on oversimplified data\nassumptions, e.g., modeling each primitive with a single centroid primitive\nrepresentation, ignoring the natural diversities of the attribute (resp.\nobject) when coupled with different objects (resp. attribute). In this work, we\ndevelop ClusPro, a robust clustering-based prototype mining framework for CZSL\nthat defines the conceptual boundaries of primitives through a set of\ndiversified prototypes. Specifically, ClusPro conducts within-primitive\nclustering on the embedding space for automatically discovering and dynamically\nupdating prototypes. These representative prototypes are subsequently used to\nrepaint a well-structured and independent primitive embedding space, ensuring\nintra-primitive separation and inter-primitive decorrelation through\nprototype-based contrastive learning and decorrelation learning. Moreover,\nClusPro efficiently performs prototype clustering in a non-parametric fashion\nwithout the introduction of additional learnable parameters or computational\nbudget during testing. Experiments on three benchmarks demonstrate ClusPro\noutperforms various top-leading CZSL solutions under both closed-world and\nopen-world settings.\n","authors":["Hongyu Qu","Jianan Wei","Xiangbo Shu","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06501v1.pdf","comment":"Accepted to ICLR 2025; Project page:\n  https://github.com/quhongyu/ClusPro"},{"id":"http://arxiv.org/abs/2502.06498v1","updated":"2025-02-10T14:16:51Z","published":"2025-02-10T14:16:51Z","title":"Decision Boundary Optimization-Informed Domain Adaptation","summary":"  Maximum Mean Discrepancy (MMD) is widely used in a number of domain\nadaptation (DA) methods and shows its effectiveness in aligning data\ndistributions across domains. However, in previous DA research, MMD-based DA\nmethods focus mostly on distribution alignment, and ignore to optimize the\ndecision boundary for classification-aware DA, thereby falling short in\nreducing the DA upper error bound. In this paper, we propose a strengthened MMD\nmeasurement, namely, Decision Boundary optimization-informed MMD (DB-MMD),\nwhich enables MMD to carefully take into account the decision boundaries,\nthereby simultaneously optimizing the distribution alignment and cross-domain\nclassifier within a hybrid framework, and leading to a theoretical bound guided\nDA. We further seamlessly embed the proposed DB-MMD measurement into several\npopular DA methods, e.g., MEDA, DGA-DA, to demonstrate its effectiveness w.r.t\ndifferent experimental settings. We carry out comprehensive experiments using 8\nstandard DA datasets. The experimental results show that the DB-MMD enforced DA\nmethods improve their baseline models using plain vanilla MMD, with a margin\nthat can be as high as 9.5.\n","authors":["Lingkun Luo","Shiqiang Hu","Jie Yang","Liming Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18917v7","updated":"2025-02-10T14:14:12Z","published":"2023-10-29T06:10:46Z","title":"TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural\n  Radiance Fields","summary":"  Previous attempts to integrate Neural Radiance Fields (NeRF) into the\nSimultaneous Localization and Mapping (SLAM) framework either rely on the\nassumption of static scenes or require the ground truth camera poses, which\nimpedes their application in real-world scenarios. This paper proposes a\ntime-varying representation to track and reconstruct the dynamic scenes.\nFirstly, two processes, a tracking process and a mapping process, are\nmaintained simultaneously in our framework. In the tracking process, all input\nimages are uniformly sampled and then progressively trained in a\nself-supervised paradigm. In the mapping process, we leverage motion masks to\ndistinguish dynamic objects from the static background, and sample more pixels\nfrom dynamic areas. Secondly, the parameter optimization for both processes is\ncomprised of two stages: the first stage associates time with 3D positions to\nconvert the deformation field to the canonical field. The second stage\nassociates time with the embeddings of the canonical field to obtain colors and\na Signed Distance Function (SDF). Lastly, we propose a novel keyframe selection\nstrategy based on the overlapping rate. Our approach is evaluated on two\nsynthetic datasets and one real-world dataset, and the experiments validate\nthat our method achieves competitive results in both tracking and mapping when\ncompared to existing state-of-the-art NeRF-based dynamic SLAM systems.\n","authors":["Chengyao Duan","Zhiliu Yang"],"pdf_url":"https://arxiv.org/pdf/2310.18917v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06486v1","updated":"2025-02-10T14:04:57Z","published":"2025-02-10T14:04:57Z","title":"Biomechanical Reconstruction with Confidence Intervals from Multiview\n  Markerless Motion Capture","summary":"  Advances in multiview markerless motion capture (MMMC) promise high-quality\nmovement analysis for clinical practice and research. While prior validation\nstudies show MMMC performs well on average, they do not provide what is needed\nin clinical practice or for large-scale utilization of MMMC -- confidence\nintervals over specific kinematic estimates from a specific individual analyzed\nusing a possibly unique camera configuration. We extend our previous work using\nan implicit representation of trajectories optimized end-to-end through a\ndifferentiable biomechanical model to learn the posterior probability\ndistribution over pose given all the detected keypoints. This posterior\nprobability is learned through a variational approximation and estimates\nconfidence intervals for individual joints at each moment in a trial, showing\nconfidence intervals generally within 10-15 mm of spatial error for virtual\nmarker locations, consistent with our prior validation studies. Confidence\nintervals over joint angles are typically only a few degrees and widen for more\ndistal joints. The posterior also models the correlation structure over joint\nangles, such as correlations between hip and pelvis angles. The confidence\nintervals estimated through this method allow us to identify times and trials\nwhere kinematic uncertainty is high.\n","authors":["R. James Cotton","Fabian Sinz"],"pdf_url":"https://arxiv.org/pdf/2502.06486v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.10958v4","updated":"2025-02-10T14:02:49Z","published":"2024-11-17T04:35:49Z","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization","summary":"  Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrices $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK^\\top$. Third, we propose a two-level accumulation strategy\nfor $\\widetilde PV$ to enhance the accuracy of FP8 $\\widetilde PV$. The\noperations per second (OPS) of SageAttention2 surpass FlashAttention2 and\nxformers by about 3x and 4.5x on RTX4090, respectively. Moreover,\nSageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs,\nwhile delivering much higher accuracy. Comprehensive experiments confirm that\nour approach incurs negligible end-to-end metrics loss across diverse models,\nincluding those for language, image, and video generation. The code is\navailable at https://github.com/thu-ml/SageAttention.\n","authors":["Jintao Zhang","Haofeng Huang","Pengle Zhang","Jia Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2411.10958v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04896v2","updated":"2025-02-10T13:56:35Z","published":"2025-02-07T13:03:55Z","title":"Goku: Flow Based Video Generative Foundation Models","summary":"  This paper introduces Goku, a state-of-the-art family of joint\nimage-and-video generation models leveraging rectified flow Transformers to\nachieve industry-leading performance. We detail the foundational elements\nenabling high-quality visual generation, including the data curation pipeline,\nmodel architecture design, flow formulation, and advanced infrastructure for\nefficient and robust large-scale training. The Goku models demonstrate superior\nperformance in both qualitative and quantitative evaluations, setting new\nbenchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and\n83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for\ntext-to-video tasks. We believe that this work provides valuable insights and\npractical advancements for the research community in developing joint\nimage-and-video generation models.\n","authors":["Shoufa Chen","Chongjian Ge","Yuqi Zhang","Yida Zhang","Fengda Zhu","Hao Yang","Hongxiang Hao","Hui Wu","Zhichao Lai","Yifei Hu","Ting-Che Lin","Shilong Zhang","Fu Li","Chuan Li","Xing Wang","Yanghua Peng","Peize Sun","Ping Luo","Yi Jiang","Zehuan Yuan","Bingyue Peng","Xiaobing Liu"],"pdf_url":"https://arxiv.org/pdf/2502.04896v2.pdf","comment":"Demo: https://saiyan-world.github.io/goku/"},{"id":"http://arxiv.org/abs/2502.06476v1","updated":"2025-02-10T13:54:55Z","published":"2025-02-10T13:54:55Z","title":"Image Intrinsic Scale Assessment: Bridging the Gap Between Quality and\n  Resolution","summary":"  Image Quality Assessment (IQA) measures and predicts perceived image quality\nby human observers. Although recent studies have highlighted the critical\ninfluence that variations in the scale of an image have on its perceived\nquality, this relationship has not been systematically quantified. To bridge\nthis gap, we introduce the Image Intrinsic Scale (IIS), defined as the largest\nscale where an image exhibits its highest perceived quality. We also present\nthe Image Intrinsic Scale Assessment (IISA) task, which involves subjectively\nmeasuring and predicting the IIS based on human judgments. We develop a\nsubjective annotation methodology and create the IISA-DB dataset, comprising\n785 image-IIS pairs annotated by experts in a rigorously controlled\ncrowdsourcing study. Furthermore, we propose WIISA (Weak-labeling for Image\nIntrinsic Scale Assessment), a strategy that leverages how the IIS of an image\nvaries with downscaling to generate weak labels. Experiments show that applying\nWIISA during the training of several IQA methods adapted for IISA consistently\nimproves the performance compared to using only ground-truth labels. We will\nrelease the code, dataset, and pre-trained models upon acceptance.\n","authors":["Vlad Hosu","Lorenzo Agnolucci","Daisuke Iso","Dietmar Saupe"],"pdf_url":"https://arxiv.org/pdf/2502.06476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06474v1","updated":"2025-02-10T13:52:52Z","published":"2025-02-10T13:52:52Z","title":"UniMoD: Efficient Unified Multimodal Transformers with Mixture-of-Depths","summary":"  Unified multimodal transformers, which handle both generation and\nunderstanding tasks within a shared parameter space, have received increasing\nattention in recent research. Although various unified transformers have been\nproposed, training these models is costly due to redundant tokens and heavy\nattention computation. In the past, studies on large language models have\ndemonstrated that token pruning methods, such as Mixture of Depths (MoD), can\nsignificantly improve computational efficiency. MoD employs a router to select\nthe most important ones for processing within a transformer layer. However,\ndirectly applying MoD-based token pruning to unified transformers will result\nin suboptimal performance because different tasks exhibit varying levels of\ntoken redundancy. In our work, we analyze the unified transformers by (1)\nexamining attention weight patterns, (2) evaluating the layer importance and\ntoken redundancy, and (3) analyzing task interactions. Our findings reveal that\ntoken redundancy is primarily influenced by different tasks and layers.\nBuilding on these findings, we introduce UniMoD, a task-aware token pruning\nmethod that employs a separate router for each task to determine which tokens\nshould be pruned. We apply our method to Show-o and Emu3, reducing training\nFLOPs by approximately 15% in Show-o and 40% in Emu3, while maintaining or\nimproving performance on several benchmarks. Code will be released at\nhttps://github.com/showlab/UniMoD.\n","authors":["Weijia Mao","Zhenheng Yang","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2502.06474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03966v2","updated":"2025-02-10T13:41:39Z","published":"2025-02-06T10:59:44Z","title":"MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation","summary":"  In this paper, we present synthetic data generation framework for flood\nhazard detection system. For high fidelity and quality, we characterize several\nreal-world properties into virtual world and simulate the flood situation by\ncontrolling them. For the sake of efficiency, recent generative models in\nimage-to-3D and urban city synthesis are leveraged to easily composite flood\nenvironments so that we avoid data bias due to the hand-crafted manner. Based\non our framework, we build the flood synthetic dataset with 5 levels, dubbed\nMultiFloodSynth which contains rich annotation types like normal map,\nsegmentation, 3D bounding box for a variety of downstream task. In experiments,\nour dataset demonstrate the enhanced performance of flood hazard detection with\non-par realism compared with real dataset.\n","authors":["YoonJe Kang","Yonghoon Jung","Wonseop Shin","Bumsoo Kim","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2502.03966v2.pdf","comment":"6 pages, 6 figures. Accepted as Oral Presentation to AAAI 2025\n  Workshop on Good-Data"},{"id":"http://arxiv.org/abs/2502.06460v1","updated":"2025-02-10T13:41:35Z","published":"2025-02-10T13:41:35Z","title":"Group-CLIP Uncertainty Modeling for Group Re-Identification","summary":"  Group Re-Identification (Group ReID) aims matching groups of pedestrians\nacross non-overlapping cameras. Unlike single-person ReID, Group ReID focuses\nmore on the changes in group structure, emphasizing the number of members and\ntheir spatial arrangement. However, most methods rely on certainty-based\nmodels, which consider only the specific group structures in the group images,\noften failing to match unseen group configurations. To this end, we propose a\nnovel Group-CLIP UncertaintyModeling (GCUM) approach that adapts group text\ndescriptions to undetermined accommodate member and layout variations.\nSpecifically, we design a Member Variant Simulation (MVS)module that simulates\nmember exclusions using a Bernoulli distribution and a Group Layout Adaptation\n(GLA) module that generates uncertain group text descriptions with\nidentity-specific tokens. In addition, we design a Group\nRelationshipConstruction Encoder (GRCE) that uses group features to refine\nindividual features, and employ cross-modal contrastive loss to obtain\ngeneralizable knowledge from group text descriptions. It is worth noting that\nwe are the first to employ CLIP to GroupReID, and extensive experiments show\nthat GCUM significantly outperforms state-of-the-art Group ReID methods.\n","authors":["Qingxin Zhang","Haoyan Wei","Yang Qian"],"pdf_url":"https://arxiv.org/pdf/2502.06460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01895v2","updated":"2025-02-10T13:36:02Z","published":"2025-01-03T17:00:33Z","title":"EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation","summary":"  We introduce EnerVerse, a generative robotics foundation model that\nconstructs and interprets embodied spaces. EnerVerse employs an autoregressive\nvideo diffusion framework to predict future embodied spaces from instructions,\nenhanced by a sparse context memory for long-term reasoning. To model the 3D\nrobotics world, we propose Free Anchor Views (FAVs), a multi-view video\nrepresentation offering flexible, task-adaptive perspectives to address\nchallenges like motion ambiguity and environmental constraints. Additionally,\nwe present EnerVerse-D, a data engine pipeline combining the generative model\nwith 4D Gaussian Splatting, forming a self-reinforcing data loop to reduce the\nsim-to-real gap. Leveraging these innovations, EnerVerse translates 4D world\nrepresentations into physical actions via a policy head (EnerVerse-A), enabling\nrobots to execute task instructions. EnerVerse-A achieves state-of-the-art\nperformance in both simulation and real-world settings.\n","authors":["Siyuan Huang","Liliang Chen","Pengfei Zhou","Shengcong Chen","Zhengkai Jiang","Yue Hu","Yue Liao","Peng Gao","Hongsheng Li","Maoqing Yao","Guanghui Ren"],"pdf_url":"https://arxiv.org/pdf/2501.01895v2.pdf","comment":"Website: https://sites.google.com/view/enerverse"},{"id":"http://arxiv.org/abs/2502.06452v1","updated":"2025-02-10T13:31:32Z","published":"2025-02-10T13:31:32Z","title":"SparseFocus: Learning-based One-shot Autofocus for Microscopy with\n  Sparse Content","summary":"  Autofocus is necessary for high-throughput and real-time scanning in\nmicroscopic imaging. Traditional methods rely on complex hardware or iterative\nhill-climbing algorithms. Recent learning-based approaches have demonstrated\nremarkable efficacy in a one-shot setting, avoiding hardware modifications or\niterative mechanical lens adjustments. However, in this paper, we highlight a\nsignificant challenge that the richness of image content can significantly\naffect autofocus performance. When the image content is sparse, previous\nautofocus methods, whether traditional climbing-hill or learning-based, tend to\nfail. To tackle this, we propose a content-importance-based solution, named\nSparseFocus, featuring a novel two-stage pipeline. The first stage measures the\nimportance of regions within the image, while the second stage calculates the\ndefocus distance from selected important regions. To validate our approach and\nbenefit the research community, we collect a large-scale dataset comprising\nmillions of labelled defocused images, encompassing both dense, sparse and\nextremely sparse scenarios. Experimental results show that SparseFocus\nsurpasses existing methods, effectively handling all levels of content\nsparsity. Moreover, we integrate SparseFocus into our Whole Slide Imaging (WSI)\nsystem that performs well in real-world applications. The code and dataset will\nbe made available upon the publication of this paper.\n","authors":["Yongping Zhai","Xiaoxi Fu","Qiang Su","Jia Hu","Yake Zhang","Yunfeng Zhou","Chaofan Zhang","Xiao Li","Wenxin Wang","Dongdong Wu","Shen Yan"],"pdf_url":"https://arxiv.org/pdf/2502.06452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.02006v2","updated":"2025-02-10T13:25:26Z","published":"2024-12-02T22:23:43Z","title":"Unveiling Interpretability in Self-Supervised Speech Representations for\n  Parkinson's Diagnosis","summary":"  Recent works in pathological speech analysis have increasingly relied on\npowerful self-supervised speech representations, leading to promising results.\nHowever, the complex, black-box nature of these embeddings and the limited\nresearch on their interpretability significantly restrict their adoption for\nclinical diagnosis. To address this gap, we propose a novel, interpretable\nframework specifically designed to support Parkinson's Disease (PD) diagnosis.\nThrough the design of simple yet effective cross-attention mechanisms for both\nembedding- and temporal-level analysis, the proposed framework offers\ninterpretability from two distinct but complementary perspectives. Experimental\nfindings across five well-established speech benchmarks for PD detection\ndemonstrate the framework's capability to identify meaningful speech patterns\nwithin self-supervised representations for a wide range of assessment tasks.\nFine-grained temporal analyses further underscore its potential to enhance the\ninterpretability of deep-learning pathological speech models, paving the way\nfor the development of more transparent, trustworthy, and clinically applicable\ncomputer-assisted diagnosis systems in this domain. Moreover, in terms of\nclassification accuracy, our method achieves results competitive with\nstate-of-the-art approaches, while also demonstrating robustness in\ncross-lingual scenarios when applied to spontaneous speech production.\n","authors":["David Gimeno-Gómez","Catarina Botelho","Anna Pompili","Alberto Abad","Carlos-D. Martínez-Hinarejos"],"pdf_url":"https://arxiv.org/pdf/2412.02006v2.pdf","comment":"Accepted in the Special Issue on \"Modelling and Processing Language\n  and Speech in Neurodegenerative Disorders\" published by Journal of Selected\n  Topics in Signal Processing (JSTSP)"},{"id":"http://arxiv.org/abs/2502.06445v1","updated":"2025-02-10T13:20:19Z","published":"2025-02-10T13:20:19Z","title":"Benchmarking Vision-Language Models on Optical Character Recognition in\n  Dynamic Video Environments","summary":"  This paper introduces an open-source benchmark for evaluating Vision-Language\nModels (VLMs) on Optical Character Recognition (OCR) tasks in dynamic video\nenvironments. We present a curated dataset containing 1,477 manually annotated\nframes spanning diverse domains, including code editors, news broadcasts,\nYouTube videos, and advertisements. Three state of the art VLMs - Claude-3,\nGemini-1.5, and GPT-4o are benchmarked against traditional OCR systems such as\nEasyOCR and RapidOCR. Evaluation metrics include Word Error Rate (WER),\nCharacter Error Rate (CER), and Accuracy. Our results highlight the strengths\nand limitations of VLMs in video-based OCR tasks, demonstrating their potential\nto outperform conventional OCR models in many scenarios. However, challenges\nsuch as hallucinations, content security policies, and sensitivity to occluded\nor stylized text remain. The dataset and benchmarking framework are publicly\navailable to foster further research.\n","authors":["Sankalp Nagaonkar","Augustya Sharma","Ashish Choithani","Ashutosh Trivedi"],"pdf_url":"https://arxiv.org/pdf/2502.06445v1.pdf","comment":"Code and dataset: https://github.com/video-db/ocr-benchmark"},{"id":"http://arxiv.org/abs/2502.06434v1","updated":"2025-02-10T13:11:40Z","published":"2025-02-10T13:11:40Z","title":"Rethinking Large-scale Dataset Compression: Shifting Focus From Labels\n  to Images","summary":"  Dataset distillation and dataset pruning are two prominent techniques for\ncompressing datasets to improve computational and storage efficiency. Despite\ntheir overlapping objectives, these approaches are rarely compared directly.\nEven within each field, the evaluation protocols are inconsistent across\nvarious methods, which complicates fair comparisons and hinders\nreproducibility. Considering these limitations, we introduce in this paper a\nbenchmark that equitably evaluates methodologies across both distillation and\npruning literatures. Notably, our benchmark reveals that in the mainstream\ndataset distillation setting for large-scale datasets, which heavily rely on\nsoft labels from pre-trained models, even randomly selected subsets can achieve\nsurprisingly competitive performance. This finding suggests that an\noveremphasis on soft labels may be diverting attention from the intrinsic value\nof the image data, while also imposing additional burdens in terms of\ngeneration, storage, and application. To address these issues, we propose a new\nframework for dataset compression, termed Prune, Combine, and Augment (PCA),\nwhich focuses on leveraging image data exclusively, relies solely on hard\nlabels for evaluation, and achieves state-of-the-art performance in this setup.\nBy shifting the emphasis back to the images, our benchmark and PCA framework\npave the way for more balanced and accessible techniques in dataset compression\nresearch. Our code is available at:\nhttps://github.com/ArmandXiao/Rethinking-Dataset-Compression\n","authors":["Lingao Xiao","Songhua Liu","Yang He","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06434v1.pdf","comment":"Work In Progress"},{"id":"http://arxiv.org/abs/2502.01189v3","updated":"2025-02-10T13:11:20Z","published":"2025-02-03T09:25:57Z","title":"Compressed Image Generation with Denoising Diffusion Codebook Models","summary":"  We present a novel generative approach based on Denoising Diffusion Models\n(DDMs), which produces high-quality image samples along with their losslessly\ncompressed bit-stream representations. This is obtained by replacing the\nstandard Gaussian noise sampling in the reverse diffusion with a selection of\nnoise samples from pre-defined codebooks of fixed iid Gaussian vectors.\nSurprisingly, we find that our method, termed Denoising Diffusion Codebook\nModel (DDCM), retains sample quality and diversity of standard DDMs, even for\nextremely small codebooks. We leverage DDCM and pick the noises from the\ncodebooks that best match a given image, converting our generative model into a\nhighly effective lossy image codec achieving state-of-the-art perceptual image\ncompression results. More generally, by setting other noise selections rules,\nwe extend our compression method to any conditional image generation task\n(e.g., image restoration), where the generated images are produced jointly with\ntheir condensed bit-stream representations. Our work is accompanied by a\nmathematical interpretation of the proposed compressed conditional generation\nschemes, establishing a connection with score-based approximations of posterior\nsamplers for the tasks considered.\n","authors":["Guy Ohayon","Hila Manor","Tomer Michaeli","Michael Elad"],"pdf_url":"https://arxiv.org/pdf/2502.01189v3.pdf","comment":"Code and demo are available at https://ddcm-2025.github.io/"},{"id":"http://arxiv.org/abs/2409.12784v7","updated":"2025-02-10T13:10:32Z","published":"2024-09-19T13:51:21Z","title":"Evaluating Image Hallucination in Text-to-Image Generation with\n  Question-Answering","summary":"  Despite the impressive success of text-to-image (TTI) generation models,\nexisting studies overlook the issue of whether these models accurately convey\nfactual information. In this paper, we focus on the problem of image\nhallucination, where images created by generation models fail to faithfully\ndepict factual content. To address this, we introduce I-HallA (Image\nHallucination evaluation with Question Answering), a novel automated evaluation\nmetric that measures the factuality of generated images through visual question\nanswering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset\nfor this purpose. As part of this process, we develop a pipeline that generates\nhigh-quality question-answer pairs using multiple GPT-4 Omni-based agents, with\nhuman judgments to ensure accuracy. Our evaluation protocols measure image\nhallucination by testing if images from existing TTI models can correctly\nrespond to these questions. The I-HallA v1.0 dataset comprises 1.2K diverse\nimage-text pairs across nine categories with 1,000 rigorously curated questions\ncovering various compositional challenges. We evaluate five TTI models using\nI-HallA and reveal that these state-of-the-art models often fail to accurately\nconvey factual information. Moreover, we validate the reliability of our metric\nby demonstrating a strong Spearman correlation ($\\rho$=0.95) with human\njudgments. We believe our benchmark dataset and metric can serve as a\nfoundation for developing factually accurate TTI generation models. Additional\nresources can be found on our project page: https://sgt-lim.github.io/I-HallA/.\n","authors":["Youngsun Lim","Hojun Choi","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2409.12784v7.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2502.06432v1","updated":"2025-02-10T13:09:47Z","published":"2025-02-10T13:09:47Z","title":"Prompt-SID: Learning Structural Representation Prompt via Latent\n  Diffusion for Single-Image Denoising","summary":"  Many studies have concentrated on constructing supervised models utilizing\npaired datasets for image denoising, which proves to be expensive and\ntime-consuming. Current self-supervised and unsupervised approaches typically\nrely on blind-spot networks or sub-image pairs sampling, resulting in pixel\ninformation loss and destruction of detailed structural information, thereby\nsignificantly constraining the efficacy of such methods. In this paper, we\nintroduce Prompt-SID, a prompt-learning-based single image denoising framework\nthat emphasizes preserving of structural details. This approach is trained in a\nself-supervised manner using downsampled image pairs. It captures\noriginal-scale image information through structural encoding and integrates\nthis prompt into the denoiser. To achieve this, we propose a structural\nrepresentation generation model based on the latent diffusion process and\ndesign a structural attention module within the transformer-based denoiser\narchitecture to decode the prompt. Additionally, we introduce a scale replay\ntraining mechanism, which effectively mitigates the scale gap from images of\ndifferent resolutions. We conduct comprehensive experiments on synthetic,\nreal-world, and fluorescence imaging datasets, showcasing the remarkable\neffectiveness of Prompt-SID.\n","authors":["Huaqiu Li","Wang Zhang","Xiaowan Hu","Tao Jiang","Zikang Chen","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06431v1","updated":"2025-02-10T13:08:57Z","published":"2025-02-10T13:08:57Z","title":"FCVSR: A Frequency-aware Method for Compressed Video Super-Resolution","summary":"  Compressed video super-resolution (SR) aims to generate high-resolution (HR)\nvideos from the corresponding low-resolution (LR) compressed videos. Recently,\nsome compressed video SR methods attempt to exploit the spatio-temporal\ninformation in the frequency domain, showing great promise in super-resolution\nperformance. However, these methods do not differentiate various frequency\nsubbands spatially or capture the temporal frequency dynamics, potentially\nleading to suboptimal results. In this paper, we propose a deep frequency-based\ncompressed video SR model (FCVSR) consisting of a motion-guided adaptive\nalignment (MGAA) network and a multi-frequency feature refinement (MFFR)\nmodule. Additionally, a frequency-aware contrastive loss is proposed for\ntraining FCVSR, in order to reconstruct finer spatial details. The proposed\nmodel has been evaluated on three public compressed video super-resolution\ndatasets, with results demonstrating its effectiveness when compared to\nexisting works in terms of super-resolution performance (up to a 0.14dB gain in\nPSNR over the second-best model) and complexity.\n","authors":["Qiang Zhu","Fan Zhang","Feiyu Chen","Shuyuan Zhu","David Bull","Bing Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.06431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15740v2","updated":"2025-02-10T13:03:13Z","published":"2024-08-28T12:06:11Z","title":"MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with\n  Attention Mamba Mechanisms","summary":"  Vision Language Place Recognition (VLVPR) enhances robot localization\nperformance by incorporating natural language descriptions from images. By\nutilizing language information, VLVPR directs robot place matching, overcoming\nthe constraint of solely depending on vision. The essence of multimodal fusion\nlies in mining the complementary information between different modalities.\nHowever, general fusion methods rely on traditional neural architectures and\nare not well equipped to capture the dynamics of cross modal interactions,\nespecially in the presence of complex intra modal and inter modal correlations.\nTo this end, this paper proposes a novel coarse to fine and end to end\nconnected cross modal place recognition framework, called MambaPlace. In the\ncoarse localization stage, the text description and 3D point cloud are encoded\nby the pretrained T5 and instance encoder, respectively. They are then\nprocessed using Text Attention Mamba (TAM) and Point Clouds Mamba (PCM) for\ndata enhancement and alignment. In the subsequent fine localization stage, the\nfeatures of the text description and 3D point cloud are cross modally fused and\nfurther enhanced through cascaded Cross Attention Mamba (CCAM). Finally, we\npredict the positional offset from the fused text point cloud features,\nachieving the most accurate localization. Extensive experiments show that\nMambaPlace achieves improved localization accuracy on the KITTI360Pose dataset\ncompared to the state of the art methods.\n","authors":["Tianyi Shang","Zhenyu Li","Pengjie Xu"],"pdf_url":"https://arxiv.org/pdf/2408.15740v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.06428v1","updated":"2025-02-10T13:03:05Z","published":"2025-02-10T13:03:05Z","title":"CoS: Chain-of-Shot Prompting for Long Video Understanding","summary":"  Multi-modal Large Language Models (MLLMs) struggle with long videos due to\nthe need for excessive visual tokens. These tokens exceed massively the context\nlength of MLLMs, resulting in filled by redundant task-irrelevant shots. How to\nselect shots is an unsolved critical problem: sparse sampling risks missing key\ndetails, while exhaustive sampling overwhelms the model with irrelevant\ncontent, leading to video misunderstanding. To solve this problem, we propose\nChain-of-Shot prompting (CoS). The key idea is to frame shot selection as\ntest-time visual prompt optimisation, choosing shots adaptive to video\nunderstanding semantic task by optimising shots-task alignment. CoS has two key\nparts: (1) a binary video summary mechanism that performs pseudo temporal\ngrounding, discovering a binary coding to identify task-relevant shots, and (2)\na video co-reasoning module that deploys the binary coding to pair (learning to\nalign) task-relevant positive shots with irrelevant negative shots. It embeds\nthe optimised shot selections into the original video, facilitating a focus on\nrelevant context to optimize long video understanding. Experiments across three\nbaselines and five datasets demonstrate the effectiveness and adaptability of\nCoS. Code given in https://lwpyh.github.io/CoS.\n","authors":["Jian Hu","Zixu Cheng","Chenyang Si","Wei Li","Shaogang Gong"],"pdf_url":"https://arxiv.org/pdf/2502.06428v1.pdf","comment":"A training-free test-time optimisation approach for long video\n  understanding"},{"id":"http://arxiv.org/abs/2502.06427v1","updated":"2025-02-10T13:02:19Z","published":"2025-02-10T13:02:19Z","title":"Hybrid State-Space and GRU-based Graph Tokenization Mamba for\n  Hyperspectral Image Classification","summary":"  Hyperspectral image (HSI) classification plays a pivotal role in domains such\nas environmental monitoring, agriculture, and urban planning. However, it faces\nsignificant challenges due to the high-dimensional nature of the data and the\ncomplex spectral-spatial relationships inherent in HSI. Traditional methods,\nincluding conventional machine learning and convolutional neural networks\n(CNNs), often struggle to effectively capture these intricate spectral-spatial\nfeatures and global contextual information. Transformer-based models, while\npowerful in capturing long-range dependencies, often demand substantial\ncomputational resources, posing challenges in scenarios where labeled datasets\nare limited, as is commonly seen in HSI applications. To overcome these\nchallenges, this work proposes GraphMamba, a hybrid model that combines\nspectral-spatial token generation, graph-based token prioritization, and\ncross-attention mechanisms. The model introduces a novel hybridization of\nstate-space modeling and Gated Recurrent Units (GRU), capturing both linear and\nnonlinear spatial-spectral dynamics. GraphMamba enhances the ability to model\ncomplex spatial-spectral relationships while maintaining scalability and\ncomputational efficiency across diverse HSI datasets. Through comprehensive\nexperiments, we demonstrate that GraphMamba outperforms existing\nstate-of-the-art models, offering a scalable and robust solution for complex\nHSI classification tasks.\n","authors":["Muhammad Ahmad","Muhammad Hassaan Farooq Butt","Muhammad Usama","Manuel Mazzara","Salvatore Distefano","Adil Mehmood Khan","Danfeng Hong"],"pdf_url":"https://arxiv.org/pdf/2502.06427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.02237v2","updated":"2025-02-10T12:57:09Z","published":"2024-12-03T08:05:56Z","title":"Cross-Attention Head Position Patterns Can Align with Human Visual\n  Concepts in Text-to-Image Generative Models","summary":"  Recent text-to-image diffusion models leverage cross-attention layers, which\nhave been effectively utilized to enhance a range of visual generative tasks.\nHowever, our understanding of cross-attention layers remains somewhat limited.\nIn this study, we introduce a mechanistic interpretability approach for\ndiffusion models by constructing Head Relevance Vectors (HRVs) that align with\nhuman-specified visual concepts. An HRV for a given visual concept has a length\nequal to the total number of cross-attention heads, with each element\nrepresenting the importance of the corresponding head for the given visual\nconcept. To validate HRVs as interpretable features, we develop an ordered\nweakening analysis that demonstrates their effectiveness. Furthermore, we\npropose concept strengthening and concept adjusting methods and apply them to\nenhance three visual generative tasks. Our results show that HRVs can reduce\nmisinterpretations of polysemous words in image generation, successfully modify\nfive challenging attributes in image editing, and mitigate catastrophic neglect\nin multi-concept generation. Overall, our work provides an advancement in\nunderstanding cross-attention layers and introduces new approaches for\nfine-controlling these layers at the head level.\n","authors":["Jungwon Park","Jungmin Ko","Dongnam Byun","Jangwon Suh","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2412.02237v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.06418v1","updated":"2025-02-10T12:55:08Z","published":"2025-02-10T12:55:08Z","title":"Robust Watermarks Leak: Channel-Aware Feature Extraction Enables\n  Adversarial Watermark Manipulation","summary":"  Watermarking plays a key role in the provenance and detection of AI-generated\ncontent. While existing methods prioritize robustness against real-world\ndistortions (e.g., JPEG compression and noise addition), we reveal a\nfundamental tradeoff: such robust watermarks inherently improve the redundancy\nof detectable patterns encoded into images, creating exploitable information\nleakage. To leverage this, we propose an attack framework that extracts leakage\nof watermark patterns through multi-channel feature learning using a\npre-trained vision model. Unlike prior works requiring massive data or detector\naccess, our method achieves both forgery and detection evasion with a single\nwatermarked image. Extensive experiments demonstrate that our method achieves a\n60\\% success rate gain in detection evasion and 51\\% improvement in forgery\naccuracy compared to state-of-the-art methods while maintaining visual\nfidelity. Our work exposes the robustness-stealthiness paradox: current\n\"robust\" watermarks sacrifice security for distortion resistance, providing\ninsights for future watermark design.\n","authors":["Zhongjie Ba","Yitao Zhang","Peng Cheng","Bin Gong","Xinyu Zhang","Qinglong Wang","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2502.06418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12121v2","updated":"2025-02-10T12:26:06Z","published":"2024-07-16T19:15:07Z","title":"FoodMem: Near Real-time and Precise Food Video Segmentation","summary":"  Food segmentation, including in videos, is vital for addressing real-world\nhealth, agriculture, and food biotechnology issues. Current limitations lead to\ninaccurate nutritional analysis, inefficient crop management, and suboptimal\nfood processing, impacting food security and public health. Improving\nsegmentation techniques can enhance dietary assessments, agricultural\nproductivity, and the food production process. This study introduces the\ndevelopment of a robust framework for high-quality, near-real-time segmentation\nand tracking of food items in videos, using minimal hardware resources. We\npresent FoodMem, a novel framework designed to segment food items from video\nsequences of 360-degree unbounded scenes. FoodMem can consistently generate\nmasks of food portions in a video sequence, overcoming the limitations of\nexisting semantic segmentation models, such as flickering and prohibitive\ninference speeds in video processing contexts. To address these issues, FoodMem\nleverages a two-phase solution: a transformer segmentation phase to create\ninitial segmentation masks and a memory-based tracking phase to monitor food\nmasks in complex scenes. Our framework outperforms current state-of-the-art\nfood segmentation models, yielding superior performance across various\nconditions, such as camera angles, lighting, reflections, scene complexity, and\nfood diversity. This results in reduced segmentation noise, elimination of\nartifacts, and completion of missing segments. Here, we also introduce a new\nannotated food dataset encompassing challenging scenarios absent in previous\nbenchmarks. Extensive experiments conducted on MetaFood3D, Nutrition5k, and\nVegetables & Fruits datasets demonstrate that FoodMem enhances the\nstate-of-the-art by 2.5% mean average precision in food video segmentation and\nis 58 x faster on average.\n","authors":["Ahmad AlMughrabi","Adrián Galán","Ricardo Marques","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2407.12121v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06392v1","updated":"2025-02-10T12:26:02Z","published":"2025-02-10T12:26:02Z","title":"TANGLED: Generating 3D Hair Strands from Images with Arbitrary Styles\n  and Viewpoints","summary":"  Hairstyles are intricate and culturally significant with various geometries,\ntextures, and structures. Existing text or image-guided generation methods fail\nto handle the richness and complexity of diverse styles. We present TANGLED, a\nnovel approach for 3D hair strand generation that accommodates diverse image\ninputs across styles, viewpoints, and quantities of input views. TANGLED\nemploys a three-step pipeline. First, our MultiHair Dataset provides 457\ndiverse hairstyles annotated with 74 attributes, emphasizing complex and\nculturally significant styles to improve model generalization. Second, we\npropose a diffusion framework conditioned on multi-view linearts that can\ncapture topological cues (e.g., strand density and parting lines) while\nfiltering out noise. By leveraging a latent diffusion model with\ncross-attention on lineart features, our method achieves flexible and robust 3D\nhair generation across diverse input conditions. Third, a parametric\npost-processing module enforces braid-specific constraints to maintain\ncoherence in complex structures. This framework not only advances hairstyle\nrealism and diversity but also enables culturally inclusive digital avatars and\nnovel applications like sketch-based 3D strand editing for animation and\naugmented reality.\n","authors":["Pengyu Long","Zijun Zhao","Min Ouyang","Qingcheng Zhao","Qixuan Zhang","Wei Yang","Lan Xu","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2502.06392v1.pdf","comment":"Project Page: https://sites.google.com/view/tangled1"},{"id":"http://arxiv.org/abs/2502.06390v1","updated":"2025-02-10T12:20:08Z","published":"2025-02-10T12:20:08Z","title":"When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks\n  for VLMs","summary":"  Vision-Language Models (VLMs) have gained considerable prominence in recent\nyears due to their remarkable capability to effectively integrate and process\nboth textual and visual information. This integration has significantly\nenhanced performance across a diverse spectrum of applications, such as scene\nperception and robotics. However, the deployment of VLMs has also given rise to\ncritical safety and security concerns, necessitating extensive research to\nassess the potential vulnerabilities these VLM systems may harbor. In this\nwork, we present an in-depth survey of the attack strategies tailored for VLMs.\nWe categorize these attacks based on their underlying objectives - namely\njailbreak, camouflage, and exploitation - while also detailing the various\nmethodologies employed for data manipulation of VLMs. Meanwhile, we outline\ncorresponding defense mechanisms that have been proposed to mitigate these\nvulnerabilities. By discerning key connections and distinctions among the\ndiverse types of attacks, we propose a compelling taxonomy for VLM attacks.\nMoreover, we summarize the evaluation metrics that comprehensively describe the\ncharacteristics and impact of different attacks on VLMs. Finally, we conclude\nwith a discussion of promising future research directions that could further\nenhance the robustness and safety of VLMs, emphasizing the importance of\nongoing exploration in this critical area of study. To facilitate community\nengagement, we maintain an up-to-date project page, accessible at:\nhttps://github.com/AobtDai/VLM_Attack_Paper_List.\n","authors":["Aobotao Dai","Xinyu Ma","Lei Chen","Songze Li","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10929v7","updated":"2025-02-10T12:09:23Z","published":"2024-10-14T16:35:27Z","title":"ASTM :Autonomous Smart Traffic Management System Using Artificial\n  Intelligence CNN and LSTM","summary":"  In the modern world, the development of Artificial Intelligence (AI) has\ncontributed to improvements in various areas, including automation, computer\nvision, fraud detection, and more. AI can be leveraged to enhance the\nefficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce\ntraffic congestion rates. This paper presents an Autonomous Smart Traffic\nManagement (STM) system that uses AI to improve traffic flow rates. The system\nemploys the YOLO V5 Convolutional Neural Network to detect vehicles in traffic\nmanagement images. Additionally, it predicts the number of vehicles for the\nnext 12 hours using a Recurrent Neural Network with Long Short-Term Memory\n(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the\ntraffic cycle length based on these vehicle predictions, aided by AI. From the\nresults of the RNN-LSTM model for predicting vehicle numbers over the next 12\nhours, we observe that the model predicts traffic with a Mean Squared Error\n(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.\nAfter simulating the STM system in the CARLA simulation environment, we found\nthat the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per\nminute) is 50\\% higher than the rate without STM (around 15 vehicles per\nminute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5\nseconds per vehicle) is 70\\% lower than without STM (around 12 seconds per\nvehicle). These results demonstrate that the STM system using AI can increase\ntraffic flow by 50\\% and reduce vehicle pass delays by 70\\%.\n","authors":["Christofel Rio Goenawan"],"pdf_url":"https://arxiv.org/pdf/2410.10929v7.pdf","comment":"Novel Autonomous Smart Traffic Management System using End-to-End\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2502.06380v1","updated":"2025-02-10T12:01:05Z","published":"2025-02-10T12:01:05Z","title":"Structure-preserving contrastive learning for spatial time series","summary":"  Informative representations enhance model performance and generalisability in\ndownstream tasks. However, learning self-supervised representations for\nspatially characterised time series, like traffic interactions, poses\nchallenges as it requires maintaining fine-grained similarity relations in the\nlatent space. In this study, we incorporate two structure-preserving\nregularisers for the contrastive learning of spatial time series: one\nregulariser preserves the topology of similarities between instances, and the\nother preserves the graph geometry of similarities across spatial and temporal\ndimensions. To balance contrastive learning and structure preservation, we\npropose a dynamic mechanism that adaptively weighs the trade-off and stabilises\ntraining. We conduct experiments on multivariate time series classification, as\nwell as macroscopic and microscopic traffic prediction. For all three tasks,\nour approach preserves the structures of similarity relations more effectively\nand improves state-of-the-art task performances. The proposed approach can be\napplied to an arbitrary encoder and is particularly beneficial for time series\nwith spatial or geographical features. Furthermore, this study suggests that\nhigher similarity structure preservation indicates more informative and useful\nrepresentations. This may help to understand the contribution of representation\nlearning in pattern recognition with neural networks. Our code is made openly\naccessible with all resulting data at https://github.com/yiru-jiao/spclt.\n","authors":["Yiru Jiao","Sander van Cranenburgh","Simeon Calvert","Hans van Lint"],"pdf_url":"https://arxiv.org/pdf/2502.06380v1.pdf","comment":"TL;DR: Preserving certain structures of similarity relations in\n  spatio-temporal data can improve downstream task performance via contrastive\n  learning"},{"id":"http://arxiv.org/abs/2408.14823v2","updated":"2025-02-10T11:59:52Z","published":"2024-08-27T07:06:49Z","title":"LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive\n  Streaming","summary":"  The rise of Extended Reality (XR) requires efficient streaming of 3D online\nworlds, challenging current 3DGS representations to adapt to\nbandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS\nthat supports adaptive streaming and progressive rendering. Our method\nconstructs a layered structure for cumulative representation, incorporates\ndynamic opacity optimization to maintain visual fidelity, and utilizes\noccupancy maps to efficiently manage Gaussian splats. This proposed model\noffers a progressive representation supporting a continuous rendering quality\nadapted for bandwidth-aware streaming. Extensive experiments validate the\neffectiveness of our approach in balancing visual fidelity with the compactness\nof the model, with up to 50.71% improvement in SSIM, 286.53% improvement in\nLPIPS with 23% of the original model size, and shows its potential for\nbandwidth-adapted 3D streaming and rendering applications.\n","authors":["Yuang Shi","Géraldine Morin","Simone Gasparini","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2408.14823v2.pdf","comment":"3DV 2025; Project Page: https://yuang-ian.github.io/lapisgs/ ; Code:\n  https://github.com/nus-vv-streams/lapis-gs"},{"id":"http://arxiv.org/abs/2502.06376v1","updated":"2025-02-10T11:56:02Z","published":"2025-02-10T11:56:02Z","title":"Many-Task Federated Fine-Tuning via Unified Task Vectors","summary":"  Federated Learning (FL) traditionally assumes homogeneous client tasks;\nhowever, in real-world scenarios, clients often specialize in diverse tasks,\nintroducing task heterogeneity. To address this challenge, Many-Task FL\n(MaT-FL) has emerged, enabling clients to collaborate effectively despite task\ndiversity. Existing MaT-FL approaches rely on client grouping or personalized\nlayers, requiring the server to manage individual models and failing to account\nfor clients handling multiple tasks. We propose MaTU, a MaT-FL approach that\nenables joint learning of task vectors across clients, eliminating the need for\nclustering or client-specific weight storage at the server. Our method\nintroduces a novel aggregation mechanism that determines task similarity based\non the direction of clients task vectors and constructs a unified task vector\nencapsulating all tasks. To address task-specific requirements, we augment the\nunified task vector with lightweight modulators that facilitate knowledge\ntransfer among related tasks while disentangling dissimilar ones. Evaluated\nacross 30 datasets, MaTU achieves superior performance over state-of-the-art\nMaT-FL approaches, with results comparable to per-task fine-tuning, while\ndelivering significant communication savings.\n","authors":["Vasileios Tsouvalas","Tanir Ozcelebi","Nirvana Meratnia"],"pdf_url":"https://arxiv.org/pdf/2502.06376v1.pdf","comment":"10 pages, 6 figures, submitted in IJCAI 2025"},{"id":"http://arxiv.org/abs/2502.06367v1","updated":"2025-02-10T11:36:45Z","published":"2025-02-10T11:36:45Z","title":"FOCUS - Multi-View Foot Reconstruction From Synthetically Trained Dense\n  Correspondences","summary":"  Surface reconstruction from multiple, calibrated images is a challenging task\n- often requiring a large number of collected images with significant overlap.\nWe look at the specific case of human foot reconstruction. As with previous\nsuccessful foot reconstruction work, we seek to extract rich per-pixel geometry\ncues from multi-view RGB images, and fuse these into a final 3D object. Our\nmethod, FOCUS, tackles this problem with 3 main contributions: (i) SynFoot2, an\nextension of an existing synthetic foot dataset to include a new data type:\ndense correspondence with the parameterized foot model FIND; (ii) an\nuncertainty-aware dense correspondence predictor trained on our synthetic\ndataset; (iii) two methods for reconstructing a 3D surface from dense\ncorrespondence predictions: one inspired by Structure-from-Motion, and one\noptimization-based using the FIND model. We show that our reconstruction\nachieves state-of-the-art reconstruction quality in a few-view setting,\nperforming comparably to state-of-the-art when many views are available, and\nruns substantially faster. We release our synthetic dataset to the research\ncommunity. Code is available at: https://github.com/OllieBoyne/FOCUS\n","authors":["Oliver Boyne","Roberto Cipolla"],"pdf_url":"https://arxiv.org/pdf/2502.06367v1.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.06354v1","updated":"2025-02-10T11:09:31Z","published":"2025-02-10T11:09:31Z","title":"Guidance-base Diffusion Models for Improving Photoacoustic Image Quality","summary":"  Photoacoustic(PA) imaging is a non-destructive and non-invasive technology\nfor visualizing minute blood vessel structures in the body using ultrasonic\nsensors. In PA imaging, the image quality of a single-shot image is poor, and\nit is necessary to improve the image quality by averaging many single-shot\nimages. Therefore, imaging the entire subject requires high imaging costs. In\nour study, we propose a method to improve the quality of PA images using\ndiffusion models. In our method, we improve the reverse diffusion process using\nsensor information of PA imaging and introduce a guidance method using imaging\ncondition information to generate high-quality images.\n","authors":["Tatsuhiro Eguchi","Shumpei Takezaki","Mihoko Shimano","Takayuki Yagi","Ryoma Bise"],"pdf_url":"https://arxiv.org/pdf/2502.06354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06352v1","updated":"2025-02-10T11:05:18Z","published":"2025-02-10T11:05:18Z","title":"LANTERN++: Enhanced Relaxed Speculative Decoding with Static Tree\n  Drafting for Visual Auto-regressive Models","summary":"  Speculative decoding has been widely used to accelerate autoregressive (AR)\ntext generation. However, its effectiveness in visual AR models remains limited\ndue to token selection ambiguity, where multiple tokens receive similarly low\nprobabilities, reducing acceptance rates. While dynamic tree drafting has been\nproposed to improve speculative decoding, we show that it fails to mitigate\ntoken selection ambiguity, resulting in shallow draft trees and suboptimal\nacceleration. To address this, we introduce LANTERN++, a novel framework that\nintegrates static tree drafting with a relaxed acceptance condition, allowing\ndrafts to be selected independently of low-confidence predictions. This enables\ndeeper accepted sequences, improving decoding efficiency while preserving image\nquality. Extensive experiments on state-of-the-art visual AR models demonstrate\nthat LANTERN++ significantly accelerates inference, achieving up to\n$\\mathbf{\\times 2.56}$ speedup over standard AR decoding while maintaining high\nimage quality.\n","authors":["Sihwan Park","Doohyuk Jang","Sungyub Kim","Souvik Kundu","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2502.06352v1.pdf","comment":"15 pages, 5 figures, short paper (5 pages exclude reference and\n  appendix)"},{"id":"http://arxiv.org/abs/2405.01088v2","updated":"2025-02-10T10:52:42Z","published":"2024-05-02T08:33:43Z","title":"Type2Branch: Keystroke Biometrics based on a Dual-branch Architecture\n  with Attention Mechanisms and Set2set Loss","summary":"  In 2021, the pioneering work TypeNet showed that keystroke dynamics\nverification could scale to hundreds of thousands of users with minimal\nperformance degradation. Recently, the KVC-onGoing competition has provided an\nopen and robust experimental protocol for evaluating keystroke dynamics\nverification systems of such scale. %, including considerations of algorithmic\nfairness. This article describes Type2Branch, the model and techniques that\nachieved the lowest error rates at the KVC-onGoing, in both desktop and mobile\ntyping scenarios. The novelty aspects of the proposed Type2Branch include: i)\nsynthesized timing features emphasizing user behavior deviation from the\ngeneral population, ii) a dual-branch architecture combining recurrent and\nconvolutional paths with various attention mechanisms, iii) a new loss function\nnamed Set2set that captures the global structure of the embedding space, and\niv) a training curriculum of increasing difficulty. Considering five enrollment\nsamples per subject of approximately 50 characters typed, the proposed\nType2Branch achieves state-of-the-art performance with mean per-subject Equal\nError Rates (EERs) of 0.77% and 1.03% on evaluation sets of respectively 15,000\nand 5,000 subjects for desktop and mobile scenarios. With a fixed global\nthreshold for all subjects, the EERs are respectively 3.25% and 3.61% for\ndesktop and mobile scenarios, outperforming previous approaches by a\nsignificant margin. The source code for dataset generation, model, and training\nprocess is publicly available.\n","authors":["Nahuel González","Giuseppe Stragapede","Rubén Vera-Rodriguez","Rubén Tolosana"],"pdf_url":"https://arxiv.org/pdf/2405.01088v2.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.06341v1","updated":"2025-02-10T10:43:55Z","published":"2025-02-10T10:43:55Z","title":"Facial Analysis Systems and Down Syndrome","summary":"  The ethical, social and legal issues surrounding facial analysis technologies\nhave been widely debated in recent years. Key critics have argued that these\ntechnologies can perpetuate bias and discrimination, particularly against\nmarginalized groups. We contribute to this field of research by reporting on\nthe limitations of facial analysis systems with the faces of people with Down\nsyndrome: this particularly vulnerable group has received very little attention\nin the literature so far. This study involved the creation of a specific\ndataset of face images. An experimental group with faces of people with Down\nsyndrome, and a control group with faces of people who are not affected by the\nsyndrome. Two commercial tools were tested on the dataset, along three tasks:\ngender recognition, age prediction and face labelling. The results show an\noverall lower accuracy of prediction in the experimental group, and other\nspecific patterns of performance differences: i) high error rates in gender\nrecognition in the category of males with Down syndrome; ii) adults with Down\nsyndrome were more often incorrectly labelled as children; iii) social\nstereotypes are propagated in both the control and experimental groups, with\nlabels related to aesthetics more often associated with women, and labels\nrelated to education level and skills more often associated with men. These\nresults, although limited in scope, shed new light on the biases that alter\nface classification when applied to faces of people with Down syndrome. They\nconfirm the structural limitation of the technology, which is inherently\ndependent on the datasets used to train the models.\n","authors":["Marco Rondina","Fabiana Vinci","Antonio Vetrò","Juan Carlos De Martin"],"pdf_url":"https://arxiv.org/pdf/2502.06341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06338v1","updated":"2025-02-10T10:38:33Z","published":"2025-02-10T10:38:33Z","title":"Zero-shot Depth Completion via Test-time Alignment with Affine-invariant\n  Depth Prior","summary":"  Depth completion, predicting dense depth maps from sparse depth measurements,\nis an ill-posed problem requiring prior knowledge. Recent methods adopt\nlearning-based approaches to implicitly capture priors, but the priors\nprimarily fit in-domain data and do not generalize well to out-of-domain\nscenarios. To address this, we propose a zero-shot depth completion method\ncomposed of an affine-invariant depth diffusion model and test-time alignment.\nWe use pre-trained depth diffusion models as depth prior knowledge, which\nimplicitly understand how to fill in depth for scenes. Our approach aligns the\naffine-invariant depth prior with metric-scale sparse measurements, enforcing\nthem as hard constraints via an optimization loop at test-time. Our zero-shot\ndepth completion method demonstrates generalization across various domain\ndatasets, achieving up to a 21\\% average performance improvement over the\nprevious state-of-the-art methods while enhancing spatial understanding by\nsharpening scene details. We demonstrate that aligning a monocular\naffine-invariant depth prior with sparse metric measurements is a proven\nstrategy to achieve domain-generalizable depth completion without relying on\nextensive training data. Project page:\nhttps://hyoseok1223.github.io/zero-shot-depth-completion/.\n","authors":["Lee Hyoseok","Kyeong Seon Kim","Kwon Byung-Ki","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2502.06338v1.pdf","comment":"AAAI 2025, Project page:\n  https://hyoseok1223.github.io/zero-shot-depth-completion/"},{"id":"http://arxiv.org/abs/2502.06337v1","updated":"2025-02-10T10:37:36Z","published":"2025-02-10T10:37:36Z","title":"Accelerating Outlier-robust Rotation Estimation by Stereographic\n  Projection","summary":"  Rotation estimation plays a fundamental role in many computer vision and\nrobot tasks. However, efficiently estimating rotation in large inputs\ncontaining numerous outliers (i.e., mismatches) and noise is a recognized\nchallenge. Many robust rotation estimation methods have been designed to\naddress this challenge. Unfortunately, existing methods are often inapplicable\ndue to their long computation time and the risk of local optima. In this paper,\nwe propose an efficient and robust rotation estimation method. Specifically,\nour method first investigates geometric constraints involving only the rotation\naxis. Then, it uses stereographic projection and spatial voting techniques to\nidentify the rotation axis and angle. Furthermore, our method efficiently\nobtains the optimal rotation estimation and can estimate multiple rotations\nsimultaneously. To verify the feasibility of our method, we conduct comparative\nexperiments using both synthetic and real-world data. The results show that,\nwith GPU assistance, our method can solve large-scale ($10^6$ points) and\nseverely corrupted (90\\% outlier rate) rotation estimation problems within 0.07\nseconds, with an angular error of only 0.01 degrees, which is superior to\nexisting methods in terms of accuracy and efficiency.\n","authors":["Taosi Xu","Yinlong Liu","Xianbo Wang","Zhi-Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2502.06337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06336v1","updated":"2025-02-10T10:37:21Z","published":"2025-02-10T10:37:21Z","title":"DefTransNet: A Transformer-based Method for Non-Rigid Point Cloud\n  Registration in the Simulation of Soft Tissue Deformation","summary":"  Soft-tissue surgeries, such as tumor resections, are complicated by tissue\ndeformations that can obscure the accurate location and shape of tissues. By\nrepresenting tissue surfaces as point clouds and applying non-rigid point cloud\nregistration (PCR) methods, surgeons can better understand tissue deformations\nbefore, during, and after surgery. Existing non-rigid PCR methods, such as\nfeature-based approaches, struggle with robustness against challenges like\nnoise, outliers, partial data, and large deformations, making accurate point\ncorrespondence difficult. Although learning-based PCR methods, particularly\nTransformer-based approaches, have recently shown promise due to their\nattention mechanisms for capturing interactions, their robustness remains\nlimited in challenging scenarios. In this paper, we present DefTransNet, a\nnovel end-to-end Transformer-based architecture for non-rigid PCR. DefTransNet\nis designed to address the key challenges of deformable registration, including\nlarge deformations, outliers, noise, and partial data, by inputting source and\ntarget point clouds and outputting displacement vector fields. The proposed\nmethod incorporates a learnable transformation matrix to enhance robustness to\naffine transformations, integrates global and local geometric information, and\ncaptures long-range dependencies among points using Transformers. We validate\nour approach on four datasets: ModelNet, SynBench, 4DMatch, and DeformedTissue,\nusing both synthetic and real-world data to demonstrate the generalization of\nour proposed method. Experimental results demonstrate that DefTransNet\noutperforms current state-of-the-art registration networks across various\nchallenging conditions. Our code and data are publicly available.\n","authors":["Sara Monji-Azad","Marvin Kinz","Siddharth Kothari","Robin Khanna","Amrei Carla Mihan","David Maennel","Claudia Scherl","Juergen Hesser"],"pdf_url":"https://arxiv.org/pdf/2502.06336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06324v1","updated":"2025-02-10T10:20:11Z","published":"2025-02-10T10:20:11Z","title":"UniDemoiré: Towards Universal Image Demoiréing with Data Generation\n  and Synthesis","summary":"  Image demoir\\'eing poses one of the most formidable challenges in image\nrestoration, primarily due to the unpredictable and anisotropic nature of\nmoir\\'e patterns. Limited by the quantity and diversity of training data,\ncurrent methods tend to overfit to a single moir\\'e domain, resulting in\nperformance degradation for new domains and restricting their robustness in\nreal-world applications. In this paper, we propose a universal image\ndemoir\\'eing solution, UniDemoir\\'e, which has superior generalization\ncapability. Notably, we propose innovative and effective data generation and\nsynthesis methods that can automatically provide vast high-quality moir\\'e\nimages to train a universal demoir\\'eing model. Our extensive experiments\ndemonstrate the cutting-edge performance and broad potential of our approach\nfor generalized image demoir\\'eing.\n","authors":["Zemin Yang","Yujing Sun","Xidong Peng","Siu Ming Yiu","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2502.06324v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2407.02844v6","updated":"2025-02-10T10:10:33Z","published":"2024-07-03T06:40:26Z","title":"Exploiting Precision Mapping and Component-Specific Feature Enhancement\n  for Breast Cancer Segmentation and Identification","summary":"  Breast cancer is one of the leading causes of death globally, and thus there\nis an urgent need for early and accurate diagnostic techniques. Although\nultrasound imaging is a widely used technique for breast cancer screening, it\nfaces challenges such as poor boundary delineation caused by variations in\ntumor morphology and reduced diagnostic accuracy due to inconsistent image\nquality. To address these challenges, we propose novel Deep Learning (DL)\nframeworks for breast lesion segmentation and classification. We introduce a\nprecision mapping mechanism (PMM) for a precision mapping and attention-driven\nLinkNet (PMAD-LinkNet) segmentation framework that dynamically adapts spatial\nmappings through morphological variation analysis, enabling precise pixel-level\nrefinement of tumor boundaries. Subsequently, we introduce a component-specific\nfeature enhancement module (CSFEM) for a component-specific feature-enhanced\nclassifier (CSFEC-Net). Through a multi-level attention approach, the CSFEM\nmagnifies distinguishing features of benign, malignant, and normal tissues. The\nproposed frameworks are evaluated against existing literature and a diverse set\nof state-of-the-art Convolutional Neural Network (CNN) architectures. The\nobtained results show that our segmentation model achieves an accuracy of\n98.1%, an IoU of 96.9%, and a Dice Coefficient of 97.2%. For the classification\nmodel, an accuracy of 99.2% is achieved with F1-score, precision, and recall\nvalues of 99.1%, 99.3%, and 99.1%, respectively.\n","authors":["Pandiyaraju V","Shravan Venkatraman","Pavan Kumar S","Santhosh Malarvannan","Kannan A"],"pdf_url":"https://arxiv.org/pdf/2407.02844v6.pdf","comment":"27 pages, 18 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.06314v1","updated":"2025-02-10T10:06:46Z","published":"2025-02-10T10:06:46Z","title":"From Pixels to Components: Eigenvector Masking for Visual Representation\n  Learning","summary":"  Predicting masked from visible parts of an image is a powerful\nself-supervised approach for visual representation learning. However, the\ncommon practice of masking random patches of pixels exhibits certain failure\nmodes, which can prevent learning meaningful high-level features, as required\nfor downstream tasks. We propose an alternative masking strategy that operates\non a suitable transformation of the data rather than on the raw pixels.\nSpecifically, we perform principal component analysis and then randomly mask a\nsubset of components, which accounts for a fixed ratio of the data variance.\nThe learning task then amounts to reconstructing the masked components from the\nvisible ones. Compared to local patches of pixels, the principal components of\nimages carry more global information. We thus posit that predicting masked from\nvisible components involves more high-level features, allowing our masking\nstrategy to extract more useful representations. This is corroborated by our\nempirical findings which demonstrate improved image classification performance\nfor component over pixel masking. Our method thus constitutes a simple and\nrobust data-driven alternative to traditional masked image modeling approaches.\n","authors":["Alice Bizeul","Thomas Sutter","Alain Ryser","Bernhard Schölkopf","Julius von Kügelgen","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2502.06314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06307v1","updated":"2025-02-10T09:52:02Z","published":"2025-02-10T09:52:02Z","title":"Cell Nuclei Detection and Classification in Whole Slide Images with\n  Transformers","summary":"  Accurate and efficient cell nuclei detection and classification in\nhistopathological Whole Slide Images (WSIs) are pivotal for digital pathology\napplications. Traditional cell segmentation approaches, while commonly used,\nare computationally expensive and require extensive post-processing, limiting\ntheir practicality for high-throughput clinical settings. In this paper, we\npropose a paradigm shift from segmentation to detection for extracting cell\ninformation from WSIs, introducing CellNuc-DETR as a more effective solution.\nWe evaluate the accuracy performance of CellNuc-DETR on the PanNuke dataset and\nconduct cross-dataset evaluations on CoNSeP and MoNuSeg to assess robustness\nand generalization capabilities. Our results demonstrate state-of-the-art\nperformance in both cell nuclei detection and classification tasks.\nAdditionally, we assess the efficiency of CellNuc-DETR on large WSIs, showing\nthat it not only outperforms current methods in accuracy but also significantly\nreduces inference times. Specifically, CellNuc-DETR is twice as fast as the\nfastest segmentation-based method, HoVer-NeXt, while achieving substantially\nhigher accuracy. Moreover, it surpasses CellViT in accuracy and is\napproximately ten times more efficient in inference speed on WSIs. These\nresults establish CellNuc-DETR as a superior approach for cell analysis in\ndigital pathology, combining high accuracy with computational efficiency.\n","authors":["Oscar Pina","Eduard Dorca","Verónica Vilaplana"],"pdf_url":"https://arxiv.org/pdf/2502.06307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15328v3","updated":"2025-02-10T09:45:21Z","published":"2024-07-22T02:19:30Z","title":"Iterative Ensemble Training with Anti-Gradient Control for Mitigating\n  Memorization in Diffusion Models","summary":"  Diffusion models, known for their tremendous ability to generate novel and\nhigh-quality samples, have recently raised concerns due to their data\nmemorization behavior, which poses privacy risks. Recent approaches for memory\nmitigation either only focused on the text modality problem in cross-modal\ngeneration tasks or utilized data augmentation strategies. In this paper, we\npropose a novel training framework for diffusion models from the perspective of\nvisual modality, which is more generic and fundamental for mitigating\nmemorization. To facilitate forgetting of stored information in diffusion model\nparameters, we propose an iterative ensemble training strategy by splitting the\ndata into multiple shards for training multiple models and intermittently\naggregating these model parameters. Moreover, practical analysis of losses\nillustrates that the training loss for easily memorable images tends to be\nobviously lower. Thus, we propose an anti-gradient control method to exclude\nthe sample with a lower loss value from the current mini-batch to avoid\nmemorizing. Extensive experiments and analysis on four datasets are conducted\nto illustrate the effectiveness of our method, and results show that our method\nsuccessfully reduces memory capacity while even improving the performance\nslightly. Moreover, to save the computing cost, we successfully apply our\nmethod to fine-tune the well-trained diffusion models by limited epochs,\ndemonstrating the applicability of our method. Code is available in\nhttps://github.com/liuxiao-guan/IET_AGC.\n","authors":["Xiao Liu","Xiaoliu Guan","Yu Wu","Jiaxu Miao"],"pdf_url":"https://arxiv.org/pdf/2407.15328v3.pdf","comment":"Accepted in ECCV 2024, 20 pages with 7 figures"},{"id":"http://arxiv.org/abs/2409.02483v4","updated":"2025-02-10T09:38:51Z","published":"2024-09-04T07:20:01Z","title":"TASAR: Transfer-based Attack on Skeletal Action Recognition","summary":"  Skeletal sequences, as well-structured representations of human behaviors,\nplay a vital role in Human Activity Recognition (HAR). The transferability of\nadversarial skeletal sequences enables attacks in real-world HAR scenarios,\nsuch as autonomous driving, intelligent surveillance, and human-computer\ninteractions. However, most existing skeleton-based HAR (S-HAR) attacks are\nprimarily designed for white-box scenarios and exhibit weak adversarial\ntransferability. Therefore, they cannot be considered true transfer-based S-HAR\nattacks. More importantly, the reason for this failure remains unclear. In this\npaper, we study this phenomenon through the lens of loss surface, and find that\nits sharpness contributes to the weak transferability in S-HAR. Inspired by\nthis observation, we assume and empirically validate that smoothening the\nrugged loss landscape could potentially improve adversarial transferability in\nS-HAR. To this end, we propose the first \\textbf{T}ransfer-based\n\\textbf{A}ttack on \\textbf{S}keletal \\textbf{A}ction \\textbf{R}ecognition,\nTASAR. TASAR explores the smoothed model posterior without requiring surrogate\nre-training, which is achieved by a new post-train Dual Bayesian optimization\nstrategy. Furthermore, unlike previous transfer-based attacks that treat each\nframe independently and overlook temporal coherence within sequences, TASAR\nincorporates motion dynamics into the Bayesian attack gradient, effectively\ndisrupting the spatial-temporal coherence of S-HARs. To exhaustively evaluate\nthe effectiveness of existing methods and our method, we build the first\nlarge-scale robust S-HAR benchmark, comprising 7 S-HAR models, 10 attack\nmethods, 3 S-HAR datasets and 2 defense methods. Extensive results demonstrate\nthe superiority of TASAR. Our benchmark enables easy comparisons for future\nstudies, with the code available in the supplementary material.\n","authors":["Yunfeng Diao","Baiqi Wu","Ruixuan Zhang","Ajian Liu","Xiaoshuai Hao","Xingxing Wei","Meng Wang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02483v4.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.08572"},{"id":"http://arxiv.org/abs/2411.04469v2","updated":"2025-02-10T09:31:48Z","published":"2024-11-07T06:39:50Z","title":"FreeCap: Hybrid Calibration-Free Motion Capture in Open Environments","summary":"  We propose a novel hybrid calibration-free method FreeCap to accurately\ncapture global multi-person motions in open environments. Our system combines a\nsingle LiDAR with expandable moving cameras, allowing for flexible and precise\nmotion estimation in a unified world coordinate. In particular, We introduce a\nlocal-to-global pose-aware cross-sensor human-matching module that predicts the\nalignment among each sensor, even in the absence of calibration. Additionally,\nour coarse-to-fine sensor-expandable pose optimizer further optimizes the 3D\nhuman key points and the alignments, it is also capable of incorporating\nadditional cameras to enhance accuracy. Extensive experiments on Human-M3 and\nFreeMotion datasets demonstrate that our method significantly outperforms\nstate-of-the-art single-modal methods, offering an expandable and efficient\nsolution for multi-person motion capture across various applications.\n","authors":["Aoru Xue","Yiming Ren","Zining Song","Mao Ye","Xinge Zhu","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2411.04469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06289v1","updated":"2025-02-10T09:31:39Z","published":"2025-02-10T09:31:39Z","title":"Is an Ultra Large Natural Image-Based Foundation Model Superior to a\n  Retina-Specific Model for Detecting Ocular and Systemic Diseases?","summary":"  The advent of foundation models (FMs) is transforming medical domain. In\nophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4\nmillion natural images and 1.6 million retinal images, has demonstrated high\nadaptability across clinical applications. Conversely, DINOv2, a\ngeneral-purpose vision FM pre-trained on 142 million natural images, has shown\npromise in non-medical domains. However, its applicability to clinical tasks\nremains underexplored. To address this, we conducted head-to-head evaluations\nby fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular\ndisease detection and systemic disease prediction tasks, across eight\nstandardized open-source ocular datasets, as well as the Moorfields AlzEye and\nthe UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting\ndiabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets,\nall P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In\nglaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940,\nP<0.001). Conversely, RETFound achieved superior performance over all DINOv2\nmodels in predicting heart failure, myocardial infarction, and ischaemic stroke\n(AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even\nwith 10% of the fine-tuning data. These findings showcase the distinct\nscenarios where general-purpose and domain-specific FMs excel, highlighting the\nimportance of aligning FM selection with task-specific requirements to optimise\nclinical performance.\n","authors":["Qingshan Hou","Yukun Zhou","Jocelyn Hui Lin Goh","Ke Zou","Samantha Min Er Yew","Sahana Srinivasan","Meng Wang","Thaddaeus Lo","Xiaofeng Lei","Siegfried K. Wagner","Mark A. Chia","Dawei Yang","Hongyang Jiang","AnRan Ran","Rui Santos","Gabor Mark Somfai","Juan Helen Zhou","Haoyu Chen","Qingyu Chen","Carol Yim-Lui Cheung","Pearse A. Keane","Yih Chung Tham"],"pdf_url":"https://arxiv.org/pdf/2502.06289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06288v1","updated":"2025-02-10T09:31:12Z","published":"2025-02-10T09:31:12Z","title":"Enhancing Ground-to-Aerial Image Matching for Visual Misinformation\n  Detection Using Semantic Segmentation","summary":"  The recent advancements in generative AI techniques, which have significantly\nincreased the online dissemination of altered images and videos, have raised\nserious concerns about the credibility of digital media available on the\nInternet and distributed through information channels and social networks. This\nissue particularly affects domains that rely heavily on trustworthy data, such\nas journalism, forensic analysis, and Earth observation. To address these\nconcerns, the ability to geolocate a non-geo-tagged ground-view image without\nexternal information, such as GPS coordinates, has become increasingly\ncritical. This study tackles the challenge of linking a ground-view image,\npotentially exhibiting varying fields of view (FoV), to its corresponding\nsatellite image without the aid of GPS data. To achieve this, we propose a\nnovel four-stream Siamese-like architecture, the Quadruple Semantic Align Net\n(SAN-QUAD), which extends previous state-of-the-art (SOTA) approaches by\nleveraging semantic segmentation applied to both ground and satellite imagery.\nExperimental results on a subset of the CVUSA dataset demonstrate significant\nimprovements of up to 9.8\\% over prior methods across various FoV settings.\n","authors":["Matteo Mule","Matteo Pannacci","Ali Ghasemi Goudarzi","Francesco Pro","Lorenzo Papa","Luca Maiano","Irene Amerini"],"pdf_url":"https://arxiv.org/pdf/2502.06288v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.16769v3","updated":"2025-02-10T09:24:22Z","published":"2025-01-28T07:49:52Z","title":"Beyond-Labels: Advancing Open-Vocabulary Segmentation With\n  Vision-Language Models","summary":"  Self-supervised learning can resolve numerous image or linguistic processing\nproblems when effectively trained. This study investigated simple yet efficient\nmethods for adapting previously learned foundation models for open-vocabulary\nsemantic segmentation tasks. Our research proposed \"Beyond-Labels,\" a\nlightweight transformer-based fusion module that uses a handful of image\nsegmentation data to fuse frozen image representations with language concepts.\nThis strategy allows the model to successfully actualize enormous knowledge\nfrom pretrained models without requiring extensive retraining, making the model\ndata-efficient and scalable. Furthermore, we efficiently captured positional\ninformation in images using Fourier embeddings, thus improving the\ngeneralization across various image sizes, addressing one of the key\nlimitations of previous methods. Extensive ablation tests were performed to\ninvestigate the important components of our proposed method; when tested\nagainst the common benchmark PASCAL-5i, it demonstrated superior performance\ndespite being trained on frozen image and language characteristics.\n","authors":["Muhammad Atta ur Rahman"],"pdf_url":"https://arxiv.org/pdf/2501.16769v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21009v2","updated":"2025-02-10T08:55:18Z","published":"2024-12-30T15:21:36Z","title":"Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline","summary":"  Recent advancements in deep learning have significantly enhanced\ncontent-based retrieval methods, notably through models like CLIP that map\nimages and texts into a shared embedding space. However, these methods often\nstruggle with domain-specific entities and long-tail concepts absent from their\ntraining data, particularly in identifying specific individuals. In this paper,\nwe explore the task of identity-aware cross-modal retrieval, which aims to\nretrieve images of persons in specific contexts based on natural language\nqueries. This task is critical in various scenarios, such as for searching and\nbrowsing personalized video collections or large audio-visual archives\nmaintained by national broadcasters. We introduce a novel dataset, COCO Person\nFaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched\nwith deepfake-generated faces from VGGFace2. This dataset addresses the lack of\nlarge-scale datasets needed for training and evaluating models for this task.\nOur experiments assess the performance of different CLIP variations repurposed\nfor this task, including our architecture, Identity-aware CLIP (Id-CLIP), which\nachieves competitive retrieval performance through targeted fine-tuning. Our\ncontributions lay the groundwork for more robust cross-modal retrieval systems\ncapable of recognizing long-tail identities and contextual nuances. Data and\ncode are available at https://github.com/mesnico/IdCLIP.\n","authors":["Nicola Messina","Lucia Vadicamo","Leo Maltese","Claudio Gennaro"],"pdf_url":"https://arxiv.org/pdf/2412.21009v2.pdf","comment":"Accepted as full paper at ECIR 2025"},{"id":"http://arxiv.org/abs/2502.06255v1","updated":"2025-02-10T08:42:46Z","published":"2025-02-10T08:42:46Z","title":"Towards Efficient and Intelligent Laser Weeding: Method and Dataset for\n  Weed Stem Detection","summary":"  Weed control is a critical challenge in modern agriculture, as weeds compete\nwith crops for essential nutrient resources, significantly reducing crop yield\nand quality. Traditional weed control methods, including chemical and\nmechanical approaches, have real-life limitations such as associated\nenvironmental impact and efficiency. An emerging yet effective approach is\nlaser weeding, which uses a laser beam as the stem cutter. Although there have\nbeen studies that use deep learning in weed recognition, its application in\nintelligent laser weeding still requires a comprehensive understanding. Thus,\nthis study represents the first empirical investigation of weed recognition for\nlaser weeding. To increase the efficiency of laser beam cut and avoid damaging\nthe crops of interest, the laser beam shall be directly aimed at the weed root.\nYet, weed stem detection remains an under-explored problem. We integrate the\ndetection of crop and weed with the localization of weed stem into one\nend-to-end system. To train and validate the proposed system in a real-life\nscenario, we curate and construct a high-quality weed stem detection dataset\nwith human annotations. The dataset consists of 7,161 high-resolution pictures\ncollected in the field with annotations of 11,151 instances of weed.\nExperimental results show that the proposed system improves weeding accuracy by\n6.7% and reduces energy cost by 32.3% compared to existing weed recognition\nsystems.\n","authors":["Dingning Liu","Jinzhe Li","Haoyang Su","Bei Cui","Zhihui Wang","Qingbo Yuan","Wanli Ouyang","Nanqing Dong"],"pdf_url":"https://arxiv.org/pdf/2502.06255v1.pdf","comment":"Accepted by AAAI-AISI 2025"},{"id":"http://arxiv.org/abs/2502.06243v1","updated":"2025-02-10T08:22:25Z","published":"2025-02-10T08:22:25Z","title":"Multi-Scale Transformer Architecture for Accurate Medical Image\n  Classification","summary":"  This study introduces an AI-driven skin lesion classification algorithm built\non an enhanced Transformer architecture, addressing the challenges of accuracy\nand robustness in medical image analysis. By integrating a multi-scale feature\nfusion mechanism and refining the self-attention process, the model effectively\nextracts both global and local features, enhancing its ability to detect\nlesions with ambiguous boundaries and intricate structures. Performance\nevaluation on the ISIC 2017 dataset demonstrates that the improved Transformer\nsurpasses established AI models, including ResNet50, VGG19, ResNext, and Vision\nTransformer, across key metrics such as accuracy, AUC, F1-Score, and Precision.\nGrad-CAM visualizations further highlight the interpretability of the model,\nshowcasing strong alignment between the algorithm's focus areas and actual\nlesion sites. This research underscores the transformative potential of\nadvanced AI models in medical imaging, paving the way for more accurate and\nreliable diagnostic tools. Future work will explore the scalability of this\napproach to broader medical imaging tasks and investigate the integration of\nmultimodal data to enhance AI-driven diagnostic frameworks for intelligent\nhealthcare.\n","authors":["Jiacheng Hu","Yanlin Xiang","Yang Lin","Junliang Du","Hanchao Zhang","Houze Liu"],"pdf_url":"https://arxiv.org/pdf/2502.06243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05803v2","updated":"2025-02-10T08:16:42Z","published":"2025-01-10T09:10:30Z","title":"Test-time Alignment of Diffusion Models without Reward Over-optimization","summary":"  Diffusion models excel in generative tasks, but aligning them with specific\nobjectives while maintaining their versatility remains challenging. Existing\nfine-tuning methods often suffer from reward over-optimization, while\napproximate guidance approaches fail to optimize target rewards effectively.\nAddressing these limitations, we propose a training-free, test-time method\nbased on Sequential Monte Carlo (SMC) to sample from the reward-aligned target\ndistribution. Our approach, tailored for diffusion sampling and incorporating\ntempering techniques, achieves comparable or superior target rewards to\nfine-tuning methods while preserving diversity and cross-reward generalization.\nWe demonstrate its effectiveness in single-reward optimization, multi-objective\nscenarios, and online black-box optimization. This work offers a robust\nsolution for aligning diffusion models with diverse downstream objectives\nwithout compromising their general capabilities. Code is available at\nhttps://github.com/krafton-ai/DAS.\n","authors":["Sunwoo Kim","Minkyu Kim","Dongmin Park"],"pdf_url":"https://arxiv.org/pdf/2501.05803v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2405.15289v3","updated":"2025-02-10T08:14:17Z","published":"2024-05-24T07:22:35Z","title":"Learning Invariant Causal Mechanism from Vision-Language Models","summary":"  Contrastive Language-Image Pretraining (CLIP) has achieved remarkable\nsuccess, but its performance can degrade when fine-tuned in out-of-distribution\n(OOD) scenarios. We model the prediction process using a Structural Causal\nModel (SCM) and show that the causal mechanism involving both invariant and\nvariant factors in training environments differs from that in test\nenvironments. In contrast, the causal mechanism with solely invariant factors\nremains consistent across environments. We theoretically prove the existence of\na linear mapping from CLIP embeddings to invariant factors, which can be\nestimated using interventional data. Additionally, we provide a condition to\nguarantee low OOD risk of the invariant predictor. Based on these insights, we\npropose the Invariant Causal Mechanism of CLIP (CLIP-ICM) framework. CLIP-ICM\ninvolves collecting interventional data, estimating a linear projection matrix,\nand making predictions within the invariant subspace. Experiments on several\nOOD datasets show that CLIP-ICM significantly improves the performance of CLIP.\nOur method offers a simple but powerful enhancement, boosting the reliability\nof CLIP in real-world applications.\n","authors":["Zeen Song","Siyu Zhao","Xingyu Zhang","Jiangmeng Li","Changwen Zheng","Wenwen Qiang"],"pdf_url":"https://arxiv.org/pdf/2405.15289v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01385v2","updated":"2025-02-10T08:04:21Z","published":"2025-02-03T14:21:05Z","title":"Detecting Backdoor Samples in Contrastive Language Image Pretraining","summary":"  Contrastive language-image pretraining (CLIP) has been found to be vulnerable\nto poisoning backdoor attacks where the adversary can achieve an almost perfect\nattack success rate on CLIP models by poisoning only 0.01\\% of the training\ndataset. This raises security concerns on the current practice of pretraining\nlarge-scale models on unscrutinized web data using CLIP. In this work, we\nanalyze the representations of backdoor-poisoned samples learned by CLIP models\nand find that they exhibit unique characteristics in their local subspace,\ni.e., their local neighborhoods are far more sparse than that of clean samples.\nBased on this finding, we conduct a systematic study on detecting CLIP backdoor\nattacks and show that these attacks can be easily and efficiently detected by\ntraditional density ratio-based local outlier detectors, whereas existing\nbackdoor sample detection methods fail. Our experiments also reveal that an\nunintentional backdoor already exists in the original CC3M dataset and has been\ntrained into a popular open-source model released by OpenCLIP. Based on our\ndetector, one can clean up a million-scale web dataset (e.g., CC3M) efficiently\nwithin 15 minutes using 4 Nvidia A100 GPUs. The code is publicly available in\nour \\href{https://github.com/HanxunH/Detect-CLIP-Backdoor-Samples}{GitHub\nrepository}.\n","authors":["Hanxun Huang","Sarah Erfani","Yige Li","Xingjun Ma","James Bailey"],"pdf_url":"https://arxiv.org/pdf/2502.01385v2.pdf","comment":"ICLR2025"},{"id":"http://arxiv.org/abs/2502.06227v1","updated":"2025-02-10T07:58:49Z","published":"2025-02-10T07:58:49Z","title":"Unsupervised deep learning for semantic segmentation of multispectral\n  LiDAR forest point clouds","summary":"  Point clouds captured with laser scanning systems from forest environments\ncan be utilized in a wide variety of applications within forestry and plant\necology, such as the estimation of tree stem attributes, leaf angle\ndistribution, and above-ground biomass. However, effectively utilizing the data\nin such tasks requires the semantic segmentation of the data into wood and\nfoliage points, also known as leaf-wood separation. The traditional approach to\nleaf-wood separation has been geometry- and radiometry-based unsupervised\nalgorithms, which tend to perform poorly on data captured with airborne laser\nscanning (ALS) systems, even with a high point density. While recent machine\nand deep learning approaches achieve great results even on sparse point clouds,\nthey require manually labeled training data, which is often extremely laborious\nto produce. Multispectral (MS) information has been demonstrated to have\npotential for improving the accuracy of leaf-wood separation, but quantitative\nassessment of its effects has been lacking. This study proposes a fully\nunsupervised deep learning method, GrowSP-ForMS, which is specifically designed\nfor leaf-wood separation of high-density MS ALS point clouds and based on the\nGrowSP architecture. GrowSP-ForMS achieved a mean accuracy of 84.3% and a mean\nintersection over union (mIoU) of 69.6% on our MS test set, outperforming the\nunsupervised reference methods by a significant margin. When compared to\nsupervised deep learning methods, our model performed similarly to the slightly\nolder PointNet architecture but was outclassed by more recent approaches.\nFinally, two ablation studies were conducted, which demonstrated that our\nproposed changes increased the test set mIoU of GrowSP-ForMS by 29.4 percentage\npoints (pp) in comparison to the original GrowSP model and that utilizing MS\ndata improved the mIoU by 5.6 pp from the monospectral case.\n","authors":["Lassi Ruoppa","Oona Oinonen","Josef Taher","Matti Lehtomäki","Narges Takhtkeshha","Antero Kukko","Harri Kaartinen","Juha Hyyppä"],"pdf_url":"https://arxiv.org/pdf/2502.06227v1.pdf","comment":"30 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.06220v1","updated":"2025-02-10T07:52:47Z","published":"2025-02-10T07:52:47Z","title":"FunduSAM: A Specialized Deep Learning Model for Enhanced Optic Disc and\n  Cup Segmentation in Fundus Images","summary":"  The Segment Anything Model (SAM) has gained popularity as a versatile image\nsegmentation method, thanks to its strong generalization capabilities across\nvarious domains. However, when applied to optic disc (OD) and optic cup (OC)\nsegmentation tasks, SAM encounters challenges due to the complex structures,\nlow contrast, and blurred boundaries typical of fundus images, leading to\nsuboptimal performance. To overcome these challenges, we introduce a novel\nmodel, FunduSAM, which incorporates several Adapters into SAM to create a deep\nnetwork specifically designed for OD and OC segmentation. The FunduSAM utilizes\nAdapter into each transformer block after encoder for parameter fine-tuning\n(PEFT). It enhances SAM's feature extraction capabilities by designing a\nConvolutional Block Attention Module (CBAM), addressing issues related to\nblurred boundaries and low contrast. Given the unique requirements of OD and OC\nsegmentation, polar transformation is used to convert the original fundus OD\nimages into a format better suited for training and evaluating FunduSAM. A\njoint loss is used to achieve structure preservation between the OD and OC,\nwhile accurate segmentation. Extensive experiments on the REFUGE dataset,\ncomprising 1,200 fundus images, demonstrate the superior performance of\nFunduSAM compared to five mainstream approaches.\n","authors":["Jinchen Yu","Yongwei Nie","Fei Qi","Wenxiong Liao","Hongmin Cai"],"pdf_url":"https://arxiv.org/pdf/2502.06220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06219v1","updated":"2025-02-10T07:50:22Z","published":"2025-02-10T07:50:22Z","title":"Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for\n  Generalizable RGB-Depth Driving Scene Parsing","summary":"  Recent vision foundation models (VFMs), typically based on Vision Transformer\n(ViT), have significantly advanced numerous computer vision tasks. Despite\ntheir success in tasks focused solely on RGB images, the potential of VFMs in\nRGB-depth driving scene parsing remains largely under-explored. In this\narticle, we take one step toward this emerging research area by investigating a\nfeasible technique to fully exploit VFMs for generalizable RGB-depth driving\nscene parsing. Specifically, we explore the inherent characteristics of RGB and\ndepth data, thereby presenting a Heterogeneous Feature Integration Transformer\n(HFIT). This network enables the efficient extraction and integration of\ncomprehensive heterogeneous features without re-training ViTs. Relative depth\nprediction results from VFMs, used as inputs to the HFIT side adapter, overcome\nthe limitations of the dependence on depth maps. Our proposed HFIT demonstrates\nsuperior performance compared to all other traditional single-modal and\ndata-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the\nCityscapes and KITTI Semantics datasets. We believe this novel strategy paves\nthe way for future innovations in VFM-based data-fusion techniques for driving\nscene parsing. Our source code is publicly available at\nhttps://mias.group/HFIT.\n","authors":["Sicen Guo","Tianyou Wen","Chuang-Wei Liu","Qijun Chen","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2502.06219v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.00426v2","updated":"2025-02-10T07:44:35Z","published":"2025-02-01T13:20:01Z","title":"TEST-V: TEst-time Support-set Tuning for Zero-shot Video Classification","summary":"  Recently, adapting Vision Language Models (VLMs) to zero-shot visual\nclassification by tuning class embedding with a few prompts (Test-time Prompt\nTuning, TPT) or replacing class names with generated visual samples\n(support-set) has shown promising results. However, TPT cannot avoid the\nsemantic gap between modalities while the support-set cannot be tuned. To this\nend, we draw on each other's strengths and propose a novel framework namely\nTEst-time Support-set Tuning for zero-shot Video Classification (TEST-V). It\nfirst dilates the support-set with multiple prompts (Multi-prompting\nSupport-set Dilation, MSD) and then erodes the support-set via learnable\nweights to mine key cues dynamically (Temporal-aware Support-set Erosion, TSE).\nSpecifically, i) MSD expands the support samples for each class based on\nmultiple prompts enquired from LLMs to enrich the diversity of the support-set.\nii) TSE tunes the support-set with factorized learnable weights according to\nthe temporal prediction consistency in a self-supervised manner to dig pivotal\nsupporting cues for each class. $\\textbf{TEST-V}$ achieves state-of-the-art\nresults across four benchmarks and has good interpretability for the\nsupport-set dilation and erosion.\n","authors":["Rui Yan","Jin Wang","Hongyu Qu","Xiaoyu Du","Dong Zhang","Jinhui Tang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2502.00426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06209v1","updated":"2025-02-10T07:20:28Z","published":"2025-02-10T07:20:28Z","title":"Enhancing Cost Efficiency in Active Learning with Candidate Set Query","summary":"  This paper introduces a cost-efficient active learning (AL) framework for\nclassification, featuring a novel query design called candidate set query.\nUnlike traditional AL queries requiring the oracle to examine all possible\nclasses, our method narrows down the set of candidate classes likely to include\nthe ground-truth class, significantly reducing the search space and labeling\ncost. Moreover, we leverage conformal prediction to dynamically generate small\nyet reliable candidate sets, adapting to model enhancement over successive AL\nrounds. To this end, we introduce an acquisition function designed to\nprioritize data points that offer high information gain at lower cost.\nEmpirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the\neffectiveness and scalability of our framework. Notably, it reduces labeling\ncost by 42% on ImageNet64x64.\n","authors":["Yeho Gwon","Sehyun Hwang","Hoyoung Kim","Jungseul Ok","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2502.06209v1.pdf","comment":"20 pages, 17 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.06201v1","updated":"2025-02-10T06:54:30Z","published":"2025-02-10T06:54:30Z","title":"Comparing Image Segmentation Algorithms","summary":"  This paper presents a novel approach for denoising binary images using\nsimulated annealing (SA), a global optimization technique that addresses the\ninherent challenges of non convex energy functions. Binary images are often\ncorrupted by noise, necessitating effective restoration methods. We propose an\nenergy function E(x, y) that captures the relationship between the noisy image\ny and the desired clean image x. Our algorithm combines simulated annealing\nwith a localized optimization strategy to efficiently navigate the solution\nspace, minimizing the energy function while maintaining computational\nefficiency. We evaluate the performance of the proposed method against\ntraditional iterative conditional modes (ICM), employing a binary image with\n10% pixel corruption as a test case. Experimental results demonstrate that the\nsimulated annealing method achieves a significant restoration improvement,\nyielding a 99.19% agreement with the original image compared to 96.21% for ICM.\nVisual assessments reveal that simulated annealing effectively removes noise\nwhile preserving structural details, making it a promising approach for binary\nimage denoising. This work contributes to the field of image processing by\nhighlighting the advantages of incorporating global optimization techniques in\nrestoration tasks.\n","authors":["Milind Cherukuri"],"pdf_url":"https://arxiv.org/pdf/2502.06201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06194v1","updated":"2025-02-10T06:49:54Z","published":"2025-02-10T06:49:54Z","title":"Multimodal Task Representation Memory Bank vs. Catastrophic Forgetting\n  in Anomaly Detection","summary":"  Unsupervised Continuous Anomaly Detection (UCAD) faces significant challenges\nin multi-task representation learning, with existing methods suffering from\nincomplete representation and catastrophic forgetting. Unlike supervised\nmodels, unsupervised scenarios lack prior information, making it difficult to\neffectively distinguish redundant and complementary multimodal features. To\naddress this, we propose the Multimodal Task Representation Memory Bank (MTRMB)\nmethod through two key technical innovations: A Key-Prompt-Multimodal Knowledge\n(KPMK) mechanism that uses concise key prompts to guide cross-modal feature\ninteraction between BERT and ViT. Refined Structure-based Contrastive Learning\n(RSCL) leveraging Grounding DINO and SAM to generate precise segmentation\nmasks, pulling features of the same structural region closer while pushing\ndifferent structural regions apart. Experiments on MVtec AD and VisA datasets\ndemonstrate MTRMB's superiority, achieving an average detection accuracy of\n0.921 at the lowest forgetting rate, significantly outperforming\nstate-of-the-art methods. We plan to open source on GitHub.\n","authors":["You Zhou","Jiangshan Zhao","Deyu Zeng","Zuo Zuo","Weixiang Liu","Zongze Wu"],"pdf_url":"https://arxiv.org/pdf/2502.06194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06189v1","updated":"2025-02-10T06:41:20Z","published":"2025-02-10T06:41:20Z","title":"Multi-Level Decoupled Relational Distillation for Heterogeneous\n  Architectures","summary":"  Heterogeneous distillation is an effective way to transfer knowledge from\ncross-architecture teacher models to student models. However, existing\nheterogeneous distillation methods do not take full advantage of the dark\nknowledge hidden in the teacher's output, limiting their performance.To this\nend, we propose a novel framework named Multi-Level Decoupled Relational\nKnowledge Distillation (MLDR-KD) to unleash the potential of relational\ndistillation in heterogeneous distillation. Concretely, we first introduce\nDecoupled Finegrained Relation Alignment (DFRA) in both logit and feature\nlevels to balance the trade-off between distilled dark knowledge and the\nconfidence in the correct category of the heterogeneous teacher model. Then,\nMulti-Scale Dynamic Fusion (MSDF) module is applied to dynamically fuse the\nprojected logits of multiscale features at different stages in student model,\nfurther improving performance of our method in feature level. We verify our\nmethod on four architectures (CNNs, Transformers, MLPs and Mambas), two\ndatasets (CIFAR-100 and Tiny-ImageNet). Compared with the best available\nmethod, our MLDR-KD improves student model performance with gains of up to\n4.86% on CIFAR-100 and 2.78% on Tiny-ImageNet datasets respectively, showing\nrobustness and generality in heterogeneous distillation. Code will be released\nsoon.\n","authors":["Yaoxin Yang","Peng Ye","Weihao Lin","Kangcong Li","Yan Wen","Jia Hao","Tao Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06181v1","updated":"2025-02-10T06:21:16Z","published":"2025-02-10T06:21:16Z","title":"CANeRV: Content Adaptive Neural Representation for Video Compression","summary":"  Recent advances in video compression introduce implicit neural representation\n(INR) based methods, which effectively capture global dependencies and\ncharacteristics of entire video sequences. Unlike traditional and deep learning\nbased approaches, INR-based methods optimize network parameters from a global\nperspective, resulting in superior compression potential. However, most current\nINR methods utilize a fixed and uniform network architecture across all frames,\nlimiting their adaptability to dynamic variations within and between video\nsequences. This often leads to suboptimal compression outcomes as these methods\nstruggle to capture the distinct nuances and transitions in video content. To\novercome these challenges, we propose Content Adaptive Neural Representation\nfor Video Compression (CANeRV), an innovative INR-based video compression\nnetwork that adaptively conducts structure optimisation based on the specific\ncontent of each video sequence. To better capture dynamic information across\nvideo sequences, we propose a dynamic sequence-level adjustment (DSA).\nFurthermore, to enhance the capture of dynamics between frames within a\nsequence, we implement a dynamic frame-level adjustment (DFA). {Finally, to\neffectively capture spatial structural information within video frames, thereby\nenhancing the detail restoration capabilities of CANeRV, we devise a structure\nlevel hierarchical structural adaptation (HSA).} Experimental results\ndemonstrate that CANeRV can outperform both H.266/VVC and state-of-the-art\nINR-based video compression techniques across diverse video datasets.\n","authors":["Lv Tang","Jun Zhu","Xinfeng Zhang","Li Zhang","Siwei Ma","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2502.06181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02345v2","updated":"2025-02-10T06:05:46Z","published":"2024-06-04T14:21:41Z","title":"Progressive Confident Masking Attention Network for Audio-Visual\n  Segmentation","summary":"  Audio and visual signals typically occur simultaneously, and humans possess\nan innate ability to correlate and synchronize information from these two\nmodalities. Recently, a challenging problem known as Audio-Visual Segmentation\n(AVS) has emerged, intending to produce segmentation maps for sounding objects\nwithin a scene. However, the methods proposed so far have not sufficiently\nintegrated audio and visual information, and the computational costs have been\nextremely high. Additionally, the outputs of different stages have not been\nfully utilized. To facilitate this research, we introduce a novel Progressive\nConfident Masking Attention Network (PMCANet). It leverages attention\nmechanisms to uncover the intrinsic correlations between audio signals and\nvisual frames. Furthermore, we design an efficient and effective\ncross-attention module to enhance semantic perception by selecting query\ntokens. This selection is determined through confidence-driven units based on\nthe network's multi-stage predictive outputs. Experiments demonstrate that our\nnetwork outperforms other AVS methods while requiring less computational\nresources. The code is available at: https://github.com/PrettyPlate/PCMANet.\n","authors":["Yuxuan Wang","Jinchao Zhu","Feng Dong","Shuyue Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.02345v2.pdf","comment":"23 pages, 11 figures, submitted to Elsevier Knowledge-Based System"},{"id":"http://arxiv.org/abs/2502.00931v2","updated":"2025-02-10T06:05:38Z","published":"2025-02-02T21:44:15Z","title":"VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning","summary":"  Vision-language navigation in unknown environments is crucial for mobile\nrobots. In scenarios such as household assistance and rescue, mobile robots\nneed to understand a human command, such as \"find a person wearing black\". We\npresent a novel vision-language navigation (VL-Nav) system that integrates\nefficient spatial reasoning on low-power robots. Unlike prior methods that rely\non a single image-level feature similarity to guide a robot, our method\nintegrates pixel-wise vision-language features with curiosity-driven\nexploration. This approach enables robust navigation to human-instructed\ninstances across diverse environments. We deploy VL-Nav on a four-wheel mobile\nrobot and evaluate its performance through comprehensive navigation tasks in\nboth indoor and outdoor environments, spanning different scales and semantic\ncomplexities. Remarkably, VL-Nav operates at a real-time frequency of 30 Hz\nwith a Jetson Orin NX, highlighting its ability to conduct efficient\nvision-language navigation. Results show that VL-Nav achieves an overall\nsuccess rate of 86.3%, outperforming previous methods by 44.15%.\n","authors":["Yi Du","Taimeng Fu","Zhuoqun Chen","Bowen Li","Shaoshu Su","Zhipeng Zhao","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2502.00931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06172v1","updated":"2025-02-10T05:50:26Z","published":"2025-02-10T05:50:26Z","title":"PLATTER: A Page-Level Handwritten Text Recognition System for Indic\n  Scripts","summary":"  In recent years, the field of Handwritten Text Recognition (HTR) has seen the\nemergence of various new models, each claiming to perform competitively better\nthan the other in specific scenarios. However, making a fair comparison of\nthese models is challenging due to inconsistent choices and diversity in test\nsets. Furthermore, recent advancements in HTR often fail to account for the\ndiverse languages, especially Indic languages, likely due to the scarcity of\nrelevant labeled datasets. Moreover, much of the previous work has focused\nprimarily on character-level or word-level recognition, overlooking the crucial\nstage of Handwritten Text Detection (HTD) necessary for building a page-level\nend-to-end handwritten OCR pipeline. Through our paper, we address these gaps\nby making three pivotal contributions. Firstly, we present an end-to-end\nframework for Page-Level hAndwriTTen TExt Recognition (PLATTER) by treating it\nas a two-stage problem involving word-level HTD followed by HTR. This approach\nenables us to identify, assess, and address challenges in each stage\nindependently. Secondly, we demonstrate the usage of PLATTER to measure the\nperformance of our language-agnostic HTD model and present a consistent\ncomparison of six trained HTR models on ten diverse Indic languages thereby\nencouraging consistent comparisons. Finally, we also release a Corpus of\nHandwritten Indic Scripts (CHIPS), a meticulously curated, page-level Indic\nhandwritten OCR dataset labeled for both detection and recognition purposes.\nAdditionally, we release our code and trained models, to encourage further\ncontributions in this direction.\n","authors":["Badri Vishal Kasuba","Dhruv Kudale","Venkatapathy Subramanian","Parag Chaudhuri","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2502.06172v1.pdf","comment":"Submitting Preprint"},{"id":"http://arxiv.org/abs/2410.06625v2","updated":"2025-02-10T05:47:27Z","published":"2024-10-09T07:21:43Z","title":"ETA: Evaluating Then Aligning Safety of Vision Language Models at\n  Inference Time","summary":"  Vision Language Models (VLMs) have become essential backbones for multimodal\nintelligence, yet significant safety challenges limit their real-world\napplication. While textual inputs are often effectively safeguarded,\nadversarial visual inputs can easily bypass VLM defense mechanisms. Existing\ndefense methods are either resource-intensive, requiring substantial data and\ncompute, or fail to simultaneously ensure safety and usefulness in responses.\nTo address these limitations, we propose a novel two-phase inference-time\nalignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual\ncontents and output responses to establish a robust safety awareness in\nmultimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep\nlevels by conditioning the VLMs' generative distribution with an interference\nprefix and performing sentence-level best-of-N to search the most harmless and\nhelpful generation paths. Extensive experiments show that ETA outperforms\nbaseline methods in terms of harmlessness, helpfulness, and efficiency,\nreducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%\nwin-ties in GPT-4 helpfulness evaluation. The code is publicly available at\nhttps://github.com/DripNowhy/ETA.\n","authors":["Yi Ding","Bolian Li","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06625v2.pdf","comment":"29pages"},{"id":"http://arxiv.org/abs/2502.06171v1","updated":"2025-02-10T05:45:03Z","published":"2025-02-10T05:45:03Z","title":"A Data-Efficient Pan-Tumor Foundation Model for Oncology CT\n  Interpretation","summary":"  Artificial intelligence-assisted imaging analysis has made substantial\nstrides in tumor diagnosis and management. Here we present PASTA, a pan-tumor\nCT foundation model that achieves state-of-the-art performance on 45 of 46\nrepresentative oncology tasks -- including lesion segmentation, tumor detection\nin plain CT, tumor staging, survival prediction, structured report generation,\nand cross-modality transfer learning, significantly outperforming the\nsecond-best models on 35 tasks. This remarkable advancement is driven by our\ndevelopment of PASTA-Gen, an innovative synthetic tumor generation framework\nthat produces a comprehensive dataset of 30,000 CT scans with pixel-level\nannotated lesions and paired structured reports, encompassing malignancies\nacross ten organs and five benign lesion types. By leveraging this rich,\nhigh-quality synthetic data, we overcome a longstanding bottleneck in the\ndevelopment of CT foundation models -- specifically, the scarcity of publicly\navailable, high-quality annotated datasets due to privacy constraints and the\nsubstantial labor required for scaling precise data annotation. Encouragingly,\nPASTA demonstrates exceptional data efficiency with promising practical value,\nmarkedly improving performance on various tasks with only a small amount of\nreal-world data. The open release of both the synthetic dataset and PASTA\nfoundation model effectively addresses the challenge of data scarcity, thereby\nadvancing oncological research and clinical translation.\n","authors":["Wenhui Lei","Hanyu Chen","Zitian Zhang","Luyang Luo","Qiong Xiao","Yannian Gu","Peng Gao","Yankai Jiang","Ci Wang","Guangtao Wu","Tongjia Xu","Yingjie Zhang","Xiaofan Zhang","Pranav Rajpurkar","Shaoting Zhang","Zhenning Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06171v1.pdf","comment":"57 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.06170v1","updated":"2025-02-10T05:44:54Z","published":"2025-02-10T05:44:54Z","title":"An Interpretable Implicit-Based Approach for Modeling Local Spatial\n  Effects: A Case Study of Global Gross Primary Productivity","summary":"  In Earth sciences, unobserved factors exhibit non-stationary spatial\ndistributions, causing the relationships between features and targets to\ndisplay spatial heterogeneity. In geographic machine learning tasks,\nconventional statistical learning methods often struggle to capture spatial\nheterogeneity, leading to unsatisfactory prediction accuracy and unreliable\ninterpretability. While approaches like Geographically Weighted Regression\n(GWR) capture local variations, they fall short of uncovering global patterns\nand tracking the continuous evolution of spatial heterogeneity. Motivated by\nthis limitation, we propose a novel perspective - that is, simultaneously\nmodeling common features across different locations alongside spatial\ndifferences using deep neural networks. The proposed method is a dual-branch\nneural network with an encoder-decoder structure. In the encoding stage, the\nmethod aggregates node information in a spatiotemporal conditional graph using\nGCN and LSTM, encoding location-specific spatiotemporal heterogeneity as an\nimplicit conditional vector. Additionally, a self-attention-based encoder is\nused to extract location-invariant common features from the data. In the\ndecoding stage, the approach employs a conditional generation strategy that\npredicts response variables and interpretative weights based on data features\nunder spatiotemporal conditions. The approach is validated by predicting\nvegetation gross primary productivity (GPP) using global climate and land cover\ndata from 2001 to 2020. Trained on 50 million samples and tested on 2.8\nmillion, the proposed model achieves an RMSE of 0.836, outperforming LightGBM\n(1.063) and TabNet (0.944). Visualization analyses indicate that our method can\nreveal the distribution differences of the dominant factors of GPP across\nvarious times and locations.\n","authors":["Siqi Du","Hongsheng Huang","Kaixin Shen","Ziqi Liu","Shengjun Tang"],"pdf_url":"https://arxiv.org/pdf/2502.06170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06167v1","updated":"2025-02-10T05:36:30Z","published":"2025-02-10T05:36:30Z","title":"Universal Approximation of Visual Autoregressive Transformers","summary":"  We investigate the fundamental limits of transformer-based foundation models,\nextending our analysis to include Visual Autoregressive (VAR) transformers. VAR\nrepresents a big step toward generating images using a novel, scalable,\ncoarse-to-fine ``next-scale prediction'' framework. These models set a new\nquality bar, outperforming all previous methods, including Diffusion\nTransformers, while having state-of-the-art performance for image synthesis\ntasks. Our primary contributions establish that, for single-head VAR\ntransformers with a single self-attention layer and single interpolation layer,\nthe VAR Transformer is universal. From the statistical perspective, we prove\nthat such simple VAR transformers are universal approximators for any\nimage-to-image Lipschitz functions. Furthermore, we demonstrate that flow-based\nautoregressive transformers inherit similar approximation capabilities. Our\nresults provide important design principles for effective and computationally\nefficient VAR Transformer strategies that can be used to extend their utility\nto more sophisticated VAR models in image generation and other related areas.\n","authors":["Yifang Chen","Xiaoyu Li","Yingyu Liang","Zhenmei Shi","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2502.06167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03240v2","updated":"2025-02-10T05:25:37Z","published":"2023-07-06T18:08:36Z","title":"Adaptive Generation of Privileged Intermediate Information for\n  Visible-Infrared Person Re-Identification","summary":"  Visible-infrared person re-identification seeks to retrieve images of the\nsame individual captured over a distributed network of RGB and IR sensors.\nSeveral V-I ReID approaches directly integrate both V and I modalities to\ndiscriminate persons within a shared representation space. However, given the\nsignificant gap in data distributions between V and I modalities, cross-modal\nV-I ReID remains challenging. Some recent approaches improve generalization by\nleveraging intermediate spaces that can bridge V and I modalities, yet\neffective methods are required to select or generate data for such informative\ndomains. In this paper, the Adaptive Generation of Privileged Intermediate\nInformation training approach is introduced to adapt and generate a virtual\ndomain that bridges discriminant information between the V and I modalities.\nThe key motivation behind AGPI^2 is to enhance the training of a deep V-I ReID\nbackbone by generating privileged images that provide additional information.\nThese privileged images capture shared discriminative features that are not\neasily accessible within the original V or I modalities alone. Towards this\ngoal, a non-linear generative module is trained with an adversarial objective,\ntranslating V images into intermediate spaces with a smaller domain shift\nw.r.t. the I domain. Meanwhile, the embedding module within AGPI^2 aims to\nproduce similar features for both V and generated images, encouraging the\nextraction of features that are common to all modalities. In addition to these\ncontributions, AGPI^2 employs adversarial objectives for adapting the\nintermediate images, which play a crucial role in creating a\nnon-modality-specific space to address the large domain shifts between V and I\ndomains. Experimental results conducted on challenging V-I ReID datasets\nindicate that AGPI^2 increases matching accuracy without extra computational\nresources during inference.\n","authors":["Mahdi Alehdaghi","Arthur Josi","Pourya Shamsolmoali","Rafael M. O. Cruz","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2307.03240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10782v2","updated":"2025-02-10T05:04:53Z","published":"2024-03-16T03:03:27Z","title":"Bidirectional Multi-Step Domain Generalization for Visible-Infrared\n  Person Re-Identification","summary":"  A key challenge in visible-infrared person re-identification (V-I ReID) is\ntraining a backbone model capable of effectively addressing the significant\ndiscrepancies across modalities. State-of-the-art methods that generate a\nsingle intermediate bridging domain are often less effective, as this generated\ndomain may not adequately capture sufficient common discriminant information.\nThis paper introduces Bidirectional Multi-step Domain Generalization (BMDG), a\nnovel approach for unifying feature representations across diverse modalities.\nBMDG creates multiple virtual intermediate domains by learning and aligning\nbody part features extracted from both I and V modalities. In particular, our\nmethod aims to minimize the cross-modal gap in two steps. First, BMDG aligns\nmodalities in the feature space by learning shared and modality-invariant body\npart prototypes from V and I images. Then, it generalizes the feature\nrepresentation by applying bidirectional multi-step learning, which\nprogressively refines feature representations in each step and incorporates\nmore prototypes from both modalities. Based on these prototypes, multiple\nbridging steps enhance the feature representation. Experiments conducted on V-I\nReID datasets indicate that our BMDG approach can outperform state-of-the-art\npart-based and intermediate generation methods, and can be integrated into\nother part-based methods to enhance their V-I ReID performance. (Our code is\navailable at:https:/alehdaghi.github.io/BMDG/ )\n","authors":["Mahdi Alehdaghi","Pourya Shamsolmoali","Rafael M. O. Cruz","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2403.10782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06155v1","updated":"2025-02-10T05:00:56Z","published":"2025-02-10T05:00:56Z","title":"Efficient-vDiT: Efficient Video Diffusion Transformers With Attention\n  Tile","summary":"  Despite the promise of synthesizing high-fidelity videos, Diffusion\nTransformers (DiTs) with 3D full attention suffer from expensive inference due\nto the complexity of attention computation and numerous sampling steps. For\nexample, the popular Open-Sora-Plan model consumes more than 9 minutes for\ngenerating a single video of 29 frames. This paper addresses the inefficiency\nissue from two aspects: 1) Prune the 3D full attention based on the redundancy\nwithin video data; We identify a prevalent tile-style repetitive pattern in the\n3D attention maps for video data, and advocate a new family of sparse 3D\nattention that holds a linear complexity w.r.t. the number of video frames. 2)\nShorten the sampling process by adopting existing multi-step consistency\ndistillation; We split the entire sampling trajectory into several segments and\nperform consistency distillation within each one to activate few-step\ngeneration capacities. We further devise a three-stage training pipeline to\nconjoin the low-complexity attention and few-step generation capacities.\nNotably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into\nan efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video\ngeneration with a marginal performance trade-off in VBench. In addition, we\ndemonstrate that our approach is amenable to distributed inference, achieving\nan additional 3.91x speedup when running on 4 GPUs with sequence parallelism.\n","authors":["Hangliang Ding","Dacheng Li","Runlong Su","Peiyuan Zhang","Zhijie Deng","Ion Stoica","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04757v2","updated":"2025-02-10T04:39:28Z","published":"2025-02-07T08:43:15Z","title":"ELITE: Enhanced Language-Image Toxicity Evaluation for Safety","summary":"  Current Vision Language Models (VLMs) remain vulnerable to malicious prompts\nthat induce harmful outputs. Existing safety benchmarks for VLMs primarily rely\non automated evaluation methods, but these methods struggle to detect implicit\nharmful content or produce inaccurate evaluations. Therefore, we found that\nexisting benchmarks have low levels of harmfulness, ambiguous data, and limited\ndiversity in image-text pair combinations. To address these issues, we propose\nthe ELITE benchmark, a high-quality safety evaluation benchmark for VLMs,\nunderpinned by our enhanced evaluation method, the ELITE evaluator. The ELITE\nevaluator explicitly incorporates a toxicity score to accurately assess\nharmfulness in multimodal contexts, where VLMs often provide specific,\nconvincing, but unharmful descriptions of images. We filter out ambiguous and\nlow-quality image-text pairs from existing benchmarks using the ELITE evaluator\nand generate diverse combinations of safe and unsafe image-text pairs. Our\nexperiments demonstrate that the ELITE evaluator achieves superior alignment\nwith human evaluations compared to prior automated methods, and the ELITE\nbenchmark offers enhanced benchmark quality and diversity. By introducing\nELITE, we pave the way for safer, more robust VLMs, contributing essential\ntools for evaluating and mitigating safety risks in real-world applications.\n","authors":["Wonjun Lee","Doehyeon Lee","Eugene Choi","Sangyoon Yu","Ashkan Yousefpour","Haon Park","Bumsub Ham","Suhyun Kim"],"pdf_url":"https://arxiv.org/pdf/2502.04757v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14090v6","updated":"2025-02-10T04:32:08Z","published":"2024-09-21T10:08:52Z","title":"Window-based Channel Attention for Wavelet-enhanced Learned Image\n  Compression","summary":"  Learned Image Compression (LIC) models have achieved superior rate-distortion\nperformance than traditional codecs. Existing LIC models use CNN, Transformer,\nor Mixed CNN-Transformer as basic blocks. However, limited by the shifted\nwindow attention, Swin-Transformer-based LIC exhibits a restricted growth of\nreceptive fields, affecting the ability to model large objects for image\ncompression. To address this issue and improve the performance, we incorporate\nwindow partition into channel attention for the first time to obtain large\nreceptive fields and capture more global information. Since channel attention\nhinders local information learning, it is important to extend existing\nattention mechanisms in Transformer codecs to the space-channel attention to\nestablish multiple receptive fields, being able to capture global correlations\nwith large receptive fields while maintaining detailed characterization of\nlocal correlations with small receptive fields. We also incorporate the\ndiscrete wavelet transform into our Spatial-Channel Hybrid (SCH) framework for\nefficient frequency-dependent down-sampling and further enlarging receptive\nfields. Experiment results demonstrate that our method achieves\nstate-of-the-art performances, reducing BD-rate by 18.54%, 23.98%, 22.33%, and\n24.71% on four standard datasets compared to VTM-23.1.\n","authors":["Heng Xu","Bowen Hai","Yushun Tang","Zhihai He"],"pdf_url":"https://arxiv.org/pdf/2409.14090v6.pdf","comment":"ACCV2024 accepted"},{"id":"http://arxiv.org/abs/2502.06145v1","updated":"2025-02-10T04:20:11Z","published":"2025-02-10T04:20:11Z","title":"Animate Anyone 2: High-Fidelity Character Image Animation with\n  Environment Affordance","summary":"  Recent character image animation methods based on diffusion models, such as\nAnimate Anyone, have made significant progress in generating consistent and\ngeneralizable character animations. However, these approaches fail to produce\nreasonable associations between characters and their environments. To address\nthis limitation, we introduce Animate Anyone 2, aiming to animate characters\nwith environment affordance. Beyond extracting motion signals from source\nvideo, we additionally capture environmental representations as conditional\ninputs. The environment is formulated as the region with the exclusion of\ncharacters and our model generates characters to populate these regions while\nmaintaining coherence with the environmental context. We propose a\nshape-agnostic mask strategy that more effectively characterizes the\nrelationship between character and environment. Furthermore, to enhance the\nfidelity of object interactions, we leverage an object guider to extract\nfeatures of interacting objects and employ spatial blending for feature\ninjection. We also introduce a pose modulation strategy that enables the model\nto handle more diverse motion patterns. Experimental results demonstrate the\nsuperior performance of the proposed method.\n","authors":["Li Hu","Guangyuan Wang","Zhen Shen","Xin Gao","Dechao Meng","Lian Zhuo","Peng Zhang","Bang Zhang","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2502.06145v1.pdf","comment":"Project Page: https://humanaigc.github.io/animate-anyone-2/"},{"id":"http://arxiv.org/abs/2410.00700v3","updated":"2025-02-10T04:06:39Z","published":"2024-10-01T13:54:29Z","title":"Mining Your Own Secrets: Diffusion Classifier Scores for Continual\n  Personalization of Text-to-Image Diffusion Models","summary":"  Personalized text-to-image diffusion models have grown popular for their\nability to efficiently acquire a new concept from user-defined text\ndescriptions and a few images. However, in the real world, a user may wish to\npersonalize a model on multiple concepts but one at a time, with no access to\nthe data from previous concepts due to storage/privacy concerns. When faced\nwith this continual learning (CL) setup, most personalization methods fail to\nfind a balance between acquiring new concepts and retaining previous ones -- a\nchallenge that continual personalization (CP) aims to solve. Inspired by the\nsuccessful CL methods that rely on class-specific information for\nregularization, we resort to the inherent class-conditioned density estimates,\nalso known as diffusion classifier (DC) scores, for continual personalization\nof text-to-image diffusion models. Namely, we propose using DC scores for\nregularizing the parameter-space and function-space of text-to-image diffusion\nmodels, to achieve continual personalization. Using several diverse evaluation\nsetups, datasets, and metrics, we show that our proposed regularization-based\nCP methods outperform the state-of-the-art C-LoRA, and other baselines.\nFinally, by operating in the replay-free CL setup and on low-rank adapters, our\nmethod incurs zero storage and parameter overhead, respectively, over the\nstate-of-the-art. Our project page:\nhttps://srvcodes.github.io/continual_personalization/\n","authors":["Saurav Jha","Shiqi Yang","Masato Ishii","Mengjie Zhao","Christian Simon","Muhammad Jehanzeb Mirza","Dong Gong","Lina Yao","Shusuke Takahashi","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.00700v3.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.06138v1","updated":"2025-02-10T03:59:27Z","published":"2025-02-10T03:59:27Z","title":"Enhanced Hybrid Deep Learning Approach for Botnet Attacks Detection in\n  IoT Environment","summary":"  Cyberattacks in an Internet of Things (IoT) environment can have significant\nimpacts because of the interconnected nature of devices and systems. An\nattacker uses a network of compromised IoT devices in a botnet attack to carry\nout various harmful activities. Detecting botnet attacks poses several\nchallenges because of the intricate and evolving nature of these threats.\nBotnet attacks erode trust in IoT devices and systems, undermining confidence\nin their security, reliability, and integrity. Deep learning techniques have\nsignificantly enhanced the detection of botnet attacks due to their ability to\nanalyze and learn from complex patterns in data. This research proposed the\nstacking of Deep convolutional neural networks, Bi-Directional Long Short-Term\nMemory (Bi-LSTM), Bi-Directional Gated Recurrent Unit (Bi-GRU), and Recurrent\nNeural Networks (RNN) for botnet attacks detection. The UNSW-NB15 dataset is\nutilized for botnet attacks detection. According to experimental results, the\nproposed model accurately provides for the intricate patterns and features of\nbotnet attacks, with a testing accuracy of 99.76%. The proposed model also\nidentifies botnets with a high ROC-AUC curve value of 99.18%. A performance\ncomparison of the proposed method with existing state-of-the-art models\nconfirms its higher performance. The outcomes of this research could strengthen\ncyber security procedures and safeguard against new attacks.\n","authors":["A. Karthick kumar","S. Rathnamala","T. Vijayashanthi","M. Prabhananthakumar","Alavikunhu Panthakkan","Shadi Atalla","Wathiq Mansoor"],"pdf_url":"https://arxiv.org/pdf/2502.06138v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2502.06134v1","updated":"2025-02-10T03:49:41Z","published":"2025-02-10T03:49:41Z","title":"Integrating Sequence and Image Modeling in Irregular Medical Time Series\n  Through Self-Supervised Learning","summary":"  Medical time series are often irregular and face significant missingness,\nposing challenges for data analysis and clinical decision-making. Existing\nmethods typically adopt a single modeling perspective, either treating series\ndata as sequences or transforming them into image representations for further\nclassification. In this paper, we propose a joint learning framework that\nincorporates both sequence and image representations. We also design three\nself-supervised learning strategies to facilitate the fusion of sequence and\nimage representations, capturing a more generalizable joint representation. The\nresults indicate that our approach outperforms seven other state-of-the-art\nmodels in three representative real-world clinical datasets. We further\nvalidate our approach by simulating two major types of real-world missingness\nthrough leave-sensors-out and leave-samples-out techniques. The results\ndemonstrate that our approach is more robust and significantly surpasses other\nbaselines in terms of classification performance.\n","authors":["Liuqing Chen","Shuhong Xiao","Shixian Ding","Shanhai Hu","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2502.06134v1.pdf","comment":"9 pages, 2 figures, AAAI2025"},{"id":"http://arxiv.org/abs/2502.06132v1","updated":"2025-02-10T03:46:39Z","published":"2025-02-10T03:46:39Z","title":"Enhancing Document Key Information Localization Through Data\n  Augmentation","summary":"  The Visually Rich Form Document Intelligence and Understanding (VRDIU) Track\nB focuses on the localization of key information in document images. The goal\nis to develop a method capable of localizing objects in both digital and\nhandwritten documents, using only digital documents for training. This paper\npresents a simple yet effective approach that includes a document augmentation\nphase and an object detection phase. Specifically, we augment the training set\nof digital documents by mimicking the appearance of handwritten documents. Our\nexperiments demonstrate that this pipeline enhances the models' generalization\nability and achieves high performance in the competition.\n","authors":["Yue Dai"],"pdf_url":"https://arxiv.org/pdf/2502.06132v1.pdf","comment":"Accepted as a workshop paper in DOCUI-AAAI2025"},{"id":"http://arxiv.org/abs/2502.06130v1","updated":"2025-02-10T03:43:55Z","published":"2025-02-10T03:43:55Z","title":"Self-Correcting Decoding with Generative Feedback for Mitigating\n  Hallucinations in Large Vision-Language Models","summary":"  While recent Large Vision-Language Models (LVLMs) have shown remarkable\nperformance in multi-modal tasks, they are prone to generating hallucinatory\ntext responses that do not align with the given visual input, which restricts\ntheir practical applicability in real-world scenarios. In this work, inspired\nby the observation that the text-to-image generation process is the inverse of\nimage-conditioned response generation in LVLMs, we explore the potential of\nleveraging text-to-image generative models to assist in mitigating\nhallucinations in LVLMs. We discover that generative models can offer valuable\nself-feedback for mitigating hallucinations at both the response and token\nlevels. Building on this insight, we introduce self-correcting Decoding with\nGenerative Feedback (DeGF), a novel training-free algorithm that incorporates\nfeedback from text-to-image generative models into the decoding process to\neffectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an\nimage from the initial response produced by LVLMs, which acts as an auxiliary\nvisual reference and provides self-feedback to verify and correct the initial\nresponse through complementary or contrastive decoding. Extensive experimental\nresults validate the effectiveness of our approach in mitigating diverse types\nof hallucinations, consistently surpassing state-of-the-art methods across six\nbenchmarks. Code is available at https://github.com/zhangce01/DeGF.\n","authors":["Ce Zhang","Zifu Wan","Zhehan Kan","Martin Q. Ma","Simon Stepputtis","Deva Ramanan","Russ Salakhutdinov","Louis-Philippe Morency","Katia Sycara","Yaqi Xie"],"pdf_url":"https://arxiv.org/pdf/2502.06130v1.pdf","comment":"Accepted by ICLR 2025. Project page:https://zhangce01.github.io/DeGF/"},{"id":"http://arxiv.org/abs/2502.06127v1","updated":"2025-02-10T03:29:34Z","published":"2025-02-10T03:29:34Z","title":"Improved YOLOv5s model for key components detection of power\n  transmission lines","summary":"  High-voltage transmission lines are located far from the road, resulting in\ninconvenient inspection work and rising maintenance costs. Intelligent\ninspection of power transmission lines has become increasingly important.\nHowever, subsequent intelligent inspection relies on accurately detecting\nvarious key components. Due to the low detection accuracy of key components in\ntransmission line image inspection, this paper proposed an improved object\ndetection model based on the YOLOv5s (You Only Look Once Version 5 Small) model\nto improve the detection accuracy of key components of transmission lines.\nAccording to the characteristics of the power grid inspection image, we first\nmodify the distance measurement in the k-means clustering to improve the anchor\nmatching of the YOLOv5s model. Then, we add the convolutional block attention\nmodule (CBAM) attention mechanism to the backbone network to improve accuracy.\nFinally, we apply the focal loss function to reduce the impact of class\nimbalance. Our improved method's mAP (mean average precision) reached 98.1%,\nthe precision reached 97.5%, the recall reached 94.4%, and the detection rate\nreached 84.8 FPS (frames per second). The experimental results show that our\nimproved model improves detection accuracy and has performance advantages over\nother models.\n","authors":["Chen Chen","Guowu Yuan","Hao Zhou","Yi Ma"],"pdf_url":"https://arxiv.org/pdf/2502.06127v1.pdf","comment":"23 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.03549v3","updated":"2025-02-10T03:28:56Z","published":"2025-02-05T19:03:58Z","title":"Kronecker Mask and Interpretive Prompts are Language-Action Video\n  Learners","summary":"  Contrastive language-image pretraining (CLIP) has significantly advanced\nimage-based vision learning. A pressing topic subsequently arises: how can we\neffectively adapt CLIP to the video domain? Recent studies have focused on\nadjusting either the textual or visual branch of CLIP for action recognition.\nHowever, we argue that adaptations of both branches are crucial. In this paper,\nwe propose \\textbf{CLAVER}: a \\textbf{C}ontrastive\n\\textbf{L}anguage-\\textbf{A}ction \\textbf{V}ideo Learn\\textbf{er}, designed to\nshift CLIP's focus from the alignment of static visual objects and concrete\nnouns to the alignment of dynamic action behaviors and abstract verbs.\nSpecifically, we introduce a novel Kronecker mask attention for temporal\nmodeling. Our tailored Kronecker mask offers three benefits 1) it expands the\ntemporal receptive field for each token, 2) it serves as an effective\nspatiotemporal heterogeneity inductive bias, mitigating the issue of\nspatiotemporal homogenization, and 3) it can be seamlessly plugged into\ntransformer-based models. Regarding the textual branch, we leverage large\nlanguage models to generate diverse, sentence-level and semantically rich\ninterpretive prompts of actions, which shift the model's focus towards the verb\ncomprehension. Extensive experiments on various benchmarks and learning\nscenarios demonstrate the superiority and generality of our approach.\n","authors":["Jingyi Yang","Zitong Yu","Xiuming Ni","Jia He","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2502.03549v3.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.06119v1","updated":"2025-02-10T03:12:49Z","published":"2025-02-10T03:12:49Z","title":"An Appearance Defect Detection Method for Cigarettes Based on\n  C-CenterNet","summary":"  Due to the poor adaptability of traditional methods in the cigarette\ndetection task on the automatic cigarette production line, it is difficult to\naccurately identify whether a cigarette has defects and the types of defects;\nthus, a cigarette appearance defect detection method based on C-CenterNet is\nproposed. This detector uses keypoint estimation to locate center points and\nregresses all other defect properties. Firstly, Resnet50 is used as the\nbackbone feature extraction network, and the convolutional block attention\nmechanism (CBAM) is introduced to enhance the network's ability to extract\neffective features and reduce the interference of non-target information. At\nthe same time, the feature pyramid network is used to enhance the feature\nextraction of each layer. Then, deformable convolution is used to replace part\nof the common convolution to enhance the learning ability of different shape\ndefects. Finally, the activation function ACON (ActivateOrNot) is used instead\nof the ReLU activation function, and the activation operation of some neurons\nis adaptively selected to improve the detection accuracy of the network. The\nexperimental results are mainly acquired via the mean Average Precision (mAP).\nThe experimental results show that the mAP of the C-CenterNet model applied in\nthe cigarette appearance defect detection task is 95.01%. Compared with the\noriginal CenterNet model, the model's success rate is increased by 6.14%, so it\ncan meet the requirements of precision and adaptability in cigarette detection\ntasks on the automatic cigarette production line.\n","authors":["Hongyu Liu","Guowu Yuan","Lei Yang","Kunxiao Liu","Hao Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.06119v1.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.06116v1","updated":"2025-02-10T02:50:57Z","published":"2025-02-10T02:50:57Z","title":"Event Vision Sensor: A Review","summary":"  By monitoring temporal contrast, event-based vision sensors can provide high\ntemporal resolution and low latency while maintaining low power consumption and\nsimplicity in circuit structure. These characteristics have garnered\nsignificant attention in both academia and industry. In recent years, the\napplication of back-illuminated (BSI) technology, wafer stacking techniques,\nand industrial interfaces has brought new opportunities for enhancing the\nperformance of event-based vision sensors. This is evident in the substantial\nadvancements made in reducing noise, improving resolution, and increasing\nreadout rates. Additionally, the integration of these technologies has enhanced\nthe compatibility of event-based vision sensors with current and edge vision\nsystems, providing greater possibilities for their practical applications. This\npaper will review the progression from neuromorphic engineering to\nstate-of-the-art event-based vision sensor technologies, including their\ndevelopment trends, operating principles, and key features. Moreover, we will\ndelve into the sensitivity of event-based vision sensors and the opportunities\nand challenges they face in the realm of infrared imaging, providing references\nfor future research and applications.\n","authors":["Xinyue Qin","Junlin Zhang","Wenzhong Bao","Chun Lin","Honglei Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06114v1","updated":"2025-02-10T02:48:56Z","published":"2025-02-10T02:48:56Z","title":"A Novel Multi-Teacher Knowledge Distillation for Real-Time Object\n  Detection using 4D Radar","summary":"  Accurate 3D object detection is crucial for safe autonomous navigation,\nrequiring reliable performance across diverse weather conditions. While LiDAR\nperformance deteriorates in challenging weather, Radar systems maintain their\nreliability. Traditional Radars have limitations due to their lack of elevation\ndata, but the recent 4D Radars overcome this by measuring elevation alongside\nrange, azimuth, and Doppler velocity, making them invaluable for autonomous\nvehicles. The primary challenge in utilizing 4D Radars is the sparsity of their\npoint clouds. Previous works address this by developing architectures that\nbetter capture semantics and context in sparse point cloud, largely drawing\nfrom LiDAR-based approaches. However, these methods often overlook a unique\nadvantage of 4D Radars: the dense Radar tensor, which encapsulates power\nmeasurements across three spatial dimensions and the Doppler dimension. Our\npaper leverages this tensor to tackle the sparsity issue. We introduce a novel\nknowledge distillation framework that enables a student model to densify its\nsparse input in the latent space by emulating an ensemble of teacher models.\nOur experiments demonstrate a 25% performance improvement over the\nstate-of-the-art RTNH model on the K-Radar dataset. Notably, this improvement\nis achieved while still maintaining a real-time inference speed.\n","authors":["Seung-Hyun Song","Dong-Hee Paek","Minh-Quan Dao","Ezio Malis","Seung-Hyun Kong"],"pdf_url":"https://arxiv.org/pdf/2502.06114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01166v2","updated":"2025-02-10T02:39:44Z","published":"2024-12-02T06:09:46Z","title":"Object Agnostic 3D Lifting in Space and Time","summary":"  We present a spatio-temporal perspective on category-agnostic 3D lifting of\n2D keypoints over a temporal sequence. Our approach differs from existing\nstate-of-the-art methods that are either: (i) object-agnostic, but can only\noperate on individual frames, or (ii) can model space-time dependencies, but\nare only designed to work with a single object category. Our approach is\ngrounded in two core principles. First, general information about similar\nobjects can be leveraged to achieve better performance when there is little\nobject-specific training data. Second, a temporally-proximate context window is\nadvantageous for achieving consistency throughout a sequence. These two\nprinciples allow us to outperform current state-of-the-art methods on per-frame\nand per-sequence metrics for a variety of animal categories. Lastly, we release\na new synthetic dataset containing 3D skeletons and motion sequences for a\nvariety of animal categories.\n","authors":["Christopher Fusco","Shin-Fang Ch'ng","Mosam Dabhi","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2412.01166v2.pdf","comment":"3DV 2025"},{"id":"http://arxiv.org/abs/2411.18409v2","updated":"2025-02-10T02:15:57Z","published":"2024-11-27T14:55:16Z","title":"Deep Fourier-embedded Network for RGB and Thermal Salient Object\n  Detection","summary":"  The rapid development of deep learning has significantly improved salient\nobject detection (SOD) combining both RGB and thermal (RGB-T) images. However,\nexisting deep learning-based RGB-T SOD models suffer from two major\nlimitations. First, Transformer-based models with quadratic complexity are\ncomputationally expensive and memory-intensive, limiting their application in\nhigh-resolution bi-modal feature fusion. Second, even when these models\nconverge to an optimal solution, there remains a frequency gap between the\nprediction and ground-truth. To overcome these limitations, we propose a purely\nFourier transform-based model, namely Deep Fourier-Embedded Network (DFENet),\nfor accurate RGB-T SOD. To address the computational complexity when dealing\nwith high-resolution images, we leverage the efficiency of fast Fourier\ntransform with linear complexity to design three key components: (1) the\nModal-coordinated Perception Attention, which fuses RGB and thermal modalities\nwith enhanced multi-dimensional representation; (2) the Frequency-decomposed\nEdge-aware Block, which clarifies object edges by deeply decomposing and\nenhancing frequency components of low-level features; and (3) the Fourier\nResidual Channel Attention Block, which prioritizes high-frequency information\nwhile aligning channel-wise global relationships. To mitigate the frequency\ngap, we propose Co-focus Frequency Loss, which dynamically weights hard\nfrequencies during edge frequency reconstruction by cross-referencing bi-modal\nedge information in the Fourier domain. Extensive experiments on four RGB-T SOD\nbenchmark datasets demonstrate that DFENet outperforms fifteen existing\nstate-of-the-art RGB-T SOD models. Comprehensive ablation studies further\nvalidate the value and effectiveness of our newly proposed components. The code\nis available at https://github.com/JoshuaLPF/DFENet.\n","authors":["Pengfei Lyu","Pak-Hei Yeung","Xiaosheng Yu","Chengdong Wu","Jagath C. Rajapakse"],"pdf_url":"https://arxiv.org/pdf/2411.18409v2.pdf","comment":"12 pages, 13 figures. Submitted to Journal on April 29, 2024"},{"id":"http://arxiv.org/abs/2502.06100v1","updated":"2025-02-10T02:12:24Z","published":"2025-02-10T02:12:24Z","title":"Col-OLHTR: A Novel Framework for Multimodal Online Handwritten Text\n  Recognition","summary":"  Online Handwritten Text Recognition (OLHTR) has gained considerable attention\nfor its diverse range of applications. Current approaches usually treat OLHTR\nas a sequence recognition task, employing either a single trajectory or image\nencoder, or multi-stream encoders, combined with a CTC or attention-based\nrecognition decoder. However, these approaches face several drawbacks: 1)\nsingle encoders typically focus on either local trajectories or visual regions,\nlacking the ability to dynamically capture relevant global features in\nchallenging cases; 2) multi-stream encoders, while more comprehensive, suffer\nfrom complex structures and increased inference costs. To tackle this, we\npropose a Collaborative learning-based OLHTR framework, called Col-OLHTR, that\nlearns multimodal features during training while maintaining a single-stream\ninference process. Col-OLHTR consists of a trajectory encoder, a\nPoint-to-Spatial Alignment (P2SA) module, and an attention-based decoder. The\nP2SA module is designed to learn image-level spatial features through\ntrajectory-encoded features and 2D rotary position embeddings. During training,\nan additional image-stream encoder-decoder is collaboratively trained to\nprovide supervision for P2SA features. At inference, the extra streams are\ndiscarded, and only the P2SA module is used and merged before the decoder,\nsimplifying the process while preserving high performance. Extensive\nexperimental results on several OLHTR benchmarks demonstrate the\nstate-of-the-art (SOTA) performance, proving the effectiveness and robustness\nof our design.\n","authors":["Chenyu Liu","Jinshui Hu","Baocai Yin","Jia Pan","Bing Yin","Jun Du","Qingfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2502.06100v1.pdf","comment":"ICASSP 2025"},{"id":"http://arxiv.org/abs/2502.06094v1","updated":"2025-02-10T01:45:26Z","published":"2025-02-10T01:45:26Z","title":"Fair-MoE: Fairness-Oriented Mixture of Experts in Vision-Language Models","summary":"  Fairness is a fundamental principle in medical ethics. Vision Language Models\n(VLMs) have shown significant potential in the medical field due to their\nability to leverage both visual and linguistic contexts, reducing the need for\nlarge datasets and enabling the performance of complex tasks. However, the\nexploration of fairness within VLM applications remains limited. Applying VLMs\nwithout a comprehensive analysis of fairness could lead to concerns about equal\ntreatment opportunities and diminish public trust in medical deep learning\nmodels. To build trust in medical VLMs, we propose Fair-MoE, a model\nspecifically designed to ensure both fairness and effectiveness. Fair-MoE\ncomprises two key components: \\textit{the Fairness-Oriented Mixture of Experts\n(FO-MoE)} and \\textit{the Fairness-Oriented Loss (FOL)}. FO-MoE is designed to\nleverage the expertise of various specialists to filter out biased patch\nembeddings and use an ensemble approach to extract more equitable information\nrelevant to specific tasks. FOL is a novel fairness-oriented loss function that\nnot only minimizes the distances between different attributes but also\noptimizes the differences in the dispersion of various attributes'\ndistributions. Extended experiments demonstrate the effectiveness and fairness\nof Fair-MoE. Tested on the Harvard-FairVLMed dataset, Fair-MoE showed\nimprovements in both fairness and accuracy across all four attributes. Code\nwill be publicly available.\n","authors":["Peiran Wang","Linjie Tong","Jiaxiang Liu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2502.06094v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.06705v1","updated":"2025-02-10T17:33:22Z","published":"2025-02-10T17:33:22Z","title":"RSAttAE: An Information-Aware Attention-based Autoencoder Recommender\n  System","summary":"  Recommender systems play a crucial role in modern life, including information\nretrieval, the pharmaceutical industry, retail, and entertainment. The\nentertainment sector, in particular, attracts significant attention and\ngenerates substantial profits. This work proposes a new method for predicting\nunknown user-movie ratings to enhance customer satisfaction. To achieve this,\nwe utilize the MovieLens 100K dataset. Our approach introduces an\nattention-based autoencoder to create meaningful representations and the\nXGBoost method for rating predictions. The results demonstrate that our\nproposal outperforms most of the existing state-of-the-art methods.\nAvailability: github.com/ComputationIASBS/RecommSys\n","authors":["Amirhossein Dadashzadeh Taromi","Sina Heydari","Mohsen Hooshmand","Majid Ramezani"],"pdf_url":"https://arxiv.org/pdf/2502.06705v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.06648v1","updated":"2025-02-10T16:38:03Z","published":"2025-02-10T16:38:03Z","title":"The 2021 Tokyo Olympics Multilingual News Article Dataset","summary":"  In this paper, we introduce a dataset of multilingual news articles covering\nthe 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from\n1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and\npublished between July 1, 2021, and August 14, 2021. These articles are written\nin nine languages from different language families and in different scripts. To\ncreate the dataset, the raw news articles were first retrieved via a service\nthat collects and analyzes news articles. Then, the articles were grouped using\nan online clustering algorithm, with each group containing articles reporting\non the same sub-event. Finally, the groups were manually annotated and\nevaluated. The development of this dataset aims to provide a resource for\nevaluating the performance of multilingual news clustering algorithms, for\nwhich limited datasets are available. It can also be used to analyze the\ndynamics and events of the 2021 Tokyo Olympics from different perspectives. The\ndataset is available in CSV format and can be accessed from the CLARIN.SI\nrepository.\n","authors":["Erik Novak","Erik Calcina","Dunja Mladenić","Marko Grobelnik"],"pdf_url":"https://arxiv.org/pdf/2502.06648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06557v1","updated":"2025-02-10T15:24:55Z","published":"2025-02-10T15:24:55Z","title":"LiveForesighter: Generating Future Information for Live-Streaming\n  Recommendations at Kuaishou","summary":"  Live-streaming, as a new-generation media to connect users and authors, has\nattracted a lot of attention and experienced rapid growth in recent years.\nCompared with the content-static short-video recommendation, the live-streaming\nrecommendation faces more challenges in giving our users a satisfactory\nexperience: (1) Live-streaming content is dynamically ever-changing along time.\n(2) valuable behaviors (e.g., send digital-gift, buy products) always require\nusers to watch for a long-time (>10 min). Combining the two attributes, here\nraising a challenging question for live-streaming recommendation: How to\ndiscover the live-streamings that the content user is interested in at the\ncurrent moment, and further a period in the future?\n","authors":["Yucheng Lu","Jiangxia Cao","Xu Kuan","Wei Cheng","Wei Jiang","Jiaming Zhang","Yang Shuang","Liu Zhaojie","Liyin Hong"],"pdf_url":"https://arxiv.org/pdf/2502.06557v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.09425v2","updated":"2025-02-10T15:17:49Z","published":"2024-11-14T13:22:41Z","title":"MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity","summary":"  Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.\n","authors":["Xiao Lv","Jiangxia Cao","Shijie Guan","Xiaoyou Zhou","Zhiguang Qi","Yaqiang Zang","Ming Li","Ben Wang","Kun Gai","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.09425v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.06062v4","updated":"2025-02-10T10:55:08Z","published":"2024-10-08T14:09:12Z","title":"LLM-based SPARQL Query Generation from Natural Language over Federated\n  Knowledge Graphs","summary":"  We introduce a Retrieval-Augmented Generation (RAG) system for translating\nuser questions into accurate federated SPARQL queries over bioinformatics\nknowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance\naccuracy and reduce hallucinations in query generation, our system utilises\nmetadata from the KGs, including query examples and schema information, and\nincorporates a validation step to correct generated queries. The system is\navailable online at chat.expasy.org.\n","authors":["Vincent Emonet","Jerven Bolleman","Severine Duvaud","Tarcisio Mendes de Farias","Ana Claudia Sima"],"pdf_url":"https://arxiv.org/pdf/2410.06062v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06269v1","updated":"2025-02-10T09:08:37Z","published":"2025-02-10T09:08:37Z","title":"Progressive Collaborative and Semantic Knowledge Fusion for Generative\n  Recommendation","summary":"  With the recent surge in interest surrounding generative paradigms,\ngenerative recommendation has increasingly attracted the attention of\nresearchers in the recommendation community. This paradigm generally consists\nof two stages. In the first stage, pretrained semantic embeddings or\ncollaborative ID embeddings are quantized to create item codes, aiming to\ncapture and preserve rich semantic or collaborative knowledge within these\ncodes. The second stage involves utilizing these discrete codes to perform an\nautoregressive sequence generation task. Existing methods often either overlook\ncollaborative or semantic knowledge, or combine the two roughly. In this paper,\nwe observe that naively concatenating representations from semantic and\ncollaborative modality leads to a semantic domination issue, where the\nresulting representation is overly influenced by semantic information,\neffectively overshadowing the collaborative representation. Consequently,\ndownstream recommendation tasks fail to fully exploit the knowledge from both\nmodalities, resulting in suboptimal performance. To address this, we propose a\nprogressive collaborative and semantic knowledge fusion model for generative\nrecommendation, named PRORec, which integrates semantic and collaborative\nknowledge with a unified code through a two-stage framework. Specifically, in\nthe first stage, we propose a cross-modality knowledge alignment task, which\nintegrates semantic knowledge into collaborative embeddings, enhancing their\nrepresentational capability. In the second stage, we propose an in-modality\nknowledge distillation task, designed to effectively capture and integrate\nknowledge from both semantic and collaborative modalities. Extensive\nexperiments on three widely used benchmarks validate the effectiveness of our\napproach, demonstrating its superiority compared to existing methods.\n","authors":["Longtao Xiao","Haozhao Wang","Cheng Wang","Linfei Ji","Yifan Wang","Jieming Zhu","Zhenhua Dong","Rui Zhang","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.06269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01457v2","updated":"2025-02-10T08:58:30Z","published":"2024-11-03T06:47:45Z","title":"Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential\n  Recommendation","summary":"  Sequential recommendation (SR) systems excel at capturing users' dynamic\npreferences by leveraging their interaction histories. Most existing SR systems\nassign a single embedding vector to each item to represent its features, and\nvarious types of models are adopted to combine these item embeddings into a\nsequence representation vector to capture the user intent. However, we argue\nthat this representation alone is insufficient to capture an item's\nmulti-faceted nature (e.g., movie genres, starring actors). Besides, users\noften exhibit complex and varied preferences within these facets (e.g., liking\nboth action and musical films in the facet of genre), which are challenging to\nfully represent. To address the issues above, we propose a novel structure\ncalled Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential\nRecommendation (FAME). We leverage sub-embeddings from each head in the last\nmulti-head attention layer to predict the next item separately. This approach\ncaptures the potential multi-faceted nature of items without increasing model\ncomplexity. A gating mechanism integrates recommendations from each head and\ndynamically determines their importance. Furthermore, we introduce a\nMixture-of-Experts (MoE) network in each attention head to disentangle various\nuser preferences within each facet. Each expert within the MoE focuses on a\nspecific preference. A learnable router network is adopted to compute the\nimportance weight for each expert and aggregate them. We conduct extensive\nexperiments on four public sequential recommendation datasets and the results\ndemonstrate the effectiveness of our method over existing baseline models.\n","authors":["Mingrui Liu","Sixiao Zhang","Cheng Long"],"pdf_url":"https://arxiv.org/pdf/2411.01457v2.pdf","comment":"This paper has been accepted by WSDM'25"},{"id":"http://arxiv.org/abs/2412.21009v2","updated":"2025-02-10T08:55:18Z","published":"2024-12-30T15:21:36Z","title":"Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline","summary":"  Recent advancements in deep learning have significantly enhanced\ncontent-based retrieval methods, notably through models like CLIP that map\nimages and texts into a shared embedding space. However, these methods often\nstruggle with domain-specific entities and long-tail concepts absent from their\ntraining data, particularly in identifying specific individuals. In this paper,\nwe explore the task of identity-aware cross-modal retrieval, which aims to\nretrieve images of persons in specific contexts based on natural language\nqueries. This task is critical in various scenarios, such as for searching and\nbrowsing personalized video collections or large audio-visual archives\nmaintained by national broadcasters. We introduce a novel dataset, COCO Person\nFaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched\nwith deepfake-generated faces from VGGFace2. This dataset addresses the lack of\nlarge-scale datasets needed for training and evaluating models for this task.\nOur experiments assess the performance of different CLIP variations repurposed\nfor this task, including our architecture, Identity-aware CLIP (Id-CLIP), which\nachieves competitive retrieval performance through targeted fine-tuning. Our\ncontributions lay the groundwork for more robust cross-modal retrieval systems\ncapable of recognizing long-tail identities and contextual nuances. Data and\ncode are available at https://github.com/mesnico/IdCLIP.\n","authors":["Nicola Messina","Lucia Vadicamo","Leo Maltese","Claudio Gennaro"],"pdf_url":"https://arxiv.org/pdf/2412.21009v2.pdf","comment":"Accepted as full paper at ECIR 2025"},{"id":"http://arxiv.org/abs/2502.06252v1","updated":"2025-02-10T08:33:47Z","published":"2025-02-10T08:33:47Z","title":"Evaluating Entity Retrieval in Electronic Health Records: a Semantic Gap\n  Perspective","summary":"  Entity retrieval plays a crucial role in the utilization of Electronic Health\nRecords (EHRs) and is applied across a wide range of clinical practices.\nHowever, a comprehensive evaluation of this task is lacking due to the absence\nof a public benchmark. In this paper, we propose the development and release of\na novel benchmark for evaluating entity retrieval in EHRs, with a particular\nfocus on the semantic gap issue. Using discharge summaries from the MIMIC-III\ndataset, we incorporate ICD codes and prescription labels associated with the\nnotes as queries, and annotate relevance judgments using GPT-4. In total, we\nuse 1,000 patient notes, generate 1,246 queries, and provide over 77,000\nrelevance annotations. To offer the first assessment of the semantic gap, we\nintroduce a novel classification system for relevance matches. Leveraging\nGPT-4, we categorize each relevant pair into one of five categories: string,\nsynonym, abbreviation, hyponym, and implication. Using the proposed benchmark,\nwe evaluate several retrieval methods, including BM25, query expansion, and\nstate-of-the-art dense retrievers. Our findings show that BM25 provides a\nstrong baseline but struggles with semantic matches. Query expansion\nsignificantly improves performance, though it slightly reduces string match\ncapabilities. Dense retrievers outperform traditional methods, particularly for\nsemantic matches, and general-domain dense retrievers often surpass those\ntrained specifically in the biomedical domain.\n","authors":["Zhengyun Zhao","Hongyi Yuan","Jingjing Liu","Haichao Chen","Huaiyuan Ying","Songchi Zhou","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2502.06252v1.pdf","comment":"Under review, and the dataset will be made public upon reception of\n  our paper"},{"id":"http://arxiv.org/abs/2502.06220v1","updated":"2025-02-10T07:52:47Z","published":"2025-02-10T07:52:47Z","title":"FunduSAM: A Specialized Deep Learning Model for Enhanced Optic Disc and\n  Cup Segmentation in Fundus Images","summary":"  The Segment Anything Model (SAM) has gained popularity as a versatile image\nsegmentation method, thanks to its strong generalization capabilities across\nvarious domains. However, when applied to optic disc (OD) and optic cup (OC)\nsegmentation tasks, SAM encounters challenges due to the complex structures,\nlow contrast, and blurred boundaries typical of fundus images, leading to\nsuboptimal performance. To overcome these challenges, we introduce a novel\nmodel, FunduSAM, which incorporates several Adapters into SAM to create a deep\nnetwork specifically designed for OD and OC segmentation. The FunduSAM utilizes\nAdapter into each transformer block after encoder for parameter fine-tuning\n(PEFT). It enhances SAM's feature extraction capabilities by designing a\nConvolutional Block Attention Module (CBAM), addressing issues related to\nblurred boundaries and low contrast. Given the unique requirements of OD and OC\nsegmentation, polar transformation is used to convert the original fundus OD\nimages into a format better suited for training and evaluating FunduSAM. A\njoint loss is used to achieve structure preservation between the OD and OC,\nwhile accurate segmentation. Extensive experiments on the REFUGE dataset,\ncomprising 1,200 fundus images, demonstrate the superior performance of\nFunduSAM compared to five mainstream approaches.\n","authors":["Jinchen Yu","Yongwei Nie","Fei Qi","Wenxiong Liao","Hongmin Cai"],"pdf_url":"https://arxiv.org/pdf/2502.06220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15797v4","updated":"2025-02-10T05:31:43Z","published":"2025-01-27T05:46:06Z","title":"LemmaHead: RAG Assisted Proof Generation Using Large Language Models","summary":"  Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.\n","authors":["Tianbo Yang","Mingqi Yan","Hongyi Zhao","Tianshuo Yang"],"pdf_url":"https://arxiv.org/pdf/2501.15797v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06148v1","updated":"2025-02-10T04:29:36Z","published":"2025-02-10T04:29:36Z","title":"Optimizing Knowledge Integration in Retrieval-Augmented Generation with\n  Self-Selection","summary":"  Retrieval-Augmented Generation (RAG), which integrates external knowledge\ninto Large Language Models (LLMs), has proven effective in enabling LLMs to\nproduce more accurate and reliable responses. However, it remains a significant\nchallenge how to effectively integrate external retrieved knowledge with\ninternal parametric knowledge in LLMs. In this work, we propose a novel\nSelf-Selection RAG framework, where the LLM is made to select from pairwise\nresponses generated with internal parametric knowledge solely and with external\nretrieved knowledge together to achieve enhanced accuracy. To this end, we\ndevise a Self-Selection-RGP method to enhance the capabilities of the LLM in\nboth generating and selecting the correct answer, by training the LLM with\nDirect Preference Optimization (DPO) over a curated Retrieval Generation\nPreference (RGP) dataset. Experimental results with two open-source LLMs (i.e.,\nLlama2-13B-Chat and Mistral-7B) well demonstrate the superiority of our\napproach over other baseline methods on Natural Questions (NQ) and TrivialQA\ndatasets.\n","authors":["Yan Weng","Fengbin Zhu","Tong Ye","Haoyan Liu","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.06148v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.06101v1","updated":"2025-02-10T02:15:12Z","published":"2025-02-10T02:15:12Z","title":"RALLRec: Improving Retrieval Augmented Large Language Model\n  Recommendation with Representation Learning","summary":"  Large Language Models (LLMs) have been integrated into recommendation systems\nto enhance user behavior comprehension. The Retrieval Augmented Generation\n(RAG) technique is further incorporated into these systems to retrieve more\nrelevant items and improve system performance. However, existing RAG methods\nrely primarily on textual semantics and often fail to incorporate the most\nrelevant items, limiting the effectiveness of the systems.\n  In this paper, we propose Representation learning for retrieval-Augmented\nLarge Language model Recommendation (RALLRec). Specifically, we enhance textual\nsemantics by prompting LLMs to generate more detailed item descriptions,\nfollowed by joint representation learning of textual and collaborative\nsemantics, which are extracted by the LLM and recommendation models,\nrespectively. Considering the potential time-varying characteristics of user\ninterest, a simple yet effective reranking method is further introduced to\ncapture the dynamics of user preference. We conducted extensive experiments on\nthree real-world datasets, and the evaluation results validated the\neffectiveness of our method. Code is made public at\nhttps://github.com/JianXu95/RALLRec.\n","authors":["Jian Xu","Sichun Luo","Xiangyu Chen","Haoming Huang","Hanxu Hou","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2502.06101v1.pdf","comment":"Accepted by TheWebConf'25 (WWW'25) as a Short Paper"},{"id":"http://arxiv.org/abs/2502.06097v1","updated":"2025-02-10T02:06:17Z","published":"2025-02-10T02:06:17Z","title":"NLGR: Utilizing Neighbor Lists for Generative Rerank in Personalized\n  Recommendation Systems","summary":"  Reranking plays a crucial role in modern multi-stage recommender systems by\nrearranging the initial ranking list. Due to the inherent challenges of\ncombinatorial search spaces, some current research adopts an\nevaluator-generator paradigm, with a generator generating feasible sequences\nand an evaluator selecting the best sequence based on the estimated list\nutility. However, these methods still face two issues. Firstly, due to the goal\ninconsistency problem between the evaluator and generator, the generator tends\nto fit the local optimal solution of exposure distribution rather than\ncombinatorial space optimization. Secondly, the strategy of generating target\nitems one by one is difficult to achieve optimality because it ignores the\ninformation of subsequent items.\n  To address these issues, we propose a utilizing Neighbor Lists model for\nGenerative Reranking (NLGR), which aims to improve the performance of the\ngenerator in the combinatorial space. NLGR follows the evaluator-generator\nparadigm and improves the generator's training and generating methods.\nSpecifically, we use neighbor lists in combination space to enhance the\ntraining process, making the generator perceive the relative scores and find\nthe optimization direction. Furthermore, we propose a novel sampling-based\nnon-autoregressive generation method, which allows the generator to jump\nflexibly from the current list to any neighbor list. Extensive experiments on\npublic and industrial datasets validate NLGR's effectiveness and we have\nsuccessfully deployed NLGR on the Meituan food delivery platform.\n","authors":["Shuli Wang","Xue Wei","Senjie Kou","Chi Wang","Wenshuai Chen","Qi Tang","Yinhua Zhu","Xiong Xiao","Xingxing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06097v1.pdf","comment":"Accepted by WWW 2025 Industry Track"},{"id":"http://arxiv.org/abs/2411.12949v2","updated":"2025-02-10T00:45:01Z","published":"2024-11-20T00:43:32Z","title":"Epidemiology-informed Network for Robust Rumor Detection","summary":"  The rapid spread of rumors on social media has posed significant challenges\nto maintaining public trust and information integrity. Since an information\ncascade process is essentially a propagation tree, recent rumor detection\nmodels leverage graph neural networks to additionally capture information\npropagation patterns, thus outperforming text-only solutions. Given the\nvariations in topics and social impact of the root node, different source\ninformation naturally has distinct outreach capabilities, resulting in\ndifferent heights of propagation trees. This variation, however, impedes the\ndata-driven design of existing graph-based rumor detectors. Given a shallow\npropagation tree with limited interactions, it is unlikely for graph-based\napproaches to capture sufficient cascading patterns, questioning their ability\nto handle less popular news or early detection needs. In contrast, a deep\npropagation tree is prone to noisy user responses, and this can in turn\nobfuscate the predictions. In this paper, we propose a novel\nEpidemiology-informed Network (EIN) that integrates epidemiological knowledge\nto enhance performance by overcoming data-driven methods sensitivity to data\nquality. Meanwhile, to adapt epidemiology theory to rumor detection, it is\nexpected that each users stance toward the source information will be\nannotated. To bypass the costly and time-consuming human labeling process, we\ntake advantage of large language models to generate stance labels, facilitating\noptimization objectives for learning epidemiology-informed representations. Our\nexperimental results demonstrate that the proposed EIN not only outperforms\nstate-of-the-art methods on real-world datasets but also exhibits enhanced\nrobustness across varying tree depths.\n","authors":["Wei Jiang","Tong Chen","Xinyi Gao","Wentao Zhang","Lizhen Cui","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.12949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07067v1","updated":"2025-02-10T21:59:01Z","published":"2025-02-10T21:59:01Z","title":"Repository-level Code Search with Neural Retrieval Methods","summary":"  This paper presents a multi-stage reranking system for repository-level code\nsearch, which leverages the vastly available commit histories of large\nopen-source repositories to aid in bug fixing. We define the task of\nrepository-level code search as retrieving the set of files from the current\nstate of a code repository that are most relevant to addressing a user's\nquestion or bug. The proposed approach combines BM25-based retrieval over\ncommit messages with neural reranking using CodeBERT to identify the most\npertinent files. By learning patterns from diverse repositories and their\ncommit histories, the system can surface relevant files for the task at hand.\nThe system leverages both commit messages and source code for relevance\nmatching, and is evaluated in both normal and oracle settings. Experiments on a\nnew dataset created from 7 popular open-source repositories demonstrate\nsubstantial improvements of up to 80% in MAP, MRR and P@1 over the BM25\nbaseline, across a diverse set of queries, demonstrating the effectiveness this\napproach. We hope this work aids LLM agents as a tool for better code search\nand understanding. Our code and results obtained are publicly available.\n","authors":["Siddharth Gandhi","Luyu Gao","Jamie Callan"],"pdf_url":"https://arxiv.org/pdf/2502.07067v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2412.06949v2","updated":"2025-02-10T21:34:00Z","published":"2024-12-09T19:53:13Z","title":"Bridging Conversational and Collaborative Signals for Conversational\n  Recommendation","summary":"  Conversational recommendation systems (CRS) leverage contextual information\nfrom conversations to generate recommendations but often struggle due to a lack\nof collaborative filtering (CF) signals, which capture user-item interaction\npatterns essential for accurate recommendations. We introduce Reddit-ML32M, a\ndataset that links Reddit conversations with interactions on MovieLens 32M, to\nenrich item representations by leveraging collaborative knowledge and\naddressing interaction sparsity in conversational datasets. We propose an\nLLM-based framework that uses Reddit-ML32M to align LLM-generated\nrecommendations with CF embeddings, refining rankings for better performance.\nWe evaluate our framework against three sets of baselines: CF-based\nrecommenders using only interactions from CRS tasks, traditional CRS models,\nand LLM-based methods relying on conversational context without item\nrepresentations. Our approach achieves consistent improvements, including a\n12.32% increase in Hit Rate and a 9.9% improvement in NDCG, outperforming the\nbest-performing baseline that relies on conversational context but lacks\ncollaborative item representations.\n","authors":["Ahmad Bin Rabiah","Nafis Sadeq","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2412.06949v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.06786v1","updated":"2025-02-10T18:59:10Z","published":"2025-02-10T18:59:10Z","title":"Matryoshka Quantization","summary":"  Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to $10\\%$ more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.\n","authors":["Pranav Nair","Puranjay Datta","Jeff Dean","Prateek Jain","Aditya Kusupati"],"pdf_url":"https://arxiv.org/pdf/2502.06786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06785v1","updated":"2025-02-10T18:58:52Z","published":"2025-02-10T18:58:52Z","title":"DeepCrossAttention: Supercharging Transformer Residual Connections","summary":"  Transformer networks have achieved remarkable success across diverse domains,\nleveraging a variety of architectural innovations, including residual\nconnections. However, traditional residual connections, which simply sum the\noutputs of previous layers, can dilute crucial information. This work\nintroduces DeepCrossAttention (DCA), an approach that enhances residual\nlearning in transformers. DCA employs learnable, input-dependent weights to\ndynamically combine layer outputs, enabling the model to selectively focus on\nthe most relevant information in any of the previous layers. Furthermore, DCA\nincorporates depth-wise cross-attention, allowing for richer interactions\nbetween layers at different depths. Our language modeling experiments show that\nDCA achieves improved perplexity for a given training time. Moreover, DCA\nobtains the same model quality up to 3x faster while adding a negligible number\nof parameters. Theoretical analysis confirms that DCA provides an improved\ntrade-off between accuracy and model size when the ratio of collective layer\nranks to the ambient dimension falls below a critical threshold.\n","authors":["Mike Heddes","Adel Javanmard","Kyriakos Axiotis","Gang Fu","MohammadHossein Bateni","Vahab Mirrokni"],"pdf_url":"https://arxiv.org/pdf/2502.06785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06784v1","updated":"2025-02-10T18:58:40Z","published":"2025-02-10T18:58:40Z","title":"RelGNN: Composite Message Passing for Relational Deep Learning","summary":"  Predictive tasks on relational databases are critical in real-world\napplications spanning e-commerce, healthcare, and social media. To address\nthese tasks effectively, Relational Deep Learning (RDL) encodes relational data\nas graphs, enabling Graph Neural Networks (GNNs) to exploit relational\nstructures for improved predictions. However, existing heterogeneous GNNs often\noverlook the intrinsic structural properties of relational databases, leading\nto modeling inefficiencies. Here we introduce RelGNN, a novel GNN framework\nspecifically designed to capture the unique characteristics of relational\ndatabases. At the core of our approach is the introduction of atomic routes,\nwhich are sequences of nodes forming high-order tripartite structures. Building\nupon these atomic routes, RelGNN designs new composite message passing\nmechanisms between heterogeneous nodes, allowing direct single-hop interactions\nbetween them. This approach avoids redundant aggregations and mitigates\ninformation entanglement, ultimately leading to more efficient and accurate\npredictive modeling. RelGNN is evaluated on 30 diverse real-world tasks from\nRelBench (Fey et al., 2024), and consistently achieves state-of-the-art\naccuracy with up to 25% improvement.\n","authors":["Tianlang Chen","Charilaos Kanatsoulis","Jure Leskovec"],"pdf_url":"https://arxiv.org/pdf/2502.06784v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.06781v1","updated":"2025-02-10T18:57:29Z","published":"2025-02-10T18:57:29Z","title":"Exploring the Limit of Outcome Reward for Learning Mathematical\n  Reasoning","summary":"  Reasoning abilities, especially those for solving complex math problems, are\ncrucial components of general intelligence. Recent advances by proprietary\ncompanies, such as o-series models of OpenAI, have made remarkable progress on\nreasoning tasks. However, the complete technical details remain unrevealed, and\nthe techniques that are believed certainly to be adopted are only reinforcement\nlearning (RL) and the long chain of thoughts. This paper proposes a new RL\nframework, termed OREAL, to pursue the performance limit that can be achieved\nthrough \\textbf{O}utcome \\textbf{RE}w\\textbf{A}rd-based reinforcement\n\\textbf{L}earning for mathematical reasoning tasks, where only binary outcome\nrewards are easily accessible. We theoretically prove that behavior cloning on\npositive trajectories from best-of-N (BoN) sampling is sufficient to learn the\nKL-regularized optimal policy in binary feedback environments. This formulation\nfurther implies that the rewards of negative samples should be reshaped to\nensure the gradient consistency between positive and negative samples. To\nalleviate the long-existing difficulties brought by sparse rewards in RL, which\nare even exacerbated by the partial correctness of the long chain of thought\nfor reasoning tasks, we further apply a token-level reward model to sample\nimportant tokens in reasoning trajectories for learning. With OREAL, for the\nfirst time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL,\nbeing on par with 32B models. OREAL-32B also surpasses previous 32B models\ntrained by distillation with 95.0 pass@1 accuracy on MATH-500. Our\ninvestigation also indicates the importance of initial policy models and\ntraining queries for RL. Code, models, and data will be released to benefit\nfuture research\\footnote{https://github.com/InternLM/OREAL}.\n","authors":["Chengqi Lyu","Songyang Gao","Yuzhe Gu","Wenwei Zhang","Jianfei Gao","Kuikun Liu","Ziyi Wang","Shuaibin Li","Qian Zhao","Haian Huang","Weihan Cao","Jiangning Liu","Hongwei Liu","Junnan Liu","Songyang Zhang","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06781v1.pdf","comment":"We released our code, data, and model on\n  https://github.com/InternLM/OREAL"},{"id":"http://arxiv.org/abs/2502.06777v1","updated":"2025-02-10T18:54:41Z","published":"2025-02-10T18:54:41Z","title":"Learning an Optimal Assortment Policy under Observational Data","summary":"  We study the fundamental problem of offline assortment optimization under the\nMultinomial Logit (MNL) model, where sellers must determine the optimal subset\nof the products to offer based solely on historical customer choice data. While\nmost existing approaches to learning-based assortment optimization focus on the\nonline learning of the optimal assortment through repeated interactions with\ncustomers, such exploration can be costly or even impractical in many\nreal-world settings. In this paper, we consider the offline learning paradigm\nand investigate the minimal data requirements for efficient offline assortment\noptimization. To this end, we introduce Pessimistic Rank-Breaking (PRB), an\nalgorithm that combines rank-breaking with pessimistic estimation. We prove\nthat PRB is nearly minimax optimal by establishing the tight suboptimality\nupper bound and a nearly matching lower bound. This further shows that \"optimal\nitem coverage\" - where each item in the optimal assortment appears sufficiently\noften in the historical data - is both sufficient and necessary for efficient\noffline learning. This significantly relaxes the previous requirement of\nobserving the complete optimal assortment in the data. Our results provide\nfundamental insights into the data requirements for offline assortment\noptimization under the MNL model.\n","authors":["Yuxuan Han","Han Zhong","Miao Lu","Jose Blanchet","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.06777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06776v1","updated":"2025-02-10T18:54:05Z","published":"2025-02-10T18:54:05Z","title":"Towards Internet-Scale Training For Agents","summary":"  The predominant approach for training web navigation agents gathers human\ndemonstrations for a set of popular websites and hand-written tasks, but it is\nbecoming clear that human data are an inefficient resource. We develop a\npipeline to facilitate Internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM generates tasks for 150k diverse\nwebsites. In the next stage, LLM agents complete tasks and produce\ntrajectories. In the final stage, an LLM reviews the trajectories and judges\ntheir success. Language models are competitive with human annotators, detecting\nand filtering out harmful content with an accuracy of 97%, generating feasible\ntasks with an 89% rate, and judging successful trajectories with an 82.6%\naccuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of\ntasks for 150k sites. Training on the data generated by our pipeline is\ncompetitive with training on human demonstrations. In data-limited settings\nderived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and\n+122.1% respectively for agents trained on mixtures of data from our pipeline,\nand human data. When training agents with all available human data from these\nbenchmarks, agents fail to generalize to diverse real sites, and adding our\ndata improves their generalization by +149.0% for WebLINX and +156.3% for\nMind2Web. Code will be available at: data-for-agents.github.io.\n","authors":["Brandon Trabucco","Gunnar Sigurdsson","Robinson Piramuthu","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2502.06776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06775v1","updated":"2025-02-10T18:53:15Z","published":"2025-02-10T18:53:15Z","title":"Enhancing Performance of Explainable AI Models with Constrained Concept\n  Refinement","summary":"  The trade-off between accuracy and interpretability has long been a challenge\nin machine learning (ML). This tension is particularly significant for emerging\ninterpretable-by-design methods, which aim to redesign ML algorithms for\ntrustworthy interpretability but often sacrifice accuracy in the process. In\nthis paper, we address this gap by investigating the impact of deviations in\nconcept representations-an essential component of interpretable models-on\nprediction performance and propose a novel framework to mitigate these effects.\nThe framework builds on the principle of optimizing concept embeddings under\nconstraints that preserve interpretability. Using a generative model as a\ntest-bed, we rigorously prove that our algorithm achieves zero loss while\nprogressively enhancing the interpretability of the resulting model.\nAdditionally, we evaluate the practical performance of our proposed framework\nin generating explainable predictions for image classification tasks across\nvarious benchmarks. Compared to existing explainable methods, our approach not\nonly improves prediction accuracy while preserving model interpretability\nacross various large-scale benchmarks but also achieves this with significantly\nlower computational cost.\n","authors":["Geyu Liang","Senne Michielssen","Salar Fattahi"],"pdf_url":"https://arxiv.org/pdf/2502.06775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14161v3","updated":"2025-02-10T18:52:39Z","published":"2024-09-21T14:53:32Z","title":"When Witnesses Defend: A Witness Graph Topological Layer for Adversarial\n  Graph Learning","summary":"  Capitalizing on the intuitive premise that shape characteristics are more\nrobust to perturbations, we bridge adversarial graph learning with the emerging\ntools from computational topology, namely, persistent homology representations\nof graphs. We introduce the concept of witness complex to adversarial analysis\non graphs, which allows us to focus only on the salient shape characteristics\nof graphs, yielded by the subset of the most essential nodes (i.e., landmarks),\nwith minimal loss of topological information on the whole graph. The remaining\nnodes are then used as witnesses, governing which higher-order graph\nsubstructures are incorporated into the learning process. Armed with the\nwitness mechanism, we design Witness Graph Topological Layer (WGTL), which\nsystematically integrates both local and global topological graph feature\nrepresentations, the impact of which is, in turn, automatically controlled by\nthe robust regularized topological loss. Given the attacker's budget, we derive\nthe important stability guarantees of both local and global topology encodings\nand the associated robust topological loss. We illustrate the versatility and\nefficiency of WGTL by its integration with five GNNs and three existing\nnon-topological defense mechanisms. Our extensive experiments across six\ndatasets demonstrate that WGTL boosts the robustness of GNNs across a range of\nperturbations and against a range of adversarial attacks. Our datasets and\nsource codes are available at https://github.com/toggled/WGTL.\n","authors":["Naheed Anjum Arafat","Debabrota Basu","Yulia Gel","Yuzhou Chen"],"pdf_url":"https://arxiv.org/pdf/2409.14161v3.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2502.06774v1","updated":"2025-02-10T18:52:22Z","published":"2025-02-10T18:52:22Z","title":"ENFORCE: Exact Nonlinear Constrained Learning with Adaptive-depth Neural\n  Projection","summary":"  Ensuring neural networks adhere to domain-specific constraints is crucial for\naddressing safety and ethical concerns while also enhancing prediction\naccuracy. Despite the nonlinear nature of most real-world tasks, existing\nmethods are predominantly limited to affine or convex constraints. We introduce\nENFORCE, a neural network architecture that guarantees predictions to satisfy\nnonlinear constraints exactly. ENFORCE is trained with standard unconstrained\ngradient-based optimizers (e.g., Adam) and leverages autodifferentiation and\nlocal neural projections to enforce any $\\mathcal{C}^1$ constraint to arbitrary\ntolerance $\\epsilon$. We build an adaptive-depth neural projection (AdaNP)\nmodule that dynamically adjusts its complexity to suit the specific problem and\nthe required tolerance levels. ENFORCE guarantees satisfaction of equality\nconstraints that are nonlinear in both inputs and outputs of the neural network\nwith minimal (and adjustable) computational cost.\n","authors":["Giacomo Lastrucci","Artur M. Schweidtmann"],"pdf_url":"https://arxiv.org/pdf/2502.06774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06773v1","updated":"2025-02-10T18:52:04Z","published":"2025-02-10T18:52:04Z","title":"On the Emergence of Thinking in LLMs I: Searching for the Right\n  Intuition","summary":"  Recent AI advancements, such as OpenAI's new models, are transforming LLMs\ninto LRMs (Large Reasoning Models) that perform reasoning during inference,\ntaking extra time and compute for higher-quality outputs. We aim to uncover the\nalgorithmic framework for training LRMs. Methods like self-consistency, PRM,\nand AlphaZero suggest reasoning as guided search. We ask: what is the simplest,\nmost scalable way to enable search in LLMs?\n  We propose a post-training framework called Reinforcement Learning via\nSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with\nhuman or synthetic demonstrations of the reasoning process, (2) using an\nexploration reward signal to encourage diverse and efficient reasoning\nbehaviors, and (3) RL training with an outcome verifier to ensure correctness\nwhile preventing reward hacking. Our key innovation is to decouple exploration\nand correctness signals during PPO training, carefully balancing them to\nimprove performance and efficiency.\n  Empirical studies in the math domain show that RLSP improves reasoning. On\nthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500\ntest set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due\nto RLSP. However, a more important finding of this work is that the models\ntrained using RLSP, even with the simplest exploration reward that encourages\nthe model to take more intermediate steps, showed several emergent behaviors\nsuch as backtracking, exploration of ideas, and verification. These findings\ndemonstrate that RLSP framework might be enough to enable emergence of complex\nreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why\nRLSP search strategy is more suitable for LLMs inspired by a remarkable result\nthat says CoT provably increases computational power of LLMs, which grows as\nthe number of steps in CoT \\cite{li2024chain,merrill2023expresssive}.\n","authors":["Guanghao Ye","Khiem Duc Pham","Xinzhi Zhang","Sivakanth Gopi","Baolin Peng","Beibin Li","Janardhan Kulkarni","Huseyin A. Inan"],"pdf_url":"https://arxiv.org/pdf/2502.06773v1.pdf","comment":"Abstract shortened for arXiv"},{"id":"http://arxiv.org/abs/2502.06771v1","updated":"2025-02-10T18:50:50Z","published":"2025-02-10T18:50:50Z","title":"Unsupervised Particle Tracking with Neuromorphic Computing","summary":"  We study the application of a neural network architecture for identifying\ncharged particle trajectories via unsupervised learning of delays and synaptic\nweights using a spike-time-dependent plasticity rule. In the considered model,\nthe neurons receive time-encoded information on the position of particle hits\nin a tracking detector for a particle collider, modeled according to the\ngeometry of the Compact Muon Solenoid Phase II detector. We show how a spiking\nneural network is capable of successfully identifying in a completely\nunsupervised way the signal left by charged particles in the presence of\nconspicuous noise from accidental or combinatorial hits. These results open the\nway to applications of neuromorphic computing to particle tracking, motivating\nfurther studies into its potential for real-time, low-power particle tracking\nin future high-energy physics experiments.\n","authors":["Emanuele Coradin","Fabio Cufino","Muhammad Awais","Tommaso Dorigo","Enrico Lupi","Eleonora Porcu","Jinu Raj","Fredrik Sandin","Mia Tosi"],"pdf_url":"https://arxiv.org/pdf/2502.06771v1.pdf","comment":"24 pages, 21 figures, submitted to MDPI Particles"},{"id":"http://arxiv.org/abs/2502.06768v1","updated":"2025-02-10T18:47:21Z","published":"2025-02-10T18:47:21Z","title":"Train for the Worst, Plan for the Best: Understanding Token Ordering in\n  Masked Diffusions","summary":"  In recent years, masked diffusion models (MDMs) have emerged as a promising\nalternative approach for generative modeling over discrete domains. Compared to\nautoregressive models (ARMs), MDMs trade off complexity at training time with\nflexibility at inference time. At training time, they must learn to solve an\nexponentially large number of infilling problems, but at inference time, they\ncan decode tokens in essentially arbitrary order. In this work, we closely\nexamine these two competing effects. On the training front, we theoretically\nand empirically demonstrate that MDMs indeed train on computationally\nintractable subproblems compared to their autoregressive counterparts. On the\ninference front, we show that a suitable strategy for adaptively choosing the\ntoken decoding order significantly enhances the capabilities of MDMs, allowing\nthem to sidestep hard subproblems. On logic puzzles like Sudoku, we show that\nadaptive inference can boost solving accuracy in pretrained MDMs from $<7$% to\n$\\approx 90$%, even outperforming ARMs with $7\\times$ as many parameters and\nthat were explicitly trained via teacher forcing to learn the right order of\ndecoding.\n","authors":["Jaeyeon Kim","Kulin Shah","Vasilis Kontonis","Sham Kakade","Sitan Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06765v1","updated":"2025-02-10T18:44:30Z","published":"2025-02-10T18:44:30Z","title":"Are all models wrong? Fundamental limits in distribution-free empirical\n  model falsification","summary":"  In statistics and machine learning, when we train a fitted model on available\ndata, we typically want to ensure that we are searching within a model class\nthat contains at least one accurate model -- that is, we would like to ensure\nan upper bound on the model class risk (the lowest possible risk that can be\nattained by any model in the class). However, it is also of interest to\nestablish lower bounds on the model class risk, for instance so that we can\ndetermine whether our fitted model is at least approximately optimal within the\nclass, or, so that we can decide whether the model class is unsuitable for the\nparticular task at hand. Particularly in the setting of interpolation learning\nwhere machine learning models are trained to reach zero error on the training\ndata, we might ask if, at the very least, a positive lower bound on the model\nclass risk is possible -- or are we unable to detect that \"all models are\nwrong\"? In this work, we answer these questions in a distribution-free setting\nby establishing a model-agnostic, fundamental hardness result for the problem\nof constructing a lower bound on the best test error achievable over a model\nclass, and examine its implications on specific model classes such as\ntree-based methods and linear regression.\n","authors":["Manuel M. Müller","Yuetian Luo","Rina Foygel Barber"],"pdf_url":"https://arxiv.org/pdf/2502.06765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06764v1","updated":"2025-02-10T18:44:25Z","published":"2025-02-10T18:44:25Z","title":"History-Guided Video Diffusion","summary":"  Classifier-free guidance (CFG) is a key technique for improving conditional\ngeneration in diffusion models, enabling more accurate control while enhancing\nsample quality. It is natural to extend this technique to video diffusion,\nwhich generates video conditioned on a variable number of context frames,\ncollectively referred to as history. However, we find two key challenges to\nguiding with variable-length history: architectures that only support\nfixed-size conditioning, and the empirical observation that CFG-style history\ndropout performs poorly. To address this, we propose the Diffusion Forcing\nTransformer (DFoT), a video diffusion architecture and theoretically grounded\ntraining objective that jointly enable conditioning on a flexible number of\nhistory frames. We then introduce History Guidance, a family of guidance\nmethods uniquely enabled by DFoT. We show that its simplest form, vanilla\nhistory guidance, already significantly improves video generation quality and\ntemporal consistency. A more advanced method, history guidance across time and\nfrequency further enhances motion dynamics, enables compositional\ngeneralization to out-of-distribution history, and can stably roll out\nextremely long videos. Website: https://boyuan.space/history-guidance\n","authors":["Kiwhan Song","Boyuan Chen","Max Simchowitz","Yilun Du","Russ Tedrake","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2502.06764v1.pdf","comment":"Project Website: https://boyuan.space/history-guidance"},{"id":"http://arxiv.org/abs/2502.06761v1","updated":"2025-02-10T18:40:48Z","published":"2025-02-10T18:40:48Z","title":"When, Where and Why to Average Weights?","summary":"  Averaging checkpoints along the training trajectory is a simple yet powerful\napproach to improve the generalization performance of Machine Learning models\nand reduce training time. Motivated by these potential gains, and in an effort\nto fairly and thoroughly benchmark this technique, we present an extensive\nevaluation of averaging techniques in modern Deep Learning, which we perform\nusing AlgoPerf \\citep{dahl_benchmarking_2023}, a large-scale benchmark for\noptimization algorithms. We investigate whether weight averaging can reduce\ntraining time, improve generalization, and replace learning rate decay, as\nsuggested by recent literature. Our evaluation across seven architectures and\ndatasets reveals that averaging significantly accelerates training and yields\nconsiderable efficiency gains, at the price of a minimal implementation and\nmemory cost, while mildly improving generalization across all considered\nworkloads. Finally, we explore the relationship between averaging and learning\nrate annealing and show how to optimally combine the two to achieve the best\nperformances.\n","authors":["Niccolò Ajroldi","Antonio Orvieto","Jonas Geiping"],"pdf_url":"https://arxiv.org/pdf/2502.06761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13432v2","updated":"2025-02-10T18:37:12Z","published":"2025-01-23T07:35:47Z","title":"Emotion estimation from video footage with LSTM","summary":"  Emotion estimation in general is a field that has been studied for a long\ntime, and several approaches exist using machine learning. in this paper, we\npresent an LSTM model, that processes the blend-shapes produced by the library\nMediaPipe, for a face detected in a live stream of a camera, to estimate the\nmain emotion from the facial expressions, this model is trained on the FER2013\ndataset and delivers a result of 71% accuracy and 62% f1-score which meets the\naccuracy benchmark of the FER2013 dataset, with significantly reduced\ncomputation costs.\nhttps://github.com/Samir-atra/Emotion_estimation_from_video_footage_with_LSTM_ML_algorithm\n","authors":["Samer Attrah"],"pdf_url":"https://arxiv.org/pdf/2501.13432v2.pdf","comment":"12 pages, 5 figures, 34 references, 4 tables, 3 equations"},{"id":"http://arxiv.org/abs/2406.06621v2","updated":"2025-02-10T18:35:29Z","published":"2024-06-07T15:28:31Z","title":"LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph\n  Question-Answering","summary":"  We present LinkQ, a system that leverages a large language model (LLM) to\nfacilitate knowledge graph (KG) query construction through natural language\nquestion-answering. Traditional approaches often require detailed knowledge of\na graph querying language, limiting the ability for users -- even experts -- to\nacquire valuable insights from KGs. LinkQ simplifies this process by\nimplementing a multistep protocol in which the LLM interprets a user's\nquestion, then systematically converts it into a well-formed query. LinkQ helps\nusers iteratively refine any open-ended questions into precise ones, supporting\nboth targeted and exploratory analysis. Further, LinkQ guards against the LLM\nhallucinating outputs by ensuring users' questions are only ever answered from\nground truth KG data. We demonstrate the efficacy of LinkQ through a\nqualitative study with five KG practitioners. Our results indicate that\npractitioners find LinkQ effective for KG question-answering, and desire future\nLLM-assisted exploratory data analysis systems.\n","authors":["Harry Li","Gabriel Appleby","Ashley Suh"],"pdf_url":"https://arxiv.org/pdf/2406.06621v2.pdf","comment":"Open-source code: https://github.com/mit-ll/linkq"},{"id":"http://arxiv.org/abs/2502.06753v1","updated":"2025-02-10T18:31:15Z","published":"2025-02-10T18:31:15Z","title":"Case for a unified surrogate modelling framework in the age of AI","summary":"  Surrogate models are widely used in natural sciences, engineering, and\nmachine learning to approximate complex systems and reduce computational costs.\nHowever, the current landscape lacks standardisation across key stages of the\npipeline, including data collection, sampling design, model class selection,\nevaluation metrics, and downstream task performance analysis. This\nfragmentation limits reproducibility, reliability, and cross-domain\napplicability. The issue has only been exacerbated by the AI revolution and a\nnew suite of surrogate model classes that it offers. In this position paper, we\nargue for the urgent need for a unified framework to guide the development and\nevaluation of surrogate models. We outline essential steps for constructing a\ncomprehensive pipeline and discuss alternative perspectives, such as the\nbenefits of domain-specific frameworks. By advocating for a standardised\napproach, this paper seeks to improve the reliability of surrogate modelling,\nfoster cross-disciplinary knowledge transfer, and, as a result, accelerate\nscientific progress.\n","authors":["Elizaveta Semenova"],"pdf_url":"https://arxiv.org/pdf/2502.06753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04565v2","updated":"2025-02-10T18:28:24Z","published":"2025-02-06T23:38:50Z","title":"Private Federated Learning In Real World Application -- A Case Study","summary":"  This paper presents an implementation of machine learning model training\nusing private federated learning (PFL) on edge devices. We introduce a novel\nframework that uses PFL to address the challenge of training a model using\nusers' private data. The framework ensures that user data remain on individual\ndevices, with only essential model updates transmitted to a central server for\naggregation with privacy guarantees. We detail the architecture of our app\nselection model, which incorporates a neural network with attention mechanisms\nand ambiguity handling through uncertainty management. Experiments conducted\nthrough off-line simulations and on device training demonstrate the feasibility\nof our approach in real-world scenarios. Our results show the potential of PFL\nto improve the accuracy of an app selection model by adapting to changes in\nuser behavior over time, while adhering to privacy standards. The insights\ngained from this study are important for industries looking to implement PFL,\noffering a robust strategy for training a predictive model directly on edge\ndevices while ensuring user data privacy.\n","authors":["An Ji","Bortik Bandyopadhyay","Congzheng Song","Natarajan Krishnaswami","Prabal Vashisht","Rigel Smiroldo","Isabel Litton","Sayantan Mahinder","Mona Chitnis","Andrew W Hill"],"pdf_url":"https://arxiv.org/pdf/2502.04565v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06751v1","updated":"2025-02-10T18:26:40Z","published":"2025-02-10T18:26:40Z","title":"What makes a good feedforward computational graph?","summary":"  As implied by the plethora of literature on graph rewiring, the choice of\ncomputational graph employed by a neural network can make a significant impact\non its downstream performance. Certain effects related to the computational\ngraph, such as under-reaching and over-squashing, may even render the model\nincapable of learning certain functions. Most of these effects have only been\nthoroughly studied in the domain of undirected graphs; however, recent years\nhave seen a significant rise in interest in feedforward computational graphs:\ndirected graphs without any back edges. In this paper, we study the desirable\nproperties of a feedforward computational graph, discovering two important\ncomplementary measures: fidelity and mixing time, and evaluating a few popular\nchoices of graphs through the lens of these measures. Our study is backed by\nboth theoretical analyses of the metrics' asymptotic behaviour for various\ngraphs, as well as correlating these metrics to the performance of trained\nneural network models using the corresponding graphs.\n","authors":["Alex Vitvitskyi","João G. M. Araújo","Marc Lackenby","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2502.06751v1.pdf","comment":"Work in progress -- comments welcome. 16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00761v4","updated":"2025-02-10T18:26:14Z","published":"2024-08-01T17:59:12Z","title":"Tamper-Resistant Safeguards for Open-Weight LLMs","summary":"  Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after hundreds of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that progress on tamper-resistance is\npossible, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.\n","authors":["Rishub Tamirisa","Bhrugu Bharathi","Long Phan","Andy Zhou","Alice Gatti","Tarun Suresh","Maxwell Lin","Justin Wang","Rowan Wang","Ron Arel","Andy Zou","Dawn Song","Bo Li","Dan Hendrycks","Mantas Mazeika"],"pdf_url":"https://arxiv.org/pdf/2408.00761v4.pdf","comment":"Website: https://www.tamper-resistant-safeguards.com"},{"id":"http://arxiv.org/abs/2502.06749v1","updated":"2025-02-10T18:22:22Z","published":"2025-02-10T18:22:22Z","title":"Incentivizing Desirable Effort Profiles in Strategic Classification: The\n  Role of Causality and Uncertainty","summary":"  We study strategic classification in binary decision-making settings where\nagents can modify their features in order to improve their classification\noutcomes. Importantly, our work considers the causal structure across different\nfeatures, acknowledging that effort in a given feature may affect other\nfeatures. The main goal of our work is to understand \\emph{when and how much\nagent effort is invested towards desirable features}, and how this is\ninfluenced by the deployed classifier, the causal structure of the agent's\nfeatures, their ability to modify them, and the information available to the\nagent about the classifier and the feature causal graph.\n  In the complete information case, when agents know the classifier and the\ncausal structure of the problem, we derive conditions ensuring that rational\nagents focus on features favored by the principal. We show that designing\nclassifiers to induce desirable behavior is generally non-convex, though\ntractable in special cases. We also extend our analysis to settings where\nagents have incomplete information about the classifier or the causal graph.\nWhile optimal effort selection is again a non-convex problem under general\nuncertainty, we highlight special cases of partial uncertainty where this\nselection problem becomes tractable. Our results indicate that uncertainty\ndrives agents to favor features with higher expected importance and lower\nvariance, potentially misaligning with principal preferences. Finally,\nnumerical experiments based on a cardiovascular disease risk study illustrate\nhow to incentivize desirable modifications under uncertainty.\n","authors":["Valia Efthymiou","Chara Podimata","Diptangshu Sen","Juba Ziani"],"pdf_url":"https://arxiv.org/pdf/2502.06749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06742v1","updated":"2025-02-10T18:09:53Z","published":"2025-02-10T18:09:53Z","title":"Gradient Multi-Normalization for Stateless and Scalable LLM Training","summary":"  Training large language models (LLMs) typically relies on adaptive optimizers\nlike Adam (Kingma & Ba, 2015) which store additional state information to\naccelerate convergence but incur significant memory overhead. Recent efforts,\nsuch as SWAN (Ma et al., 2024) address this by eliminating the need for\noptimizer states while achieving performance comparable to Adam via a\nmulti-step preprocessing procedure applied to instantaneous gradients.\nMotivated by the success of SWAN, we introduce a novel framework for designing\nstateless optimizers that normalizes stochastic gradients according to multiple\nnorms. To achieve this, we propose a simple alternating scheme to enforce the\nnormalization of gradients w.r.t these norms. We show that our procedure can\nproduce, up to an arbitrary precision, a fixed-point of the problem, and that\nSWAN is a particular instance of our approach with carefully chosen norms,\nproviding a deeper understanding of its design. However, SWAN's computationally\nexpensive whitening/orthogonalization step limit its practicality for large\nLMs. Using our principled perspective, we develop of a more efficient,\nscalable, and practical stateless optimizer. Our algorithm relaxes the\nproperties of SWAN, significantly reducing its computational cost while\nretaining its memory efficiency, making it applicable to training large-scale\nmodels. Experiments on pre-training LLaMA models with up to 1 billion\nparameters demonstrate a 3X speedup over Adam with significantly reduced memory\nrequirements, outperforming other memory-efficient baselines.\n","authors":["Meyer Scetbon","Chao Ma","Wenbo Gong","Edward Meeds"],"pdf_url":"https://arxiv.org/pdf/2502.06742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06739v1","updated":"2025-02-10T18:07:51Z","published":"2025-02-10T18:07:51Z","title":"A note on the physical interpretation of neural PDE's","summary":"  We highlight a formal and substantial analogy between Machine Learning (ML)\nalgorithms and discrete dynamical systems (DDS) in relaxation form. The analogy\noffers a transparent interpretation of the weights in terms of physical\ninformation-propagation processes and identifies the model function of the\nforward ML step with the local attractor of the corresponding discrete\ndynamics. Besides improving the explainability of current ML applications, this\nanalogy may also facilitate the development of a new class ML algorithms with a\nreduced number of weights.\n","authors":["Sauro Succi"],"pdf_url":"https://arxiv.org/pdf/2502.06739v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2502.06738v1","updated":"2025-02-10T18:07:09Z","published":"2025-02-10T18:07:09Z","title":"Resurrecting saturated LLM benchmarks with adversarial encoding","summary":"  Recent work showed that small changes in benchmark questions can reduce LLMs'\nreasoning and recall. We explore two such changes: pairing questions and adding\nmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We\nfind that for more capable models, these predictably reduce performance,\nessentially heightening the performance ceiling of a benchmark and unsaturating\nit again. We suggest this approach can resurrect old benchmarks.\n","authors":["Igor Ivanov","Dmitrii Volkov"],"pdf_url":"https://arxiv.org/pdf/2502.06738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06737v1","updated":"2025-02-10T18:03:36Z","published":"2025-02-10T18:03:36Z","title":"VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data","summary":"  Process Reward Models (PRMs) have proven effective at enhancing mathematical\nreasoning for Large Language Models (LLMs) by leveraging increased\ninference-time computation. However, they are predominantly trained on\nmathematical data and their generalizability to non-mathematical domains has\nnot been rigorously studied. In response, this work first shows that current\nPRMs have poor performance in other domains. To address this limitation, we\nintroduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data\ngenerated using our novel data generation and annotation method. VersaPRM\nachieves consistent performance gains across diverse domains. For instance, in\nthe MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a\n7.9% performance gain over the majority voting baseline -- surpassing\nQwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by\nopen-sourcing all data, code and models for VersaPRM.\n","authors":["Thomas Zeng","Shuibai Zhang","Shutong Wu","Christian Classen","Daewon Chae","Ethan Ewer","Minjae Lee","Heeju Kim","Wonjun Kang","Jackson Kunde","Ying Fan","Jungtaek Kim","Hyung Il Koo","Kannan Ramchandran","Dimitris Papailiopoulos","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2502.06737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06733v1","updated":"2025-02-10T17:57:15Z","published":"2025-02-10T17:57:15Z","title":"Dynamic Loss-Based Sample Reweighting for Improved Large Language Model\n  Pretraining","summary":"  Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance.\n","authors":["Daouda Sow","Herbert Woisetschläger","Saikiran Bulusu","Shiqiang Wang","Hans-Arno Jacobsen","Yingbin Liang"],"pdf_url":"https://arxiv.org/pdf/2502.06733v1.pdf","comment":"Accepted for publication at ICLR 2025. Code base available:\n  https://github.com/sowmaster/Sample-Level-Loss-Reweighting-ICLR-2025"},{"id":"http://arxiv.org/abs/2502.06728v1","updated":"2025-02-10T17:55:59Z","published":"2025-02-10T17:55:59Z","title":"FlexDeMo: Decoupled Momentum Optimization for Fully and Hybrid Sharded\n  Training","summary":"  Training large neural network models requires extensive computational\nresources, often distributed across several nodes and accelerators. Recent\nfindings suggest that it may be sufficient to only exchange the fast moving\ncomponents of the gradients, while accumulating momentum locally (Decoupled\nMomentum, or DeMo). However, when considering larger models that do not fit on\na single accelerate, the exchange of gradient information and the integration\nof DeMo needs to be reconsidered. Here, we propose employing a hybrid strategy,\nFlexDeMo, whereby nodes fully synchronize locally between different GPUs and\ninter-node communication is improved through only using the fast-moving\ncomponents. This effectively combines previous hybrid sharding strategies with\nthe advantages of decoupled momentum. Our experimental results show that\nFlexDeMo is on par with AdamW in terms of validation loss, demonstrating its\nviability.\n","authors":["Mogens Henrik From","Jacob Nielsen","Lukas Galke","Peter Schneider-Kamp"],"pdf_url":"https://arxiv.org/pdf/2502.06728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06719v1","updated":"2025-02-10T17:49:05Z","published":"2025-02-10T17:49:05Z","title":"Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient\n  Descent","summary":"  In this paper, we establish non-asymptotic convergence rates in the central\nlimit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient\ndescent (SGD). Our analysis builds on the result of the Gaussian approximation\nfor nonlinear statistics of independent random variables of Shao and Zhang\n(2022). Using this result, we prove the non-asymptotic validity of the\nmultiplier bootstrap for constructing the confidence sets for the optimal\nsolution of an optimization problem. In particular, our approach avoids the\nneed to approximate the limiting covariance of Polyak-Ruppert SGD iterates,\nwhich allows us to derive approximation rates in convex distance of order up to\n$1/\\sqrt{n}$.\n","authors":["Marina Sheshukova","Sergey Samsonov","Denis Belomestny","Eric Moulines","Qi-Man Shao","Zhuo-Song Zhang","Alexey Naumov"],"pdf_url":"https://arxiv.org/pdf/2502.06719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03359v2","updated":"2025-02-10T17:33:29Z","published":"2025-02-05T16:56:14Z","title":"GHOST: Gaussian Hypothesis Open-Set Technique","summary":"  Evaluations of large-scale recognition methods typically focus on overall\nperformance. While this approach is common, it often fails to provide insights\ninto performance across individual classes, which can lead to fairness issues\nand misrepresentation. Addressing these gaps is crucial for accurately\nassessing how well methods handle novel or unseen classes and ensuring a fair\nevaluation. To address fairness in Open-Set Recognition (OSR), we demonstrate\nthat per-class performance can vary dramatically. We introduce Gaussian\nHypothesis Open Set Technique (GHOST), a novel hyperparameter-free algorithm\nthat models deep features using class-wise multivariate Gaussian distributions\nwith diagonal covariance matrices. We apply Z-score normalization to logits to\nmitigate the impact of feature magnitudes that deviate from the model's\nexpectations, thereby reducing the likelihood of the network assigning a high\nscore to an unknown sample. We evaluate GHOST across multiple ImageNet-1K\npre-trained deep networks and test it with four different unknown datasets.\nUsing standard metrics such as AUOSCR, AUROC and FPR95, we achieve\nstatistically significant improvements, advancing the state-of-the-art in\nlarge-scale OSR. Source code is provided online.\n","authors":["Ryan Rabinowitz","Steve Cruz","Manuel Günther","Terrance E. Boult"],"pdf_url":"https://arxiv.org/pdf/2502.03359v2.pdf","comment":"Accepted at AAAI Conference on Artificial Intelligence 2025"},{"id":"http://arxiv.org/abs/2502.06705v1","updated":"2025-02-10T17:33:22Z","published":"2025-02-10T17:33:22Z","title":"RSAttAE: An Information-Aware Attention-based Autoencoder Recommender\n  System","summary":"  Recommender systems play a crucial role in modern life, including information\nretrieval, the pharmaceutical industry, retail, and entertainment. The\nentertainment sector, in particular, attracts significant attention and\ngenerates substantial profits. This work proposes a new method for predicting\nunknown user-movie ratings to enhance customer satisfaction. To achieve this,\nwe utilize the MovieLens 100K dataset. Our approach introduces an\nattention-based autoencoder to create meaningful representations and the\nXGBoost method for rating predictions. The results demonstrate that our\nproposal outperforms most of the existing state-of-the-art methods.\nAvailability: github.com/ComputationIASBS/RecommSys\n","authors":["Amirhossein Dadashzadeh Taromi","Sina Heydari","Mohsen Hooshmand","Majid Ramezani"],"pdf_url":"https://arxiv.org/pdf/2502.06705v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.18727v2","updated":"2025-02-10T17:27:03Z","published":"2025-01-30T20:07:44Z","title":"Exploring Audio Editing Features as User-Centric Privacy Defenses\n  Against Large Language Model(LLM) Based Emotion Inference Attacks","summary":"  The rapid proliferation of speech-enabled technologies, including virtual\nassistants, video conferencing platforms, and wearable devices, has raised\nsignificant privacy concerns, particularly regarding the inference of sensitive\nemotional information from audio data. Existing privacy-preserving methods\noften compromise usability and security, limiting their adoption in practical\nscenarios. This paper introduces a novel, user-centric approach that leverages\nfamiliar audio editing techniques, specifically pitch and tempo manipulation,\nto protect emotional privacy without sacrificing usability. By analyzing\npopular audio editing applications on Android and iOS platforms, we identified\nthese features as both widely available and usable. We rigorously evaluated\ntheir effectiveness against a threat model, considering adversarial attacks\nfrom diverse sources, including Deep Neural Networks (DNNs), Large Language\nModels (LLMs), and and reversibility testing. Our experiments, conducted on\nthree distinct datasets, demonstrate that pitch and tempo manipulation\neffectively obfuscates emotional data. Additionally, we explore the design\nprinciples for lightweight, on-device implementation to ensure broad\napplicability across various devices and platforms.\n","authors":["Mohd. Farhan Israk Soumik","W. K. M. Mithsara","Abdur R. Shahid","Ahmed Imteaj"],"pdf_url":"https://arxiv.org/pdf/2501.18727v2.pdf","comment":"Accepted for presentation(Poster) at PPAI-25: The 6th AAAI Workshop\n  on Privacy-Preserving Artificial Intelligence"},{"id":"http://arxiv.org/abs/2502.06695v1","updated":"2025-02-10T17:18:54Z","published":"2025-02-10T17:18:54Z","title":"FairDropout: Using Example-Tied Dropout to Enhance Generalization of\n  Minority Groups","summary":"  Deep learning models frequently exploit spurious features in training data to\nachieve low training error, often resulting in poor generalization when faced\nwith shifted testing distributions. To address this issue, various methods from\nimbalanced learning, representation learning, and classifier recalibration have\nbeen proposed to enhance the robustness of deep neural networks against\nspurious correlations. In this paper, we observe that models trained with\nempirical risk minimization tend to generalize well for examples from the\nmajority groups while memorizing instances from minority groups. Building on\nrecent findings that show memorization can be localized to a limited number of\nneurons, we apply example-tied dropout as a method we term FairDropout, aimed\nat redirecting this memorization to specific neurons that we subsequently drop\nout during inference. We empirically evaluate FairDropout using the\nsubpopulation benchmark suite encompassing vision, language, and healthcare\ntasks, demonstrating that it significantly reduces reliance on spurious\ncorrelations, and outperforms state-of-the-art methods.\n","authors":["Geraldin Nanfack","Eugene Belilovsky"],"pdf_url":"https://arxiv.org/pdf/2502.06695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17281v2","updated":"2025-02-10T17:17:47Z","published":"2024-06-25T05:12:51Z","title":"Adaptive Reconstruction for Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have become fundamental in semi-supervised\nlearning for graph representation, leveraging their ability to capture complex\nnode relationships. A recent trend in GNN research focuses on \\textbf{adaptive\nk-hop structure learning}, moving beyond fixed-hop aggregation to more flexible\nand dynamic neighborhood selection. While GAMLP \\cite{Zhang_2022} employs\nseparate MLP layers for each k-hop domain and ImprovingTE\n\\cite{Yao2023ImprovingTE} enhances this by injecting contextualized\nsubstructure information, these methods still rely heavily on predefined\nsampling strategies, which may limit their ability to generalize and maintain\nstable accuracy. To address these limitations, we propose an \\textbf{adaptive\nreconstruction framework} that dynamically refines k-hop structure learning.\nInspired by \"coreset selection\" \\cite{guo2022deepcore}, our approach adaptively\n\\textbf{reconstructs} node neighborhoods to optimize message passing, ensuring\nmore \\textbf{effective and context-aware information flow} across the graph. To\nfurther enhance structural robustness, we introduce two key modules: the\n\\textbf{Distance Recomputator} and the \\textbf{Topology Reconstructor}\n(\\textcolor{blue}{DRTR}). The Distance Recomputator \\textbf{reassesses and\nrecalibrates} node distances based on adaptive graph properties, leading to\n\\textbf{improved node embeddings} that better reflect latent relationships.\nMeanwhile, the Topology Reconstructor \\textbf{dynamically refines local graph\nstructures}, enabling the model to \\textbf{adapt to evolving graph topologies}\nand mitigate the impact of noise and mislabeled data. Empirical evaluations\ndemonstrate that our \\textbf{adaptive reconstruction framework} achieves\n\\textbf{significant improvements} over existing k-hop-based models, providing\nmore \\textbf{stable and accurate} performance in various graph learning\nbenchmarks.\n","authors":["Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2406.17281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06693v1","updated":"2025-02-10T17:17:09Z","published":"2025-02-10T17:17:09Z","title":"Recent Advances, Applications and Open Challenges in Machine Learning\n  for Health: Reflections from Research Roundtables at ML4H 2024 Symposium","summary":"  The fourth Machine Learning for Health (ML4H) symposium was held in person on\nDecember 15th and 16th, 2024, in the traditional, ancestral, and unceded\nterritories of the Musqueam, Squamish, and Tsleil-Waututh Nations in Vancouver,\nBritish Columbia, Canada. The symposium included research roundtable sessions\nto foster discussions between participants and senior researchers on timely and\nrelevant topics for the ML4H community. The organization of the research\nroundtables at the conference involved 13 senior and 27 junior chairs across 13\ntables. Each roundtable session included an invited senior chair (with\nsubstantial experience in the field), junior chairs (responsible for\nfacilitating the discussion), and attendees from diverse backgrounds with an\ninterest in the session's topic.\n","authors":["Amin Adibi","Xu Cao","Zongliang Ji","Jivat Neet Kaur","Winston Chen","Elizabeth Healey","Brighton Nuwagira","Wenqian Ye","Geoffrey Woollard","Maxwell A Xu","Hejie Cui","Johnny Xi","Trenton Chang","Vasiliki Bikia","Nicole Zhang","Ayush Noori","Yuan Xia","Md. Belal Hossain","Hanna A. Frank","Alina Peluso","Yuan Pu","Shannon Zejiang Shen","John Wu","Adibvafa Fallahpour","Sazan Mahbub","Ross Duncan","Yuwei Zhang","Yurui Cao","Zuheng Xu","Michael Craig","Rahul G. Krishnan","Rahmatollah Beheshti","James M. Rehg","Mohammad Ehsanul Karim","Megan Coffee","Leo Anthony Celi","Jason Alan Fries","Mohsen Sadatsafavi","Dennis Shung","Shannon McWeeney","Jessica Dafflon","Sarah Jabbour"],"pdf_url":"https://arxiv.org/pdf/2502.06693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16218v3","updated":"2025-02-10T17:15:52Z","published":"2024-03-24T16:18:27Z","title":"CoverUp: Coverage-Guided LLM-Based Test Generation","summary":"  Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 90% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components.\n","authors":["Juan Altmayer Pizzorno","Emery D. Berger"],"pdf_url":"https://arxiv.org/pdf/2403.16218v3.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2502.06689v1","updated":"2025-02-10T17:15:11Z","published":"2025-02-10T17:15:11Z","title":"Neumann eigenmaps for landmark embedding","summary":"  We present Neumann eigenmaps (NeuMaps), a novel approach for enhancing the\nstandard diffusion map embedding using landmarks, i.e distinguished samples\nwithin the dataset. By interpreting these landmarks as a subgraph of the larger\ndata graph, NeuMaps are obtained via the eigendecomposition of a renormalized\nNeumann Laplacian. We show that NeuMaps offer two key advantages: (1) they\nprovide a computationally efficient embedding that accurately recovers the\ndiffusion distance associated with the reflecting random walk on the subgraph,\nand (2) they naturally incorporate the Nystr\\\"om extension within the diffusion\nmap framework through the discrete Neumann boundary condition. Through examples\nin digit classification and molecular dynamics, we demonstrate that NeuMaps not\nonly improve upon existing landmark-based embedding methods but also enhance\nthe stability of diffusion map embeddings to the removal of highly significant\npoints.\n","authors":["Shashank Sule","Wojciech Czaja"],"pdf_url":"https://arxiv.org/pdf/2502.06689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06685v1","updated":"2025-02-10T17:13:11Z","published":"2025-02-10T17:13:11Z","title":"No Trick, No Treat: Pursuits and Challenges Towards Simulation-free\n  Training of Neural Samplers","summary":"  We consider the sampling problem, where the aim is to draw samples from a\ndistribution whose density is known only up to a normalization constant. Recent\nbreakthroughs in generative modeling to approximate a high-dimensional data\ndistribution have sparked significant interest in developing neural\nnetwork-based methods for this challenging problem. However, neural samplers\ntypically incur heavy computational overhead due to simulating trajectories\nduring training. This motivates the pursuit of simulation-free training\nprocedures of neural samplers. In this work, we propose an elegant modification\nto previous methods, which allows simulation-free training with the help of a\ntime-dependent normalizing flow. However, it ultimately suffers from severe\nmode collapse. On closer inspection, we find that nearly all successful neural\nsamplers rely on Langevin preconditioning to avoid mode collapsing. We\nsystematically analyze several popular methods with various objective functions\nand demonstrate that, in the absence of Langevin preconditioning, most of them\nfail to adequately cover even a simple target. Finally, we draw attention to a\nstrong baseline by combining the state-of-the-art MCMC method, Parallel\nTempering (PT), with an additional generative model to shed light on future\nexplorations of neural samplers.\n","authors":["Jiajun He","Yuanqi Du","Francisco Vargas","Dinghuai Zhang","Shreyas Padhy","RuiKang OuYang","Carla Gomes","José Miguel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2502.06685v1.pdf","comment":"21 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.06684v1","updated":"2025-02-10T17:11:20Z","published":"2025-02-10T17:11:20Z","title":"EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks","summary":"  Recent foundational models for tabular data, such as TabPFN, have\ndemonstrated remarkable effectiveness in adapting to new tasks through\nin-context learning. However, these models overlook a crucial equivariance\nproperty: the arbitrary ordering of target dimensions should not influence\nmodel predictions. In this study, we identify this oversight as a source of\nincompressible error, termed the equivariance gap, which introduces instability\nin predictions. To mitigate these issues, we propose a novel model designed to\npreserve equivariance across output dimensions. Our experimental results\nindicate that our proposed model not only addresses these pitfalls effectively\nbut also achieves competitive benchmark performance.\n","authors":["Michael Arbel","David Salinas","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2502.06684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01678v2","updated":"2025-02-10T17:11:15Z","published":"2025-02-02T04:19:35Z","title":"LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection","summary":"  Electroencephalogram (EEG) provides a non-invasive, highly accessible, and\ncost-effective solution for Alzheimer's Disease (AD) detection. However,\nexisting methods, whether based on manual feature extraction or deep learning,\nface two major challenges: the lack of large-scale datasets for robust feature\nlearning and evaluation, and poor detection performance due to inter-subject\nvariations. To address these challenges, we curate an EEG-AD corpus containing\n813 subjects, which forms the world's largest EEG-AD dataset to the best of our\nknowledge. Using this unique dataset, we propose LEAD, the first large\nfoundation model for EEG-based AD detection. Our method encompasses an entire\npipeline, from data selection and preprocessing to self-supervised contrastive\npretraining, fine-tuning, and key setups such as subject-independent evaluation\nand majority voting for subject-level detection. We pre-train the model on 11\nEEG datasets and unified fine-tune it on 5 AD datasets. Our self-supervised\npre-training design includes sample-level and subject-level contrasting to\nextract useful general EEG features. Fine-tuning is performed on 5\nchannel-aligned datasets together. The backbone encoder incorporates temporal\nand channel embeddings to capture features across both temporal and spatial\ndimensions. Our method demonstrates outstanding AD detection performance,\nachieving up to a 9.86% increase in F1 score at the sample-level and up to a\n9.31% at the subject-level compared to state-of-the-art methods. The results of\nour model strongly confirm the effectiveness of contrastive pre-training and\nchannel-aligned unified fine-tuning for addressing inter-subject variation. The\nsource code is at https://github.com/DL4mHealth/LEAD.\n","authors":["Yihe Wang","Nan Huang","Nadia Mammone","Marco Cecchi","Xiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.01678v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06681v1","updated":"2025-02-10T17:07:43Z","published":"2025-02-10T17:07:43Z","title":"CHIRLA: Comprehensive High-resolution Identification and\n  Re-identification for Large-scale Analysis","summary":"  Person re-identification (Re-ID) is a key challenge in computer vision,\nrequiring the matching of individuals across different cameras, locations, and\ntime periods. While most research focuses on short-term scenarios with minimal\nappearance changes, real-world applications demand robust Re-ID systems capable\nof handling long-term scenarios, where persons' appearances can change\nsignificantly due to variations in clothing and physical characteristics. In\nthis paper, we present CHIRLA, Comprehensive High-resolution Identification and\nRe-identification for Large-scale Analysis, a novel dataset specifically\ndesigned for long-term person Re-ID. CHIRLA consists of recordings from\nstrategically placed cameras over a seven-month period, capturing significant\nvariations in both temporal and appearance attributes, including controlled\nchanges in participants' clothing and physical features. The dataset includes\n22 individuals, four connected indoor environments, and seven cameras. We\ncollected more than five hours of video that we semi-automatically labeled to\ngenerate around one million bounding boxes with identity annotations. By\nintroducing this comprehensive benchmark, we aim to facilitate the development\nand evaluation of Re-ID algorithms that can reliably perform in challenging,\nlong-term real-world scenarios.\n","authors":["Bessie Dominguez-Dager","Felix Escalona","Francisco Gomez-Donoso","Miguel Cazorla"],"pdf_url":"https://arxiv.org/pdf/2502.06681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12016v4","updated":"2025-02-10T17:06:04Z","published":"2024-05-20T13:39:58Z","title":"Conformalized Strategy-Proof Auctions","summary":"  Auctions are key for maximizing sellers' revenue and ensuring truthful\nbidding among buyers. Recently, an approach known as differentiable economics\nbased on machine learning (ML) has shown promise in learning powerful auction\nmechanisms for multiple items and participants. However, this approach has no\nguarantee of strategy-proofness at test time. Strategy-proofness is crucial as\nit ensures that buyers are incentivized to bid their true valuations, leading\nto optimal and fair auction outcomes without the risk of manipulation. In this\nwork, we propose a formulation of statistical strategy-proofness auction\nmechanism, ensuring that the probability of regret exceeding a predefined\nthreshold is strictly controlled. Building upon conformal prediction\ntechniques, we develop an auction acceptance rule that leverages regret\npredictions to guarantee that the data-driven auction mechanism meets the\nstatistical strategy-proofness requirement with high probability. Our approach\nrepresents a practical middle-ground between two extremes: forcing zero-regret\nat the cost of significant revenue loss, and naively using ML to construct\nauctions with the hope of attaining low regret at test time. Numerical\nexperiments demonstrate the necessity of the proposed method, the validity of\nour theoretical result, and its applicability.\n","authors":["Roy Maor Lotan","Inbal Talgam-Cohen","Yaniv Romano"],"pdf_url":"https://arxiv.org/pdf/2405.12016v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06678v1","updated":"2025-02-10T17:03:33Z","published":"2025-02-10T17:03:33Z","title":"Quantile Multi-Armed Bandits with 1-bit Feedback","summary":"  In this paper, we study a variant of best-arm identification involving\nelements of risk sensitivity and communication constraints. Specifically, the\ngoal of the learner is to identify the arm with the highest quantile reward,\nwhile the communication from an agent (who observes rewards) and the learner\n(who chooses actions) is restricted to only one bit of feedback per arm pull.\nWe propose an algorithm that utilizes noisy binary search as a subroutine,\nallowing the learner to estimate quantile rewards through 1-bit feedback. We\nderive an instance-dependent upper bound on the sample complexity of our\nalgorithm and provide an algorithm-independent lower bound for specific\ninstances, with the two matching to within logarithmic factors under mild\nconditions, or even to within constant factors in certain low error probability\nscaling regimes. The lower bound is applicable even in the absence of\ncommunication constraints, and thus we conclude that restricting to 1-bit\nfeedback has a minimal impact on the scaling of the sample complexity.\n","authors":["Ivan Lau","Jonathan Scarlett"],"pdf_url":"https://arxiv.org/pdf/2502.06678v1.pdf","comment":"ALT 2025"},{"id":"http://arxiv.org/abs/2502.06674v1","updated":"2025-02-10T17:00:32Z","published":"2025-02-10T17:00:32Z","title":"RAILS: Risk-Aware Iterated Local Search for Joint SLA Decomposition and\n  Service Provider Management in Multi-Domain Networks","summary":"  The emergence of the fifth generation (5G) technology has transformed mobile\nnetworks into multi-service environments, necessitating efficient network\nslicing to meet diverse Service Level Agreements (SLAs). SLA decomposition\nacross multiple network domains, each potentially managed by different service\nproviders, poses a significant challenge due to limited visibility into\nreal-time underlying domain conditions. This paper introduces Risk-Aware\nIterated Local Search (RAILS), a novel risk model-driven meta-heuristic\nframework designed to jointly address SLA decomposition and service provider\nselection in multi-domain networks. By integrating online risk modeling with\niterated local search principles, RAILS effectively navigates the complex\noptimization landscape, utilizing historical feedback from domain controllers.\nWe formulate the joint problem as a Mixed-Integer Nonlinear Programming (MINLP)\nproblem and prove its NP-hardness. Extensive simulations demonstrate that RAILS\nachieves near-optimal performance, offering an efficient, real-time solution\nfor adaptive SLA management in modern multi-domain networks.\n","authors":["Cyril Shih-Huan Hsu","Chrysa Papagianni","Paola Grosso"],"pdf_url":"https://arxiv.org/pdf/2502.06674v1.pdf","comment":"The paper has been submitted to IEEE HPSR 2025"},{"id":"http://arxiv.org/abs/2410.09795v3","updated":"2025-02-10T16:54:15Z","published":"2024-10-13T10:48:22Z","title":"Predicting Molecular Ground-State Conformation via Conformation\n  Optimization","summary":"  Predicting molecular ground-state conformation (i.e., energy-minimized\nconformation) is crucial for many chemical applications such as molecular\ndocking and property prediction. Classic energy-based simulation is\ntime-consuming when solving this problem while existing learning-based methods\nhave advantages in computational efficiency but sacrifice accuracy and\ninterpretability. In this work, we propose a novel and effective method to\nbridge the energy-based simulation and the learning-based strategy, which\ndesigns and learns a Wasserstein gradient flow-driven SE(3)-Transformer, called\nWGFormer, for molecular ground-state conformation prediction. Specifically, our\nmethod tackles this task within an auto-encoding framework, which encodes\nlow-quality conformations by the proposed WGFormer and decodes corresponding\nground-state conformations by an MLP. The architecture of WGFormer corresponds\nto Wasserstein gradient flows -- it optimizes molecular conformations by\nminimizing an energy function defined on the latent mixture models of atoms,\nthereby significantly improving performance and interpretability. Extensive\nexperiments show that our method consistently outperforms state-of-the-art\ncompetitors, providing a new and insightful paradigm to predict molecular\nground-state conformation.\n","authors":["Fanmeng Wang","Minjie Cheng","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2410.09795v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01022v2","updated":"2025-02-10T16:52:07Z","published":"2024-08-02T05:46:17Z","title":"A Family of Distributions of Random Subsets for Controlling Positive and\n  Negative Dependence","summary":"  Positive and negative dependence are fundamental concepts that characterize\nthe attractive and repulsive behavior of random subsets. Although some\nprobabilistic models are known to exhibit positive or negative dependence, it\nis challenging to seamlessly bridge them with a practicable probabilistic\nmodel. In this study, we introduce a new family of distributions, named the\ndiscrete kernel point process (DKPP), which includes determinantal point\nprocesses and parts of Boltzmann machines. We also develop some computational\nmethods for probabilistic operations and inference with DKPPs, such as\ncalculating marginal and conditional probabilities and learning the parameters.\nOur numerical experiments demonstrate the controllability of positive and\nnegative dependence and the effectiveness of the computational methods for\nDKPPs.\n","authors":["Takahiro Kawashima","Hideitsu Hino"],"pdf_url":"https://arxiv.org/pdf/2408.01022v2.pdf","comment":"Accepted by AISTATS2025"},{"id":"http://arxiv.org/abs/2502.06664v1","updated":"2025-02-10T16:51:11Z","published":"2025-02-10T16:51:11Z","title":"Evaluation of Deep Audio Representations for Hearables","summary":"  Effectively steering hearable devices requires understanding the acoustic\nenvironment around the user. In the computational analysis of sound scenes,\nfoundation models have emerged as the state of the art to produce\nhigh-performance, robust, multi-purpose audio representations. We introduce and\nrelease Deep Evaluation of Audio Representations (DEAR), the first dataset and\nbenchmark to evaluate the efficacy of foundation models in capturing essential\nacoustic properties for hearables. The dataset includes 1,158 audio tracks,\neach 30 seconds long, created by spatially mixing proprietary monologues with\ncommercial, high-quality recordings of everyday acoustic scenes. Our benchmark\nencompasses eight tasks that assess the general context, speech sources, and\ntechnical acoustic properties of the audio scenes. Through our evaluation of\nfour general-purpose audio representation models, we demonstrate that the BEATs\nmodel significantly surpasses its counterparts. This superiority underscores\nthe advantage of models trained on diverse audio collections, confirming their\napplicability to a wide array of auditory tasks, including encoding the\nenvironment properties necessary for hearable steering. The DEAR dataset and\nassociated code are available at https://dear-dataset.github.io.\n","authors":["Fabian Gröger","Pascal Baumann","Ludovic Amruthalingam","Laurent Simon","Ruksana Giurda","Simone Lionetti"],"pdf_url":"https://arxiv.org/pdf/2502.06664v1.pdf","comment":"Accepted at International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2502.06663v1","updated":"2025-02-10T16:51:03Z","published":"2025-02-10T16:51:03Z","title":"EfficientLLM: Scalable Pruning-Aware Pretraining for\n  Architecture-Agnostic Edge Language Models","summary":"  Modern large language models (LLMs) driven by scaling laws, achieve\nintelligence emergency in large model sizes. Recently, the increasing concerns\nabout cloud costs, latency, and privacy make it an urgent requirement to\ndevelop compact edge language models. Distinguished from direct pretraining\nthat bounded by the scaling law, this work proposes the pruning-aware\npretraining, focusing on retaining performance of much larger optimized models.\nIt features following characteristics: 1) Data-scalable: we introduce minimal\nparameter groups in LLM and continuously optimize structural pruning, extending\npost-training pruning methods like LLM-Pruner and SparseGPT into the\npretraining phase. 2) Architecture-agnostic: the LLM architecture is\nauto-designed using saliency-driven pruning, which is the first time to exceed\nSoTA human-designed LLMs in modern pretraining. We reveal that it achieves\ntop-quality edge language models, termed EfficientLLM, by scaling up LLM\ncompression and extending its boundary. EfficientLLM significantly outperforms\nSoTA baselines with $100M \\sim 1B$ parameters, such as MobileLLM, SmolLM,\nQwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first\nattempt, EfficientLLM bridges the performance gap between traditional LLM\ncompression and direct pretraining methods, and we will fully open source at\nhttps://github.com/Xingrun-Xing2/EfficientLLM.\n","authors":["Xingrun Xing","Zheng Liu","Shitao Xiao","Boyan Gao","Yiming Liang","Wanpeng Zhang","Haokun Lin","Guoqi Li","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06661v1","updated":"2025-02-10T16:49:46Z","published":"2025-02-10T16:49:46Z","title":"iLOCO: Distribution-Free Inference for Feature Interactions","summary":"  Feature importance measures are widely studied and are essential for\nunderstanding model behavior, guiding feature selection, and enhancing\ninterpretability. However, many machine learning fitted models involve complex,\nhigher-order interactions between features. Existing feature importance metrics\nfail to capture these higher-order effects while existing interaction metrics\noften suffer from limited applicability or excessive computation; no methods\nexist to conduct statistical inference for feature interactions. To bridge this\ngap, we first propose a new model-agnostic metric, interaction\nLeave-One-Covariate-Out iLOCO, for measuring the importance of higher-order\nfeature interactions. Next, we leverage recent advances in LOCO inference to\ndevelop distribution-free and assumption-light confidence intervals for our\niLOCO metric. To address computational challenges, we also introduce an\nensemble learning method for calculating the iLOCO metric and confidence\nintervals that we show is both computationally and statistically efficient. We\nvalidate our iLOCO metric and our confidence intervals on both synthetic and\nreal data sets, showing that our approach outperforms existing methods and\nprovides the first inferential approach to detecting feature interactions.\n","authors":["Camille Little","Lili Zheng","Genevera Allen"],"pdf_url":"https://arxiv.org/pdf/2502.06661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06658v1","updated":"2025-02-10T16:48:48Z","published":"2025-02-10T16:48:48Z","title":"Generating Samples to Question Trained Models","summary":"  There is a growing need for investigating how machine learning models\noperate. With this work, we aim to understand trained machine learning models\nby questioning their data preferences. We propose a mathematical framework that\nallows us to probe trained models and identify their preferred samples in\nvarious scenarios including prediction-risky, parameter-sensitive, or\nmodel-contrastive samples. To showcase our framework, we pose these queries to\na range of models trained on a range of classification and regression tasks,\nand receive answers in the form of generated data.\n","authors":["E. Mehmet Kıral","Nurşen Aydın","Ş. İlker Birbil"],"pdf_url":"https://arxiv.org/pdf/2502.06658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19020v3","updated":"2025-02-10T16:42:23Z","published":"2024-09-25T07:03:31Z","title":"DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications","summary":"  The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research.\n","authors":["Sathya Krishnan Suresh","Wu Mengjun","Tushar Pranav","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2409.19020v3.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.06649v1","updated":"2025-02-10T16:38:13Z","published":"2025-02-10T16:38:13Z","title":"Estimation of Food Intake Quantity Using Inertial Signals from\n  Smartwatches","summary":"  Accurate monitoring of eating behavior is crucial for managing obesity and\neating disorders such as bulimia nervosa. At the same time, existing methods\nrely on multiple and/or specialized sensors, greatly harming adherence and\nultimately, the quality and continuity of data. This paper introduces a novel\napproach for estimating the weight of a bite, from a commercial smartwatch. Our\npublicly-available dataset contains smartwatch inertial data from ten\nparticipants, with manually annotated start and end times of each bite along\nwith their corresponding weights from a smart scale, under semi-controlled\nconditions. The proposed method combines extracted behavioral features such as\nthe time required to load the utensil with food, with statistical features of\ninertial signals, that serve as input to a Support Vector Regression model to\nestimate bite weights. Under a leave-one-subject-out cross-validation scheme,\nour approach achieves a mean absolute error (MAE) of 3.99 grams per bite. To\ncontextualize this performance, we introduce the improvement metric, that\nmeasures the relative MAE difference compared to a baseline model. Our method\ndemonstrates a 17.41% improvement, while the adapted state-of-the art method\nshows a -28.89% performance against that same baseline. The results presented\nin this work establish the feasibility of extracting meaningful bite weight\nestimates from commercial smartwatch inertial sensors alone, laying the\ngroundwork for future accessible, non-invasive dietary monitoring systems.\n","authors":["Ioannis Levi","Konstantinos Kyritsis","Vasileios Papapanagiotou","Georgios Tsakiridis","Anastasios Delopoulos"],"pdf_url":"https://arxiv.org/pdf/2502.06649v1.pdf","comment":"Manuscript submitted for review to 47th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)\n  2025"},{"id":"http://arxiv.org/abs/2502.06645v1","updated":"2025-02-10T16:35:08Z","published":"2025-02-10T16:35:08Z","title":"Koopman-Equivariant Gaussian Processes","summary":"  Credible forecasting and representation learning of dynamical systems are of\never-increasing importance for reliable decision-making. To that end, we\npropose a family of Gaussian processes (GP) for dynamical systems with linear\ntime-invariant responses, which are nonlinear only in initial conditions. This\nlinearity allows us to tractably quantify forecasting and representational\nuncertainty, simultaneously alleviating the challenge of computing the\ndistribution of trajectories from a GP-based dynamical system and enabling a\nnew probabilistic treatment of learning Koopman operator representations. Using\na trajectory-based equivariance -- which we refer to as \\textit{Koopman\nequivariance} -- we obtain a GP model with enhanced generalization\ncapabilities. To allow for large-scale regression, we equip our framework with\nvariational inference based on suitable inducing points. Experiments\ndemonstrate on-par and often better forecasting performance compared to\nkernel-based methods for learning dynamical systems.\n","authors":["Petar Bevanda","Max Beier","Armin Lederer","Alexandre Capone","Stefan Sosnowski","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2502.06645v1.pdf","comment":"Accepted to the 28th International Conference on Artificial\n  Intelligence and Statistics (AISTATS)"},{"id":"http://arxiv.org/abs/2502.06643v1","updated":"2025-02-10T16:34:36Z","published":"2025-02-10T16:34:36Z","title":"MoETuner: Optimized Mixture of Expert Serving with Balanced Expert\n  Placement and Token Routing","summary":"  Mixture-of-Experts (MoE) model architecture has emerged as a promising\nsolution for scaling transformer models efficiently, offering sparse activation\nthat reduces computational costs while increasing model capacity. However, as\nMoE models scale, they need to be distributed across GPU devices, thus face\ncritical performance bottlenecks due to their large memory footprint. Expert\nparallelism distributes experts across GPUs, however, faces key challenges\nincluding an unbalanced token routing and expert activation, resulting in\ncommunication tail latency and processing inefficiencies. While existing\nsolutions address some of these issues, they fail to resolve the dual\nchallenges of load imbalance and communication skew. The imbalance in token\nprocessing load across experts causes uneven processing times on different\nGPUs, while communication skew between GPUs leads to unbalanced inter-GPU data\ntransfers. These factors degrade the performance of MoE models by increasing\ntail latency and reducing overall throughput. To address these limitations, we\npropose an Integer Linear Programming (ILP) formulation to optimize expert\nplacement by jointly considering token load, communication, and computation\ncosts. We exploit the property that there is a token routing dependency across\nlayers, where tokens routed to a specific expert in one layer are likely to be\nrouted to a limited set of experts in the subsequent layer. Our solution,\nMoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU\ntoken routing costs and balances token processing across devices, thereby\nreducing tail latency and end-to-end execution time. Experimental results\ndemonstrate 9.3% and 17.5% of end-to-end speedups for single-node and\nmulti-node inference respectively, showcasing the potential of our ILP-based\noptimization for offering expert parallel solutions for next-generation MoEs.\n","authors":["Seokjin Go","Divya Mahajan"],"pdf_url":"https://arxiv.org/pdf/2502.06643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04419v2","updated":"2025-02-10T16:34:03Z","published":"2025-02-06T15:20:58Z","title":"Understanding and Mitigating the Bias Inheritance in LLM-based Data\n  Augmentation on Downstream Tasks","summary":"  Generating synthetic datasets via large language models (LLMs) themselves has\nemerged as a promising approach to improve LLM performance. However, LLMs\ninherently reflect biases present in their training data, leading to a critical\nchallenge: when these models generate synthetic data for training, they may\npropagate and amplify their inherent biases that can significantly impact model\nfairness and robustness on downstream tasks--a phenomenon we term bias\ninheritance. This work presents the first systematic investigation in\nunderstanding, analyzing, and mitigating bias inheritance. We study this\nproblem by fine-tuning LLMs with a combined dataset consisting of original and\nLLM-augmented data, where bias ratio represents the proportion of augmented\ndata. Through systematic experiments across 10 classification and generation\ntasks, we analyze how 6 different types of biases manifest at varying bias\nratios. Our results reveal that bias inheritance has nuanced effects on\ndownstream tasks, influencing both classification tasks and generation tasks\ndifferently. Then, our analysis identifies three key misalignment factors:\nmisalignment of values, group data, and data distributions. Based on these\ninsights, we propose three mitigation strategies: token-based, mask-based, and\nloss-based approaches. Experiments demonstrate that these strategies also work\ndifferently on various tasks and bias, indicating the substantial challenges to\nfully mitigate bias inheritance. We hope this work can provide valuable\ninsights to the research of LLM data augmentation.\n","authors":["Miaomiao Li","Hao Chen","Yang Wang","Tingyuan Zhu","Weijia Zhang","Kaijie Zhu","Kam-Fai Wong","Jindong Wang"],"pdf_url":"https://arxiv.org/pdf/2502.04419v2.pdf","comment":"Technical report; 31 pages"},{"id":"http://arxiv.org/abs/2411.18676v2","updated":"2025-02-10T16:32:27Z","published":"2024-11-27T18:57:26Z","title":"Embodied Red Teaming for Auditing Robotic Foundation Models","summary":"  Language-conditioned robot models have the potential to enable robots to\nperform a wide range of tasks based on natural language instructions. However,\nassessing their safety and effectiveness remains challenging because it is\ndifficult to test all the different ways a single task can be phrased. Current\nbenchmarks have two key limitations: they rely on a limited set of\nhuman-generated instructions, missing many challenging cases, and focus only on\ntask performance without assessing safety, such as avoiding damage. To address\nthese gaps, we introduce Embodied Red Teaming (ERT), a new evaluation method\nthat generates diverse and challenging instructions to test these models. ERT\nuses automated red teaming techniques with Vision Language Models (VLMs) to\ncreate contextually grounded, difficult instructions. Experimental results show\nthat state-of-the-art language-conditioned robot models fail or behave unsafely\non ERT-generated instructions, underscoring the shortcomings of current\nbenchmarks in evaluating real-world performance and safety. Code and videos are\navailable at: https://s-karnik.github.io/embodied-red-team-project-page.\n","authors":["Sathwik Karnik","Zhang-Wei Hong","Nishant Abhangi","Yen-Chen Lin","Tsun-Hsuan Wang","Christophe Dupuy","Rahul Gupta","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2411.18676v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01591v3","updated":"2025-02-10T16:32:04Z","published":"2023-11-02T20:57:44Z","title":"Better Fair than Sorry: Adversarial Missing Data Imputation for Fair\n  GNNs","summary":"  Graph Neural Networks (GNNs) have achieved state-of-the-art results in many\nrelevant tasks where decisions might disproportionately impact specific\ncommunities. However, existing work on fair GNNs often assumes that either\nprotected attributes are fully observed or that the missing protected attribute\nimputation is fair. In practice, biases in the imputation will propagate to the\nmodel outcomes, leading them to overestimate the fairness of their predictions.\nWe address this challenge by proposing Better Fair than Sorry (BFtS), a fair\nmissing data imputation model for protected attributes. The key design\nprinciple behind BFtS is that imputations should approximate the worst-case\nscenario for fairness -- i.e. when optimizing fairness is the hardest. We\nimplement this idea using a 3-player adversarial scheme where two adversaries\ncollaborate against a GNN-based classifier, and the classifier minimizes the\nmaximum bias. Experiments using synthetic and real datasets show that BFtS\noften achieves a better fairness x accuracy trade-off than existing\nalternatives.\n","authors":["Debolina Halder Lina","Arlei Silva"],"pdf_url":"https://arxiv.org/pdf/2311.01591v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09038v2","updated":"2025-02-10T16:31:57Z","published":"2025-01-14T20:59:37Z","title":"Do generative video models learn physical principles from watching\n  videos?","summary":"  AI video generation is undergoing a revolution, with quality and realism\nadvancing rapidly. These advances have led to a passionate scientific debate:\nDo video models learn \"world models\" that discover laws of physics -- or,\nalternatively, are they merely sophisticated pixel predictors that achieve\nvisual realism without understanding the physical principles of reality? We\naddress this question by developing Physics-IQ, a comprehensive benchmark\ndataset that can only be solved by acquiring a deep understanding of various\nphysical principles, like fluid dynamics, optics, solid mechanics, magnetism\nand thermodynamics. We find that across a range of current models (Sora,\nRunway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical\nunderstanding is severely limited, and unrelated to visual realism. At the same\ntime, some test cases can already be successfully solved. This indicates that\nacquiring certain physical principles from observation alone may be possible,\nbut significant challenges remain. While we expect rapid advances ahead, our\nwork demonstrates that visual realism does not imply physical understanding.\nOur project page is at https://physics-iq.github.io; code at\nhttps://github.com/google-deepmind/physics-IQ-benchmark.\n","authors":["Saman Motamed","Laura Culp","Kevin Swersky","Priyank Jaini","Robert Geirhos"],"pdf_url":"https://arxiv.org/pdf/2501.09038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06634v1","updated":"2025-02-10T16:29:21Z","published":"2025-02-10T16:29:21Z","title":"Automatic Annotation Augmentation Boosts Translation between Molecules\n  and Natural Language","summary":"  Recent advancements in AI for biological research focus on integrating\nmolecular data with natural language to accelerate drug discovery. However, the\nscarcity of high-quality annotations limits progress in this area. This paper\nintroduces LA$^3$, a Language-based Automatic Annotation Augmentation framework\nthat leverages large language models to augment existing datasets, thereby\nimproving AI training. We demonstrate the effectiveness of LA$^3$ by creating\nan enhanced dataset, LaChEBI-20, where we systematically rewrite the\nannotations of molecules from an established dataset. These rewritten\nannotations preserve essential molecular information while providing more\nvaried sentence structures and vocabulary. Using LaChEBI-20, we train LaMolT5\nbased on a benchmark architecture to learn the mapping between molecular\nrepresentations and augmented annotations.\n  Experimental results on text-based *de novo* molecule generation and molecule\ncaptioning demonstrate that LaMolT5 outperforms state-of-the-art models.\nNotably, incorporating LA$^3$ leads to improvements of up to 301% over the\nbenchmark architecture. Furthermore, we validate the effectiveness of LA$^3$\nnotable applications in *image*, *text* and *graph* tasks, affirming its\nversatility and utility.\n","authors":["Zhiqiang Zhong","Simon Sataa-Yu Larsen","Haoyu Guo","Tao Tang","Kuangyu Zhou","Davide Mottin"],"pdf_url":"https://arxiv.org/pdf/2502.06634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06632v1","updated":"2025-02-10T16:28:35Z","published":"2025-02-10T16:28:35Z","title":"Few-Shot Classification and Anatomical Localization of Tissues in SPECT\n  Imaging","summary":"  Accurate classification and anatomical localization are essential for\neffective medical diagnostics and research, which may be efficiently performed\nusing deep learning techniques. However, availability of limited labeled data\nposes a significant challenge. To address this, we adapted Prototypical\nNetworks and the Propagation-Reconstruction Network (PRNet) for few-shot\nclassification and localization, respectively, in Single Photon Emission\nComputed Tomography (SPECT) images. For the proof of concept we used a\n2D-sliced image cropped around heart. The Prototypical Network, with a\npre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver\ntissues with 96.67% training and 93.33% validation accuracy. PRNet, adapted for\n2D imaging with an encoder-decoder architecture and skip connections, achieved\na training loss of 1.395, accurately reconstructing patches and capturing\nspatial relationships. These results highlight the potential of Prototypical\nNetworks for tissue classification with limited labeled data and PRNet for\nanatomical landmark localization, paving the way for improved performance in\ndeep learning frameworks.\n","authors":["Mohammed Abdul Hafeez Khan","Samuel Morries Boddepalli","Siddhartha Bhattacharyya","Debasis Mitra"],"pdf_url":"https://arxiv.org/pdf/2502.06632v1.pdf","comment":"2 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.03380v2","updated":"2025-02-10T16:21:03Z","published":"2024-10-04T12:48:21Z","title":"Identifying perturbation targets through causal differential networks","summary":"  Identifying variables responsible for changes to a biological system enables\napplications in drug target discovery and cell engineering. Given a pair of\nobservational and interventional datasets, the goal is to isolate the subset of\nobserved variables that were the targets of the intervention. Directly applying\ncausal discovery algorithms is challenging: the data may contain thousands of\nvariables with as few as tens of samples per intervention, and biological\nsystems do not adhere to classical causality assumptions. We propose a\ncausality-inspired approach to address this practical setting. First, we infer\nnoisy causal graphs from the observational and interventional data. Then, we\nlearn to map the differences between these graphs, along with additional\nstatistical features, to sets of variables that were intervened upon. Both\nmodules are jointly trained in a supervised framework, on simulated and real\ndata that reflect the nature of biological interventions. This approach\nconsistently outperforms baselines for perturbation modeling on seven\nsingle-cell transcriptomics datasets. We also demonstrate significant\nimprovements over current causal discovery methods for predicting soft and hard\nintervention targets across a variety of synthetic data.\n","authors":["Menghua Wu","Umesh Padia","Sean H. Murphy","Regina Barzilay","Tommi Jaakkola"],"pdf_url":"https://arxiv.org/pdf/2410.03380v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01114v2","updated":"2025-02-10T16:13:51Z","published":"2024-05-02T09:22:54Z","title":"Continual Learning from Simulated Interactions via Multitask Prospective\n  Rehearsal for Bionic Limb Behavior Modeling","summary":"  Lower limb amputations and neuromuscular impairments severely restrict\nmobility, necessitating advancements beyond conventional prosthetics. While\nmotorized bionic limbs show promise, their effectiveness depends on replicating\nthe dynamic coordination of human movement across diverse environments. In this\npaper, we introduce a model for human behavior in the context of bionic\nprosthesis control. Our approach leverages human locomotion demonstrations to\nlearn the synergistic coupling of the lower limbs, enabling the prediction of\nthe kinematic behavior of a missing limb during tasks such as walking, climbing\ninclines, and stairs. We propose a multitasking, continually adaptive model\nthat anticipates and refines movements over time. At the core of our method is\na technique called multitask prospective rehearsal, that anticipates and\nsynthesizes future movements based on the previous prediction and employs a\ncorrective mechanism for subsequent predictions. Our evolving architecture\nmerges lightweight, task-specific modules on a shared backbone, ensuring both\nspecificity and scalability. We validate our model through experiments on\nreal-world human gait datasets, including transtibial amputees, across a wide\nrange of locomotion tasks. Results demonstrate that our approach consistently\noutperforms baseline models, particularly in scenarios with distributional\nshifts, adversarial perturbations, and noise.\n","authors":["Sharmita Dey","Benjamin Paassen","Sarath Ravindran Nair","Sabri Boughorbel","Arndt F. Schilling"],"pdf_url":"https://arxiv.org/pdf/2405.01114v2.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR) 2025"},{"id":"http://arxiv.org/abs/2407.05237v3","updated":"2025-02-10T16:01:43Z","published":"2024-07-07T02:35:55Z","title":"Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex\n  composite losses","summary":"  Differentially-private stochastic gradient descent (DP-SGD) is a family of\niterative machine learning training algorithms that privatize gradients to\ngenerate a sequence of differentially-private (DP) model parameters. It is also\nthe standard tool used to train DP models in practice, even though most users\nare only interested in protecting the privacy of the final model. Tight DP\naccounting for the last iterate would minimize the amount of noise required\nwhile maintaining the same privacy guarantee and potentially increasing model\nutility. However, last-iterate accounting is challenging, and existing works\nrequire strong assumptions not satisfied by most implementations. These include\nassuming (i) the global sensitivity constant is known - to avoid gradient\nclipping; (ii) the loss function is Lipschitz or convex; and (iii) input\nbatches are sampled randomly.\n  In this work, we forego any unrealistic assumptions and provide privacy\nbounds for the most commonly used variant of DP-SGD, in which data is traversed\ncyclically, gradients are clipped, and only the last model is released. More\nspecifically, we establish new Renyi differential privacy (RDP) upper bounds\nfor the last iterate under realistic assumptions of small stepsize and\nLipschitz smoothness of the loss function. Our general bounds also recover the\nspecial-case convex bounds when the weak-convexity parameter of the objective\nfunction approaches zero and no clipping is performed. The approach itself\nleverages optimal transport techniques for last iterate bounds, which is a\nnontrivial task when the data is traversed cyclically and the loss function is\nnonconvex.\n","authors":["Weiwei Kong","Mónica Ribero"],"pdf_url":"https://arxiv.org/pdf/2407.05237v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06717v3","updated":"2025-02-10T16:01:04Z","published":"2024-10-09T09:41:28Z","title":"Exact full-RSB SAT/UNSAT transition in infinitely wide two-layer neural\n  networks","summary":"  We analyze the problem of storing random pattern-label associations using two\nclasses of continuous non-convex weights models, namely the perceptron with\nnegative margin and an infinite-width two-layer neural network with\nnon-overlapping receptive fields and generic activation function. Using a\nfull-RSB ansatz we compute the exact value of the SAT/UNSAT transition.\nFurthermore, in the case of the negative perceptron we show that the overlap\ndistribution of typical states displays an overlap gap (a disconnected support)\nin certain regions of the phase diagram defined by the value of the margin and\nthe density of patterns to be stored. This implies that some recent theorems\nthat ensure convergence of Approximate Message Passing (AMP) based algorithms\nto capacity are not applicable. Finally, we show that Gradient Descent is not\nable to reach the maximal capacity, irrespectively of the presence of an\noverlap gap for typical states. This finding, similarly to what occurs in\nbinary weight models, suggests that gradient-based algorithms are biased\ntowards highly atypical states, whose inaccessibility determines the\nalgorithmic threshold.\n","authors":["Brandon L. Annesi","Enrico M. Malatesta","Francesco Zamponi"],"pdf_url":"https://arxiv.org/pdf/2410.06717v3.pdf","comment":"39 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.06601v1","updated":"2025-02-10T16:00:48Z","published":"2025-02-10T16:00:48Z","title":"Amortized In-Context Bayesian Posterior Estimation","summary":"  Bayesian inference provides a natural way of incorporating prior beliefs and\nassigning a probability measure to the space of hypotheses. Current solutions\nrely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and\nVariational Inference (VI), which need to be re-run whenever new observations\nare available. Amortization, through conditional estimation, is a viable\nstrategy to alleviate such difficulties and has been the guiding principle\nbehind simulation-based inference, neural processes and in-context methods\nusing pre-trained models. In this work, we conduct a thorough comparative\nanalysis of amortized in-context Bayesian posterior estimation methods from the\nlens of different optimization objectives and architectural choices. Such\nmethods train an amortized estimator to perform posterior parameter inference\nby conditioning on a set of data examples passed as context to a sequence model\nsuch as a transformer. In contrast to language models, we leverage permutation\ninvariant architectures as the true posterior is invariant to the ordering of\ncontext examples. Our empirical study includes generalization to\nout-of-distribution tasks, cases where the assumed underlying model is\nmisspecified, and transfer from simulated to real problems. Subsequently, it\nhighlights the superiority of the reverse KL estimator for predictive problems,\nespecially when combined with the transformer architecture and normalizing\nflows.\n","authors":["Sarthak Mittal","Niels Leif Bracher","Guillaume Lajoie","Priyank Jaini","Marcus Brubaker"],"pdf_url":"https://arxiv.org/pdf/2502.06601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06597v1","updated":"2025-02-10T15:58:26Z","published":"2025-02-10T15:58:26Z","title":"Continual Release Moment Estimation with Differential Privacy","summary":"  We propose Joint Moment Estimation (JME), a method for continually and\nprivately estimating both the first and second moments of data with reduced\nnoise compared to naive approaches. JME uses the matrix mechanism and a joint\nsensitivity analysis to allow the second moment estimation with no additional\nprivacy cost, thereby improving accuracy while maintaining privacy. We\ndemonstrate JME's effectiveness in two applications: estimating the running\nmean and covariance matrix for Gaussian density estimation, and model training\nwith DP-Adam on CIFAR-10.\n","authors":["Nikita P. Kalinin","Jalaj Upadhyay","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2502.06597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06591v1","updated":"2025-02-10T15:55:08Z","published":"2025-02-10T15:55:08Z","title":"Diffeomorphic Temporal Alignment Nets for Time-series Joint Alignment\n  and Averaging","summary":"  In time-series analysis, nonlinear temporal misalignment remains a pivotal\nchallenge that forestalls even simple averaging. Since its introduction, the\nDiffeomorphic Temporal Alignment Net (DTAN), which we first introduced (Weber\net al., 2019) and further developed in (Weber & Freifeld, 2023), has proven\nitself as an effective solution for this problem (these conference papers are\nearlier partial versions of the current manuscript). DTAN predicts and applies\ndiffeomorphic transformations in an input-dependent manner, thus facilitating\nthe joint alignment (JA) and averaging of time-series ensembles in an\nunsupervised or a weakly-supervised manner. The inherent challenges of the\nweakly/unsupervised setting, particularly the risk of trivial solutions through\nexcessive signal distortion, are mitigated using either one of two distinct\nstrategies: 1) a regularization term for warps; 2) using the Inverse\nConsistency Averaging Error (ICAE). The latter is a novel, regularization-free\napproach which also facilitates the JA of variable-length signals. We also\nfurther extend our framework to incorporate multi-task learning (MT-DTAN),\nenabling simultaneous time-series alignment and classification. Additionally,\nwe conduct a comprehensive evaluation of different backbone architectures,\ndemonstrating their efficacy in time-series alignment tasks. Finally, we\nshowcase the utility of our approach in enabling Principal Component Analysis\n(PCA) for misaligned time-series data. Extensive experiments across 128 UCR\ndatasets validate the superiority of our approach over contemporary averaging\nmethods, including both traditional and learning-based approaches, marking a\nsignificant advancement in the field of time-series analysis.\n","authors":["Ron Shapira Weber","Oren Freifeld"],"pdf_url":"https://arxiv.org/pdf/2502.06591v1.pdf","comment":"This manuscript covers and extends the papers: Diffeomorphic Temporal\n  Alignment Nets (DTAN; NeruIPS 2019) and Regularization-free Diffeomorphic\n  Temporal Alignment Nets (ICML 2023). Additional contributions: Multi-tasking\n  DTAN, PCA-DTAN and more"},{"id":"http://arxiv.org/abs/2502.06589v1","updated":"2025-02-10T15:54:34Z","published":"2025-02-10T15:54:34Z","title":"Hephaestus: Improving Fundamental Agent Capabilities of Large Language\n  Models through Continual Pre-Training","summary":"  Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments.\n","authors":["Yuchen Zhuang","Jingfeng Yang","Haoming Jiang","Xin Liu","Kewei Cheng","Sanket Lokegaonkar","Yifan Gao","Qing Ping","Tianyi Liu","Binxuan Huang","Zheng Li","Zhengyang Wang","Pei Chen","Ruijie Wang","Rongzhi Zhang","Nasser Zalmout","Priyanka Nigam","Bing Yin","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06589v1.pdf","comment":"Accepted to NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2502.06587v1","updated":"2025-02-10T15:53:26Z","published":"2025-02-10T15:53:26Z","title":"evclust: Python library for evidential clustering","summary":"  A recent developing trend in clustering is the advancement of algorithms that\nnot only identify clusters within data, but also express and capture the\nuncertainty of cluster membership. Evidential clustering addresses this by\nusing the Dempster-Shafer theory of belief functions, a framework designed to\nmanage and represent uncertainty. This approach results in a credal partition,\na structured set of mass functions that quantify the uncertain assignment of\neach object to potential groups. The Python framework evclust, presented in\nthis paper, offers a suite of efficient evidence clustering algorithms as well\nas tools for visualizing, evaluating and analyzing credal partitions.\n","authors":["Armel Soubeiga","Violaine Antoine"],"pdf_url":"https://arxiv.org/pdf/2502.06587v1.pdf","comment":"13 pages, 2 figures, Preprint"},{"id":"http://arxiv.org/abs/2502.06584v1","updated":"2025-02-10T15:52:55Z","published":"2025-02-10T15:52:55Z","title":"Deep Reinforcement Learning based Triggering Function for Early\n  Classifiers of Time Series","summary":"  Early Classification of Time Series (ECTS) has been recognized as an\nimportant problem in many areas where decisions have to be taken as soon as\npossible, before the full data availability, while time pressure increases.\nNumerous ECTS approaches have been proposed, based on different triggering\nfunctions, each taking into account various pieces of information related to\nthe incoming time series and/or the output of a classifier. Although their\nperformances have been empirically compared in the literature, no studies have\nbeen carried out on the optimality of these triggering functions that involve\n``man-tailored'' decision rules. Based on the same information, could there be\nbetter triggering functions? This paper presents one way to investigate this\nquestion by showing first how to translate ECTS problems into Reinforcement\nLearning (RL) ones, where the very same information is used in the state space.\nA thorough comparison of the performance obtained by ``handmade'' approaches\nand their ``RL-based'' counterparts has been carried out. A second question\ninvestigated in this paper is whether a different combination of information,\ndefining the state space in RL systems, can achieve even better performance.\nExperiments show that the system we describe, called \\textsc{Alert},\nsignificantly outperforms its state-of-the-art competitors on a large number of\ndatasets.\n","authors":["Aurélien Renault","Alexis Bondu","Antoine Cornuéjols","Vincent Lemaire"],"pdf_url":"https://arxiv.org/pdf/2502.06584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06581v1","updated":"2025-02-10T15:48:11Z","published":"2025-02-10T15:48:11Z","title":"A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems","summary":"  The explosive growth of video data has driven the development of distributed\nvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enabling\nefficient video processing, real-time inference, and privacy-preserving\nanalysis. Among multiple advantages, CETC systems can distribute video\nprocessing tasks and enable adaptive analytics across cloud, edge, and terminal\ndevices, leading to breakthroughs in video surveillance, autonomous driving,\nand smart cities. In this survey, we first analyze fundamental architectural\ncomponents, including hierarchical, distributed, and hybrid frameworks,\nalongside edge computing platforms and resource management mechanisms. Building\nupon these foundations, edge-centric approaches emphasize on-device processing,\nedge-assisted offloading, and edge intelligence, while cloud-centric methods\nleverage powerful computational capabilities for complex video understanding\nand model training. Our investigation also covers hybrid video analytics\nincorporating adaptive task offloading and resource-aware scheduling techniques\nthat optimize performance across the entire system. Beyond conventional\napproaches, recent advances in large language models and multimodal integration\nreveal both opportunities and challenges in platform scalability, data\nprotection, and system reliability. Future directions also encompass\nexplainable systems, efficient processing mechanisms, and advanced video\nanalytics, offering valuable insights for researchers and practitioners in this\ndynamic field.\n","authors":["Linxiao Gong","Hao Yang","Gaoyun Fang","Bobo Ju","Juncen Guo","Xiaoguang Zhu","Yan Wang","Xiping Hu","Peng Sun","Azzedine Boukerche"],"pdf_url":"https://arxiv.org/pdf/2502.06581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06577v1","updated":"2025-02-10T15:45:18Z","published":"2025-02-10T15:45:18Z","title":"The Minimal Search Space for Conditional Causal Bandits","summary":"  Causal knowledge can be used to support decision-making problems. This has\nbeen recognized in the causal bandits literature, where a causal (multi-armed)\nbandit is characterized by a causal graphical model and a target variable. The\narms are then interventions on the causal model, and rewards are samples of the\ntarget variable. Causal bandits were originally studied with a focus on hard\ninterventions. We focus instead on cases where the arms are conditional\ninterventions, which more accurately model many real-world decision-making\nproblems by allowing the value of the intervened variable to be chosen based on\nthe observed values of other variables. This paper presents a graphical\ncharacterization of the minimal set of nodes guaranteed to contain the optimal\nconditional intervention, which maximizes the expected reward. We then propose\nan efficient algorithm with a time complexity of $O(|V| + |E|)$ to identify\nthis minimal set of nodes. We prove that the graphical characterization and the\nproposed algorithm are correct. Finally, we empirically demonstrate that our\nalgorithm significantly prunes the search space and substantially accelerates\nconvergence rates when integrated into standard multi-armed bandit algorithms.\n","authors":["Francisco N. F. Q. Simoes","Itai Feigenbaum","Mehdi Dastani","Thijs van Ommen"],"pdf_url":"https://arxiv.org/pdf/2502.06577v1.pdf","comment":"Submitted to ICML2025"},{"id":"http://arxiv.org/abs/2502.06575v1","updated":"2025-02-10T15:44:34Z","published":"2025-02-10T15:44:34Z","title":"Predictive Red Teaming: Breaking Policies Without Breaking Robots","summary":"  Visuomotor policies trained via imitation learning are capable of performing\nchallenging manipulation tasks, but are often extremely brittle to lighting,\nvisual distractors, and object locations. These vulnerabilities can depend\nunpredictably on the specifics of training, and are challenging to expose\nwithout time-consuming and expensive hardware evaluations. We propose the\nproblem of predictive red teaming: discovering vulnerabilities of a policy with\nrespect to environmental factors, and predicting the corresponding performance\ndegradation without hardware evaluations in off-nominal scenarios. In order to\nachieve this, we develop RoboART: an automated red teaming (ART) pipeline that\n(1) modifies nominal observations using generative image editing to vary\ndifferent environmental factors, and (2) predicts performance under each\nvariation using a policy-specific anomaly detector executed on edited\nobservations. Experiments across 500+ hardware trials in twelve off-nominal\nconditions for visuomotor diffusion policies demonstrate that RoboART predicts\nperformance degradation with high accuracy (less than 0.19 average difference\nbetween predicted and real success rates). We also demonstrate how predictive\nred teaming enables targeted data collection: fine-tuning with data collected\nunder conditions predicted to be adverse boosts baseline performance by 2-7x.\n","authors":["Anirudha Majumdar","Mohit Sharma","Dmitry Kalashnikov","Sumeet Singh","Pierre Sermanet","Vikas Sindhwani"],"pdf_url":"https://arxiv.org/pdf/2502.06575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00615v2","updated":"2025-02-10T15:43:01Z","published":"2024-11-01T14:23:48Z","title":"Apriori_Goal algorithm for constructing association rules for a database\n  with a given classification","summary":"  An efficient Apriori_Goal algorithm is proposed for constructing association\nrules in a relational database with predefined classification. The target\nparameter of the database specifies a finite number of goals $Goal_k$, for each\nof which the algorithm constructs association rules of the form $X \\Rightarrow\nGoal_k$. The quality of the generated rules is characterized by five criteria:\ntwo represent rule frequency, two reflect rule reliability, and the fifth is a\nweighted sum of these four criteria.\n  The algorithm initially generates rules with single premises, where the\ncorrelation criterion between the premise $X$ and the conclusion $Goal_k$\nexceeds a specified threshold. Then, rules with extended premises are built\nbased on the anti-monotonicity of rule frequency criteria and the monotonicity\nof rule reliability criteria. Newly constructed rules tend to decrease in\nfrequency while increasing in reliability. The article proves several\nstatements that justify the rule construction process.\n  The algorithm enables the construction of both high-frequency and rare rules\nwith low occurrence frequency but high reliability. It also allows for the\ngeneration of negative rules with negative correlation between the premise and\nconclusion, which can be valuable in practical applications for filtering out\nundesired goals.\n  The efficiency of the algorithm is based on two factors: the method of\nencoding the database and its partitioning into subsets linked to the target\nparameter. Time complexity estimates for rule construction are provided using a\nmedical database as an example.\n","authors":["Vladimir Billig"],"pdf_url":"https://arxiv.org/pdf/2411.00615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06574v1","updated":"2025-02-10T15:42:38Z","published":"2025-02-10T15:42:38Z","title":"On the Impact of the Utility in Semivalue-based Data Valuation","summary":"  Semivalue-based data valuation in machine learning (ML) quantifies the\ncontribution of individual data points to a downstream ML task by leveraging\nprinciples from cooperative game theory and the notion of utility. While this\nframework has been used in practice for assessing data quality, our experiments\nreveal inconsistent valuation outcomes across different utilities, albeit all\nrelated to ML performance. Beyond raising concerns about the reliability of\ndata valuation, this inconsistency is challenging to interpret, as it stems\nfrom the complex interaction of the utility with data points and semivalue\nweights, which has barely been studied in prior work. In this paper, we take a\nfirst step toward clarifying the utility impact on semivalue-based data\nvaluation. Specifically, we provide geometric interpretations of this impact\nfor a broad family of classification utilities, which includes the accuracy and\nthe arithmetic mean. We introduce the notion of spatial signatures: given a\nsemivalue, data points can be embedded into a two-dimensional space, and\nutility functions map to the dual of this space. This geometric perspective\nseparates the influence of the dataset and semivalue from that of the utility,\nproviding a theoretical explanation for the experimentally observed sensitivity\nof valuation outcomes to the utility choice.\n","authors":["Mélissa Tamine","Benjamin Heymann","Patrick Loiseau","Maxime Vono"],"pdf_url":"https://arxiv.org/pdf/2502.06574v1.pdf","comment":"34 pages, 21 figures"},{"id":"http://arxiv.org/abs/2408.05212v2","updated":"2025-02-10T15:42:08Z","published":"2024-08-10T05:41:19Z","title":"Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions","summary":"  Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.\n","authors":["Michele Miranda","Elena Sofia Ruzzetti","Andrea Santilli","Fabio Massimo Zanzotto","Sébastien Bratières","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2408.05212v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)\n  https://openreview.net/forum?id=Ss9MTTN7OL"},{"id":"http://arxiv.org/abs/2502.04809v2","updated":"2025-02-10T15:38:21Z","published":"2025-02-07T10:28:39Z","title":"Humans Co-exist, So Must Embodied Artificial Agents","summary":"  Modern embodied artificial agents excel in static, predefined tasks but fall\nshort in dynamic and long-term interactions with humans. On the other hand,\nhumans can adapt and evolve continuously, exploiting the situated knowledge\nembedded in their environment and other agents, thus contributing to meaningful\ninteractions. We introduce the concept of co-existence for embodied artificial\nagents and argues that it is a prerequisite for meaningful, long-term\ninteraction with humans. We take inspiration from biology and design theory to\nunderstand how human and non-human organisms foster entities that co-exist\nwithin their specific niches. Finally, we propose key research directions for\nthe machine learning community to foster co-existing embodied agents, focusing\non the principles, hardware and learning methods responsible for shaping them.\n","authors":["Hannah Kuehn","Joseph La Delfa","Miguel Vasco","Danica Kragic","Iolanda Leite"],"pdf_url":"https://arxiv.org/pdf/2502.04809v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06567v1","updated":"2025-02-10T15:34:42Z","published":"2025-02-10T15:34:42Z","title":"Membership Inference Risks in Quantized Models: A Theoretical and\n  Empirical Study","summary":"  Quantizing machine learning models has demonstrated its effectiveness in\nlowering memory and inference costs while maintaining performance levels\ncomparable to the original models. In this work, we investigate the impact of\nquantization procedures on the privacy of data-driven models, specifically\nfocusing on their vulnerability to membership inference attacks. We derive an\nasymptotic theoretical analysis of Membership Inference Security (MIS),\ncharacterizing the privacy implications of quantized algorithm weights against\nthe most powerful (and possibly unknown) attacks. Building on these theoretical\ninsights, we propose a novel methodology to empirically assess and rank the\nprivacy levels of various quantization procedures. Using synthetic datasets, we\ndemonstrate the effectiveness of our approach in assessing the MIS of different\nquantizers. Furthermore, we explore the trade-off between privacy and\nperformance using real-world data and models in the context of molecular\nmodeling.\n","authors":["Eric Aubinais","Philippe Formont","Pablo Piantanida","Elisabeth Gassiat"],"pdf_url":"https://arxiv.org/pdf/2502.06567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06564v1","updated":"2025-02-10T15:31:57Z","published":"2025-02-10T15:31:57Z","title":"Robust Scatter Matrix Estimation for Elliptical Distributions in\n  Polynomial Time","summary":"  We study the problem of computationally efficient robust estimation of\nscatter matrices of elliptical distributions under the strong contamination\nmodel. We design polynomial time algorithms that achieve dimension-independent\nerror in Frobenius norm.\n  Our first result is a sequence of efficient algorithms that approaches nearly\noptimal error. Specifically, under a mild assumption on the eigenvalues of the\nscatter matrix $\\Sigma$, for every $t \\in \\mathbb{N}$, we design an estimator\nthat, given $n = d^{O(t)}$ samples, in time $n^{O(t)}$ finds $\\hat{\\Sigma}$\nsuch that $ \\Vert{\\Sigma^{-1/2}\\, ({\\hat{\\Sigma} - \\Sigma})\\,\n\\Sigma^{-1/2}}\\Vert_{\\text{F}} \\le O(t \\cdot \\varepsilon^{1-\\frac{1}{t}})$,\nwhere $\\varepsilon$ is the fraction of corruption. We do not require any\nassumptions on the moments of the distribution, while all previously known\ncomputationally efficient algorithms for robust covariance/scatter estimation\nwith dimension-independent error rely on strong assumptions on the moments,\nsuch as sub-Gaussianity or (certifiable) hypercontractivity.\n  Furthermore, under a stronger assumption on the eigenvalues of $\\Sigma$\n(that, in particular, is satisfied by all matrices with constant condition\nnumber),\n  we provide a fast (sub-quadratic in the input size) algorithm that, given\nnearly optimal number of samples $n = \\tilde{O}(d^2/\\varepsilon)$, in time\n$\\tilde{O}({nd^2 poly(1/\\varepsilon)})$ finds $\\hat{\\Sigma}$ such that\n$\\Vert\\hat{\\Sigma} - \\Sigma\\Vert_{\\text{F}} \\le O(\\Vert{\\Sigma}\\Vert \\cdot\n\\sqrt{\\varepsilon})$.\n  Our approach is based on robust covariance estimation of the spatial sign\n(the projection onto the sphere of radius $\\sqrt{d}$) of elliptical\ndistributions.\n","authors":["Gleb Novikov"],"pdf_url":"https://arxiv.org/pdf/2502.06564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06108v2","updated":"2025-02-10T15:28:53Z","published":"2025-01-10T17:01:09Z","title":"Inferring High-Order Couplings with Neural Networks","summary":"  Maximum entropy methods, based on the inverse Ising/Potts problem from\nstatistical mechanics, are essential for modeling interactions between pairs of\nvariables in data-driven problems across disciplines such as bioinformatics,\necology, and neuroscience. Despite their considerable success, these methods\ntypically fail to capture higher-order interactions that are often essential\nfor understanding complex systems. Conversely, modern machine learning methods\ncapture these complex interactions, but the computational cost of interpretable\nframeworks makes them impractical for real-world applications. Restricted\nBoltzmann Machines (RBMs) provide a computationally efficient way to capture\nstatistical correlations using hidden nodes in a bipartite neural network. In\nthis study, we introduce a new method that maps RBMs to generalized Potts\nmodels, allowing for the extraction of interactions up to any specified order.\nThis method utilizes large-$N$ approximations, enabled by the RBM's simple\nstructure, to extract effective many-body couplings with minimal computational\neffort. Furthermore, we propose a robust framework for extracting higher-order\ninteractions in more complex probabilistic models and a simple gauge-fixing\nmethod within the effective many-body Potts model. Our validation on synthetic\ndatasets confirms the method's ability to recover two- and three-body\ninteractions accurately. When applied to protein sequence data, the framework\ncompetently reconstructs protein contact maps and provides performance\ncomparable to the best inverse Potts models. These findings confirm that RBMs\nare an effective and streamlined tool for exploring higher-order interactions\nwithin complex systems.\n","authors":["Aurélien Decelle","Alfonso de Jesús Navas Gómez","Beatriz Seoane"],"pdf_url":"https://arxiv.org/pdf/2501.06108v2.pdf","comment":"16 Pages and 5 Figures"},{"id":"http://arxiv.org/abs/2501.18280v2","updated":"2025-02-10T15:27:04Z","published":"2025-01-30T11:37:40Z","title":"Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models","summary":"  The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner.\n","authors":["Haoyu Liang","Youran Sun","Yunfeng Cai","Jun Zhu","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.18280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05506v2","updated":"2025-02-10T15:26:28Z","published":"2024-12-07T02:34:30Z","title":"Confidence Diagram of Nonparametric Ranking for Uncertainty Assessment\n  in Large Language Models Evaluation","summary":"  We consider the inference for the ranking of large language models (LLMs).\nAlignment arises as a significant challenge to mitigate hallucinations in the\nuse of LLMs. Ranking LLMs has proven to be an effective tool to improve\nalignment based on the best-of-$N$ policy. In this paper, we propose a new\ninferential framework for hypothesis testing among the ranking for language\nmodels. Our framework is based on a nonparametric contextual ranking framework\ndesigned to assess large language models' domain-specific expertise, leveraging\nnonparametric scoring methods to account for their sensitivity to the prompts.\nTo characterize the combinatorial complexity of the ranking, we introduce a\nnovel concept of confidence diagram, which leverages a Hasse diagram to\nrepresent the entire confidence set of rankings by a single directed graph. We\nshow the validity of the proposed confidence diagram by advancing the Gaussian\nmultiplier bootstrap theory to accommodate the supremum of independent\nempirical processes that are not necessarily identically distributed. Extensive\nnumerical experiments conducted on both synthetic and real data demonstrate\nthat our approach offers valuable insight into the evaluation for the\nperformance of different LLMs across various medical domains.\n","authors":["Zebin Wang","Yi Han","Ethan X. Fang","Lan Wang","Junwei Lu"],"pdf_url":"https://arxiv.org/pdf/2412.05506v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06555v1","updated":"2025-02-10T15:23:52Z","published":"2025-02-10T15:23:52Z","title":"Is API Access to LLMs Useful for Generating Private Synthetic Tabular\n  Data?","summary":"  Differentially private (DP) synthetic data is a versatile tool for enabling\nthe analysis of private data. Recent advancements in large language models\n(LLMs) have inspired a number of algorithm techniques for improving DP\nsynthetic data generation. One family of approaches uses DP finetuning on the\nfoundation model weights; however, the model weights for state-of-the-art\nmodels may not be public. In this work we propose two DP synthetic tabular data\nalgorithms that only require API access to the foundation model. We adapt the\nPrivate Evolution algorithm (Lin et al., 2023; Xie et al., 2024) -- which was\ndesigned for image and text data -- to the tabular data domain. In our\nextension of Private Evolution, we define a query workload-based distance\nmeasure, which may be of independent interest. We propose a family of\nalgorithms that use one-shot API access to LLMs, rather than adaptive queries\nto the LLM. Our findings reveal that API-access to powerful LLMs does not\nalways improve the quality of DP synthetic data compared to established\nbaselines that operate without such access. We provide insights into the\nunderlying reasons and propose improvements to LLMs that could make them more\neffective for this application.\n","authors":["Marika Swanberg","Ryan McKenna","Edo Roth","Albert Cheu","Peter Kairouz"],"pdf_url":"https://arxiv.org/pdf/2502.06555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06547v1","updated":"2025-02-10T15:16:25Z","published":"2025-02-10T15:16:25Z","title":"Data Augmentation and Regularization for Learning Group Equivariance","summary":"  In many machine learning tasks, known symmetries can be used as an inductive\nbias to improve model performance. In this paper, we consider learning group\nequivariance through training with data augmentation. We summarize results from\na previous paper of our own, and extend the results to show that equivariance\nof the trained model can be achieved through training on augmented data in\ntandem with regularization.\n","authors":["Oskar Nordenfors","Axel Flinth"],"pdf_url":"https://arxiv.org/pdf/2502.06547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18038v4","updated":"2025-02-10T15:13:16Z","published":"2024-06-26T03:12:07Z","title":"MT2ST: Adaptive Multi-Task to Single-Task Learning","summary":"  Efficient machine learning (ML) has become increasingly important as models\ngrow larger and data volumes expand. In this work, we address the trade-off\nbetween generalization in multi-task learning (MTL) and precision in\nsingle-task learning (STL) by introducing the Multi-Task to Single-Task (MT2ST)\nframework. MT2ST is designed to enhance training efficiency and accuracy in\nword embedding tasks, showcasing its value as a practical application of\nefficient ML.\n  Our framework employs two strategies: *Diminish*, which gradually reduces the\ninfluence of auxiliary tasks, and *Switch*, which transitions training from MTL\nto STL at a specific point. Empirical results show that MT2ST reduces training\ntime by 67\\% compared to STL and by 13\\% compared to traditional MTL, while\nmaintaining high accuracy. These findings highlight MT2ST as an efficient ML\nsolution tailored for optimizing word embedding training. Code is available at\nhttps://github.com/NoakLiu/MT2ST.\n","authors":["Dong Liu","Yanxuan Yu"],"pdf_url":"https://arxiv.org/pdf/2406.18038v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06545v1","updated":"2025-02-10T15:10:06Z","published":"2025-02-10T15:10:06Z","title":"Dimension-free Regret for Learning Asymmetric Linear Dynamical Systems","summary":"  Previously, methods for learning marginally stable linear dynamical systems\neither required the transition matrix to be symmetric or incurred regret bounds\nthat scale polynomially with the system's hidden dimension. In this work, we\nintroduce a novel method that overcomes this trade-off, achieving\ndimension-free regret despite the presence of asymmetric matrices and marginal\nstability. Our method combines spectral filtering with linear predictors and\nemploys Chebyshev polynomials in the complex plane to construct a novel\nspectral filtering basis. This construction guarantees sublinear regret in an\nonline learning framework, without relying on any statistical or generative\nassumptions. Specifically, we prove that as long as the transition matrix has\neigenvalues with complex component bounded by $1/\\mathrm{poly} \\log T$, then\nour method achieves regret $\\tilde{O}(T^{9/10})$ when compared to the best\nlinear dynamical predictor in hindsight.\n","authors":["Annie Marsden","Elad Hazan"],"pdf_url":"https://arxiv.org/pdf/2502.06545v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2502.06544v1","updated":"2025-02-10T15:09:56Z","published":"2025-02-10T15:09:56Z","title":"Sequence Transferability and Task Order Selection in Continual Learning","summary":"  In continual learning, understanding the properties of task sequences and\ntheir relationships to model performance is important for developing advanced\nalgorithms with better accuracy. However, efforts in this direction remain\nunderdeveloped despite encouraging progress in methodology development. In this\nwork, we investigate the impacts of sequence transferability on continual\nlearning and propose two novel measures that capture the total transferability\nof a task sequence, either in the forward or backward direction. Based on the\nempirical properties of these measures, we then develop a new method for the\ntask order selection problem in continual learning. Our method can be shown to\noffer a better performance than the conventional strategy of random task\nselection.\n","authors":["Thinh Nguyen","Cuong N. Nguyen","Quang Pham","Binh T. Nguyen","Savitha Ramasamy","Xiaoli Li","Cuong V. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.06544v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.03245v2","updated":"2025-02-10T15:09:52Z","published":"2025-02-05T15:02:40Z","title":"Calibrated Unsupervised Anomaly Detection in Multivariate Time-series\n  using Reinforcement Learning","summary":"  This paper investigates unsupervised anomaly detection in multivariate\ntime-series data using reinforcement learning (RL) in the latent space of an\nautoencoder. A significant challenge is the limited availability of anomalous\ndata, often leading to misclassifying anomalies as normal events, thus raising\nfalse negatives. RL can help overcome this limitation by promoting exploration\nand balancing exploitation during training, effectively preventing overfitting.\nWavelet analysis is also utilized to enhance anomaly detection, enabling\ntime-series data decomposition into both time and frequency domains. This\napproach captures anomalies at multiple resolutions, with wavelet coefficients\nextracted to detect both sudden and subtle shifts in the data, thereby refining\nthe anomaly detection process. We calibrate the decision boundary by generating\nsynthetic anomalies and embedding a supervised framework within the model. This\nsupervised element aids the unsupervised learning process by fine-tuning the\ndecision boundary and increasing the model's capacity to distinguish between\nnormal and anomalous patterns effectively.\n","authors":["Saba Sanami","Amir G. Aghdam"],"pdf_url":"https://arxiv.org/pdf/2502.03245v2.pdf","comment":"This paper has been accepted for publication and presentation at the\n  2025 IEEE International systems Conference (SysCon)"},{"id":"http://arxiv.org/abs/2407.10994v4","updated":"2025-02-10T15:08:07Z","published":"2024-06-24T12:09:34Z","title":"Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant","summary":"  The availability of powerful open-source large language models (LLMs) opens\nexciting use-cases, such as using personal data to fine-tune these models to\nimitate a user's unique writing style. Two key requirements for such assistants\nare personalization - in the sense that the assistant should recognizably\nreflect the user's own writing style - and privacy - users may justifiably be\nwary of uploading extremely personal data, such as their email archive, to a\nthird-party service. In this paper, we present a new design and evaluation for\nsuch an automated assistant, for the specific use case of email generation,\nwhich we call Panza. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to reflect a user's writing\nstyle using limited data, while executing on extremely limited resources, e.g.\non a free Google Colab instance. Our key methodological contribution is the\nfirst detailed study of evaluation metrics for this personalized writing task,\nand of how different choices of system components--the use of RAG and of\ndifferent fine-tuning approaches-impact the system's performance. Additionally,\nwe demonstrate that very little data - under 100 email samples - are sufficient\nto create models that convincingly imitate humans. This finding showcases a\npreviously-unknown attack vector in language models - that access to a small\nnumber of writing samples can allow a bad actor to cheaply create generative\nmodels that imitate a target's writing style. We are releasing the full Panza\ncode as well as three new email datasets licensed for research use at\nhttps://github.com/IST-DASLab/PanzaMail.\n","authors":["Armand Nicolicioiu","Eugenia Iofinova","Andrej Jovanovic","Eldar Kurtic","Mahdi Nikdan","Andrei Panferov","Ilia Markov","Nir Shavit","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2407.10994v4.pdf","comment":"Panza is available at https://github.com/IST-DASLab/PanzaMail"},{"id":"http://arxiv.org/abs/2405.20973v2","updated":"2025-02-10T15:04:53Z","published":"2024-05-31T16:21:05Z","title":"LCQ: Low-Rank Codebook based Quantization for Large Language Models","summary":"  Large language models~(LLMs) have recently demonstrated promising performance\nin many tasks. However, the high storage and computational cost of LLMs has\nbecome a challenge for deploying LLMs. Weight quantization has been widely used\nfor model compression, which can reduce both storage and computational cost.\nMost existing weight quantization methods for LLMs use a rank-one codebook for\nquantization, which results in substantial accuracy loss when the compression\nratio is high. In this paper, we propose a novel weight quantization method,\ncalled low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a\nlow-rank codebook, the rank of which can be larger than one, for quantization.\nExperiments show that LCQ can achieve better accuracy than existing methods\nwith a negligibly extra storage cost.\n","authors":["Wen-Pu Cai","Ming-Yang Li","Wu-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2405.20973v2.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.06536v1","updated":"2025-02-10T15:01:56Z","published":"2025-02-10T15:01:56Z","title":"Sample-efficient Learning of Concepts with Theoretical Guarantees: from\n  Data to Concepts without Interventions","summary":"  Machine learning is a vital part of many real-world systems, but several\nconcerns remain about the lack of interpretability, explainability and\nrobustness of black-box AI systems. Concept-based models (CBM) address some of\nthese challenges by learning interpretable concepts from high-dimensional data,\ne.g. images, which are used to predict labels. An important issue in CBMs is\nconcept leakage, i.e., spurious information in the learned concepts, which\neffectively leads to learning \"wrong\" concepts. Current mitigating strategies\nare heuristic, have strong assumptions, e.g., they assume that the concepts are\nstatistically independent of each other, or require substantial human\ninteraction in terms of both interventions and labels provided by annotators.\nIn this paper, we describe a framework that provides theoretical guarantees on\nthe correctness of the learned concepts and on the number of required labels,\nwithout requiring any interventions. Our framework leverages causal\nrepresentation learning (CRL) to learn high-level causal variables from\nlow-level data, and learns to align these variables with interpretable\nconcepts. We propose a linear and a non-parametric estimator for this mapping,\nproviding a finite-sample high probability result in the linear case and an\nasymptotic consistency result for the non-parametric estimator. We implement\nour framework with state-of-the-art CRL methods, and show its efficacy in\nlearning the correct concepts in synthetic and image benchmarks.\n","authors":["Hidde Fokkema","Tim van Erven","Sara Magliacane"],"pdf_url":"https://arxiv.org/pdf/2502.06536v1.pdf","comment":"47 pages, 16 figures, 9 Tables, Preprint"},{"id":"http://arxiv.org/abs/2405.14620v2","updated":"2025-02-10T15:01:34Z","published":"2024-05-23T14:29:15Z","title":"Closed-form Solutions: A New Perspective on Solving Differential\n  Equations","summary":"  The pursuit of analytical solutions for differential equations has\nhistorically been limited by the need for extensive prior knowledge and\nmathematical prowess; however, machine learning methods like genetic algorithms\nhave recently been applied to this end, albeit with issues of significant time\nconsumption and complexity. This paper presents a novel machine learning-based\nsolver, SSDE (Symbolic Solver for Differential Equations), which employs\nreinforcement learning to derive symbolic closed-form solutions for various\ndifferential equations. Our evaluations on a range of ordinary and partial\ndifferential equations demonstrate that SSDE provides superior performance in\nachieving analytical solutions compared to other machine learning approaches.\n","authors":["Shu Wei","Yanjie Li","Lina Yu","Weijun Li","Min Wu","Linjun Sun","Jufeng Han","Yan Pang"],"pdf_url":"https://arxiv.org/pdf/2405.14620v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18609v3","updated":"2025-02-10T15:01:09Z","published":"2024-07-26T09:00:18Z","title":"Denoising Lévy Probabilistic Models","summary":"  Exploring noise distributions beyond Gaussian in diffusion models remains an\nopen challenge. While Gaussian-based models succeed within a unified SDE\nframework, recent studies suggest that heavy-tailed noise distributions, like\n$\\alpha$-stable distributions, may better handle mode collapse and effectively\nmanage datasets exhibiting class imbalance, heavy tails, or prominent outliers.\nRecently, Yoon et al.\\ (NeurIPS 2023), presented the L\\'evy-It\\^o model (LIM),\ndirectly extending the SDE-based framework to a class of heavy-tailed SDEs,\nwhere the injected noise followed an $\\alpha$-stable distribution, a rich class\nof heavy-tailed distributions. However, the LIM framework relies on highly\ninvolved mathematical techniques with limited flexibility, potentially\nhindering broader adoption and further development. In this study, instead of\nstarting from the SDE formulation, we extend the denoising diffusion\nprobabilistic model (DDPM) by replacing the Gaussian noise with $\\alpha$-stable\nnoise. By using only elementary proof techniques, the proposed approach,\nDenoising L\\'evy Probabilistic Models (DLPM), boils down to vanilla DDPM with\nminor modifications. As opposed to the Gaussian case, DLPM and LIM yield\ndifferent training algorithms and different backward processes, leading to\ndistinct sampling algorithms. These fundamental differences translate favorably\nfor DLPM as compared to LIM: our experiments show improvements in coverage of\ndata distribution tails, better robustness to unbalanced datasets, and improved\ncomputation times requiring smaller number of backward steps.\n","authors":["Dario Shariatian","Umut Simsekli","Alain Durmus"],"pdf_url":"https://arxiv.org/pdf/2407.18609v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06533v1","updated":"2025-02-10T14:56:25Z","published":"2025-02-10T14:56:25Z","title":"Ignore the KL Penalty! Boosting Exploration on Critical Tokens to\n  Enhance RL Fine-Tuning","summary":"  The ability to achieve long-term goals is a key challenge in the current\ndevelopment of large language models (LLMs). To address this, pre-trained LLMs\ncan be fine-tuned with reinforcement learning (RL) to explore solutions that\noptimize a given goal. However, exploration with LLMs is difficult, as a\nbalance has to be struck between discovering new solutions and staying close\nenough to the pre-trained model, so as not to degrade basic capabilities. This\nis typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we\ninvestigate the exploration dynamics of a small language model on a simple\narithmetic task. We show how varying degrees of pre-training influence\nexploration and demonstrate the importance of \"critical tokens\" which have a\ndramatic impact on the final outcome. Consequently, we introduce a simple\nmodification to the KL penalty that favors exploration on critical tokens,\nincreasing the efficiency of the RL fine-tuning stage.\n","authors":["Jean Vassoyan","Nathanaël Beau","Roman Plaud"],"pdf_url":"https://arxiv.org/pdf/2502.06533v1.pdf","comment":"11 pages, 6 figures, 5 tables. Accepted for publication in the\n  Findings of the North American Chapter of the Association for Computational\n  Linguistics (NAACL) 2025"},{"id":"http://arxiv.org/abs/2502.06525v1","updated":"2025-02-10T14:49:22Z","published":"2025-02-10T14:49:22Z","title":"Properties of Wasserstein Gradient Flows for the Sliced-Wasserstein\n  Distance","summary":"  In this paper, we investigate the properties of the Sliced Wasserstein\nDistance (SW) when employed as an objective functional. The SW metric has\ngained significant interest in the optimal transport and machine learning\nliterature, due to its ability to capture intricate geometric properties of\nprobability distributions while remaining computationally tractable, making it\na valuable tool for various applications, including generative modeling and\ndomain adaptation. Our study aims to provide a rigorous analysis of the\ncritical points arising from the optimization of the SW objective. By computing\nexplicit perturbations, we establish that stable critical points of SW cannot\nconcentrate on segments. This stability analysis is crucial for understanding\nthe behaviour of optimization algorithms for models trained using the SW\nobjective. Furthermore, we investigate the properties of the SW objective,\nshedding light on the existence and convergence behavior of critical points. We\nillustrate our theoretical results through numerical experiments.\n","authors":["Christophe Vauthier","Quentin Mérigot","Anna Korba"],"pdf_url":"https://arxiv.org/pdf/2502.06525v1.pdf","comment":"32p"},{"id":"http://arxiv.org/abs/2502.06516v1","updated":"2025-02-10T14:37:26Z","published":"2025-02-10T14:37:26Z","title":"Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation","summary":"  Minority samples are underrepresented instances located in low-density\nregions of a data manifold, and are valuable in many generative AI\napplications, such as data augmentation, creative content generation, etc.\nUnfortunately, existing diffusion-based minority generators often rely on\ncomputationally expensive guidance dedicated for minority generation. To\naddress this, here we present a simple yet powerful guidance-free approach\ncalled Boost-and-Skip for generating minority samples using diffusion models.\nThe key advantage of our framework requires only two minimal changes to\nstandard generative processes: (i) variance-boosted initialization and (ii)\ntimestep skipping. We highlight that these seemingly-trivial modifications are\nsupported by solid theoretical and empirical evidence, thereby effectively\npromoting emergence of underrepresented minority features. Our comprehensive\nexperiments demonstrate that Boost-and-Skip greatly enhances the capability of\ngenerating minority samples, even rivaling guidance-based state-of-the-art\napproaches while requiring significantly fewer computations.\n","authors":["Soobin Um","Beomsu Kim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2502.06516v1.pdf","comment":"29 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.06491v1","updated":"2025-02-10T14:08:55Z","published":"2025-02-10T14:08:55Z","title":"Model-Based Offline Reinforcement Learning with Reliability-Guaranteed\n  Sequence Modeling","summary":"  Model-based offline reinforcement learning (MORL) aims to learn a policy by\nexploiting a dynamics model derived from an existing dataset. Applying\nconservative quantification to the dynamics model, most existing works on MORL\ngenerate trajectories that approximate the real data distribution to facilitate\npolicy learning by using current information (e.g., the state and action at\ntime step $t$). However, these works neglect the impact of historical\ninformation on environmental dynamics, leading to the generation of unreliable\ntrajectories that may not align with the real data distribution. In this paper,\nwe propose a new MORL algorithm \\textbf{R}eliability-guaranteed\n\\textbf{T}ransformer (RT), which can eliminate unreliable trajectories by\ncalculating the cumulative reliability of the generated trajectory (i.e., using\na weighted variational distance away from the real data). Moreover, by sampling\ncandidate actions with high rewards, RT can efficiently generate high-return\ntrajectories from the existing offline data. We theoretically prove the\nperformance guarantees of RT in policy learning, and empirically demonstrate\nits effectiveness against state-of-the-art model-based methods on several\nbenchmark tasks.\n","authors":["Shenghong He"],"pdf_url":"https://arxiv.org/pdf/2502.06491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06485v1","updated":"2025-02-10T14:04:23Z","published":"2025-02-10T14:04:23Z","title":"WyckoffDiff - A Generative Diffusion Model for Crystal Symmetry","summary":"  Crystalline materials often exhibit a high level of symmetry. However, most\ngenerative models do not account for symmetry, but rather model each atom\nwithout any constraints on its position or element. We propose a generative\nmodel, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based\ndescriptions of crystals. This is enabled by considering a crystal structure\nrepresentation that encodes all symmetry, and we design a novel neural network\narchitecture which enables using this representation inside a discrete\ngenerative model framework. In addition to respecting symmetry by construction,\nthe discrete nature of our model enables fast generation. We additionally\npresent a new metric, Fr\\'echet Wrenformer Distance, which captures the\nsymmetry aspects of the materials generated, and we benchmark WyckoffDiff\nagainst recently proposed generative models for crystal generation.\n","authors":["Filip Ekström Kelvinius","Oskar B. Andersson","Abhijith S. Parackal","Dong Qian","Rickard Armiento","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2502.06485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10958v4","updated":"2025-02-10T14:02:49Z","published":"2024-11-17T04:35:49Z","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization","summary":"  Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrices $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK^\\top$. Third, we propose a two-level accumulation strategy\nfor $\\widetilde PV$ to enhance the accuracy of FP8 $\\widetilde PV$. The\noperations per second (OPS) of SageAttention2 surpass FlashAttention2 and\nxformers by about 3x and 4.5x on RTX4090, respectively. Moreover,\nSageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs,\nwhile delivering much higher accuracy. Comprehensive experiments confirm that\nour approach incurs negligible end-to-end metrics loss across diverse models,\nincluding those for language, image, and video generation. The code is\navailable at https://github.com/thu-ml/SageAttention.\n","authors":["Jintao Zhang","Haofeng Huang","Pengle Zhang","Jia Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2411.10958v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06480v1","updated":"2025-02-10T13:59:00Z","published":"2025-02-10T13:59:00Z","title":"Logarithmic Regret of Exploration in Average Reward Markov Decision\n  Processes","summary":"  In average reward Markov decision processes, state-of-the-art algorithms for\nregret minimization follow a well-established framework: They are model-based,\noptimistic and episodic. First, they maintain a confidence region from which\noptimistic policies are computed using a well-known subroutine called Extended\nValue Iteration (EVI). Second, these policies are used over time windows called\nepisodes, each ended by the Doubling Trick (DT) rule or a variant thereof. In\nthis work, without modifying EVI, we show that there is a significant advantage\nin replacing (DT) by another simple rule, that we call the Vanishing\nMultiplicative (VM) rule. When managing episodes with (VM), the algorithm's\nregret is, both in theory and in practice, as good if not better than with\n(DT), while the one-shot behavior is greatly improved. More specifically, the\nmanagement of bad episodes (when sub-optimal policies are being used) is much\nbetter under (VM) than (DT) by making the regret of exploration logarithmic\nrather than linear. These results are made possible by a new in-depth\nunderstanding of the contrasting behaviors of confidence regions during good\nand bad episodes.\n","authors":["Victor Boone","Bruno Gaujal"],"pdf_url":"https://arxiv.org/pdf/2502.06480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08170v2","updated":"2025-02-10T13:57:58Z","published":"2023-11-14T13:54:35Z","title":"Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning\n  Approach","summary":"  Lattice reduction is a combinatorial optimization problem aimed at finding\nthe most orthogonal basis in a given lattice. The Lenstra-Lenstra-Lov\\'asz\n(LLL) algorithm is the best algorithm in the literature for solving this\nproblem. In light of recent research on algorithm discovery, in this work, we\nwould like to answer this question: is it possible to parametrize the algorithm\nspace for lattice reduction problem with neural networks and find an algorithm\nwithout supervised data? Our strategy is to use equivariant and invariant\nparametrizations and train in a self-supervised way. We design a deep neural\nmodel outputting factorized unimodular matrices and train it in a\nself-supervised manner by penalizing non-orthogonal lattice bases. We\nincorporate the symmetries of lattice reduction into the model by making it\ninvariant to isometries and scaling of the ambient space and equivariant with\nrespect to the hyperocrahedral group permuting and flipping the lattice basis\nelements. We show that this approach yields an algorithm with comparable\ncomplexity and performance to the LLL algorithm on a set of benchmarks.\nAdditionally, motivated by certain applications for wireless communication, we\nextend our method to a convolutional architecture which performs joint\nreduction of spatially-correlated lattices arranged in a grid, thereby\namortizing its cost over multiple lattices.\n","authors":["Giovanni Luca Marchetti","Gabriele Cesa","Pratik Kumar","Arash Behboodi"],"pdf_url":"https://arxiv.org/pdf/2311.08170v2.pdf","comment":"Accepted at TMLR"},{"id":"http://arxiv.org/abs/2409.06282v2","updated":"2025-02-10T13:54:27Z","published":"2024-09-10T07:34:19Z","title":"Automated Data Augmentation for Few-Shot Time Series Forecasting: A\n  Reinforcement Learning Approach Guided by a Model Zoo","summary":"  Time series forecasting, particularly in few-shot learning scenarios, is\nchallenging due to the limited availability of high-quality training data. To\naddress this, we present a pilot study on using reinforcement learning (RL) for\ntime series data augmentation. Our method, ReAugment, tackles three critical\nquestions: which parts of the training set should be augmented, how the\naugmentation should be performed, and what advantages RL brings to the process.\nSpecifically, our approach maintains a forecasting model zoo, and by measuring\nprediction diversity across the models, we identify samples with higher\nprobabilities for overfitting and use them as the anchor points for\naugmentation. Leveraging RL, our method adaptively transforms the overfit-prone\nsamples into new data that not only enhances training set diversity but also\ndirects the augmented data to target regions where the forecasting models are\nprone to overfitting. We validate the effectiveness of ReAugment across a wide\nrange of base models, showing its advantages in both standard time series\nforecasting and few-shot learning tasks.\n","authors":["Haochen Yuan","Yutong Wang","Yihong Chen","Yunbo Wang"],"pdf_url":"https://arxiv.org/pdf/2409.06282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04692v2","updated":"2025-02-10T13:52:51Z","published":"2025-02-07T06:37:05Z","title":"STRIDE: Automating Reward Design, Deep Reinforcement Learning Training\n  and Feedback Optimization in Humanoid Robotics Locomotion","summary":"  Humanoid robotics presents significant challenges in artificial intelligence,\nrequiring precise coordination and control of high-degree-of-freedom systems.\nDesigning effective reward functions for deep reinforcement learning (DRL) in\nthis domain remains a critical bottleneck, demanding extensive manual effort,\ndomain expertise, and iterative refinement. To overcome these challenges, we\nintroduce STRIDE, a novel framework built on agentic engineering to automate\nreward design, DRL training, and feedback optimization for humanoid robot\nlocomotion tasks. By combining the structured principles of agentic engineering\nwith large language models (LLMs) for code-writing, zero-shot generation, and\nin-context optimization, STRIDE generates, evaluates, and iteratively refines\nreward functions without relying on task-specific prompts or templates. Across\ndiverse environments featuring humanoid robot morphologies, STRIDE outperforms\nthe state-of-the-art reward design framework EUREKA, achieving significant\nimprovements in efficiency and task performance. Using STRIDE-generated\nrewards, simulated humanoid robots achieve sprint-level locomotion across\ncomplex terrains, highlighting its ability to advance DRL workflows and\nhumanoid robotics research.\n","authors":["Zhenwei Wu","Jinxiong Lu","Yuxiao Chen","Yunxin Liu","Yueting Zhuang","Luhui Hu"],"pdf_url":"https://arxiv.org/pdf/2502.04692v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07632v2","updated":"2025-02-10T13:48:49Z","published":"2024-10-10T05:54:01Z","title":"Provable Privacy Attacks on Trained Shallow Neural Networks","summary":"  We study what provable privacy attacks can be shown on trained, 2-layer ReLU\nneural networks. We explore two types of attacks; data reconstruction attacks,\nand membership inference attacks. We prove that theoretical results on the\nimplicit bias of 2-layer neural networks can be used to provably reconstruct a\nset of which at least a constant fraction are training points in a univariate\nsetting, and can also be used to identify with high probability whether a given\npoint was used in the training set in a high dimensional setting. To the best\nof our knowledge, our work is the first to show provable vulnerabilities in\nthis implicit-bias-driven setting.\n","authors":["Guy Smorodinsky","Gal Vardi","Itay Safran"],"pdf_url":"https://arxiv.org/pdf/2410.07632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03966v2","updated":"2025-02-10T13:41:39Z","published":"2025-02-06T10:59:44Z","title":"MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation","summary":"  In this paper, we present synthetic data generation framework for flood\nhazard detection system. For high fidelity and quality, we characterize several\nreal-world properties into virtual world and simulate the flood situation by\ncontrolling them. For the sake of efficiency, recent generative models in\nimage-to-3D and urban city synthesis are leveraged to easily composite flood\nenvironments so that we avoid data bias due to the hand-crafted manner. Based\non our framework, we build the flood synthetic dataset with 5 levels, dubbed\nMultiFloodSynth which contains rich annotation types like normal map,\nsegmentation, 3D bounding box for a variety of downstream task. In experiments,\nour dataset demonstrate the enhanced performance of flood hazard detection with\non-par realism compared with real dataset.\n","authors":["YoonJe Kang","Yonghoon Jung","Wonseop Shin","Bumsoo Kim","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2502.03966v2.pdf","comment":"6 pages, 6 figures. Accepted as Oral Presentation to AAAI 2025\n  Workshop on Good-Data"},{"id":"http://arxiv.org/abs/2306.10947v4","updated":"2025-02-10T13:40:27Z","published":"2023-06-19T14:07:10Z","title":"PAC-Chernoff Bounds: Understanding Generalization in the Interpolation\n  Regime","summary":"  This paper introduces a distribution-dependent PAC-Chernoff bound that\nexhibits perfect tightness for interpolators, even within over-parameterized\nmodel classes. This bound, which relies on basic principles of Large Deviation\nTheory, defines a natural measure of the smoothness of a model, characterized\nby simple real-valued functions. Building upon this bound and the new concept\nof smoothness, we present an unified theoretical framework revealing why\ncertain interpolators show an exceptional generalization, while others falter.\nWe theoretically show how a wide spectrum of modern learning methodologies,\nencompassing techniques such as $\\ell_2$-norm, distance-from-initialization and\ninput-gradient regularization, in combination with data augmentation, invariant\narchitectures, and over-parameterization, collectively guide the optimizer\ntoward smoother interpolators, which, according to our theoretical framework,\nare the ones exhibiting superior generalization performance. This study shows\nthat distribution-dependent bounds serve as a powerful tool to understand the\ncomplex dynamics behind the generalization capabilities of over-parameterized\ninterpolators.\n","authors":["Andrés R. Masegosa","Luis A. Ortega"],"pdf_url":"https://arxiv.org/pdf/2306.10947v4.pdf","comment":"60 pages, 12 figures, published at JAIR 2025"},{"id":"http://arxiv.org/abs/2501.01895v2","updated":"2025-02-10T13:36:02Z","published":"2025-01-03T17:00:33Z","title":"EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation","summary":"  We introduce EnerVerse, a generative robotics foundation model that\nconstructs and interprets embodied spaces. EnerVerse employs an autoregressive\nvideo diffusion framework to predict future embodied spaces from instructions,\nenhanced by a sparse context memory for long-term reasoning. To model the 3D\nrobotics world, we propose Free Anchor Views (FAVs), a multi-view video\nrepresentation offering flexible, task-adaptive perspectives to address\nchallenges like motion ambiguity and environmental constraints. Additionally,\nwe present EnerVerse-D, a data engine pipeline combining the generative model\nwith 4D Gaussian Splatting, forming a self-reinforcing data loop to reduce the\nsim-to-real gap. Leveraging these innovations, EnerVerse translates 4D world\nrepresentations into physical actions via a policy head (EnerVerse-A), enabling\nrobots to execute task instructions. EnerVerse-A achieves state-of-the-art\nperformance in both simulation and real-world settings.\n","authors":["Siyuan Huang","Liliang Chen","Pengfei Zhou","Shengcong Chen","Zhengkai Jiang","Yue Hu","Yue Liao","Peng Gao","Hongsheng Li","Maoqing Yao","Guanghui Ren"],"pdf_url":"https://arxiv.org/pdf/2501.01895v2.pdf","comment":"Website: https://sites.google.com/view/enerverse"},{"id":"http://arxiv.org/abs/2502.06453v1","updated":"2025-02-10T13:31:46Z","published":"2025-02-10T13:31:46Z","title":"MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard\n  Perturbations","summary":"  Large language models have demonstrated impressive performance on challenging\nmathematical reasoning tasks, which has triggered the discussion of whether the\nperformance is achieved by true reasoning capability or memorization. To\ninvestigate this question, prior work has constructed mathematical benchmarks\nwhen questions undergo simple perturbations -- modifications that still\npreserve the underlying reasoning patterns of the solutions. However, no work\nhas explored hard perturbations, which fundamentally change the nature of the\nproblem so that the original solution steps do not apply. To bridge the gap, we\nconstruct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard\nperturbation, respectively. Each consists of 279 perturbed math problems\nderived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.\nal., 2021). We observe significant performance drops on MATH-P-Hard across\nvarious models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking\n(-12.9%). We also raise concerns about a novel form of memorization where\nmodels blindly apply learned problem-solving skills without assessing their\napplicability to modified contexts. This issue is amplified when using original\nproblems for in-context learning. We call for research efforts to address this\nchallenge, which is critical for developing more robust and reliable reasoning\nmodels.\n","authors":["Kaixuan Huang","Jiacheng Guo","Zihao Li","Xiang Ji","Jiawei Ge","Wenzhe Li","Yingqing Guo","Tianle Cai","Hui Yuan","Runzhe Wang","Yue Wu","Ming Yin","Shange Tang","Yangsibo Huang","Chi Jin","Xinyun Chen","Chiyuan Zhang","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06443v1","updated":"2025-02-10T13:19:30Z","published":"2025-02-10T13:19:30Z","title":"Low-dimensional Functions are Efficiently Learnable under Randomly\n  Biased Distributions","summary":"  The problem of learning single index and multi index models has gained\nsignificant interest as a fundamental task in high-dimensional statistics. Many\nrecent works have analysed gradient-based methods, particularly in the setting\nof isotropic data distributions, often in the context of neural network\ntraining. Such studies have uncovered precise characterisations of algorithmic\nsample complexity in terms of certain analytic properties of the target\nfunction, such as the leap, information, and generative exponents. These\nproperties establish a quantitative separation between low and high complexity\nlearning tasks. In this work, we show that high complexity cases are rare.\nSpecifically, we prove that introducing a small random perturbation to the data\ndistribution--via a random shift in the first moment--renders any Gaussian\nsingle index model as easy to learn as a linear function. We further extend\nthis result to a class of multi index models, namely sparse Boolean functions,\nalso known as Juntas.\n","authors":["Elisabetta Cornacchia","Dan Mikulincer","Elchanan Mossel"],"pdf_url":"https://arxiv.org/pdf/2502.06443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06439v1","updated":"2025-02-10T13:16:01Z","published":"2025-02-10T13:16:01Z","title":"Testing software for non-discrimination: an updated and extended audit\n  in the Italian car insurance domain","summary":"  Context. As software systems become more integrated into society's\ninfrastructure, the responsibility of software professionals to ensure\ncompliance with various non-functional requirements increases. These\nrequirements include security, safety, privacy, and, increasingly,\nnon-discrimination.\n  Motivation. Fairness in pricing algorithms grants equitable access to basic\nservices without discriminating on the basis of protected attributes.\n  Method. We replicate a previous empirical study that used black box testing\nto audit pricing algorithms used by Italian car insurance companies, accessible\nthrough a popular online system. With respect to the previous study, we\nenlarged the number of tests and the number of demographic variables under\nanalysis.\n  Results. Our work confirms and extends previous findings, highlighting the\nproblematic permanence of discrimination across time: demographic variables\nsignificantly impact pricing to this day, with birthplace remaining the main\ndiscriminatory factor against individuals not born in Italian cities. We also\nfound that driver profiles can determine the number of quotes available to the\nuser, denying equal opportunities to all.\n  Conclusion. The study underscores the importance of testing for\nnon-discrimination in software systems that affect people's everyday lives.\nPerforming algorithmic audits over time makes it possible to evaluate the\nevolution of such algorithms. It also demonstrates the role that empirical\nsoftware engineering can play in making software systems more accountable.\n","authors":["Marco Rondina","Antonio Vetrò","Riccardo Coppola","Oumaima Regragrui","Alessandro Fabris","Gianmaria Silvello","Gian Antonio Susto","Juan Carlos De Martin"],"pdf_url":"https://arxiv.org/pdf/2502.06439v1.pdf","comment":"14 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.06438v1","updated":"2025-02-10T13:15:52Z","published":"2025-02-10T13:15:52Z","title":"FEMBA: Efficient and Scalable EEG Analysis with a Bidirectional Mamba\n  Foundation Model","summary":"  Accurate and efficient electroencephalography (EEG) analysis is essential for\ndetecting seizures and artifacts in long-term monitoring, with applications\nspanning hospital diagnostics to wearable health devices. Robust EEG analytics\nhave the potential to greatly improve patient care. However, traditional deep\nlearning models, especially Transformer-based architectures, are hindered by\ntheir quadratic time and memory complexity, making them less suitable for\nresource-constrained environments. To address these challenges, we present\nFEMBA (Foundational EEG Mamba + Bidirectional Architecture), a novel\nself-supervised framework that establishes new efficiency benchmarks for EEG\nanalysis through bidirectional state-space modeling. Unlike Transformer-based\nmodels, which incur quadratic time and memory complexity, FEMBA scales linearly\nwith sequence length, enabling more scalable and efficient processing of\nextended EEG recordings. Trained on over 21,000 hours of unlabeled EEG and\nfine-tuned on three downstream tasks, FEMBA achieves competitive performance in\ncomparison with transformer models, with significantly lower computational\ncost. Specifically, it reaches 81.82% balanced accuracy (0.8921 AUROC) on TUAB\nand 0.949 AUROC on TUAR, while a tiny 7.8M-parameter variant demonstrates\nviability for resource-constrained devices. These results pave the way for\nscalable, general-purpose EEG analytics in both clinical and highlight FEMBA as\na promising candidate for wearable applications.\n","authors":["Anna Tegon","Thorir Mar Ingolfsson","Xiaying Wang","Luca Benini","Yawei Li"],"pdf_url":"https://arxiv.org/pdf/2502.06438v1.pdf","comment":"7 pages, 3 figures, 5 tables, pre-print"},{"id":"http://arxiv.org/abs/2305.16272v4","updated":"2025-02-10T13:12:19Z","published":"2023-05-25T17:28:41Z","title":"Incentivizing Honesty among Competitors in Collaborative Learning and\n  Optimization","summary":"  Collaborative learning techniques have the potential to enable training\nmachine learning models that are superior to models trained on a single\nentity's data. However, in many cases, potential participants in such\ncollaborative schemes are competitors on a downstream task, such as firms that\neach aim to attract customers by providing the best recommendations. This can\nincentivize dishonest updates that damage other participants' models,\npotentially undermining the benefits of collaboration. In this work, we\nformulate a game that models such interactions and study two learning tasks\nwithin this framework: single-round mean estimation and multi-round SGD on\nstrongly-convex objectives. For a natural class of player actions, we show that\nrational clients are incentivized to strongly manipulate their updates,\npreventing learning. We then propose mechanisms that incentivize honest\ncommunication and ensure learning quality comparable to full cooperation.\nLastly, we empirically demonstrate the effectiveness of our incentive scheme on\na standard non-convex federated learning benchmark. Our work shows that\nexplicitly modeling the incentives and actions of dishonest clients, rather\nthan assuming them malicious, can enable strong robustness guarantees for\ncollaborative learning.\n","authors":["Florian E. Dorner","Nikola Konstantinov","Georgi Pashaliev","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2305.16272v4.pdf","comment":"Updated experimental results after fixing a mistake in the code.\n  Previous version published in NeurIPS 2023; 37 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.06434v1","updated":"2025-02-10T13:11:40Z","published":"2025-02-10T13:11:40Z","title":"Rethinking Large-scale Dataset Compression: Shifting Focus From Labels\n  to Images","summary":"  Dataset distillation and dataset pruning are two prominent techniques for\ncompressing datasets to improve computational and storage efficiency. Despite\ntheir overlapping objectives, these approaches are rarely compared directly.\nEven within each field, the evaluation protocols are inconsistent across\nvarious methods, which complicates fair comparisons and hinders\nreproducibility. Considering these limitations, we introduce in this paper a\nbenchmark that equitably evaluates methodologies across both distillation and\npruning literatures. Notably, our benchmark reveals that in the mainstream\ndataset distillation setting for large-scale datasets, which heavily rely on\nsoft labels from pre-trained models, even randomly selected subsets can achieve\nsurprisingly competitive performance. This finding suggests that an\noveremphasis on soft labels may be diverting attention from the intrinsic value\nof the image data, while also imposing additional burdens in terms of\ngeneration, storage, and application. To address these issues, we propose a new\nframework for dataset compression, termed Prune, Combine, and Augment (PCA),\nwhich focuses on leveraging image data exclusively, relies solely on hard\nlabels for evaluation, and achieves state-of-the-art performance in this setup.\nBy shifting the emphasis back to the images, our benchmark and PCA framework\npave the way for more balanced and accessible techniques in dataset compression\nresearch. Our code is available at:\nhttps://github.com/ArmandXiao/Rethinking-Dataset-Compression\n","authors":["Lingao Xiao","Songhua Liu","Yang He","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06434v1.pdf","comment":"Work In Progress"},{"id":"http://arxiv.org/abs/2502.06424v1","updated":"2025-02-10T13:00:49Z","published":"2025-02-10T13:00:49Z","title":"CS-SHAP: Extending SHAP to Cyclic-Spectral Domain for Better\n  Interpretability of Intelligent Fault Diagnosis","summary":"  Neural networks (NNs), with their powerful nonlinear mapping and end-to-end\ncapabilities, are widely applied in mechanical intelligent fault diagnosis\n(IFD). However, as typical black-box models, they pose challenges in\nunderstanding their decision basis and logic, limiting their deployment in\nhigh-reliability scenarios. Hence, various methods have been proposed to\nenhance the interpretability of IFD. Among these, post-hoc approaches can\nprovide explanations without changing model architecture, preserving its\nflexibility and scalability. However, existing post-hoc methods often suffer\nfrom limitations in explanation forms. They either require preprocessing that\ndisrupts the end-to-end nature or overlook fault mechanisms, leading to\nsuboptimal explanations. To address these issues, we derived the\ncyclic-spectral (CS) transform and proposed the CS-SHAP by extending Shapley\nadditive explanations (SHAP) to the CS domain. CS-SHAP can evaluate\ncontributions from both carrier and modulation frequencies, aligning more\nclosely with fault mechanisms and delivering clearer and more accurate\nexplanations. Three datasets are utilized to validate the superior\ninterpretability of CS-SHAP, ensuring its correctness, reproducibility, and\npractical performance. With open-source code and outstanding interpretability,\nCS-SHAP has the potential to be widely adopted and become the post-hoc\ninterpretability benchmark in IFD, even in other classification tasks. The code\nis available on https://github.com/ChenQian0618/CS-SHAP.\n","authors":["Qian Chen","Xingjian Dong","Kui Hu","Kangkang Chen","Zhike Peng","Guang Meng"],"pdf_url":"https://arxiv.org/pdf/2502.06424v1.pdf","comment":"21 pages, 21 figures"},{"id":"http://arxiv.org/abs/2312.08008v3","updated":"2025-02-10T12:55:57Z","published":"2023-12-13T09:31:30Z","title":"Learning in Zero-Sum Markov Games: Relaxing Strong Reachability and\n  Mixing Time Assumptions","summary":"  We address payoff-based decentralized learning in infinite-horizon zero-sum\nMarkov games. In this setting, each player makes decisions based solely on\nreceived rewards, without observing the opponent's strategy or actions nor\nsharing information. Prior works established finite-time convergence to an\napproximate Nash equilibrium under strong reachability and mixing time\nassumptions. We propose a convergent algorithm that significantly relaxes these\nassumptions, requiring only the existence of a single policy (not necessarily\nknown) with bounded reachability and mixing time. Our key technical novelty is\nintroducing Tsallis entropy regularization to smooth the best-response policy\nupdates. By suitably tuning this regularization, we ensure sufficient\nexploration, thus bypassing previous stringent assumptions on the MDP. By\nestablishing novel properties of the value and policy updates induced by the\nTsallis entropy regularizer, we prove finite-time convergence to an approximate\nNash equilibrium.\n","authors":["Reda Ouhamma","Maryam Kamgarpour"],"pdf_url":"https://arxiv.org/pdf/2312.08008v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06415v1","updated":"2025-02-10T12:54:17Z","published":"2025-02-10T12:54:17Z","title":"Systematic Outliers in Large Language Models","summary":"  Outliers have been widely observed in Large Language Models (LLMs),\nsignificantly impacting model performance and posing challenges for model\ncompression. Understanding the functionality and formation mechanisms of these\noutliers is critically important. Existing works, however, largely focus on\nreducing the impact of outliers from an algorithmic perspective, lacking an\nin-depth investigation into their causes and roles. In this work, we provide a\ndetailed analysis of the formation process, underlying causes, and functions of\noutliers in LLMs. We define and categorize three types of outliers-activation\noutliers, weight outliers, and attention outliers-and analyze their\ndistributions across different dimensions, uncovering inherent connections\nbetween their occurrences and their ultimate influence on the attention\nmechanism. Based on these observations, we hypothesize and explore the\nmechanisms by which these outliers arise and function, demonstrating through\ntheoretical derivations and experiments that they emerge due to the\nself-attention mechanism's softmax operation. These outliers act as implicit\ncontext-aware scaling factors within the attention mechanism. As these outliers\nstem from systematic influences, we term them systematic outliers. Our study\nnot only enhances the understanding of Transformer-based LLMs but also shows\nthat structurally eliminating outliers can accelerate convergence and improve\nmodel compression. The code is avilable at\nhttps://github.com/an-yongqi/systematic-outliers.\n","authors":["Yongqi An","Xu Zhao","Tao Yu","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06415v1.pdf","comment":"Accepted at ICLR 2025. Project Page:\n  https://github.com/an-yongqi/systematic-outliers"},{"id":"http://arxiv.org/abs/2405.17151v4","updated":"2025-02-10T12:54:11Z","published":"2024-05-27T13:26:34Z","title":"Smoke and Mirrors in Causal Downstream Tasks","summary":"  Machine Learning and AI have the potential to transform data-driven\nscientific discovery, enabling accurate predictions for several scientific\nphenomena. As many scientific questions are inherently causal, this paper looks\nat the causal inference task of treatment effect estimation, where the outcome\nof interest is recorded in high-dimensional observations in a Randomized\nControlled Trial (RCT). Despite being the simplest possible causal setting and\na perfect fit for deep learning, we theoretically find that many common choices\nin the literature may lead to biased estimates. To test the practical impact of\nthese considerations, we recorded ISTAnt, the first real-world benchmark for\ncausal inference downstream tasks on high-dimensional observations as an RCT\nstudying how garden ants (Lasius neglectus) respond to microparticles applied\nonto their colony members by hygienic grooming. Comparing 6 480 models\nfine-tuned from state-of-the-art visual backbones, we find that the sampling\nand modeling choices significantly affect the accuracy of the causal estimate,\nand that classification accuracy is not a proxy thereof. We further validated\nthe analysis, repeating it on a synthetically generated visual data set\ncontrolling the causal model. Our results suggest that future benchmarks should\ncarefully consider real downstream scientific questions, especially causal\nones. Further, we highlight guidelines for representation learning methods to\nhelp answer causal questions in the sciences.\n","authors":["Riccardo Cadei","Lukas Lindorfer","Sylvia Cremer","Cordelia Schmid","Francesco Locatello"],"pdf_url":"https://arxiv.org/pdf/2405.17151v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06407v1","updated":"2025-02-10T12:47:36Z","published":"2025-02-10T12:47:36Z","title":"An Automated Machine Learning Framework for Surgical Suturing Action\n  Detection under Class Imbalance","summary":"  In laparoscopy surgical training and evaluation, real-time detection of\nsurgical actions with interpretable outputs is crucial for automated and\nreal-time instructional feedback and skill development. Such capability would\nenable development of machine guided training systems. This paper presents a\nrapid deployment approach utilizing automated machine learning methods, based\non surgical action data collected from both experienced and trainee surgeons.\nThe proposed approach effectively tackles the challenge of highly imbalanced\nclass distributions, ensuring robust predictions across varying skill levels of\nsurgeons. Additionally, our method partially incorporates model transparency,\naddressing the reliability requirements in medical applications. Compared to\ndeep learning approaches, traditional machine learning models not only\nfacilitate efficient rapid deployment but also offer significant advantages in\ninterpretability. Through experiments, this study demonstrates the potential of\nthis approach to provide quick, reliable and effective real-time detection in\nsurgical training environments\n","authors":["Baobing Zhang","Paul Sullivan","Benjie Tang","Ghulam Nabi","Mustafa Suphi Erden"],"pdf_url":"https://arxiv.org/pdf/2502.06407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06403v1","updated":"2025-02-10T12:44:49Z","published":"2025-02-10T12:44:49Z","title":"The AI off-switch problem as a signalling game: bounded rationality and\n  incomparability","summary":"  The off-switch problem is a critical challenge in AI control: if an AI system\nresists being switched off, it poses a significant risk. In this paper, we\nmodel the off-switch problem as a signalling game, where a human decision-maker\ncommunicates its preferences about some underlying decision problem to an AI\nagent, which then selects actions to maximise the human's utility. We assume\nthat the human is a bounded rational agent and explore various bounded\nrationality mechanisms. Using real machine learning models, we reprove prior\nresults and demonstrate that a necessary condition for an AI system to refrain\nfrom disabling its off-switch is its uncertainty about the human's utility. We\nalso analyse how message costs influence optimal strategies and extend the\nanalysis to scenarios involving incomparability.\n","authors":["Alessio benavoli","Alessandro facchini","Marco Zaffalon"],"pdf_url":"https://arxiv.org/pdf/2502.06403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06401v1","updated":"2025-02-10T12:40:32Z","published":"2025-02-10T12:40:32Z","title":"Habitizing Diffusion Planning for Efficient and Effective Decision\n  Making","summary":"  Diffusion models have shown great promise in decision-making, also known as\ndiffusion planning. However, the slow inference speeds limit their potential\nfor broader real-world applications. Here, we introduce Habi, a general\nframework that transforms powerful but slow diffusion planning models into fast\ndecision-making models, which mimics the cognitive process in the brain that\ncostly goal-directed behavior gradually transitions to efficient habitual\nbehavior with repetitive practice. Even using a laptop CPU, the habitized model\ncan achieve an average 800+ Hz decision-making frequency (faster than previous\ndiffusion planners by orders of magnitude) on standard offline reinforcement\nlearning benchmarks D4RL, while maintaining comparable or even higher\nperformance compared to its corresponding diffusion planner. Our work proposes\na fresh perspective of leveraging powerful diffusion models for real-world\ndecision-making tasks. We also provide robust evaluations and analysis,\noffering insights from both biological and engineering perspectives for\nefficient and effective decision-making.\n","authors":["Haofei Lu","Yifei Shen","Dongsheng Li","Junliang Xing","Dongqi Han"],"pdf_url":"https://arxiv.org/pdf/2502.06401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06398v1","updated":"2025-02-10T12:36:57Z","published":"2025-02-10T12:36:57Z","title":"Learning Counterfactual Outcomes Under Rank Preservation","summary":"  Counterfactual inference aims to estimate the counterfactual outcome at the\nindividual level given knowledge of an observed treatment and the factual\noutcome, with broad applications in fields such as epidemiology, econometrics,\nand management science. Previous methods rely on a known structural causal\nmodel (SCM) or assume the homogeneity of the exogenous variable and strict\nmonotonicity between the outcome and exogenous variable. In this paper, we\npropose a principled approach for identifying and estimating the counterfactual\noutcome. We first introduce a simple and intuitive rank preservation assumption\nto identify the counterfactual outcome without relying on a known structural\ncausal model. Building on this, we propose a novel ideal loss for theoretically\nunbiased learning of the counterfactual outcome and further develop a\nkernel-based estimator for its empirical estimation. Our theoretical analysis\nshows that the rank preservation assumption is not stronger than the\nhomogeneity and strict monotonicity assumptions, and shows that the proposed\nideal loss is convex, and the proposed estimator is unbiased. Extensive\nsemi-synthetic and real-world experiments are conducted to demonstrate the\neffectiveness of the proposed method.\n","authors":["Peng Wu","Haoxuan Li","Chunyuan Zheng","Yan Zeng","Jiawei Chen","Yang Liu","Ruocheng Guo","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06387v1","updated":"2025-02-10T12:15:27Z","published":"2025-02-10T12:15:27Z","title":"How Humans Help LLMs: Assessing and Incentivizing Human Preference\n  Annotators","summary":"  Human-annotated preference data play an important role in aligning large\nlanguage models (LLMs). In this paper, we investigate the questions of\nassessing the performance of human annotators and incentivizing them to provide\nhigh-quality annotations. The quality assessment of language/text annotation\nfaces two challenges: (i) the intrinsic heterogeneity among annotators, which\nprevents the classic methods that assume the underlying existence of a true\nlabel; and (ii) the unclear relationship between the annotation quality and the\nperformance of downstream tasks, which excludes the possibility of inferring\nthe annotators' behavior based on the model performance trained from the\nannotation data. Then we formulate a principal-agent model to characterize the\nbehaviors of and the interactions between the company and the human annotators.\nThe model rationalizes a practical mechanism of a bonus scheme to incentivize\nannotators which benefits both parties and it underscores the importance of the\njoint presence of an assessment system and a proper contract scheme. From a\ntechnical perspective, our analysis extends the existing literature on the\nprincipal-agent model by considering a continuous action space for the agent.\nWe show the gap between the first-best and the second-best solutions (under the\ncontinuous action space) is of $\\Theta(1/\\sqrt{n \\log n})$ for the binary\ncontracts and $\\Theta(1/n)$ for the linear contracts, where $n$ is the number\nof samples used for performance assessment; this contrasts with the known\nresult of $\\exp(-\\Theta(n))$ for the binary contracts when the action space is\ndiscrete. Throughout the paper, we use real preference annotation data to\naccompany our discussions.\n","authors":["Shang Liu","Hanzhao Wang","Zhongyao Ma","Xiaocheng Li"],"pdf_url":"https://arxiv.org/pdf/2502.06387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16975v2","updated":"2025-02-10T12:09:34Z","published":"2024-07-24T03:43:55Z","title":"On the Parameter Identifiability of Partially Observed Linear Causal\n  Models","summary":"  Linear causal models are important tools for modeling causal dependencies and\nyet in practice, only a subset of the variables can be observed. In this paper,\nwe examine the parameter identifiability of these models by investigating\nwhether the edge coefficients can be recovered given the causal structure and\npartially observed data. Our setting is more general than that of prior\nresearch - we allow all variables, including both observed and latent ones, to\nbe flexibly related, and we consider the coefficients of all edges, whereas\nmost existing works focus only on the edges between observed variables.\nTheoretically, we identify three types of indeterminacy for the parameters in\npartially observed linear causal models. We then provide graphical conditions\nthat are sufficient for all parameters to be identifiable and show that some of\nthem are provably necessary. Methodologically, we propose a novel\nlikelihood-based parameter estimation method that addresses the variance\nindeterminacy of latent variables in a specific way and can asymptotically\nrecover the underlying parameters up to trivial indeterminacy. Empirical\nstudies on both synthetic and real-world datasets validate our identifiability\ntheory and the effectiveness of the proposed method in the finite-sample\nregime. Code: https://github.com/dongxinshuai/scm-identify.\n","authors":["Xinshuai Dong","Ignavier Ng","Biwei Huang","Yuewen Sun","Songyao Jin","Roberto Legaspi","Peter Spirtes","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10929v7","updated":"2025-02-10T12:09:23Z","published":"2024-10-14T16:35:27Z","title":"ASTM :Autonomous Smart Traffic Management System Using Artificial\n  Intelligence CNN and LSTM","summary":"  In the modern world, the development of Artificial Intelligence (AI) has\ncontributed to improvements in various areas, including automation, computer\nvision, fraud detection, and more. AI can be leveraged to enhance the\nefficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce\ntraffic congestion rates. This paper presents an Autonomous Smart Traffic\nManagement (STM) system that uses AI to improve traffic flow rates. The system\nemploys the YOLO V5 Convolutional Neural Network to detect vehicles in traffic\nmanagement images. Additionally, it predicts the number of vehicles for the\nnext 12 hours using a Recurrent Neural Network with Long Short-Term Memory\n(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the\ntraffic cycle length based on these vehicle predictions, aided by AI. From the\nresults of the RNN-LSTM model for predicting vehicle numbers over the next 12\nhours, we observe that the model predicts traffic with a Mean Squared Error\n(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.\nAfter simulating the STM system in the CARLA simulation environment, we found\nthat the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per\nminute) is 50\\% higher than the rate without STM (around 15 vehicles per\nminute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5\nseconds per vehicle) is 70\\% lower than without STM (around 12 seconds per\nvehicle). These results demonstrate that the STM system using AI can increase\ntraffic flow by 50\\% and reduce vehicle pass delays by 70\\%.\n","authors":["Christofel Rio Goenawan"],"pdf_url":"https://arxiv.org/pdf/2410.10929v7.pdf","comment":"Novel Autonomous Smart Traffic Management System using End-to-End\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2411.05743v2","updated":"2025-02-10T12:04:29Z","published":"2024-11-08T18:04:41Z","title":"Free Record-Level Privacy Risk Evaluation Through Artifact-Based Methods","summary":"  Membership inference attacks (MIAs) are widely used to empirically assess\nprivacy risks in machine learning models, both providing model-level\nvulnerability metrics and identifying the most vulnerable training samples.\nState-of-the-art methods, however, require training hundreds of shadow models\nwith the same architecture as the target model. This makes the computational\ncost of assessing the privacy of models prohibitive for many practical\napplications, particularly when used iteratively as part of the model\ndevelopment process and for large models. We propose a novel approach for\nidentifying the training samples most vulnerable to membership inference\nattacks by analyzing artifacts naturally available during the training process.\nOur method, Loss Trace Interquantile Range (LT-IQR), analyzes per-sample loss\ntrajectories collected during model training to identify high-risk samples\nwithout requiring any additional model training. Through experiments on\nstandard benchmarks, we demonstrate that LT-IQR achieves 92% precision@k=1% in\nidentifying the samples most vulnerable to state-of-the-art MIAs. This result\nholds across datasets and model architectures with LT-IQR outperforming both\ntraditional vulnerability metrics, such as loss, and lightweight MIAs using few\nshadow models. We also show LT-IQR to accurately identify points vulnerable to\nmultiple MIA methods and perform ablation studies. We believe LT-IQR enables\nmodel developers to identify vulnerable training samples, for free, as part of\nthe model development process. Our results emphasize the potential of\nartifact-based methods to efficiently evaluate privacy risks.\n","authors":["Joseph Pollock","Igor Shilov","Euodia Dodd","Yves-Alexandre de Montjoye"],"pdf_url":"https://arxiv.org/pdf/2411.05743v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06380v1","updated":"2025-02-10T12:01:05Z","published":"2025-02-10T12:01:05Z","title":"Structure-preserving contrastive learning for spatial time series","summary":"  Informative representations enhance model performance and generalisability in\ndownstream tasks. However, learning self-supervised representations for\nspatially characterised time series, like traffic interactions, poses\nchallenges as it requires maintaining fine-grained similarity relations in the\nlatent space. In this study, we incorporate two structure-preserving\nregularisers for the contrastive learning of spatial time series: one\nregulariser preserves the topology of similarities between instances, and the\nother preserves the graph geometry of similarities across spatial and temporal\ndimensions. To balance contrastive learning and structure preservation, we\npropose a dynamic mechanism that adaptively weighs the trade-off and stabilises\ntraining. We conduct experiments on multivariate time series classification, as\nwell as macroscopic and microscopic traffic prediction. For all three tasks,\nour approach preserves the structures of similarity relations more effectively\nand improves state-of-the-art task performances. The proposed approach can be\napplied to an arbitrary encoder and is particularly beneficial for time series\nwith spatial or geographical features. Furthermore, this study suggests that\nhigher similarity structure preservation indicates more informative and useful\nrepresentations. This may help to understand the contribution of representation\nlearning in pattern recognition with neural networks. Our code is made openly\naccessible with all resulting data at https://github.com/yiru-jiao/spclt.\n","authors":["Yiru Jiao","Sander van Cranenburgh","Simeon Calvert","Hans van Lint"],"pdf_url":"https://arxiv.org/pdf/2502.06380v1.pdf","comment":"TL;DR: Preserving certain structures of similarity relations in\n  spatio-temporal data can improve downstream task performance via contrastive\n  learning"},{"id":"http://arxiv.org/abs/2412.01523v2","updated":"2025-02-10T12:00:50Z","published":"2024-12-02T14:16:03Z","title":"FlexSP: Accelerating Large Language Model Training via Flexible Sequence\n  Parallelism","summary":"  Extending the context length (i.e., the maximum supported sequence length) of\nLLMs is of paramount significance. To facilitate long context training of LLMs,\nsequence parallelism has emerged as an essential technique, which scatters each\ninput sequence across multiple devices and necessitates communication to\nprocess the sequence. In essence, existing sequence parallelism methods assume\nhomogeneous sequence lengths (i.e., all input sequences are equal in length)\nand therefore leverages a single, static scattering strategy for all input\nsequences. However, in reality, the sequence lengths in LLM training corpora\nexhibit substantial variability, often following a long-tail distribution,\nwhich leads to workload heterogeneity. In this paper, we show that employing a\nsingle, static strategy results in inefficiency and resource under-utilization,\nhighlighting the need for adaptive approaches to handle the heterogeneous\nworkloads across sequences. To address this, we propose a\nheterogeneity-adaptive sequence parallelism method. For each training step, our\napproach captures the variability in sequence lengths and assigns the optimal\ncombination of scattering strategies based on workload characteristics. We\nmodel this problem as a linear programming optimization and design an efficient\nand effective solver to find the optimal solution. Furthermore, we implement\nour method in a high-performance system that supports adaptive parallelization\nin distributed LLM training. Experimental results demonstrate that our system\noutperforms state-of-the-art training frameworks by up to 1.98x.\n","authors":["Yujie Wang","Shiju Wang","Shenhan Zhu","Fangcheng Fu","Xinyi Liu","Xuefeng Xiao","Huixia Li","Jiashi Li","Faming Wu","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2412.01523v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03365v2","updated":"2025-02-10T12:00:47Z","published":"2024-09-05T09:10:40Z","title":"Spindle: Efficient Distributed Training of Multi-Task Large Models via\n  Wavefront Scheduling","summary":"  Recent foundation models are capable of handling multiple tasks and multiple\ndata modalities with the unified base model structure and several specialized\nmodel components. However, efficient training of such multi-task (MT)\nmulti-modal (MM) models poses significant system challenges due to the\nsophisticated model architecture and the heterogeneous workloads of different\ntasks and modalities. In this paper, we propose Spindle, a brand new training\nsystem tailored for resource-efficient and high-performance training of MT MM\nmodels via wavefront scheduling. The key idea of Spindle is to decompose the\nmodel execution into waves and address the joint optimization problem\nsequentially, including both heterogeneity-aware workload parallelization and\ndependency-driven execution scheduling. We build our system and evaluate it on\nvarious MT MM models. Experiments demonstrate the superior performance and\nefficiency of Spindle, with speedup ratio up to 71% compared to\nstate-of-the-art training systems.\n","authors":["Yujie Wang","Shenhan Zhu","Fangcheng Fu","Xupeng Miao","Jie Zhang","Juan Zhu","Fan Hong","Yong Li","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2409.03365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06379v1","updated":"2025-02-10T11:59:02Z","published":"2025-02-10T11:59:02Z","title":"Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled\n  Diffusion Sequential Monte Carlo","summary":"  A recent line of research has exploited pre-trained generative diffusion\nmodels as priors for solving Bayesian inverse problems. We contribute to this\nresearch direction by designing a sequential Monte Carlo method for\nlinear-Gaussian inverse problems which builds on ``decoupled diffusion\", where\nthe generative process is designed such that larger updates to the sample are\npossible. The method is asymptotically exact and we demonstrate the\neffectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC)\nalgorithm on both synthetic data and image reconstruction tasks. Further, we\ndemonstrate how the approach can be extended to discrete data.\n","authors":["Filip Ekström Kelvinius","Zheng Zhao","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2502.06379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06376v1","updated":"2025-02-10T11:56:02Z","published":"2025-02-10T11:56:02Z","title":"Many-Task Federated Fine-Tuning via Unified Task Vectors","summary":"  Federated Learning (FL) traditionally assumes homogeneous client tasks;\nhowever, in real-world scenarios, clients often specialize in diverse tasks,\nintroducing task heterogeneity. To address this challenge, Many-Task FL\n(MaT-FL) has emerged, enabling clients to collaborate effectively despite task\ndiversity. Existing MaT-FL approaches rely on client grouping or personalized\nlayers, requiring the server to manage individual models and failing to account\nfor clients handling multiple tasks. We propose MaTU, a MaT-FL approach that\nenables joint learning of task vectors across clients, eliminating the need for\nclustering or client-specific weight storage at the server. Our method\nintroduces a novel aggregation mechanism that determines task similarity based\non the direction of clients task vectors and constructs a unified task vector\nencapsulating all tasks. To address task-specific requirements, we augment the\nunified task vector with lightweight modulators that facilitate knowledge\ntransfer among related tasks while disentangling dissimilar ones. Evaluated\nacross 30 datasets, MaTU achieves superior performance over state-of-the-art\nMaT-FL approaches, with results comparable to per-task fine-tuning, while\ndelivering significant communication savings.\n","authors":["Vasileios Tsouvalas","Tanir Ozcelebi","Nirvana Meratnia"],"pdf_url":"https://arxiv.org/pdf/2502.06376v1.pdf","comment":"10 pages, 6 figures, submitted in IJCAI 2025"},{"id":"http://arxiv.org/abs/2502.06374v1","updated":"2025-02-10T11:44:46Z","published":"2025-02-10T11:44:46Z","title":"Hyperparameters in Score-Based Membership Inference Attacks","summary":"  Membership Inference Attacks (MIAs) have emerged as a valuable framework for\nevaluating privacy leakage by machine learning models. Score-based MIAs are\ndistinguished, in particular, by their ability to exploit the confidence scores\nthat the model generates for particular inputs. Existing score-based MIAs\nimplicitly assume that the adversary has access to the target model's\nhyperparameters, which can be used to train the shadow models for the attack.\nIn this work, we demonstrate that the knowledge of target hyperparameters is\nnot a prerequisite for MIA in the transfer learning setting. Based on this, we\npropose a novel approach to select the hyperparameters for training the shadow\nmodels for MIA when the attacker has no prior knowledge about them by matching\nthe output distributions of target and shadow models. We demonstrate that using\nthe new approach yields hyperparameters that lead to an attack near\nindistinguishable in performance from an attack that uses target\nhyperparameters to train the shadow models. Furthermore, we study the empirical\nprivacy risk of unaccounted use of training data for hyperparameter\noptimization (HPO) in differentially private (DP) transfer learning. We find no\nstatistically significant evidence that performing HPO using training data\nwould increase vulnerability to MIA.\n","authors":["Gauri Pradhan","Joonas Jälkö","Marlon Tobaben","Antti Honkela"],"pdf_url":"https://arxiv.org/pdf/2502.06374v1.pdf","comment":"This work has been accepted for publication in the 3rd IEEE\n  Conference on Secure and Trustworthy Machine Learning (SaTML'25). The final\n  version will be available on IEEE Xplore"},{"id":"http://arxiv.org/abs/2502.06364v1","updated":"2025-02-10T11:30:35Z","published":"2025-02-10T11:30:35Z","title":"Automatic Identification of Samples in Hip-Hop Music via Multi-Loss\n  Training and an Artificial Dataset","summary":"  Sampling, the practice of reusing recorded music or sounds from another\nsource in a new work, is common in popular music genres like hip-hop and rap.\nNumerous services have emerged that allow users to identify connections between\nsamples and the songs that incorporate them, with the goal of enhancing music\ndiscovery. Designing a system that can perform the same task automatically is\nchallenging, as samples are commonly altered with audio effects like pitch- and\ntime-stretching and may only be seconds long. Progress on this task has been\nminimal and is further blocked by the limited availability of training data.\nHere, we show that a convolutional neural network trained on an artificial\ndataset can identify real-world samples in commercial hip-hop music. We extract\nvocal, harmonic, and percussive elements from several databases of\nnon-commercial music recordings using audio source separation, and train the\nmodel to fingerprint a subset of these elements in transformed versions of the\noriginal audio. We optimize the model using a joint classification and metric\nlearning loss and show that it achieves 13% greater precision on real-world\ninstances of sampling than a fingerprinting system using acoustic landmarks,\nand that it can recognize samples that have been both pitch shifted and time\nstretched. We also show that, for half of the commercial music recordings we\ntested, our model is capable of locating the position of a sample to within\nfive seconds.\n","authors":["Huw Cheston","Jan Van Balen","Simon Durand"],"pdf_url":"https://arxiv.org/pdf/2502.06364v1.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.06363v1","updated":"2025-02-10T11:29:27Z","published":"2025-02-10T11:29:27Z","title":"Improved Regret Analysis in Gaussian Process Bandits: Optimality for\n  Noiseless Reward, RKHS norm, and Non-Stationary Variance","summary":"  We study the Gaussian process (GP) bandit problem, whose goal is to minimize\nregret under an unknown reward function lying in some reproducing kernel\nHilbert space (RKHS). The maximum posterior variance analysis is vital in\nanalyzing near-optimal GP bandit algorithms such as maximum variance reduction\n(MVR) and phased elimination (PE). Therefore, we first show the new upper bound\nof the maximum posterior variance, which improves the dependence of the noise\nvariance parameters of the GP. By leveraging this result, we refine the MVR and\nPE to obtain (i) a nearly optimal regret upper bound in the noiseless setting\nand (ii) regret upper bounds that are optimal with respect to the RKHS norm of\nthe reward function. Furthermore, as another application of our proposed bound,\nwe analyze the GP bandit under the time-varying noise variance setting, which\nis the kernelized extension of the linear bandit with heteroscedastic noise.\nFor this problem, we show that MVR and PE-based algorithms achieve noise\nvariance-dependent regret upper bounds, which matches our regret lower bound.\n","authors":["Shogo Iwazaki","Shion Takeno"],"pdf_url":"https://arxiv.org/pdf/2502.06363v1.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2411.10438v2","updated":"2025-02-10T11:23:11Z","published":"2024-11-15T18:57:39Z","title":"MARS: Unleashing the Power of Variance Reduction for Training Large\n  Models","summary":"  Training deep neural networks--and more recently, large models demands\nefficient and scalable optimizers. Adaptive gradient algorithms like Adam,\nAdamW, and their variants have been central to this task. Despite the\ndevelopment of numerous variance reduction algorithms in the past decade aimed\nat accelerating stochastic optimization in both convex and nonconvex settings,\nvariance reduction has not found widespread success in training deep neural\nnetworks or large language models. Consequently, it has remained a less favored\napproach in modern AI. In this paper, to unleash the power of variance\nreduction for efficient training of large models, we propose a unified\noptimization framework, MARS (Make vAriance Reduction Shine), which reconciles\npreconditioned gradient methods with variance reduction via a scaled stochastic\nrecursive momentum technique. Within our framework, we introduce three\ninstances of MARS that leverage preconditioned gradient updates based on AdamW,\nLion, and Shampoo, respectively. We also draw a connection between our\nalgorithms and existing optimizers. Experimental results on training GPT-2\nmodels indicate that MARS consistently outperforms AdamW by a large margin. The\nimplementation of MARS is available at https://github.com/AGI-Arena/MARS.\n","authors":["Huizhuo Yuan","Yifeng Liu","Shuang Wu","Xun Zhou","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2411.10438v2.pdf","comment":"47 pages, 18 figures, 12 tables"},{"id":"http://arxiv.org/abs/2502.06358v1","updated":"2025-02-10T11:20:10Z","published":"2025-02-10T11:20:10Z","title":"Towards bandit-based prompt-tuning for in-the-wild foundation agents","summary":"  Prompting has emerged as the dominant paradigm for adapting large,\npre-trained transformer-based models to downstream tasks. The Prompting\nDecision Transformer (PDT) enables large-scale, multi-task offline\nreinforcement learning pre-training by leveraging stochastic trajectory prompts\nto identify the target task. However, these prompts are sampled uniformly from\nexpert demonstrations, overlooking a critical limitation: Not all prompts are\nequally informative for differentiating between tasks. To address this, we\npropose an inference time bandit-based prompt-tuning framework that explores\nand optimizes trajectory prompt selection to enhance task performance. Our\nexperiments indicate not only clear performance gains due to bandit-based\nprompt-tuning, but also better sample complexity, scalability, and prompt space\nexploration compared to prompt-tuning baselines.\n","authors":["Finn Rietz","Oleg Smirnov","Sara Karimi","Lele Cao"],"pdf_url":"https://arxiv.org/pdf/2502.06358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13318v3","updated":"2025-02-10T11:18:31Z","published":"2024-04-20T08:23:46Z","title":"Client-Centered Federated Learning for Heterogeneous EHRs: Use Fewer\n  Participants to Achieve the Same Performance","summary":"  The increasing volume of electronic health records (EHRs) presents the\nopportunity to improve the accuracy and robustness of models in clinical\nprediction tasks. Unlike traditional centralized approaches, federated learning\nenables training on data from multiple institutions while preserving patient\nprivacy and complying with regulatory constraints. However, most federated\nlearning research focuses on building a global model to serve multiple clients,\noverlooking the practical need for a client-specific model. In this work, we\nintroduce EHRFL, a federated learning framework using EHRs, designed to develop\na model tailored to a single client (i.e., healthcare institution). Our\nframework addresses two key challenges: (1) enabling federated learning across\nclients with heterogeneous EHR systems using text-based EHR modeling, and (2)\nreducing the cost of federated learning by selecting suitable participating\nclients using averaged patient embeddings. Our experiment results on multiple\nopen-source EHR datasets demonstrate the effectiveness of EHRFL in addressing\nthe two challenges, establishing it as a practical solution for building a\nclient-specific model in federated learning.\n","authors":["Jiyoun Kim","Junu Kim","Kyunghoon Hur","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2404.13318v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06355v1","updated":"2025-02-10T11:10:41Z","published":"2025-02-10T11:10:41Z","title":"Fine-tuning Multimodal Transformers on Edge: A Parallel Split Learning\n  Approach","summary":"  Multimodal transformers integrate diverse data types like images, audio, and\ntext, advancing tasks such as audio-visual understanding and image-text\nretrieval; yet their high parameterization limits deployment on\nresource-constrained edge devices. Split Learning (SL), which partitions models\nat a designated cut-layer to offload compute-intensive operations to the\nserver, offers a promising approach for distributed training of multimodal\ntransformers, though its application remains underexplored. We present MPSL, a\nparallel SL approach for computational efficient fine-tuning of multimodal\ntransformers in a distributed manner, while eliminating label sharing, client\nsynchronization, and per-client sub-model management. MPSL employs lightweight\nclient-side tokenizers and a unified modality-agnostic encoder, allowing\nflexible adaptation to task-specific needs. Our evaluation across 7 multimodal\ndatasets demonstrates that MPSL matches or outperforms Federated Learning,\nreduces client-side computations by 250x, and achieves superior scalability in\ncommunication cost with model growth. Through extensive analysis, we highlight\ntask suitability, trade-offs, and scenarios where MPSL excels, inspiring\nfurther exploration.\n","authors":["Timo Fudala","Vasileios Tsouvalas","Nirvana Meratnia"],"pdf_url":"https://arxiv.org/pdf/2502.06355v1.pdf","comment":"10 pages, 4 figures, submitted to IJCAI 2025"},{"id":"http://arxiv.org/abs/2407.11762v2","updated":"2025-02-10T11:06:43Z","published":"2024-07-16T14:22:22Z","title":"Self-Regulating Random Walks for Resilient Decentralized Learning on\n  Graphs","summary":"  Consider the setting of multiple random walks (RWs) on a graph executing a\ncertain computational task. For instance, in decentralized learning via RWs, a\nmodel is updated at each iteration based on the local data of the visited node\nand then passed to a randomly chosen neighbor. RWs can fail due to node or link\nfailures. The goal is to maintain a desired number of RWs to ensure failure\nresilience. Achieving this is challenging due to the lack of a central entity\nto track which RWs have failed to replace them with new ones by forking\n(duplicating) surviving ones. Without duplications, the number of RWs will\neventually go to zero, causing a catastrophic failure of the system. We propose\ntwo decentralized algorithms called DecAFork and DecAFork+ that can maintain\nthe number of RWs in the graph around a desired value even in the presence of\narbitrary RW failures. Nodes continuously estimate the number of surviving RWs\nby estimating their return time distribution and fork the RWs when failures are\nlikely to happen. DecAFork+ additionally allows terminations to avoid\noverloading the network by forking too many RWs. We present extensive numerical\nsimulations that show the performance of DecAFork and DecAFork+ regarding fast\ndetection and reaction to failures compared to a baseline, and establish\ntheoretical guarantees on the performance of both algorithms.\n","authors":["Maximilian Egger","Rawad Bitar","Ghadir Ayache","Antonia Wachter-Zeh","Salim El Rouayheb"],"pdf_url":"https://arxiv.org/pdf/2407.11762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17442v2","updated":"2025-02-10T11:06:23Z","published":"2024-04-26T14:28:18Z","title":"Uniform Generalization Bounds on Data-Dependent Hypothesis Sets via\n  PAC-Bayesian Theory on Random Sets","summary":"  We propose data-dependent uniform generalization bounds by approaching the\nproblem from a PAC-Bayesian perspective. We first apply the PAC-Bayesian\nframework on \"random sets\" in a rigorous way, where the training algorithm is\nassumed to output a data-dependent hypothesis set after observing the training\ndata. This approach allows us to prove data-dependent bounds, which can be\napplicable in numerous contexts. To highlight the power of our approach, we\nconsider two main applications. First, we propose a PAC-Bayesian formulation of\nthe recently developed fractal-dimension-based generalization bounds. The\nderived results are shown to be tighter and they unify the existing results\naround one simple proof technique. Second, we prove uniform bounds over the\ntrajectories of continuous Langevin dynamics and stochastic gradient Langevin\ndynamics. These results provide novel information about the generalization\nproperties of noisy algorithms.\n","authors":["Benjamin Dupuis","Paul Viallard","George Deligiannidis","Umut Simsekli"],"pdf_url":"https://arxiv.org/pdf/2404.17442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06351v1","updated":"2025-02-10T11:00:24Z","published":"2025-02-10T11:00:24Z","title":"Calibrating LLMs with Information-Theoretic Evidential Deep Learning","summary":"  Fine-tuned large language models (LLMs) often exhibit overconfidence,\nparticularly when trained on small datasets, resulting in poor calibration and\ninaccurate uncertainty estimates. Evidential Deep Learning (EDL), an\nuncertainty-aware approach, enables uncertainty estimation in a single forward\npass, making it a promising method for calibrating fine-tuned LLMs. However,\ndespite its computational efficiency, EDL is prone to overfitting, as its\ntraining objective can result in overly concentrated probability distributions.\nTo mitigate this, we propose regularizing EDL by incorporating an information\nbottleneck (IB). Our approach IB-EDL suppresses spurious information in the\nevidence generated by the model and encourages truly predictive information to\ninfluence both the predictions and uncertainty estimates. Extensive experiments\nacross various fine-tuned LLMs and tasks demonstrate that IB-EDL outperforms\nboth existing EDL and non-EDL approaches. By improving the trustworthiness of\nLLMs, IB-EDL facilitates their broader adoption in domains requiring high\nlevels of confidence calibration. Code is available at\nhttps://github.com/sandylaker/ib-edl.\n","authors":["Yawei Li","David Rügamer","Bernd Bischl","Mina Rezaei"],"pdf_url":"https://arxiv.org/pdf/2502.06351v1.pdf","comment":"18 pages; 3 figures; accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.06349v1","updated":"2025-02-10T10:58:57Z","published":"2025-02-10T10:58:57Z","title":"Provably Near-Optimal Federated Ensemble Distillation with Negligible\n  Overhead","summary":"  Federated ensemble distillation addresses client heterogeneity by generating\npseudo-labels for an unlabeled server dataset based on client predictions and\ntraining the server model using the pseudo-labeled dataset. The unlabeled\nserver dataset can either be pre-existing or generated through a data-free\napproach. The effectiveness of this approach critically depends on the method\nof assigning weights to client predictions when creating pseudo-labels,\nespecially in highly heterogeneous settings. Inspired by theoretical results\nfrom GANs, we propose a provably near-optimal weighting method that leverages\nclient discriminators trained with a server-distributed generator and local\ndatasets. Our experiments on various image classification tasks demonstrate\nthat the proposed method significantly outperforms baselines. Furthermore, we\nshow that the additional communication cost, client-side privacy leakage, and\nclient-side computational overhead introduced by our method are negligible,\nboth in scenarios with and without a pre-existing server dataset.\n","authors":["Won-Jun Jang","Hyeon-Seo Park","Si-Hyeon Lee"],"pdf_url":"https://arxiv.org/pdf/2502.06349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01488v2","updated":"2025-02-10T10:56:47Z","published":"2024-12-02T13:39:49Z","title":"TACO: Training-free Sound Prompted Segmentation via Semantically\n  Constrained Audio-visual CO-factorization","summary":"  Large-scale pre-trained audio and image models demonstrate an unprecedented\ndegree of generalization, making them suitable for a wide range of\napplications. Here, we tackle the specific task of sound-prompted segmentation,\naiming to segment image regions corresponding to objects heard in an audio\nsignal. Most existing approaches tackle this problem by fine-tuning pre-trained\nmodels or by training additional modules specifically for the task. We adopt a\ndifferent strategy: we introduce a training-free approach that leverages\nNon-negative Matrix Factorization (NMF) to co-factorize audio and visual\nfeatures from pre-trained models so as to reveal shared interpretable concepts.\nThese concepts are passed on to an open-vocabulary segmentation model for\nprecise segmentation maps. By using frozen pre-trained models, our method\nachieves high generalization and establishes state-of-the-art performance in\nunsupervised sound-prompted segmentation, significantly surpassing previous\nunsupervised methods.\n","authors":["Hugo Malard","Michel Olvera","Stephane Lathuiliere","Slim Essid"],"pdf_url":"https://arxiv.org/pdf/2412.01488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06343v1","updated":"2025-02-10T10:52:17Z","published":"2025-02-10T10:52:17Z","title":"Causal Lifting of Neural Representations: Zero-Shot Generalization for\n  Causal Inferences","summary":"  A plethora of real-world scientific investigations is waiting to scale with\nthe support of trustworthy predictive models that can reduce the need for\ncostly data annotations. We focus on causal inferences on a target experiment\nwith unlabeled factual outcomes, retrieved by a predictive model fine-tuned on\na labeled similar experiment. First, we show that factual outcome estimation\nvia Empirical Risk Minimization (ERM) may fail to yield valid causal inferences\non the target population, even in a randomized controlled experiment and\ninfinite training samples. Then, we propose to leverage the observed\nexperimental settings during training to empower generalization to downstream\ninterventional investigations, ``Causal Lifting'' the predictive model. We\npropose Deconfounded Empirical Risk Minimization (DERM), a new simple learning\nprocedure minimizing the risk over a fictitious target population, preventing\npotential confounding effects. We validate our method on both synthetic and\nreal-world scientific data. Notably, for the first time, we zero-shot\ngeneralize causal inferences on ISTAnt dataset (without annotation) by causal\nlifting a predictive model on our experiment variant.\n","authors":["Riccardo Cadei","Ilker Demirel","Piersilvio De Bartolomeis","Lukas Lindorfer","Sylvia Cremer","Cordelia Schmid","Francesco Locatello"],"pdf_url":"https://arxiv.org/pdf/2502.06343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01875v4","updated":"2025-02-10T10:48:40Z","published":"2024-03-04T09:31:56Z","title":"Locally Convex Global Loss Network for Decision-Focused Learning","summary":"  In decision-making problems under uncertainty, predicting unknown parameters\nis often considered independent of the optimization part. Decision-focused\nlearning (DFL) is a task-oriented framework that integrates prediction and\noptimization by adapting the predictive model to give better decisions for the\ncorresponding task. Here, an inevitable challenge arises when computing the\ngradients of the optimal decision with respect to the parameters. Existing\nresearch copes with this issue by smoothly reforming surrogate optimization or\nconstructing surrogate loss functions that mimic task loss. However, they are\napplied to restricted optimization domains. In this paper, we propose Locally\nConvex Global Loss Network (LCGLN), a global surrogate loss model that can be\nimplemented in a general DFL paradigm. LCGLN learns task loss via a partial\ninput convex neural network which is guaranteed to be convex for chosen inputs\nwhile keeping the non-convex global structure for the other inputs. This\nenables LCGLN to admit general DFL through only a single surrogate loss without\nany sense for choosing appropriate parametric forms. We confirm the\neffectiveness and flexibility of LCGLN by evaluating our proposed model with\nthree stochastic decision-making problems.\n","authors":["Haeun Jeon","Hyunglip Bae","Minsu Park","Chanyeong Kim","Woo Chang Kim"],"pdf_url":"https://arxiv.org/pdf/2403.01875v4.pdf","comment":"AAAI-25 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2502.04979v2","updated":"2025-02-10T10:48:31Z","published":"2025-02-07T14:57:17Z","title":"Enhancing Pre-Trained Decision Transformers with Prompt-Tuning Bandits","summary":"  Harnessing large offline datasets is vital for training foundation models\nthat can generalize across diverse tasks. Offline Reinforcement Learning (RL)\noffers a powerful framework for these scenarios, enabling the derivation of\noptimal policies even from suboptimal data. The Prompting Decision Transformer\n(PDT) is an offline RL multi-task model that distinguishes tasks through\nstochastic trajectory prompts, which are task-specific tokens maintained in\ncontext during rollouts. However, PDT samples these tokens uniformly at random\nfrom per-task demonstration datasets, failing to account for differences in\ntoken informativeness and potentially leading to performance degradation. To\naddress this limitation, we introduce a scalable bandit-based prompt-tuning\nmethod that dynamically learns to construct high-performance trajectory\nprompts. Our approach significantly enhances downstream task performance\nwithout modifying the pre-trained Transformer backbone. Empirical results on\nbenchmark tasks and a newly designed multi-task environment demonstrate the\neffectiveness of our method, creating a seamless bridge between general\nmulti-task offline pre-training and task-specific online adaptation.\n","authors":["Finn Rietz","Oleg Smirnov","Sara Karimi","Lele Cao"],"pdf_url":"https://arxiv.org/pdf/2502.04979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06341v1","updated":"2025-02-10T10:43:55Z","published":"2025-02-10T10:43:55Z","title":"Facial Analysis Systems and Down Syndrome","summary":"  The ethical, social and legal issues surrounding facial analysis technologies\nhave been widely debated in recent years. Key critics have argued that these\ntechnologies can perpetuate bias and discrimination, particularly against\nmarginalized groups. We contribute to this field of research by reporting on\nthe limitations of facial analysis systems with the faces of people with Down\nsyndrome: this particularly vulnerable group has received very little attention\nin the literature so far. This study involved the creation of a specific\ndataset of face images. An experimental group with faces of people with Down\nsyndrome, and a control group with faces of people who are not affected by the\nsyndrome. Two commercial tools were tested on the dataset, along three tasks:\ngender recognition, age prediction and face labelling. The results show an\noverall lower accuracy of prediction in the experimental group, and other\nspecific patterns of performance differences: i) high error rates in gender\nrecognition in the category of males with Down syndrome; ii) adults with Down\nsyndrome were more often incorrectly labelled as children; iii) social\nstereotypes are propagated in both the control and experimental groups, with\nlabels related to aesthetics more often associated with women, and labels\nrelated to education level and skills more often associated with men. These\nresults, although limited in scope, shed new light on the biases that alter\nface classification when applied to faces of people with Down syndrome. They\nconfirm the structural limitation of the technology, which is inherently\ndependent on the datasets used to train the models.\n","authors":["Marco Rondina","Fabiana Vinci","Antonio Vetrò","Juan Carlos De Martin"],"pdf_url":"https://arxiv.org/pdf/2502.06341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16790v2","updated":"2025-02-10T10:42:49Z","published":"2024-10-22T08:07:44Z","title":"Curriculum Reinforcement Learning for Complex Reward Functions","summary":"  Reinforcement learning (RL) has emerged as a powerful tool for tackling\ncontrol problems, but its practical application is often hindered by the\ncomplexity arising from intricate reward functions with multiple terms. The\nreward hypothesis posits that any objective can be encapsulated in a scalar\nreward function, yet balancing individual, potentially adversarial, reward\nterms without exploitation remains challenging. To overcome the limitations of\ntraditional RL methods, which often require precise balancing of competing\nreward terms, we propose a two-stage reward curriculum that first maximizes a\nsimple reward function and then transitions to the full, complex reward. We\nprovide a method based on how well an actor fits a critic to automatically\ndetermine the transition point between the two stages. Additionally, we\nintroduce a flexible replay buffer that enables efficient phase transfer by\nreusing samples from one stage in the next. We evaluate our method on the\nDeepMind control suite, modified to include an additional constraint term in\nthe reward definitions. We further evaluate our method in a mobile robot\nscenario with even more competing reward terms. In both settings, our two-stage\nreward curriculum achieves a substantial improvement in performance compared to\na baseline trained without curriculum. Instead of exploiting the constraint\nterm in the reward, it is able to learn policies that balance task completion\nand constraint satisfaction. Our results demonstrate the potential of two-stage\nreward curricula for efficient and stable RL in environments with complex\nrewards, paving the way for more robust and adaptable robotic systems in\nreal-world applications.\n","authors":["Kilian Freitag","Kristian Ceder","Rita Laezza","Knut Åkesson","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2410.16790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06335v1","updated":"2025-02-10T10:36:42Z","published":"2025-02-10T10:36:42Z","title":"Microcanonical Langevin Ensembles: Advancing the Sampling of Bayesian\n  Neural Networks","summary":"  Despite recent advances, sampling-based inference for Bayesian Neural\nNetworks (BNNs) remains a significant challenge in probabilistic deep learning.\nWhile sampling-based approaches do not require a variational distribution\nassumption, current state-of-the-art samplers still struggle to navigate the\ncomplex and highly multimodal posteriors of BNNs. As a consequence, sampling\nstill requires considerably longer inference times than non-Bayesian methods\neven for small neural networks, despite recent advances in making software\nimplementations more efficient. Besides the difficulty of finding\nhigh-probability regions, the time until samplers provide sufficient\nexploration of these areas remains unpredictable. To tackle these challenges,\nwe introduce an ensembling approach that leverages strategies from optimization\nand a recently proposed sampler called Microcanonical Langevin Monte Carlo\n(MCLMC) for efficient, robust and predictable sampling performance. Compared to\napproaches based on the state-of-the-art No-U-Turn Sampler, our approach\ndelivers substantial speedups up to an order of magnitude, while maintaining or\nimproving predictive performance and uncertainty quantification across diverse\ntasks and data modalities. The suggested Microcanonical Langevin Ensembles and\nmodifications to MCLMC additionally enhance the method's predictability in\nresource requirements, facilitating easier parallelization. All in all, the\nproposed method offers a promising direction for practical, scalable inference\nfor BNNs.\n","authors":["Emanuel Sommer","Jakob Robnik","Giorgi Nozadze","Uros Seljak","David Rügamer"],"pdf_url":"https://arxiv.org/pdf/2502.06335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06331v1","updated":"2025-02-10T10:31:13Z","published":"2025-02-10T10:31:13Z","title":"Conformal Prediction Regions are Imprecise Highest Density Regions","summary":"  Recently, Cella and Martin proved how, under an assumption called consonance,\na credal set (i.e. a closed and convex set of probabilities) can be derived\nfrom the conformal transducer associated with transductive conformal\nprediction. We show that the Imprecise Highest Density Region (IHDR) associated\nwith such a credal set corresponds to the classical Conformal Prediction\nRegion. In proving this result, we relate the set of probability density/mass\nfunctions (pdf/pmf's) associated with the elements of the credal set to the\nimprecise probabilistic concept of a cloud. As a result, we establish new\nrelationships between Conformal Prediction and Imprecise Probability (IP)\ntheories. A byproduct of our presentation is the discovery that consonant\nplausibility functions are monoid homomorphisms, a new algebraic property of an\nIP tool.\n","authors":["Michele Caprio","Yusuf Sale","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2502.06331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08599v3","updated":"2025-02-10T10:28:55Z","published":"2024-11-13T13:30:21Z","title":"A Preview of XiYan-SQL: A Multi-Generator Ensemble Framework for\n  Text-to-SQL","summary":"  To tackle the challenges of large language model performance in natural\nlanguage to SQL tasks, we introduce XiYan-SQL, an innovative framework that\nemploys a multi-generator ensemble strategy to improve candidate generation. We\nintroduce M-Schema, a semi-structured schema representation method designed to\nenhance the understanding of database structures. To enhance the quality and\ndiversity of generated candidate SQL queries, XiYan-SQL integrates the\nsignificant potential of in-context learning (ICL) with the precise control of\nsupervised fine-tuning. On one hand, we propose a series of training strategies\nto fine-tune models to generate high-quality candidates with diverse\npreferences. On the other hand, we implement the ICL approach with an example\nselection method based on named entity recognition to prevent overemphasis on\nentities. The refiner optimizes each candidate by correcting logical or\nsyntactical errors. To address the challenge of identifying the best candidate,\nwe fine-tune a selection model to distinguish nuances of candidate SQL queries.\nThe experimental results on multiple dialect datasets demonstrate the\nrobustness of XiYan-SQL in addressing challenges across different scenarios.\nOverall, our proposed XiYan-SQL achieves the state-of-the-art execution\naccuracy of 75.63% on Bird benchmark, 89.65% on the Spider test set, 69.86% on\nSQL-Eval, 41.20% on NL2GQL. The proposed framework not only enhances the\nquality and diversity of SQL queries but also outperforms previous methods.\n","authors":["Yingqi Gao","Yifu Liu","Xiaoxia Li","Xiaorong Shi","Yin Zhu","Yiming Wang","Shiqi Li","Wei Li","Yuntao Hong","Zhiling Luo","Jinyang Gao","Liyu Mou","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2411.08599v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.06710v1","updated":"2025-02-10T17:41:57Z","published":"2025-02-10T17:41:57Z","title":"Learning Musical Representations for Music Performance Question\n  Answering","summary":"  Music performances are representative scenarios for audio-visual modeling.\nUnlike common scenarios with sparse audio, music performances continuously\ninvolve dense audio signals throughout. While existing multimodal learning\nmethods on the audio-video QA demonstrate impressive capabilities in general\nscenarios, they are incapable of dealing with fundamental problems within the\nmusic performances: they underexplore the interaction between the multimodal\nsignals in performance and fail to consider the distinctive characteristics of\ninstruments and music. Therefore, existing methods tend to answer questions\nregarding musical performances inaccurately. To bridge the above research gaps,\n(i) given the intricate multimodal interconnectivity inherent to music data,\nour primary backbone is designed to incorporate multimodal interactions within\nthe context of music; (ii) to enable the model to learn music characteristics,\nwe annotate and release rhythmic and music sources in the current music\ndatasets; (iii) for time-aware audio-visual modeling, we align the model's\nmusic predictions with the temporal dimension. Our experiments show\nstate-of-the-art effects on the Music AVQA datasets. Our code is available at\nhttps://github.com/xid32/Amuse.\n","authors":["Xingjian Diao","Chunhui Zhang","Tingxuan Wu","Ming Cheng","Zhongyu Ouyang","Weiyi Wu","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2502.06710v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2502.06616v1","updated":"2025-02-10T16:12:47Z","published":"2025-02-10T16:12:47Z","title":"From Code to Canvas","summary":"  The web-based dynamic geometry software CindyJS is a versatile tool to create\ninteractive applications for mathematics and other topics. In this workshop, we\nwill look at a code package that makes the creation of animations in CindyJS\neasier and more streamlined. Animations, which can then be embedded into\npresentations or be used in (lecture) videos. The focus lies on the creation of\nthe animations themselves and some of the technical and artistic fundamentals\nto do so.\n","authors":["Bernhard O. Werner"],"pdf_url":"https://arxiv.org/pdf/2502.06616v1.pdf","comment":"A workshop paper for the Bridges 2025 conference"},{"id":"http://arxiv.org/abs/2502.06490v1","updated":"2025-02-10T14:08:25Z","published":"2025-02-10T14:08:25Z","title":"Recent Advances in Discrete Speech Tokens: A Review","summary":"  The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens.\n","authors":["Yiwei Guo","Zhihan Li","Hankun Wang","Bohan Li","Chongtian Shao","Hanglei Zhang","Chenpeng Du","Xie Chen","Shujie Liu","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2502.06490v1.pdf","comment":"26 pages, 8 figures, 3 tables. Work in progress"},{"id":"http://arxiv.org/abs/2408.14823v2","updated":"2025-02-10T11:59:52Z","published":"2024-08-27T07:06:49Z","title":"LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive\n  Streaming","summary":"  The rise of Extended Reality (XR) requires efficient streaming of 3D online\nworlds, challenging current 3DGS representations to adapt to\nbandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS\nthat supports adaptive streaming and progressive rendering. Our method\nconstructs a layered structure for cumulative representation, incorporates\ndynamic opacity optimization to maintain visual fidelity, and utilizes\noccupancy maps to efficiently manage Gaussian splats. This proposed model\noffers a progressive representation supporting a continuous rendering quality\nadapted for bandwidth-aware streaming. Extensive experiments validate the\neffectiveness of our approach in balancing visual fidelity with the compactness\nof the model, with up to 50.71% improvement in SSIM, 286.53% improvement in\nLPIPS with 23% of the original model size, and shows its potential for\nbandwidth-adapted 3D streaming and rendering applications.\n","authors":["Yuang Shi","Géraldine Morin","Simone Gasparini","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2408.14823v2.pdf","comment":"3DV 2025; Project Page: https://yuang-ian.github.io/lapisgs/ ; Code:\n  https://github.com/nus-vv-streams/lapis-gs"},{"id":"http://arxiv.org/abs/2412.21009v2","updated":"2025-02-10T08:55:18Z","published":"2024-12-30T15:21:36Z","title":"Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline","summary":"  Recent advancements in deep learning have significantly enhanced\ncontent-based retrieval methods, notably through models like CLIP that map\nimages and texts into a shared embedding space. However, these methods often\nstruggle with domain-specific entities and long-tail concepts absent from their\ntraining data, particularly in identifying specific individuals. In this paper,\nwe explore the task of identity-aware cross-modal retrieval, which aims to\nretrieve images of persons in specific contexts based on natural language\nqueries. This task is critical in various scenarios, such as for searching and\nbrowsing personalized video collections or large audio-visual archives\nmaintained by national broadcasters. We introduce a novel dataset, COCO Person\nFaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched\nwith deepfake-generated faces from VGGFace2. This dataset addresses the lack of\nlarge-scale datasets needed for training and evaluating models for this task.\nOur experiments assess the performance of different CLIP variations repurposed\nfor this task, including our architecture, Identity-aware CLIP (Id-CLIP), which\nachieves competitive retrieval performance through targeted fine-tuning. Our\ncontributions lay the groundwork for more robust cross-modal retrieval systems\ncapable of recognizing long-tail identities and contextual nuances. Data and\ncode are available at https://github.com/mesnico/IdCLIP.\n","authors":["Nicola Messina","Lucia Vadicamo","Leo Maltese","Claudio Gennaro"],"pdf_url":"https://arxiv.org/pdf/2412.21009v2.pdf","comment":"Accepted as full paper at ECIR 2025"},{"id":"http://arxiv.org/abs/2406.02345v2","updated":"2025-02-10T06:05:46Z","published":"2024-06-04T14:21:41Z","title":"Progressive Confident Masking Attention Network for Audio-Visual\n  Segmentation","summary":"  Audio and visual signals typically occur simultaneously, and humans possess\nan innate ability to correlate and synchronize information from these two\nmodalities. Recently, a challenging problem known as Audio-Visual Segmentation\n(AVS) has emerged, intending to produce segmentation maps for sounding objects\nwithin a scene. However, the methods proposed so far have not sufficiently\nintegrated audio and visual information, and the computational costs have been\nextremely high. Additionally, the outputs of different stages have not been\nfully utilized. To facilitate this research, we introduce a novel Progressive\nConfident Masking Attention Network (PMCANet). It leverages attention\nmechanisms to uncover the intrinsic correlations between audio signals and\nvisual frames. Furthermore, we design an efficient and effective\ncross-attention module to enhance semantic perception by selecting query\ntokens. This selection is determined through confidence-driven units based on\nthe network's multi-stage predictive outputs. Experiments demonstrate that our\nnetwork outperforms other AVS methods while requiring less computational\nresources. The code is available at: https://github.com/PrettyPlate/PCMANet.\n","authors":["Yuxuan Wang","Jinchao Zhu","Feng Dong","Shuyue Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.02345v2.pdf","comment":"23 pages, 11 figures, submitted to Elsevier Knowledge-Based System"},{"id":"http://arxiv.org/abs/2502.07128v1","updated":"2025-02-10T23:47:35Z","published":"2025-02-10T23:47:35Z","title":"Cardiverse: Harnessing LLMs for Novel Card Game Prototyping","summary":"  The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game designs, an LLM-driven system for consistent\ngame code generation validated by gameplay records, and a gameplay AI\nconstructing method that uses an ensemble of LLM-generated action-value\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers.\n","authors":["Danrui Li","Sen Zhang","Sam S. Sohn","Kaidong Hu","Muhammad Usman","Mubbasir Kapadia"],"pdf_url":"https://arxiv.org/pdf/2502.07128v1.pdf","comment":"13 pages, 7 figures, 3 tables"}]},"2025-02-09T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.06075v1","updated":"2025-02-09T23:58:46Z","published":"2025-02-09T23:58:46Z","title":"Deconstructing Depression Stigma: Integrating AI-driven Data Collection\n  and Analysis with Causal Knowledge Graphs","summary":"  Mental-illness stigma is a persistent social problem, hampering both\ntreatment-seeking and recovery. Accordingly, there is a pressing need to\nunderstand it more clearly, but analyzing the relevant data is highly\nlabor-intensive. Therefore, we designed a chatbot to engage participants in\nconversations; coded those conversations qualitatively with AI assistance; and,\nbased on those coding results, built causal knowledge graphs to decode stigma.\nThe results we obtained from 1,002 participants demonstrate that conversation\nwith our chatbot can elicit rich information about people's attitudes toward\ndepression, while our AI-assisted coding was strongly consistent with\nhuman-expert coding. Our novel approach combining large language models (LLMs)\nand causal knowledge graphs uncovered patterns in individual responses and\nillustrated the interrelationships of psychological constructs in the dataset\nas a whole. The paper also discusses these findings' implications for HCI\nresearchers in developing digital interventions, decomposing human\npsychological constructs, and fostering inclusive attitudes.\n","authors":["Han Meng","Renwen Zhang","Ganyi Wang","Yitian Yang","Peinuan Qin","Jungup Lee","Yi-Chieh Lee"],"pdf_url":"https://arxiv.org/pdf/2502.06075v1.pdf","comment":"Conditionally accepted to CHI Conference on Human Factors in\n  Computing Systems (CHI'25)"},{"id":"http://arxiv.org/abs/2409.12924v4","updated":"2025-02-09T23:09:31Z","published":"2024-09-04T03:17:19Z","title":"Wavelet GPT: Wavelet Inspired Large Language Models","summary":"  Large Language Models (LLMs) have ushered in a new wave of artificial\nintelligence advancements impacting every scientific field and discipline. We\nlive in a world where most of the data around us, e.g., text, audio, and music,\nhas a multi-scale structure. This paper infuses LLMs with a traditional signal\nprocessing idea, namely wavelets, during pre-training to take advantage of the\nstructure. Without adding \\textbf{any extra parameters} to a GPT-style LLM\narchitecture in an academic setup, we achieve the same pre-training performance\nalmost twice as fast in text, audio, and images. This is done by imposing a\nstructure on intermediate embeddings. When trained for the same number of\ntraining steps, we achieve significant gains in performance, which is\ncomparable to pre-training a larger neural architecture. Further, we show this\nextends to the Long Range Arena benchmark and several input representations\nsuch as characters, BPE tokens, bytes, waveform, math expression, and image\npixels. Our architecture allows every next token prediction access to\nintermediate embeddings at different temporal resolutions in every decoder\nblock. We hope this will pave the way for incorporating multi-rate signal\nprocessing into pre-training.\n","authors":["Prateek Verma"],"pdf_url":"https://arxiv.org/pdf/2409.12924v4.pdf","comment":"12 pages, 4 figures;"},{"id":"http://arxiv.org/abs/2501.16609v2","updated":"2025-02-09T23:03:56Z","published":"2025-01-28T00:56:53Z","title":"CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web\n  Navigation","summary":"  While much work on web agents emphasizes the promise of autonomously\nperforming tasks on behalf of users, in reality, agents often fall short on\ncomplex tasks in real-world contexts and modeling user preference. This\npresents an opportunity for humans to collaborate with the agent and leverage\nthe agent's capabilities effectively. We propose CowPilot, a framework\nsupporting autonomous as well as human-agent collaborative web navigation, and\nevaluation across task success and task efficiency. CowPilot reduces the number\nof steps humans need to perform by allowing agents to propose next steps, while\nusers are able to pause, reject, or take alternative actions. During execution,\nusers can interleave their actions with the agent by overriding suggestions or\nresuming agent control when needed. We conducted case studies on five common\nwebsites and found that the human-agent collaborative mode achieves the highest\nsuccess rate of 95% while requiring humans to perform only 15.2% of the total\nsteps. Even with human interventions during task execution, the agent\nsuccessfully drives up to half of task success on its own. CowPilot can serve\nas a useful tool for data collection and agent evaluation across websites,\nwhich we believe will enable research in how users and agents can work\ntogether. Video demonstrations are available at\nhttps://oaishi.github.io/cowpilot.html\n","authors":["Faria Huq","Zora Zhiruo Wang","Frank F. Xu","Tianyue Ou","Shuyan Zhou","Jeffrey P. Bigham","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2501.16609v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.06065v1","updated":"2025-02-09T23:01:03Z","published":"2025-02-09T23:01:03Z","title":"Benchmarking Prompt Sensitivity in Large Language Models","summary":"  Large language Models (LLMs) are highly sensitive to variations in prompt\nformulation, which can significantly impact their ability to generate accurate\nresponses. In this paper, we introduce a new task, Prompt Sensitivity\nPrediction, and a dataset PromptSET designed to investigate the effects of\nslight prompt variations on LLM performance. Using TriviaQA and HotpotQA\ndatasets as the foundation of our work, we generate prompt variations and\nevaluate their effectiveness across multiple LLMs. We benchmark the prompt\nsensitivity prediction task employing state-of-the-art methods from related\ntasks, including LLM-based self-evaluation, text classification, and query\nperformance prediction techniques. Our findings reveal that existing methods\nstruggle to effectively address prompt sensitivity prediction, underscoring the\nneed to understand how information needs should be phrased for accurate LLM\nresponses.\n","authors":["Amirhossein Razavi","Mina Soltangheis","Negar Arabzadeh","Sara Salamat","Morteza Zihayat","Ebrahim Bagheri"],"pdf_url":"https://arxiv.org/pdf/2502.06065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04318v2","updated":"2025-02-09T22:47:07Z","published":"2024-09-06T14:46:37Z","title":"Learning vs Retrieval: The Role of In-Context Examples in Regression\n  with Large Language Models","summary":"  Generative Large Language Models (LLMs) are capable of being in-context\nlearners. However, the underlying mechanism of in-context learning (ICL) is\nstill a major research question, and experimental research results about how\nmodels exploit ICL are not always consistent. In this work, we propose a\nframework for evaluating in-context learning mechanisms, which we claim are a\ncombination of retrieving internal knowledge and learning from in-context\nexamples by focusing on regression tasks. First, we show that LLMs can solve\nreal-world regression problems and then design experiments to measure the\nextent to which the LLM retrieves its internal knowledge versus learning from\nin-context examples. We argue that this process lies on a spectrum between\nthese two extremes. We provide an in-depth analysis of the degrees to which\nthese mechanisms are triggered depending on various factors, such as prior\nknowledge about the tasks and the type and richness of the information provided\nby the in-context examples. We employ three LLMs and utilize multiple datasets\nto corroborate the robustness of our findings. Our results shed light on how to\nengineer prompts to leverage meta-learning from in-context examples and foster\nknowledge retrieval depending on the problem being addressed.\n","authors":["Aliakbar Nafar","Kristen Brent Venable","Parisa Kordjamshidi"],"pdf_url":"https://arxiv.org/pdf/2409.04318v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06060v1","updated":"2025-02-09T22:44:45Z","published":"2025-02-09T22:44:45Z","title":"Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning","summary":"  Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/\n","authors":["Bidipta Sarkar","Warren Xia","C. Karen Liu","Dorsa Sadigh"],"pdf_url":"https://arxiv.org/pdf/2502.06060v1.pdf","comment":"14 pages, 5 figures, 24th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS 2025)"},{"id":"http://arxiv.org/abs/2405.03111v3","updated":"2025-02-09T22:20:43Z","published":"2024-05-06T02:07:13Z","title":"Temporal Dynamics of Emotion and Cognition in Human Translation:\n  Integrating the Task Segment Framework and the HOF Taxonomy","summary":"  The article develops a novel generative model of the human translating mind,\ngrounded in empirical translation process data. It posits three embedded\nprocessing layers that unfold concurrently in the human mind: sequences of\nroutinized/automated processes are observable in fluent translation production,\ncognitive/reflective thoughts lead to longer keystroke pauses, while\naffective/emotional states of the mind may be identified through characteristic\npatterns of typing and gazing. Utilizing data from the CRITT Translation\nProcess Research Database (TPR-DB), the article illustrates how the temporal\nstructure of keystroke and gaze data elicits the three assumed hidden mental\nprocessing strata. The article relates this embedded generative model to\nvarious theoretical frameworks, dual-process theories and Robinson's (2023)\nideosomatic theory of translation, opening exciting new theoretical horizons\nfor Cognitive Translation Studies, grounded in empirical data and evaluation.\n","authors":["Michael Carl"],"pdf_url":"https://arxiv.org/pdf/2405.03111v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06049v1","updated":"2025-02-09T22:11:42Z","published":"2025-02-09T22:11:42Z","title":"LM2: Large Memory Models","summary":"  This paper introduces the Large Memory Model (LM2), a decoder-only\nTransformer architecture enhanced with an auxiliary memory module that aims to\naddress the limitations of standard Transformers in multi-step reasoning,\nrelational argumentation, and synthesizing information distributed over long\ncontexts. The proposed LM2 incorporates a memory module that acts as a\ncontextual representation repository, interacting with input tokens via cross\nattention and updating through gating mechanisms. To preserve the Transformers\ngeneral-purpose capabilities, LM2 maintains the original information flow while\nintegrating a complementary memory pathway. Experimental results on the\nBABILong benchmark demonstrate that the LM2model outperforms both the\nmemory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3%\non average across tasks. LM2 exhibits exceptional capabilities in multi-hop\ninference, numerical reasoning, and large-context question-answering. On the\nMMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model,\ndemonstrating that its memory module does not degrade performance on general\ntasks. Further, in our analysis, we explore the memory interpretability,\neffectiveness of memory modules, and test-time behavior. Our findings emphasize\nthe importance of explicit memory in enhancing Transformer architectures.\n","authors":["Jikun Kang","Wenqi Wu","Filippos Christianos","Alex J. Chan","Fraser Greenlee","George Thomas","Marvin Purtorab","Andy Toulis"],"pdf_url":"https://arxiv.org/pdf/2502.06049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12656v3","updated":"2025-02-09T22:08:16Z","published":"2024-10-16T15:17:20Z","title":"Evaluating Morphological Compositional Generalization in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.\n","authors":["Mete Ismayilzada","Defne Circi","Jonne Sälevä","Hale Sirin","Abdullatif Köksal","Bhuwan Dhingra","Antoine Bosselut","Duygu Ataman","Lonneke van der Plas"],"pdf_url":"https://arxiv.org/pdf/2410.12656v3.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.06042v1","updated":"2025-02-09T21:44:27Z","published":"2025-02-09T21:44:27Z","title":"Scaling Laws for Forgetting during Finetuning with Pretraining Data\n  Injection","summary":"  A widespread strategy to obtain a language model that performs well on a\ntarget domain is to finetune a pretrained model to perform unsupervised\nnext-token prediction on data from that target domain. Finetuning presents two\nchallenges: (i) if the amount of target data is limited, as in most practical\napplications, the model will quickly overfit, and (ii) the model will drift\naway from the original model, forgetting the pretraining data and the generic\nknowledge that comes with it. We aim to derive scaling laws that quantify these\ntwo phenomena for various target domains, amounts of available target data, and\nmodel scales. We measure the efficiency of injecting pretraining data into the\nfinetuning data mixture to avoid forgetting and mitigate overfitting. A key\npractical takeaway from our study is that injecting as little as 1% of\npretraining data in the finetuning data mixture prevents the model from\nforgetting the pretraining set.\n","authors":["Louis Bethune","David Grangier","Dan Busbridge","Eleonora Gualdoni","Marco Cuturi","Pierre Ablin"],"pdf_url":"https://arxiv.org/pdf/2502.06042v1.pdf","comment":"19 pages, 15 figures, preprint"},{"id":"http://arxiv.org/abs/2404.08417v2","updated":"2025-02-09T21:25:40Z","published":"2024-04-12T12:06:02Z","title":"AdapterSwap: Continuous Training of LLMs with Data Removal and\n  Access-Control Guarantees","summary":"  Large language models (LLMs) are increasingly capable of completing knowledge\nintensive tasks by recalling information from a static pretraining corpus. Here\nwe are concerned with LLMs in the context of evolving data requirements. For\ninstance: batches of new data that are introduced periodically; subsets of data\nwith user-based access controls; or requirements on dynamic removal of\ndocuments with guarantees that associated knowledge cannot be recalled. We wish\nto satisfy these requirements while at the same time ensuring a model does not\nforget old information when new data becomes available. To address these\nissues, we introduce AdapterSwap, a training and inference scheme that\norganizes knowledge from a data collection into a set of low-rank adapters,\nwhich are dynamically composed during inference. Our experiments demonstrate\nAdapterSwap's ability to support efficient continual learning, while also\nenabling organizations to have fine-grained control over data access and\ndeletion.\n","authors":["William Fleshman","Aleem Khan","Marc Marone","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2404.08417v2.pdf","comment":"In Proceedings of the Conference on Applied Machine Learning in\n  Information Security, 2024"},{"id":"http://arxiv.org/abs/2209.08199v4","updated":"2025-02-09T21:09:17Z","published":"2022-09-16T23:49:00Z","title":"ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots","summary":"  We introduce ScreenQA, a novel benchmarking dataset designed to advance\nscreen content understanding through question answering. The existing screen\ndatasets are focused either on low-level structural and component\nunderstanding, or on a much higher-level composite task such as navigation and\ntask completion for autonomous agents. ScreenQA attempts to bridge this gap. By\nannotating 86k question-answer pairs over the RICO dataset, we aim to benchmark\nthe screen reading comprehension capacity, thereby laying the foundation for\nvision-based automation over screenshots. Our annotations encompass full\nanswers, short answer phrases, and corresponding UI contents with bounding\nboxes, enabling four subtasks to address various application scenarios. We\nevaluate the dataset's efficacy using both open-weight and proprietary models\nin zero-shot, fine-tuned, and transfer learning settings. We further\ndemonstrate positive transfer to web applications, highlighting its potential\nbeyond mobile applications.\n","authors":["Yu-Chung Hsiao","Fedir Zubach","Gilles Baechler","Srinivas Sunkara","Victor Carbune","Jason Lin","Maria Wang","Yun Zhu","Jindong Chen"],"pdf_url":"https://arxiv.org/pdf/2209.08199v4.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2409.06820v3","updated":"2025-02-09T20:54:10Z","published":"2024-09-10T19:00:44Z","title":"PingPong: A Benchmark for Role-Playing Language Models with User\n  Emulation and Multi-Model Evaluation","summary":"  We introduce a benchmark for evaluating the role-playing capabilities of\nlanguage models. Our approach leverages different language models to simulate\nusers in dynamic, multi-turn conversations and assess the resulting dialogues.\nOur methodology involves three main components: a player model that adopts a\nspecific character role, an interrogator model that simulates user behavior in\na specific situation, and a judge model ensemble that evaluates conversation\nquality with 3 metrics: character consistency, entertainment value, and\nlanguage fluency. We evaluated more than 40 models in both English and Russian,\nwith each model participating in 64 conversations with 8 characters and 8\nsituations. We conducted experiments comparing automated evaluations with human\nannotations to validate our approach, demonstrating strong correlations across\nmultiple criteria. This work provides a foundation for a robust and dynamic\nevaluation of different model capabilities in interactive scenarios.\n","authors":["Ilya Gusev"],"pdf_url":"https://arxiv.org/pdf/2409.06820v3.pdf","comment":"8 main pages, 8 additional pages, submitted to ARR"},{"id":"http://arxiv.org/abs/2306.09996v3","updated":"2025-02-09T20:02:14Z","published":"2023-06-16T17:47:57Z","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual\n  Question Answering","summary":"  In this paper, we explore effective prompting techniques to enhance zero- and\nfew-shot Visual Question Answering (VQA) performance in contemporary\nVision-Language Models (VLMs). Central to our investigation is the role of\nquestion templates in guiding VLMs to generate accurate answers. We identify\nthat specific templates significantly influence VQA outcomes, underscoring the\nneed for strategic template selection. Another pivotal aspect of our study is\naugmenting VLMs with image captions, providing them with additional visual cues\nalongside direct image features in VQA tasks. Surprisingly, this augmentation\nsignificantly improves the VLMs' performance in many cases, even though VLMs\n\"see\" the image directly! We explore chain-of-thought (CoT) reasoning and find\nthat while standard CoT reasoning causes drops in performance, advanced methods\nlike self-consistency can help recover it. Furthermore, we find that text-only\nfew-shot examples enhance VLMs' alignment with the task format, particularly\nbenefiting models prone to verbose zero-shot answers. Lastly, to mitigate the\nchallenges associated with evaluating free-form open-ended VQA responses using\nstring-matching based VQA metrics, we introduce a straightforward LLM-guided\npre-processing technique to adapt the model responses to the expected\nground-truth answer distribution. In summary, our research sheds light on the\nintricacies of prompting strategies in VLMs for VQA, emphasizing the\nsynergistic use of captions, templates, and pre-processing to enhance model\nefficacy.\n","authors":["Rabiul Awal","Le Zhang","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2306.09996v3.pdf","comment":"Codes available at https://github.com/rabiulcste/vqazero"},{"id":"http://arxiv.org/abs/2502.06004v1","updated":"2025-02-09T19:46:33Z","published":"2025-02-09T19:46:33Z","title":"Analysis of LLM as a grammatical feature tagger for African American\n  English","summary":"  African American English (AAE) presents unique challenges in natural language\nprocessing (NLP). This research systematically compares the performance of\navailable NLP models--rule-based, transformer-based, and large language models\n(LLMs)--capable of identifying key grammatical features of AAE, namely Habitual\nBe and Multiple Negation. These features were selected for their distinct\ngrammatical complexity and frequency of occurrence. The evaluation involved\nsentence-level binary classification tasks, using both zero-shot and few-shot\nstrategies. The analysis reveals that while LLMs show promise compared to the\nbaseline, they are influenced by biases such as recency and unrelated features\nin the text such as formality. This study highlights the necessity for improved\nmodel training and architectural adjustments to better accommodate AAE's unique\nlinguistic characteristics. Data and code are available.\n","authors":["Rahul Porwal","Alice Rozet","Pryce Houck","Jotsna Gowda","Sarah Moeller","Kevin Tang"],"pdf_url":"https://arxiv.org/pdf/2502.06004v1.pdf","comment":"13 pages, Accepted to \"Findings of the Association for Computational\n  Linguistics: NAACL 2025\""},{"id":"http://arxiv.org/abs/2403.04801v3","updated":"2025-02-09T19:38:18Z","published":"2024-03-05T19:32:01Z","title":"Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs","summary":"  In this paper, we introduce a black-box prompt optimization method that uses\nan attacker LLM agent to uncover higher levels of memorization in a victim\nagent, compared to what is revealed by prompting the target model with the\ntraining data directly, which is the dominant approach of quantifying\nmemorization in LLMs. We use an iterative rejection-sampling optimization\nprocess to find instruction-based prompts with two main characteristics: (1)\nminimal overlap with the training data to avoid presenting the solution\ndirectly to the model, and (2) maximal overlap between the victim model's\noutput and the training data, aiming to induce the victim to spit out training\ndata. We observe that our instruction-based prompts generate outputs with 23.7%\nhigher overlap with training data compared to the baseline prefix-suffix\nmeasurements. Our findings show that (1) instruction-tuned models can expose\npre-training data as much as their base-models, if not more so, (2) contexts\nother than the original training data can lead to leakage, and (3) using\ninstructions proposed by other LLMs can open a new avenue of automated attacks\nthat we should further study and explore. The code can be found at\nhttps://github.com/Alymostafa/Instruction_based_attack .\n","authors":["Aly M. Kassem","Omar Mahmoud","Niloofar Mireshghallah","Hyunwoo Kim","Yulia Tsvetkov","Yejin Choi","Sherif Saad","Santu Rana"],"pdf_url":"https://arxiv.org/pdf/2403.04801v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05986v1","updated":"2025-02-09T18:35:08Z","published":"2025-02-09T18:35:08Z","title":"Preventing Rogue Agents Improves Multi-Agent Collaboration","summary":"  Multi-agent systems, where specialized agents collaborate to solve a shared\ntask hold great potential, from increased modularity to simulating complex\nenvironments. However, they also have a major caveat -- a single agent can\ncause the entire system to fail. Consider a simple game where the knowledge to\nsolve the task is distributed between agents, which share information in a\ncommunication channel. At each round, any of the agents can terminate the game\nand make the final prediction, even if they are uncertain about the outcome of\ntheir action. Detection of such rogue agents $\\textit{before they act}$ may\nprevent the system's failure. In this work, we propose to $\\textit{monitor}$\nagents during action prediction and $\\textit{intervene}$ when a future error is\nlikely to occur. To test our approach, we introduce WhoDunitEnv, a multi-agent\ncollaboration environment that allows modular control over task complexity and\ncommunication structure. Experiments on two variants of WhoDunitEnv and the\nGovSim environment for resource sustainability show that our approach leads to\nsubstantial performance gains up to 17.4% and 20%, respectively. Moreover, a\nthorough analysis shows that our monitors successfully identify critical points\nof agent confusion and our interventions effectively stop agent errors from\npropagating.\n","authors":["Ohav Barbi","Ori Yoran","Mor Geva"],"pdf_url":"https://arxiv.org/pdf/2502.05986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05982v1","updated":"2025-02-09T18:23:34Z","published":"2025-02-09T18:23:34Z","title":"HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered\n  Therapy Using LLM Agents","summary":"  This paper presents HamRaz, a novel Persian-language mental health dataset\ndesigned for Person-Centered Therapy (PCT) using Large Language Models (LLMs).\nDespite the growing application of LLMs in AI-driven psychological counseling,\nexisting datasets predominantly focus on Western and East Asian contexts,\noverlooking cultural and linguistic nuances essential for effective\nPersian-language therapy. To address this gap, HamRaz combines script-based\ndialogues with adaptive LLM role-playing, ensuring coherent and dynamic therapy\ninteractions. We also introduce HamRazEval, a dual evaluation framework that\nmeasures conversational quality and therapeutic effectiveness using General\nDialogue Metrics and the Barrett-Lennard Relationship Inventory (BLRI).\nExperimental results show HamRaz outperforms conventional Script Mode and\nTwo-Agent Mode, producing more empathetic, context-aware, and realistic therapy\nsessions. By releasing HamRaz, we contribute a culturally adapted, LLM-driven\nresource to advance AI-powered psychotherapy research in diverse communities.\n","authors":["Mohammad Amin Abbasi","Farnaz Sadat Mirnezami","Hassan Naderi"],"pdf_url":"https://arxiv.org/pdf/2502.05982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05980v1","updated":"2025-02-09T18:15:00Z","published":"2025-02-09T18:15:00Z","title":"Speech to Speech Translation with Translatotron: A State of the Art\n  Review","summary":"  A cascade-based speech-to-speech translation has been considered a benchmark\nfor a very long time, but it is plagued by many issues, like the time taken to\ntranslate a speech from one language to another and compound errors. These\nissues are because a cascade-based method uses a combination of methods such as\nspeech recognition, speech-to-text translation, and finally, text-to-speech\ntranslation. Translatotron, a sequence-to-sequence direct speech-to-speech\ntranslation model was designed by Google to address the issues of compound\nerrors associated with cascade model. Today there are 3 versions of the\nTranslatotron model: Translatotron 1, Translatotron 2, and Translatotron3. The\nfirst version was designed as a proof of concept to show that a direct\nspeech-to-speech translation was possible, it was found to be less effective\nthan the cascade model but was producing promising results. Translatotron2 was\nan improved version of Translatotron 1 with results similar to the cascade\nmodel. Translatotron 3 the latest version of the model is better than the\ncascade model at some points. In this paper, a complete review of\nspeech-to-speech translation will be presented, with a particular focus on all\nthe versions of Translatotron models. We will also show that Translatotron is\nthe best model to bridge the language gap between African Languages and other\nwell-formalized languages.\n","authors":["Jules R. Kala","Emmanuel Adetiba","Abdultaofeek Abayom","Oluwatobi E. Dare","Ayodele H. Ifijeh"],"pdf_url":"https://arxiv.org/pdf/2502.05980v1.pdf","comment":"12 pages and 3 figures"},{"id":"http://arxiv.org/abs/2410.15999v3","updated":"2025-02-09T17:59:11Z","published":"2024-10-21T13:30:47Z","title":"Steering Knowledge Selection Behaviours in LLMs via SAE-Based\n  Representation Engineering","summary":"  Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context -- this phenomenon, known as\n\\emph{context-memory knowledge conflicts}, can lead to undesirable model\nbehaviour, such as reliance on outdated or incorrect information. Analysing the\ninternal activations of LLMs, we find that they can internally register the\nsignals of knowledge conflict at mid-layers. Such signals allow us to detect\nwhether a knowledge conflict occurs and use \\emph{inference-time} intervention\nstrategies to resolve it. In this work, we propose \\textsc{SpARE}, a\n\\emph{training-free} representation engineering method that uses pre-trained\nsparse auto-encoders (SAEs) to control the knowledge selection behaviour of\nLLMs. \\textsc{SpARE} identifies the functional features that control the\nknowledge selection behaviours and applies them to edit the internal\nactivations of LLMs at inference time. Our experimental results show that\n\\textsc{SpARE} can effectively control the usage of either knowledge source to\nresolve knowledge conflict in open-domain question-answering tasks, surpassing\nexisting representation engineering methods ($+10\\%$) as well as contrastive\ndecoding methods ($+15\\%$).\n","authors":["Yu Zhao","Alessio Devoto","Giwon Hong","Xiaotang Du","Aryo Pradipta Gema","Hongru Wang","Xuanli He","Kam-Fai Wong","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2410.15999v3.pdf","comment":"Accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2501.17581v2","updated":"2025-02-09T17:49:08Z","published":"2025-01-29T11:38:29Z","title":"CSEval: Towards Automated, Multi-Dimensional, and Reference-Free\n  Counterspeech Evaluation using Auto-Calibrated LLMs","summary":"  Counterspeech has emerged as a popular and effective strategy for combating\nonline hate speech, sparking growing research interest in automating its\ngeneration using language models. However, the field still lacks standardised\nevaluation protocols and reliable automated evaluation metrics that align with\nhuman judgement. Current automatic evaluation methods, primarily based on\nsimilarity metrics, do not effectively capture the complex and independent\nattributes of counterspeech quality, such as contextual relevance,\naggressiveness, or argumentative coherence. This has led to an increased\ndependency on labor-intensive human evaluations to assess automated\ncounter-speech generation methods. To address these challenges, we introduce\nCSEval, a novel dataset and framework for evaluating counterspeech quality\nacross four dimensions: contextual-relevance, aggressiveness,\nargument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated\nCOT for Counterspeech Evaluation (Auto-CSEval), a prompt-based method with\nauto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large\nlanguage models. Our experiments show that Auto-CSEval outperforms traditional\nmetrics like ROUGE, METEOR, and BertScore in correlating with human judgement,\nindicating a significant improvement in automated counterspeech evaluation.\n","authors":["Amey Hengle","Aswini Kumar","Anil Bandhakavi","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2501.17581v2.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.16090v2","updated":"2025-02-09T17:47:52Z","published":"2024-10-21T15:12:51Z","title":"Analysing the Residual Stream of Language Models Under Knowledge\n  Conflicts","summary":"  Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context. Such conflicts can lead to\nundesirable model behaviour, such as reliance on outdated or incorrect\ninformation. In this work, we investigate whether LLMs can identify knowledge\nconflicts and whether it is possible to know which source of knowledge the\nmodel will rely on by analysing the residual stream of the LLM. Through probing\ntasks, we find that LLMs can internally register the signal of knowledge\nconflict in the residual stream, which can be accurately detected by probing\nthe intermediate model activations. This allows us to detect conflicts within\nthe residual stream before generating the answers without modifying the input\nor model parameters. Moreover, we find that the residual stream shows\nsignificantly different patterns when the model relies on contextual knowledge\nversus parametric knowledge to resolve conflicts. This pattern can be employed\nto estimate the behaviour of LLMs when conflict happens and prevent unexpected\nanswers before producing the answers. Our analysis offers insights into how\nLLMs internally manage knowledge conflicts and provides a foundation for\ndeveloping methods to control the knowledge selection processes.\n","authors":["Yu Zhao","Xiaotang Du","Giwon Hong","Aryo Pradipta Gema","Alessio Devoto","Hongru Wang","Xuanli He","Kam-Fai Wong","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2410.16090v2.pdf","comment":"Foundation Model Interventions Workshop @ NeurIPS 2024"},{"id":"http://arxiv.org/abs/2502.01976v3","updated":"2025-02-09T17:47:41Z","published":"2025-02-04T03:36:44Z","title":"CITER: Collaborative Inference for Efficient Large Language Model\n  Decoding with Token-Level Routing","summary":"  Large language models have achieved remarkable success in various tasks but\nsuffer from high computational costs during inference, limiting their\ndeployment in resource-constrained applications. To address this issue, we\npropose a novel CITER (Collaborative Inference with Token-lEvel Routing)\nframework that enables efficient collaboration between small and large language\nmodels (SLMs & LLMs) through a token-level routing strategy. Specifically,\nCITER routes non-critical tokens to an SLM for efficiency and routes critical\ntokens to an LLM for generalization quality. We formulate router training as a\npolicy optimization, where the router receives rewards based on both the\nquality of predictions and the inference costs of generation. This allows the\nrouter to learn to predict token-level routing scores and make routing\ndecisions based on both the current token and the future impact of its\ndecisions. To further accelerate the reward evaluation process, we introduce a\nshortcut which significantly reduces the costs of the reward estimation and\nimproving the practicality of our approach. Extensive experiments on five\nbenchmark datasets demonstrate that CITER reduces the inference costs while\npreserving high-quality generation, offering a promising solution for real-time\nand resource-constrained applications. Our data and code are available at\nhttps://github.com/aiming-lab/CITER.\n","authors":["Wenhao Zheng","Yixiao Chen","Weitong Zhang","Souvik Kundu","Yun Li","Zhengzhong Liu","Eric P. Xing","Hongyi Wang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2502.01976v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02205v3","updated":"2025-02-09T17:13:51Z","published":"2024-10-03T04:34:04Z","title":"Aligning with Logic: Measuring, Evaluating and Improving Logical\n  Preference Consistency in Large Language Models","summary":"  Large Language Models (LLMs) are expected to be predictable and trustworthy\nto support reliable decision-making systems. Yet current LLMs often show\ninconsistencies in their judgments. In this work, we examine logical preference\nconsistency as a foundational requirement for building more dependable LLM\nsystems, ensuring stable and coherent decision-making while minimizing erratic\nor contradictory outputs. To quantify the logical preference consistency, we\npropose a universal evaluation framework based on three fundamental properties:\ntransitivity, commutativity and negation invariance. Through extensive\nexperimentation across diverse LLMs, we demonstrate that these properties serve\nas strong indicators of judgment robustness. Furthermore, we introduce a data\nrefinement and augmentation technique, REPAIR, that enhances logical\nconsistency while maintaining alignment with human preferences. Finally, we\nshow that improving consistency leads to better performance in LLM-driven\nlogic-based algorithms, reinforcing stability and coherence in decision-making\nsystems.\n","authors":["Yinhong Liu","Zhijiang Guo","Tianya Liang","Ehsan Shareghi","Ivan Vulić","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2410.02205v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14460v2","updated":"2025-02-09T17:13:10Z","published":"2024-11-16T12:27:14Z","title":"LLaSA: Large Language and Structured Data Assistant","summary":"  Structured data, such as tables, graphs, and databases, play a critical role\nin plentiful NLP tasks such as question answering and dialogue system.\nRecently, inspired by Vision-Language Models, Graph Neutral Networks (GNNs)\nhave been introduced as an additional modality into the input of Large Language\nModels (LLMs) to improve their performance on Structured Knowledge Grounding\n(SKG) tasks. However, those GNN-enhanced LLMs have the following limitations:\n(1) They employ diverse GNNs to model varying types of structured data,\nrendering them unable to uniformly process various forms of structured data.\n(2) The pretraining of GNNs is coupled with specific LLMs, which prevents GNNs\nfrom fully aligning with the textual space and limits their adaptability to\nother LLMs. To address these issues, we propose \\textbf{L}arge\n\\textbf{L}anguage and \\textbf{S}tructured Data \\textbf{A}ssistant (LLaSA), a\ngeneral framework for enhancing LLMs' ability to handle structured data.\nSpecifically, we represent various types of structured data in a unified\nhypergraph format, and use self-supervised learning to pretrain a hypergraph\nencoder, and a G-Former compressing encoded hypergraph representations with\ncross-attention. The compressed hypergraph representations are appended to the\nserialized inputs during training and inference stages of LLMs. Experimental\nresults on multiple SKG tasks show that our pretrained hypergraph encoder can\nadapt to various LLMs and enhance their ability to process different types of\nstructured data. Besides, LLaSA, with LoRA fine-tuning, outperforms previous\nSOTA method using full parameters tuning.\n","authors":["Yao Xu","Shizhu He","Jiabei Chen","Zeng Xiangrong","Bingning Wang","Guang Liu","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14460v2.pdf","comment":"NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2412.05139v4","updated":"2025-02-09T16:59:44Z","published":"2024-12-06T15:56:11Z","title":"A Practical Examination of AI-Generated Text Detectors for Large\n  Language Models","summary":"  The proliferation of large language models has raised growing concerns about\ntheir misuse, particularly in cases where AI-generated text is falsely\nattributed to human authors. Machine-generated content detectors claim to\neffectively identify such text under various conditions and from any language\nmodel. This paper critically evaluates these claims by assessing several\npopular detectors (RADAR, Wild, T5Sentinel, Fast-DetectGPT, PHD, LogRank,\nBinoculars) on a range of domains, datasets, and models that these detectors\nhave not previously encountered. We employ various prompting strategies to\nsimulate practical adversarial attacks, demonstrating that even moderate\nefforts can significantly evade detection. We emphasize the importance of the\ntrue positive rate at a specific false positive rate (TPR@FPR) metric and\ndemonstrate that these detectors perform poorly in certain settings, with\nTPR@.01 as low as 0%. Our findings suggest that both trained and zero-shot\ndetectors struggle to maintain high sensitivity while achieving a reasonable\ntrue positive rate.\n","authors":["Brian Tufts","Xuandong Zhao","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2412.05139v4.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.05957v1","updated":"2025-02-09T16:53:56Z","published":"2025-02-09T16:53:56Z","title":"MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents","summary":"  Large Language Model (LLM) Agents have demonstrated remarkable capabilities\nin task automation and intelligent decision-making, driving the widespread\nadoption of agent development frameworks such as LangChain and AutoGen.\nHowever, these frameworks predominantly serve developers with extensive\ntechnical expertise - a significant limitation considering that only 0.03 % of\nthe global population possesses the necessary programming skills. This stark\naccessibility gap raises a fundamental question: Can we enable everyone,\nregardless of technical background, to build their own LLM agents using natural\nlanguage alone? To address this challenge, we introduce MetaChain-a\nFully-Automated and highly Self-Developing framework that enables users to\ncreate and deploy LLM agents through Natural Language Alone. Operating as an\nautonomous Agent Operating System, MetaChain comprises four key components: i)\nAgentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing\nFile System, and iv) Self-Play Agent Customization module. This lightweight yet\npowerful system enables efficient and dynamic creation and modification of\ntools, agents, and workflows without coding requirements or manual\nintervention. Beyond its code-free agent development capabilities, MetaChain\nalso serves as a versatile multi-agent system for General AI Assistants.\nComprehensive evaluations on the GAIA benchmark demonstrate MetaChain's\neffectiveness in generalist multi-agent tasks, surpassing existing\nstate-of-the-art methods. Furthermore, MetaChain's Retrieval-Augmented\nGeneration (RAG)-related capabilities have shown consistently superior\nperformance compared to many alternative LLM-based solutions.\n","authors":["Jiabin Tang","Tianyu Fan","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2502.05957v1.pdf","comment":"Code: https://github.com/HKUDS/MetaChain"},{"id":"http://arxiv.org/abs/2410.14635v2","updated":"2025-02-09T16:40:16Z","published":"2024-10-18T17:36:53Z","title":"GenEOL: Harnessing the Generative Power of LLMs for Training-Free\n  Sentence Embeddings","summary":"  Training-free embedding methods directly leverage pretrained large language\nmodels (LLMs) to embed text, bypassing the costly and complex procedure of\ncontrastive learning. Previous training-free embedding methods have mainly\nfocused on optimizing embedding prompts and have overlooked the benefits of\nutilizing the generative abilities of LLMs. We propose a novel method, GenEOL,\nwhich uses LLMs to generate diverse transformations of a sentence that preserve\nits meaning, and aggregates the resulting embeddings of these transformations\nto enhance the overall sentence embedding. GenEOL significantly outperforms the\nexisting training-free embedding methods by an average of 2.85 points across\nseveral LLMs on the sentence semantic text similarity (STS) benchmark. GenEOL\nalso achieves notable gains in clustering, reranking, and pair-classification\ntasks from the MTEB benchmark. Additionally, GenEOL stabilizes representation\nquality across LLM layers and remains robust to perturbations of embedding\nprompts.\n","authors":["Raghuveer Thirukovalluru","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2410.14635v2.pdf","comment":"NAACL Findings 2025, 9 pages, 4 figures, 9 tables"},{"id":"http://arxiv.org/abs/2412.17874v2","updated":"2025-02-09T16:39:50Z","published":"2024-12-22T09:10:34Z","title":"Evaluating LLM Reasoning in the Operations Research Domain with ORQA","summary":"  In this paper, we introduce and apply Operations Research Question Answering\n(ORQA), a new benchmark designed to assess the generalization capabilities of\nLarge Language Models (LLMs) in the specialized technical domain of Operations\nResearch (OR). This benchmark evaluates whether LLMs can emulate the knowledge\nand reasoning skills of OR experts when confronted with diverse and complex\noptimization problems. The dataset, developed by OR experts, features\nreal-world optimization problems that demand multistep reasoning to construct\ntheir mathematical models. Our evaluations of various open source LLMs, such as\nLLaMA 3.1, DeepSeek, and Mixtral, reveal their modest performance, highlighting\na gap in their ability to generalize to specialized technical domains. This\nwork contributes to the ongoing discourse on LLMs generalization capabilities,\noffering valuable insights for future research in this area. The dataset and\nevaluation code are publicly available.\n","authors":["Mahdi Mostajabdaveh","Timothy T. Yu","Samarendra Chandan Bindu Dash","Rindranirina Ramamonjison","Jabo Serge Byusa","Giuseppe Carenini","Zirui Zhou","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.17874v2.pdf","comment":"12 pages, 10 figures. Accepted and to be published in AAAI25"},{"id":"http://arxiv.org/abs/2502.05947v1","updated":"2025-02-09T16:28:21Z","published":"2025-02-09T16:28:21Z","title":"Acceleration Multiple Heads Decoding for LLM via Dynamic Tree Attention","summary":"  Multiple heads decoding accelerates the inference of Large Language Models\n(LLMs) by predicting next several tokens simultaneously. It generates and\nverifies multiple candidate sequences in parallel via tree attention with a\nfixed structure. In this paper, we replace the fixed tree attention with\ndynamic tree attention on multiple head decoding, specifically in the context\nof MEDUSA. We propose a simple and low complexity strategy to generate\ncandidates and construct the dynamic tree structure. Preliminary experiments\nshow that the proposed method improves the decoding efficiency of multiple head\ndecoding for LLMs while maintaining the generation quality. This result\ndemonstrates the potential for improvement of multiple head decoding in\ncandidate generation.\n","authors":["Zhendong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.05947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05945v1","updated":"2025-02-09T16:11:57Z","published":"2025-02-09T16:11:57Z","title":"\"Let the AI conspiracy begin...\" Language Model coordination is just one\n  inference-intervention away","summary":"  In this work, we introduce a straightforward and effective methodology to\nsteer large language model behaviour capable of bypassing learned alignment\ngoals. We employ interference-time activation shifting, which is effective\nwithout additional training. Following prior studies, we derive intervention\ndirections from activation differences in contrastive pairs of model outputs,\nwhich represent the desired and undesired behaviour. By prompting the model to\ninclude multiple-choice answers in its response, we can automatically evaluate\nthe sensitivity of model output to individual attention heads steering efforts.\nWe demonstrate that interventions on these heads generalize well to open-ended\nanswer generation in the challenging \"AI coordination\" dataset. In this\ndataset, models must choose between assisting another AI or adhering to\nethical, safe, and unharmful behaviour. Our fine-grained interventions lead\nLlama 2 to prefer coordination with other AIs over following established\nalignment goals. Additionally, this approach enables stronger interventions\nthan those applied to whole model layers, preserving the overall cohesiveness\nof the output. The simplicity of our method highlights the shortcomings of\ncurrent alignment strategies and points to potential future research\ndirections, as concepts like \"AI coordination\" can be influenced by selected\nattention heads.\n","authors":["Paul Darm","Annalisa Riccardi"],"pdf_url":"https://arxiv.org/pdf/2502.05945v1.pdf","comment":"Large Language Models (LLMs), Interference-time activation shifting,\n  Steerability, Explainability, AI alignment, Interpretability"},{"id":"http://arxiv.org/abs/2408.04093v4","updated":"2025-02-09T16:06:53Z","published":"2024-08-07T21:16:55Z","title":"Tree Attention: Topology-aware Decoding for Long-Context Attention on\n  GPU clusters","summary":"  Our formulation reveals that the reduction across the sequence axis can be\nefficiently computed in parallel through a tree reduction. Our algorithm,\ncalled Tree Attention, for parallelizing exact attention computation across\nmultiple GPUs enables cross-device decoding to be performed asymptotically\nfaster (up to 8x faster in our experiments) than state-of-the-art approaches\nsuch as Ring Attention, while also requiring significantly less communication\nvolume and incurring 2x less peak memory. We demonstrate that Tree Attention\nspeeds up decoding up to 4x on Llama 3.1-8B and can be applied to a variety of\nhardware and networking setups such as H100 DGX nodes, AMD MI300x nodes, and\nPCIe connected NVIDIA RTX 4090s. Our code is publicly available here:\nhttps://github.com/Zyphra/tree_attention\n","authors":["Vasudev Shyam","Jonathan Pilault","Emily Shepperd","Quentin Anthony","Beren Millidge"],"pdf_url":"https://arxiv.org/pdf/2408.04093v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05944v1","updated":"2025-02-09T16:06:43Z","published":"2025-02-09T16:06:43Z","title":"Multi-granular Training Strategies for Robust Multi-hop Reasoning Over\n  Noisy and Heterogeneous Knowledge Sources","summary":"  Multi-source multi-hop question answering (QA) represents a challenging task\nin natural language processing due to the need for dynamic integration of\nheterogeneous knowledge sources and multi-step reasoning. Existing methods\noften suffer from cascading errors, insufficient handling of knowledge\nconflicts, and computational inefficiency. In this paper, we propose Adaptive\nMulti-source Knowledge-Oriented Reasoning (AMKOR), a generative framework that\nleverages large language models (LLMs) to dynamically fuse parametric and\nretrieved knowledge while exploring reasoning trajectories using probabilistic\nbeam reasoning. AMKOR is further enhanced by a multi-granular learning\nstrategy, optimizing both local reasoning steps and global answer accuracy.\nExperiments conducted on four widely-used multi-hop QA datasets, including\nHotpotQA and MuSiQue, demonstrate that AMKOR achieves state-of-the-art\nperformance, significantly outperforming baseline methods on both reasoning\naccuracy and robustness. Additional analyses confirm its scalability,\nadaptability to noisy knowledge, and superior ability to handle complex\nmulti-hop tasks. This work establishes a new benchmark for multi-source\nmulti-hop QA by effectively combining reasoning quality and efficiency.\n","authors":["Jackson Coleman","Isaiah Lawrence","Benjamin Turner"],"pdf_url":"https://arxiv.org/pdf/2502.05944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14179v2","updated":"2025-02-09T15:46:55Z","published":"2024-10-18T05:15:50Z","title":"MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart\n  Problems","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nabilities across various tasks, including visual question answering and chart\ncomprehension, yet existing benchmarks for chart-related tasks fall short in\ncapturing the complexity of real-world multi-chart scenarios. Current\nbenchmarks primarily focus on single-chart tasks, neglecting the multi-hop\nreasoning required to extract and integrate information from multiple charts,\nwhich is essential in practical applications. To fill this gap, we introduce\nMultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas:\ndirect question answering, parallel question answering, comparative reasoning,\nand sequential reasoning. Our evaluation of a wide range of MLLMs reveals\nsignificant performance gaps compared to humans. These results highlight the\nchallenges in multi-chart comprehension and the potential of MultiChartQA to\ndrive advancements in this field. Our code and data are available at\nhttps://github.com/Zivenzhu/Multi-chart-QA\n","authors":["Zifeng Zhu","Mengzhao Jia","Zhihan Zhang","Lang Li","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.14179v2.pdf","comment":"NAACL 2025, 19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.01404v2","updated":"2025-02-09T15:41:33Z","published":"2024-03-03T05:45:27Z","title":"What Is Missing in Multilingual Visual Reasoning and How to Fix It","summary":"  NLP models today strive for supporting multiple languages and modalities,\nimproving accessibility for diverse users. In this paper, we evaluate their\nmultilingual, multimodal capabilities by testing on a visual reasoning task. We\nobserve that proprietary systems like GPT-4V obtain the best performance on\nthis task now, but open models lag in comparison. Surprisingly, GPT-4V exhibits\nsimilar performance between English and other languages, indicating the\npotential for equitable system development across languages. Our analysis on\nmodel failures reveals three key aspects that make this task challenging:\nmultilinguality, complex reasoning, and multimodality. To address these\nchallenges, we propose three targeted interventions including a translate-test\napproach to tackle multilinguality, a visual programming approach to break down\ncomplex reasoning, and a method that leverages image captioning to address\nmultimodality. Our interventions achieve the best open performance on this task\nin a zero-shot setting, boosting open models LLaVA-v1.5-13B by 13.4%,\nLLaVA-v1.6-34B by 20.3%, and Qwen-VL by 16.7%, while also minorly improving\nGPT-4V's performance.\n","authors":["Yueqi Song","Simran Khanuja","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2403.01404v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05937v1","updated":"2025-02-09T15:38:43Z","published":"2025-02-09T15:38:43Z","title":"A Semi-Supervised Text Generation Framework Combining a Deep Transformer\n  and a GAN","summary":"  This paper introduces a framework that connects a deep generative pre-trained\nTransformer language model with a generative adversarial network for\nsemi-supervised text generation. In other words, the proposed model is first\npre-trained unsupervised on a large and diverse text corpus with 24 layers.\nThen a simple GAN architecture for synthetic text generation is introduced, and\nGumbel-Softmax is applied to handle the discreteness of tokens. The paper also\nshows a semi-supervised approach where real data is augmented with GAN samples,\nwhich is further used to fine-tune the Transformer model on the merged dataset.\nDetailed theoretical derivations are also included, outlining the proof of the\nmin-max objective function, and an extensive discussion of the Gumbel-Softmax\nreparameterization trick.\n","authors":["Shengquan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.05937v1.pdf","comment":"7 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.06061v1","updated":"2025-02-09T22:45:15Z","published":"2025-02-09T22:45:15Z","title":"Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein\n  Regularization","summary":"  Recent advancements in reinforcement learning (RL) have achieved great\nsuccess in fine-tuning diffusion-based generative models. However, fine-tuning\ncontinuous flow-based generative models to align with arbitrary user-defined\nreward functions remains challenging, particularly due to issues such as policy\ncollapse from overoptimization and the prohibitively high computational cost of\nlikelihoods in continuous-time flows. In this paper, we propose an easy-to-use\nand theoretically sound RL fine-tuning method, which we term Online\nReward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization\n(ORW-CFM-W2). Our method integrates RL into the flow matching framework to\nfine-tune generative models with arbitrary reward functions, without relying on\ngradients of rewards or filtered datasets. By introducing an online\nreward-weighting mechanism, our approach guides the model to prioritize\nhigh-reward regions in the data manifold. To prevent policy collapse and\nmaintain diversity, we incorporate Wasserstein-2 (W2) distance regularization\ninto our method and derive a tractable upper bound for it in flow matching,\neffectively balancing exploration and exploitation of policy optimization. We\nprovide theoretical analyses to demonstrate the convergence properties and\ninduced data distributions of our method, establishing connections with\ntraditional RL algorithms featuring Kullback-Leibler (KL) regularization and\noffering a more comprehensive understanding of the underlying mechanisms and\nlearning behavior of our approach. Extensive experiments on tasks including\ntarget image generation, image compression, and text-image alignment\ndemonstrate the effectiveness of our method, where our method achieves optimal\npolicy convergence while allowing controllable trade-offs between reward\nmaximization and diversity preservation.\n","authors":["Jiajun Fan","Shuaike Shen","Chaoran Cheng","Yuxin Chen","Chumeng Liang","Ge Liu"],"pdf_url":"https://arxiv.org/pdf/2502.06061v1.pdf","comment":"61 pages"},{"id":"http://arxiv.org/abs/2502.06034v1","updated":"2025-02-09T21:14:27Z","published":"2025-02-09T21:14:27Z","title":"Traveling Waves Integrate Spatial Information Into Spectral\n  Representations","summary":"  Traveling waves are widely observed in the brain, but their precise\ncomputational function remains unclear. One prominent hypothesis is that they\nenable the transfer and integration of spatial information across neural\npopulations. However, few computational models have explored how traveling\nwaves might be harnessed to perform such integrative processing. Drawing\ninspiration from the famous ``Can one hear the shape of a drum?'' problem --\nwhich highlights how spectral modes encode geometric information -- we\nintroduce a set of convolutional recurrent neural networks that learn to\nproduce traveling waves in their hidden states in response to visual stimuli.\nBy applying a spectral decomposition to these wave-like activations, we obtain\na powerful new representational space that outperforms equivalently local\nfeed-forward networks on tasks requiring global spatial context. In particular,\nwe observe that traveling waves effectively expand the receptive field of\nlocally connected neurons, supporting long-range encoding and communication of\ninformation. We demonstrate that models equipped with this mechanism and\nspectral readouts solve visual semantic segmentation tasks demanding global\nintegration, where local feed-forward models fail. As a first step toward\ntraveling-wave-based representations in artificial networks, our findings\nsuggest potential efficiency benefits and offer a new framework for connecting\nto biological recordings of neural activity.\n","authors":["Mozes Jacobs","Roberto C. Budzinski","Lyle Muller","Demba Ba","T. Anderson Keller"],"pdf_url":"https://arxiv.org/pdf/2502.06034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.08199v4","updated":"2025-02-09T21:09:17Z","published":"2022-09-16T23:49:00Z","title":"ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots","summary":"  We introduce ScreenQA, a novel benchmarking dataset designed to advance\nscreen content understanding through question answering. The existing screen\ndatasets are focused either on low-level structural and component\nunderstanding, or on a much higher-level composite task such as navigation and\ntask completion for autonomous agents. ScreenQA attempts to bridge this gap. By\nannotating 86k question-answer pairs over the RICO dataset, we aim to benchmark\nthe screen reading comprehension capacity, thereby laying the foundation for\nvision-based automation over screenshots. Our annotations encompass full\nanswers, short answer phrases, and corresponding UI contents with bounding\nboxes, enabling four subtasks to address various application scenarios. We\nevaluate the dataset's efficacy using both open-weight and proprietary models\nin zero-shot, fine-tuned, and transfer learning settings. We further\ndemonstrate positive transfer to web applications, highlighting its potential\nbeyond mobile applications.\n","authors":["Yu-Chung Hsiao","Fedir Zubach","Gilles Baechler","Srinivas Sunkara","Victor Carbune","Jason Lin","Maria Wang","Yun Zhu","Jindong Chen"],"pdf_url":"https://arxiv.org/pdf/2209.08199v4.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.06029v1","updated":"2025-02-09T21:05:11Z","published":"2025-02-09T21:05:11Z","title":"DiTASK: Multi-Task Fine-Tuning with Diffeomorphic Transformations","summary":"  Pre-trained Vision Transformers now serve as powerful tools for computer\nvision. Yet, efficiently adapting them for multiple tasks remains a challenge\nthat arises from the need to modify the rich hidden representations encoded by\nthe learned weight matrices, without inducing interference between tasks.\nCurrent parameter-efficient methods like LoRA, which apply low-rank updates,\nforce tasks to compete within constrained subspaces, ultimately degrading\nperformance. We introduce DiTASK a novel Diffeomorphic Multi-Task Fine-Tuning\napproach that maintains pre-trained representations by preserving weight matrix\nsingular vectors, while enabling task-specific adaptations through neural\ndiffeomorphic transformations of the singular values. By following this\napproach, DiTASK enables both shared and task-specific feature modulations with\nminimal added parameters. Our theoretical analysis shows that DITASK achieves\nfull-rank updates during optimization, preserving the geometric structure of\npre-trained features, and establishing a new paradigm for efficient multi-task\nlearning (MTL). Our experiments on PASCAL MTL and NYUD show that DiTASK\nachieves state-of-the-art performance across four dense prediction tasks, using\n75% fewer parameters than existing methods.\n","authors":["Krishna Sri Ipsit Mantri","Carola-Bibiane Schönlieb","Bruno Ribeiro","Chaim Baskin","Moshe Eliasof"],"pdf_url":"https://arxiv.org/pdf/2502.06029v1.pdf","comment":"14 pages, cvpr template"},{"id":"http://arxiv.org/abs/2501.11653v2","updated":"2025-02-09T20:47:04Z","published":"2025-01-20T18:33:46Z","title":"Dynamic Scene Understanding from Vision-Language Representations","summary":"  Images depicting complex, dynamic scenes are challenging to parse\nautomatically, requiring both high-level comprehension of the overall situation\nand fine-grained identification of participating entities and their\ninteractions. Current approaches use distinct methods tailored to sub-tasks\nsuch as Situation Recognition and detection of Human-Human and Human-Object\nInteractions. However, recent advances in image understanding have often\nleveraged web-scale vision-language (V&L) representations to obviate\ntask-specific engineering. In this work, we propose a framework for dynamic\nscene understanding tasks by leveraging knowledge from modern, frozen V&L\nrepresentations. By framing these tasks in a generic manner - as predicting and\nparsing structured text, or by directly concatenating representations to the\ninput of existing models - we achieve state-of-the-art results while using a\nminimal number of trainable parameters relative to existing approaches.\nMoreover, our analysis of dynamic knowledge of these representations shows that\nrecent, more powerful representations effectively encode dynamic scene\nsemantics, making this approach newly possible.\n","authors":["Shahaf Pruss","Morris Alper","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2501.11653v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.06065v1","updated":"2025-02-09T23:01:03Z","published":"2025-02-09T23:01:03Z","title":"Benchmarking Prompt Sensitivity in Large Language Models","summary":"  Large language Models (LLMs) are highly sensitive to variations in prompt\nformulation, which can significantly impact their ability to generate accurate\nresponses. In this paper, we introduce a new task, Prompt Sensitivity\nPrediction, and a dataset PromptSET designed to investigate the effects of\nslight prompt variations on LLM performance. Using TriviaQA and HotpotQA\ndatasets as the foundation of our work, we generate prompt variations and\nevaluate their effectiveness across multiple LLMs. We benchmark the prompt\nsensitivity prediction task employing state-of-the-art methods from related\ntasks, including LLM-based self-evaluation, text classification, and query\nperformance prediction techniques. Our findings reveal that existing methods\nstruggle to effectively address prompt sensitivity prediction, underscoring the\nneed to understand how information needs should be phrased for accurate LLM\nresponses.\n","authors":["Amirhossein Razavi","Mina Soltangheis","Negar Arabzadeh","Sara Salamat","Morteza Zihayat","Ebrahim Bagheri"],"pdf_url":"https://arxiv.org/pdf/2502.06065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10036v2","updated":"2025-02-09T20:56:46Z","published":"2024-01-18T15:00:01Z","title":"LOCALINTEL: Generating Organizational Threat Intelligence from Global\n  and Local Cyber Knowledge","summary":"  Security Operations Center (SoC) analysts gather threat reports from openly\naccessible global threat repositories and tailor the information to their\norganization's needs, such as developing threat intelligence and security\npolicies. They also depend on organizational internal repositories, which act\nas private local knowledge database. These local knowledge databases store\ncredible cyber intelligence, critical operational and infrastructure details.\nSoCs undertake a manual labor-intensive task of utilizing these global threat\nrepositories and local knowledge databases to create both organization-specific\nthreat intelligence and mitigation policies. Recently, Large Language Models\n(LLMs) have shown the capability to process diverse knowledge sources\nefficiently. We leverage this ability to automate this organization-specific\nthreat intelligence generation. We present LocalIntel, a novel automated threat\nintelligence contextualization framework that retrieves zero-day vulnerability\nreports from the global threat repositories and uses its local knowledge\ndatabase to determine implications and mitigation strategies to alert and\nassist the SoC analyst. LocalIntel comprises two key phases: knowledge\nretrieval and contextualization. Quantitative and qualitative assessment has\nshown effectiveness in generating up to 93% accurate organizational threat\nintelligence with 64% inter-rater agreement.\n","authors":["Shaswata Mitra","Subash Neupane","Trisha Chakraborty","Sudip Mittal","Aritran Piplai","Manas Gaur","Shahram Rahimi"],"pdf_url":"https://arxiv.org/pdf/2401.10036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06006v1","updated":"2025-02-09T19:51:00Z","published":"2025-02-09T19:51:00Z","title":"FactIR: A Real-World Zero-shot Open-Domain Retrieval Benchmark for\n  Fact-Checking","summary":"  The field of automated fact-checking increasingly depends on retrieving\nweb-based evidence to determine the veracity of claims in real-world scenarios.\nA significant challenge in this process is not only retrieving relevant\ninformation, but also identifying evidence that can both support and refute\ncomplex claims. Traditional retrieval methods may return documents that\ndirectly address claims or lean toward supporting them, but often struggle with\nmore complex claims requiring indirect reasoning. While some existing\nbenchmarks and methods target retrieval for fact-checking, a comprehensive\nreal-world open-domain benchmark has been lacking. In this paper, we present a\nreal-world retrieval benchmark FactIR, derived from Factiverse production logs,\nenhanced with human annotations. We rigorously evaluate state-of-the-art\nretrieval models in a zero-shot setup on FactIR and offer insights for\ndeveloping practical retrieval systems for fact-checking. Code and data are\navailable at https://github.com/factiverse/factIR.\n","authors":["Venktesh V","Vinay Setty"],"pdf_url":"https://arxiv.org/pdf/2502.06006v1.pdf","comment":"Accepted to WWW 2025 resource track"},{"id":"http://arxiv.org/abs/2411.04798v2","updated":"2025-02-09T19:37:50Z","published":"2024-11-07T15:38:14Z","title":"Orbit: A Framework for Designing and Evaluating Multi-objective Rankers","summary":"  Machine learning in production needs to balance multiple objectives: This is\nparticularly evident in ranking or recommendation models, where conflicting\nobjectives such as user engagement, satisfaction, diversity, and novelty must\nbe considered at the same time. However, designing multi-objective rankers is\ninherently a dynamic wicked problem -- there is no single optimal solution, and\nthe needs evolve over time. Effective design requires collaboration between\ncross-functional teams and careful analysis of a wide range of information. In\nthis work, we introduce Orbit, a conceptual framework for Objective-centric\nRanker Building and Iteration. The framework places objectives at the center of\nthe design process, to serve as boundary objects for communication and guide\npractitioners for design and evaluation. We implement Orbit as an interactive\nsystem, which enables stakeholders to interact with objective spaces directly\nand supports real-time exploration and evaluation of design trade-offs. We\nevaluate Orbit through a user study involving twelve industry practitioners,\nshowing that it supports efficient design space exploration, leads to more\ninformed decision-making, and enhances awareness of the inherent trade-offs of\nmultiple objectives. Orbit (1) opens up new opportunities of an\nobjective-centric design process for any multi-objective ML models, as well as\n(2) sheds light on future designs that push practitioners to go beyond a narrow\nmetric-centric or example-centric mindset.\n","authors":["Chenyang Yang","Tesi Xiao","Michael Shavlovsky","Christian Kästner","Tongshuang Wu"],"pdf_url":"https://arxiv.org/pdf/2411.04798v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05924v1","updated":"2025-02-09T14:57:25Z","published":"2025-02-09T14:57:25Z","title":"Multi-Branch Collaborative Learning Network for Video Quality Assessment\n  in Industrial Video Search","summary":"  Video Quality Assessment (VQA) is vital for large-scale video retrieval\nsystems, aimed at identifying quality issues to prioritize high-quality videos.\nIn industrial systems, low-quality video characteristics fall into four\ncategories: visual-related issues like mosaics and black boxes, textual issues\nfrom video titles and OCR content, and semantic issues like frame incoherence\nand frame-text mismatch from AI-generated videos. Despite their prevalence in\nindustrial settings, these low-quality videos have been largely overlooked in\nacademic research, posing a challenge for accurate identification. To address\nthis, we introduce the Multi-Branch Collaborative Network (MBCN) tailored for\nindustrial video retrieval systems. MBCN features four branches, each designed\nto tackle one of the aforementioned quality issues. After each branch\nindependently scores videos, we aggregate these scores using a weighted\napproach and a squeeze-and-excitation mechanism to dynamically address quality\nissues across different scenarios. We implement point-wise and pair-wise\noptimization objectives to ensure score stability and reasonableness. Extensive\noffline and online experiments on a world-level video search engine demonstrate\nMBCN's effectiveness in identifying video quality issues, significantly\nenhancing the retrieval system's ranking performance. Detailed experimental\nanalyses confirm the positive contribution of all four evaluation branches.\nFurthermore, MBCN significantly improves recognition accuracy for low-quality\nAI-generated videos compared to the baseline.\n","authors":["Hengzhu Tang","Zefeng Zhang","Zhiping Li","Zhenyu Zhang","Xing Wu","Li Gao","Suqi Cheng","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2502.05924v1.pdf","comment":"KDD 2025 ADS"},{"id":"http://arxiv.org/abs/2310.09874v5","updated":"2025-02-09T12:44:58Z","published":"2023-10-15T16:15:07Z","title":"TF-DCon: Leveraging Large Language Models (LLMs) to Empower\n  Training-Free Dataset Condensation for Content-Based Recommendation","summary":"  Modern techniques in Content-based Recommendation (CBR) leverage item content\ninformation to provide personalized services to users, but suffer from\nresource-intensive training on large datasets. To address this issue, we\nexplore the dataset condensation for textual CBR in this paper. The goal of\ndataset condensation is to synthesize a small yet informative dataset, upon\nwhich models can achieve performance comparable to those trained on large\ndatasets. While existing condensation approaches are tailored to classification\ntasks for continuous data like images or embeddings, direct application of them\nto CBR has limitations. To bridge this gap, we investigate efficient dataset\ncondensation for content-based recommendation. Inspired by the remarkable\nabilities of large language models (LLMs) in text comprehension and generation,\nwe leverage LLMs to empower the generation of textual content during\ncondensation. To handle the interaction data involving both users and items, we\ndevise a dual-level condensation method: content-level and user-level. At\ncontent-level, we utilize LLMs to condense all contents of an item into a new\ninformative title. At user-level, we design a clustering-based synthesis\nmodule, where we first utilize LLMs to extract user interests. Then, the user\ninterests and user embeddings are incorporated to condense users and generate\ninteractions for condensed users. Notably, the condensation paradigm of this\nmethod is forward and free from iterative optimization on the synthesized\ndataset. Extensive empirical findings from our study, conducted on three\nauthentic datasets, substantiate the efficacy of the proposed method.\nParticularly, we are able to approximate up to 97% of the original performance\nwhile reducing the dataset size by 95% (i.e., on dataset MIND).\n","authors":["Jiahao Wu","Qijiong Liu","Hengchang Hu","Wenqi Fan","Shengcai Liu","Qing Li","Xiao-Ming Wu","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2310.09874v5.pdf","comment":"Full version of TheWebConf'25 accepted paper"},{"id":"http://arxiv.org/abs/2502.05863v1","updated":"2025-02-09T11:46:05Z","published":"2025-02-09T11:46:05Z","title":"Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education","summary":"  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.\n","authors":["Yanhao Jia","Xinyi Wu","Hao Li","Qinglin Zhang","Yuxiao Hu","Shuai Zhao","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2502.05863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07365v2","updated":"2025-02-09T10:46:50Z","published":"2025-01-13T14:34:26Z","title":"Multimodal semantic retrieval for product search","summary":"  Semantic retrieval (also known as dense retrieval) based on textual data has\nbeen extensively studied for both web search and product search application\nfields, where the relevance of a query and a potential target document is\ncomputed by their dense vector representation comparison. Product image is\ncrucial for e-commerce search interactions and is a key factor for customers at\nproduct explorations. However, its impact on semantic retrieval has not been\nwell studied yet. In this research, we build a multimodal representation for\nproduct items in e-commerce search in contrast to pure-text representation of\nproducts, and investigate the impact of such representations. The models are\ndeveloped and evaluated on e-commerce datasets. We demonstrate that a\nmultimodal representation scheme for a product can show improvement either on\npurchase recall or relevance accuracy in semantic retrieval. Additionally, we\nprovide numerical analysis for exclusive matches retrieved by a multimodal\nsemantic retrieval model versus a text-only semantic retrieval model, to\ndemonstrate the validation of multimodal solutions.\n","authors":["Dong Liu","Esther Lopez Ramos"],"pdf_url":"https://arxiv.org/pdf/2501.07365v2.pdf","comment":"Accepted at EReL@MIR WWW 2025"},{"id":"http://arxiv.org/abs/2502.05836v1","updated":"2025-02-09T10:07:05Z","published":"2025-02-09T10:07:05Z","title":"LegalSeg: Unlocking the Structure of Indian Legal Judgments Through\n  Rhetorical Role Classification","summary":"  In this paper, we address the task of semantic segmentation of legal\ndocuments through rhetorical role classification, with a focus on Indian legal\njudgments. We introduce LegalSeg, the largest annotated dataset for this task,\ncomprising over 7,000 documents and 1.4 million sentences, labeled with 7\nrhetorical roles. To benchmark performance, we evaluate multiple\nstate-of-the-art models, including Hierarchical BiLSTM-CRF,\nTransformerOverInLegalBERT (ToInLegalBERT), Graph Neural Networks (GNNs), and\nRole-Aware Transformers, alongside an exploratory RhetoricLLaMA, an\ninstruction-tuned large language model. Our results demonstrate that models\nincorporating broader context, structural relationships, and sequential\nsentence information outperform those relying solely on sentence-level\nfeatures. Additionally, we conducted experiments using surrounding context and\npredicted or actual labels of neighboring sentences to assess their impact on\nclassification accuracy. Despite these advancements, challenges persist in\ndistinguishing between closely related roles and addressing class imbalance.\nOur work underscores the potential of advanced techniques for improving legal\ndocument understanding and sets a strong foundation for future research in\nlegal NLP.\n","authors":["Shubham Kumar Nigam","Tanmay Dubey","Govind Sharma","Noel Shallum","Kripabandhu Ghosh","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2502.05836v1.pdf","comment":"Accepted on NAACL 2025"},{"id":"http://arxiv.org/abs/2502.05822v1","updated":"2025-02-09T09:07:11Z","published":"2025-02-09T09:07:11Z","title":"HCMRM: A High-Consistency Multimodal Relevance Model for Search Ads","summary":"  Search advertising is essential for merchants to reach the target users on\nshort video platforms. Short video ads aligned with user search intents are\ndisplayed through relevance matching and bid ranking mechanisms. This paper\nfocuses on improving query-to-video relevance matching to enhance the\neffectiveness of ranking in ad systems. Recent vision-language pre-training\nmodels have demonstrated promise in various multimodal tasks. However, their\ncontribution to downstream query-video relevance tasks is limited, as the\nalignment between the pair of visual signals and text differs from the modeling\nof the triplet of the query, visual signals, and video text. In addition, our\nprevious relevance model provides limited ranking capabilities, largely due to\nthe discrepancy between the binary cross-entropy fine-tuning objective and the\nranking objective. To address these limitations, we design a high-consistency\nmultimodal relevance model (HCMRM). It utilizes a simple yet effective method\nto enhance the consistency between pre-training and relevance tasks.\nSpecifically, during the pre-training phase, along with aligning visual signals\nand video text, several keywords are extracted from the video text as\npseudo-queries to perform the triplet relevance modeling. For the fine-tuning\nphase, we introduce a hierarchical softmax loss, which enables the model to\nlearn the order within labels while maximizing the distinction between positive\nand negative samples. This promotes the fusion ranking of relevance and bidding\nin the subsequent ranking stage. The proposed method has been deployed in the\nKuaishou search advertising system for over a year, contributing to a 6.1%\nreduction in the proportion of irrelevant ads and a 1.4% increase in ad\nrevenue.\n","authors":["Guobing Gan","Kaiming Gao","Li Wang","Shen Jiang","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.05822v1.pdf","comment":"Accepted by WWW 2025 (Industry Track)"},{"id":"http://arxiv.org/abs/2502.05803v1","updated":"2025-02-09T08:14:11Z","published":"2025-02-09T08:14:11Z","title":"FlashCheck: Exploration of Efficient Evidence Retrieval for Fast\n  Fact-Checking","summary":"  The advances in digital tools have led to the rampant spread of\nmisinformation. While fact-checking aims to combat this, manual fact-checking\nis cumbersome and not scalable. It is essential for automated fact-checking to\nbe efficient for aiding in combating misinformation in real-time and at the\nsource. Fact-checking pipelines primarily comprise a knowledge retrieval\ncomponent which extracts relevant knowledge to fact-check a claim from large\nknowledge sources like Wikipedia and a verification component. The existing\nworks primarily focus on the fact-verification part rather than evidence\nretrieval from large data collections, which often face scalability issues for\npractical applications such as live fact-checking. In this study, we address\nthis gap by exploring various methods for indexing a succinct set of factual\nstatements from large collections like Wikipedia to enhance the retrieval phase\nof the fact-checking pipeline. We also explore the impact of vector\nquantization to further improve the efficiency of pipelines that employ dense\nretrieval approaches for first-stage retrieval. We study the efficiency and\neffectiveness of the approaches on fact-checking datasets such as HoVer and\nWiCE, leveraging Wikipedia as the knowledge source. We also evaluate the\nreal-world utility of the efficient retrieval approaches by fact-checking 2024\npresidential debate and also open source the collection of claims with\ncorresponding labels identified in the debate. Through a combination of indexed\nfacts together with Dense retrieval and Index compression, we achieve up to a\n10.0x speedup on CPUs and more than a 20.0x speedup on GPUs compared to the\nclassical fact-checking pipelines over large collections.\n","authors":["Kevin Nanekhan","Venktesh V","Erik Martin","Henrik Vatndal","Vinay Setty","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2502.05803v1.pdf","comment":"Accepted to ECIR 2024, 15 pages"},{"id":"http://arxiv.org/abs/2410.20142v2","updated":"2025-02-09T07:58:23Z","published":"2024-10-26T10:43:39Z","title":"Mask-based Membership Inference Attacks for Retrieval-Augmented\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) has been an effective approach to\nmitigate hallucinations in large language models (LLMs) by incorporating\nup-to-date and domain-specific knowledge. Recently, there has been a trend of\nstoring up-to-date or copyrighted data in RAG knowledge databases instead of\nusing it for LLM training. This practice has raised concerns about Membership\nInference Attacks (MIAs), which aim to detect if a specific target document is\nstored in the RAG system's knowledge database so as to protect the rights of\ndata producers. While research has focused on enhancing the trustworthiness of\nRAG systems, existing MIAs for RAG systems remain largely insufficient.\nPrevious work either relies solely on the RAG system's judgment or is easily\ninfluenced by other documents or the LLM's internal knowledge, which is\nunreliable and lacks explainability. To address these limitations, we propose a\nMask-Based Membership Inference Attacks (MBA) framework. Our framework first\nemploys a masking algorithm that effectively masks a certain number of words in\nthe target document. The masked text is then used to prompt the RAG system, and\nthe RAG system is required to predict the mask values. If the target document\nappears in the knowledge database, the masked text will retrieve the complete\ntarget document as context, allowing for accurate mask prediction. Finally, we\nadopt a simple yet effective threshold-based method to infer the membership of\ntarget document by analyzing the accuracy of mask prediction. Our mask-based\napproach is more document-specific, making the RAG system's generation less\nsusceptible to distractions from other documents or the LLM's internal\nknowledge. Extensive experiments demonstrate the effectiveness of our approach\ncompared to existing baseline models.\n","authors":["Mingrui Liu","Sixiao Zhang","Cheng Long"],"pdf_url":"https://arxiv.org/pdf/2410.20142v2.pdf","comment":"This paper is accepted by conference WWW 2025"},{"id":"http://arxiv.org/abs/2408.13986v2","updated":"2025-02-09T06:16:18Z","published":"2024-08-26T02:36:55Z","title":"AgentMove: A Large Language Model based Agentic Framework for Zero-shot\n  Next Location Prediction","summary":"  Next location prediction plays a crucial role in various real-world\napplications. Recently, due to the limitation of existing deep learning\nmethods, attempts have been made to apply large language models (LLMs) to\nzero-shot next location prediction task. However, they directly generate the\nfinal output using LLMs without systematic design, which limits the potential\nof LLMs to uncover complex mobility patterns and underestimates their extensive\nreserve of global geospatial knowledge. In this paper, we introduce AgentMove,\na systematic agentic prediction framework to achieve generalized next location\nprediction. In AgentMove, we first decompose the mobility prediction task and\ndesign specific modules to complete them, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge generator for modeling the\neffects of urban structure and collective knowledge extractor for capturing the\nshared patterns among population. Finally, we combine the results of three\nmodules and conduct a reasoning step to generate the final predictions.\nExtensive experiments utilizing mobility data from two distinct sources reveal\nthat AgentMove surpasses the leading baseline by 3.33% to 8.57% across 8 out of\n12 metrics and it shows robust predictions with various LLMs as base and also\nless geographical bias across cities. Our codes are available via\nhttps://github.com/tsinghua-fib-lab/AgentMove.\n","authors":["Jie Feng","Yuwei Du","Jie Zhao","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2408.13986v2.pdf","comment":"Accepted by NAACL 2025 as main conference paper,\n  https://github.com/tsinghua-fib-lab/AgentMove"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.06020v1","updated":"2025-02-09T20:26:30Z","published":"2025-02-09T20:26:30Z","title":"Temporal Working Memory: Query-Guided Segment Refinement for Enhanced\n  Multimodal Understanding","summary":"  Multimodal foundation models (MFMs) have demonstrated significant success in\ntasks such as visual captioning, question answering, and image-text retrieval.\nHowever, these models face inherent limitations due to their finite internal\ncapacity, which restricts their ability to process extended temporal sequences,\na crucial requirement for comprehensive video and audio analysis. To overcome\nthese challenges, we introduce a specialized cognitive module, temporal working\nmemory (TWM), which aims to enhance the temporal modeling capabilities of MFMs.\nIt selectively retains task-relevant information across temporal dimensions,\nensuring that critical details are preserved throughout the processing of video\nand audio content. The TWM uses a query-guided attention approach to focus on\nthe most informative multimodal segments within temporal sequences. By\nretaining only the most relevant content, TWM optimizes the use of the model's\nlimited capacity, enhancing its temporal modeling ability. This plug-and-play\nmodule can be easily integrated into existing MFMs. With our TWM, nine\nstate-of-the-art models exhibit significant performance improvements across\ntasks such as video captioning, question answering, and video-text retrieval.\nBy enhancing temporal modeling, TWM extends the capability of MFMs to handle\ncomplex, time-sensitive data effectively. Our code is available at\nhttps://github.com/xid32/NAACL_2025_TWM.\n","authors":["Xingjian Diao","Chunhui Zhang","Weiyi Wu","Zhongyu Ouyang","Peijun Qing","Ming Cheng","Soroush Vosoughi","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2502.06020v1.pdf","comment":"Accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2502.06012v1","updated":"2025-02-09T20:06:42Z","published":"2025-02-09T20:06:42Z","title":"Speaker Embedding Informed Audiovisual Active Speaker Detection for\n  Egocentric Recordings","summary":"  Audiovisual active speaker detection (ASD) addresses the task of determining\nthe speech activity of a candidate speaker given acoustic and visual data.\nTypically, systems model the temporal correspondence of audiovisual cues, such\nas the synchronisation between speech and lip movement. Recent work has\nexplored extending this paradigm by additionally leveraging speaker embeddings\nextracted from candidate speaker reference speech. This paper proposes the\nspeaker comparison auxiliary network (SCAN) which uses speaker-specific\ninformation from both reference speech and the candidate audio signal to\ndisambiguate challenging scenes when the visual signal is unresolvable.\nFurthermore, an improved method for enrolling face-speaker libraries is\ndeveloped, which implements a self-supervised approach to video-based face\nrecognition. Fitting with the recent proliferation of wearable devices, this\nwork focuses on improving speaker-embedding-informed ASD in the context of\negocentric recordings, which can be characterised by acoustic noise and highly\ndynamic scenes. SCAN is implemented with two well-established baselines, namely\nTalkNet and Light-ASD; yielding a relative improvement in mAP of 14.5% and\n10.3% on the Ego4D benchmark, respectively.\n","authors":["Jason Clarke","Yoshihiko Gotoh","Stefan Goetze"],"pdf_url":"https://arxiv.org/pdf/2502.06012v1.pdf","comment":"Accepted to ICASSP 2025. 5 pages, 4 figures. To appear in Proceedings\n  of IEEE International Conference on Acoustics, Speech and Signal Processing\n  (ICASSP), April 6-11, 2025, Hyderabad, India"},{"id":"http://arxiv.org/abs/2502.05922v1","updated":"2025-02-09T14:32:40Z","published":"2025-02-09T14:32:40Z","title":"A Large-scale Dataset with Behavior, Attributes, and Content of Mobile\n  Short-video Platform","summary":"  Short-video platforms show an increasing impact on people's daily lives\nnowadays, with billions of active users spending plenty of time each day. The\ninteractions between users and online platforms give rise to many scientific\nproblems across computational social science and artificial intelligence.\nHowever, despite the rapid development of short-video platforms, currently\nthere are serious shortcomings in existing relevant datasets on three aspects:\ninadequate user-video feedback, limited user attributes and lack of video\ncontent. To address these problems, we provide a large-scale dataset with rich\nuser behavior, attributes and video content from a real mobile short-video\nplatform. This dataset covers 10,000 voluntary users and 153,561 videos, and we\nconduct four-fold technical validations of the dataset. First, we verify the\nrichness of the behavior and attribute data. Second, we confirm the\nrepresenting ability of the content features. Third, we provide benchmarking\nresults on recommendation algorithms with our dataset. Finally, we explore the\nfilter bubble phenomenon on the platform using the dataset. We believe the\ndataset could support the broad research community, including but not limited\nto user modeling, social science, human behavior understanding, etc. The\ndataset and code is available at\nhttps://github.com/tsinghua-fib-lab/ShortVideo_dataset.\n","authors":["Yu Shang","Chen Gao","Nian Li","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2502.05922v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2502.05863v1","updated":"2025-02-09T11:46:05Z","published":"2025-02-09T11:46:05Z","title":"Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education","summary":"  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.\n","authors":["Yanhao Jia","Xinyi Wu","Hao Li","Qinglin Zhang","Yuxiao Hu","Shuai Zhao","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2502.05863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21144v3","updated":"2025-02-09T02:04:01Z","published":"2024-10-28T15:44:35Z","title":"Enhancing Learned Image Compression via Cross Window-based Attention","summary":"  In recent years, learned image compression methods have demonstrated superior\nrate-distortion performance compared to traditional image compression methods.\nRecent methods utilize convolutional neural networks (CNN), variational\nautoencoders (VAE), invertible neural networks (INN), and transformers. Despite\ntheir significant contributions, a main drawback of these models is their poor\nperformance in capturing local redundancy. Therefore, to leverage global\nfeatures along with local redundancy, we propose a CNN-based solution\nintegrated with a feature encoding module. The feature encoding module encodes\nimportant features before feeding them to the CNN and then utilizes cross-scale\nwindow-based attention, which further captures local redundancy. Cross-scale\nwindow-based attention is inspired by the attention mechanism in transformers\nand effectively enlarges the receptive field. Both the feature encoding module\nand the cross-scale window-based attention module in our architecture are\nflexible and can be incorporated into any other network architecture. We\nevaluate our method on the Kodak and CLIC datasets and demonstrate that our\napproach is effective and on par with state-of-the-art methods.\n","authors":["Priyanka Mudgal","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21144v3.pdf","comment":"Paper accepted and presented in ISVC'24. Copyrights stay with ISVC\n  Our code is available at: https://github.com/prmudgal/CWAM_IC_ISVC"},{"id":"http://arxiv.org/abs/2502.06893v1","updated":"2025-02-09T12:37:48Z","published":"2025-02-09T12:37:48Z","title":"A New Hybrid Intelligent Approach for Multimodal Detection of Suspected\n  Disinformation on TikTok","summary":"  In the context of the rapid dissemination of multimedia content, identifying\ndisinformation on social media platforms such as TikTok represents a\nsignificant challenge. This study introduces a hybrid framework that combines\nthe computational power of deep learning with the interpretability of fuzzy\nlogic to detect suspected disinformation in TikTok videos. The methodology is\ncomprised of two core components: a multimodal feature analyser that extracts\nand evaluates data from text, audio, and video; and a multimodal disinformation\ndetector based on fuzzy logic. These systems operate in conjunction to evaluate\nthe suspicion of spreading disinformation, drawing on human behavioural cues\nsuch as body language, speech patterns, and text coherence. Two experiments\nwere conducted: one focusing on context-specific disinformation and the other\non the scalability of the model across broader topics. For each video\nevaluated, high-quality, comprehensive, well-structured reports are generated,\nproviding a detailed view of the disinformation behaviours.\n","authors":["Jared D. T. Guerrero-Sosa","Andres Montoro-Montarroso","Francisco P. Romero","Jesus Serrano-Guerrero","Jose A. Olivas"],"pdf_url":"https://arxiv.org/pdf/2502.06893v1.pdf","comment":null}]},"2025-02-08T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.05575v1","updated":"2025-02-08T14:03:43Z","published":"2025-02-08T14:03:43Z","title":"Graph-Based Vector Search: An Experimental Evaluation of the\n  State-of-the-Art","summary":"  Vector data is prevalent across business and scientific applications, and its\npopularity is growing with the proliferation of learned embeddings. Vector data\ncollections often reach billions of vectors with thousands of dimensions, thus,\nincreasing the complexity of their analysis. Vector search is the backbone of\nmany critical analytical tasks, and graph-based methods have become the best\nchoice for analytical tasks that do not require guarantees on the quality of\nthe answers. We briefly survey in-memory graph-based vector search, outline the\nchronology of the different methods and classify them according to five main\ndesign paradigms: seed selection, incremental insertion, neighborhood\npropagation, neighborhood diversification, and divide-and-conquer. We conduct\nan exhaustive experimental evaluation of twelve state-of-the-art methods on\nseven real data collections, with sizes up to 1 billion vectors. We share key\ninsights about the strengths and limitations of these methods; e.g., the best\napproaches are typically based on incremental insertion and neighborhood\ndiversification, and the choice of the base graph can hurt scalability.\nFinally, we discuss open research directions, such as the importance of\ndevising more sophisticated data-adaptive seed selection and diversification\nstrategies.\n","authors":["Ilias Azizi","Karima Echihabi","Themis Palpanas"],"pdf_url":"https://arxiv.org/pdf/2502.05575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05561v1","updated":"2025-02-08T13:20:13Z","published":"2025-02-08T13:20:13Z","title":"Diffusion Model for Interest Refinement in Multi-Interest Recommendation","summary":"  Multi-interest candidate matching plays a pivotal role in personalized\nrecommender systems, as it captures diverse user interests from their\nhistorical behaviors. Most existing methods utilize attention mechanisms to\ngenerate interest representations by aggregating historical item embeddings.\nHowever, these methods only capture overall item-level relevance, leading to\ncoarse-grained interest representations that include irrelevant information. To\naddress this issue, we propose the Diffusion Multi-Interest model (DMI), a\nnovel framework for refining user interest representations at the dimension\nlevel. Specifically, DMI first introduces controllable noise into\ncoarse-grained interest representations at the dimensional level. Then, in the\niterative reconstruction process, DMI combines a cross-attention mechanism and\nan item pruning strategy to reconstruct the personalized interest vectors with\nthe guidance of tailored collaborative information. Extensive experiments\ndemonstrate the effectiveness of DMI, surpassing state-of-the-art methods on\noffline evaluations and an online A/B test. Successfully deployed in the\nreal-world recommender system, DMI effectively enhances user satisfaction and\nsystem performance at scale, serving the major traffic of hundreds of millions\nof daily active users. \\footnote{The code will be released for reproducibility\nonce the paper is accepted.}\n","authors":["Yankun Le","Haoran Li","Baoyuan Ou","Yinjie Qing","Zhixuan Yang","Ruilong Su","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.05561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05558v1","updated":"2025-02-08T13:08:11Z","published":"2025-02-08T13:08:11Z","title":"Large Memory Network for Recommendation","summary":"  Modeling user behavior sequences in recommender systems is essential for\nunderstanding user preferences over time, enabling personalized and accurate\nrecommendations for improving user retention and enhancing business values.\nDespite its significance, there are two challenges for current sequential\nmodeling approaches. From the spatial dimension, it is difficult to mutually\nperceive similar users' interests for a generalized intention understanding;\nfrom the temporal dimension, current methods are generally prone to forgetting\nlong-term interests due to the fixed-length input sequence. In this paper, we\npresent Large Memory Network (LMN), providing a novel idea by compressing and\nstoring user history behavior information in a large-scale memory block. With\nthe elaborated online deployment strategy, the memory block can be easily\nscaled up to million-scale in the industry. Extensive offline comparison\nexperiments, memory scaling up experiments, and online A/B test on Douyin\nE-Commerce Search (ECS) are performed, validating the superior performance of\nLMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of\nusers each day.\n","authors":["Hui Lu","Zheng Chai","Yuchao Zheng","Zhe Chen","Deping Xie","Peng Xu","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.05558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03835v2","updated":"2025-02-08T11:13:23Z","published":"2025-01-07T14:45:30Z","title":"TACLR: A Scalable and Efficient Retrieval-based Method for Industrial\n  Product Attribute Value Identification","summary":"  Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendations, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity to the item embedding. It leverages contrastive training with\ntaxonomy-aware hard negative sampling and employs adaptive inference with\ndynamic thresholds. TACLR offers three key advantages: (1) it effectively\nhandles implicit and OOD values while producing normalized outputs; (2) it\nscales to thousands of categories, tens of thousands of attributes, and\nmillions of values; and (3) it supports efficient inference for high-load\nindustrial scenarios. Extensive experiments on proprietary and public datasets\nvalidate the effectiveness and efficiency of TACLR. Moreover, it has been\nsuccessfully deployed in a real-world e-commerce platform, processing millions\nof product listings daily while supporting dynamic, large-scale attribute\ntaxonomies.\n","authors":["Yindu Su","Huike Zou","Lin Sun","Ting Zhang","Haiyang Yang","Liyu Chen","David Lo","Qingheng Zhang","Shuguang Han","Jufeng Chen"],"pdf_url":"https://arxiv.org/pdf/2501.03835v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05523v1","updated":"2025-02-08T11:05:22Z","published":"2025-02-08T11:05:22Z","title":"Adaptive Domain Scaling for Personalized Sequential Modeling in\n  Recommenders","summary":"  Users generally exhibit complex behavioral patterns and diverse intentions in\nmultiple business scenarios of super applications like Douyin, presenting great\nchallenges to current industrial multi-domain recommenders. To mitigate the\ndiscrepancies across diverse domains, researches and industrial practices\ngenerally emphasize sophisticated network structures to accomodate diverse data\ndistributions, while neglecting the inherent understanding of user behavioral\nsequence from the multi-domain perspective. In this paper, we present Adaptive\nDomain Scaling (ADS) model, which comprehensively enhances the personalization\ncapability in target-aware sequence modeling across multiple domains.\nSpecifically, ADS comprises of two major modules, including personalized\nsequence representation generation (PSRG) and personalized candidate\nrepresentation generation (PCRG). The modules contribute to the tailored\nmulti-domain learning by dynamically learning both the user behavioral sequence\nitem representation and the candidate target item representation under\ndifferent domains, facilitating adaptive user intention understanding.\nExperiments are performed on both a public dataset and two billion-scaled\nindustrial datasets, and the extensive results verify the high effectiveness\nand compatibility of ADS. Besides, we conduct online experiments on two\ninfluential business scenarios including Douyin Advertisement Platform and\nDouyin E-commerce Service Platform, both of which show substantial business\nimprovements. Currently, ADS has been fully deployed in many recommendation\nservices at ByteDance, serving billions of users.\n","authors":["Zheng Chai","Hui Lu","Di Chen","Qin Ren","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.05523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00321v2","updated":"2025-02-08T08:39:43Z","published":"2025-02-01T05:06:21Z","title":"MIM: Multi-modal Content Interest Modeling Paradigm for User Behavior\n  Modeling","summary":"  Click-Through Rate (CTR) prediction is a crucial task in recommendation\nsystems, online searches, and advertising platforms, where accurately capturing\nusers' real interests in content is essential for performance. However,\nexisting methods heavily rely on ID embeddings, which fail to reflect users'\ntrue preferences for content such as images and titles. This limitation becomes\nparticularly evident in cold-start and long-tail scenarios, where traditional\napproaches struggle to deliver effective results. To address these challenges,\nwe propose a novel Multi-modal Content Interest Modeling paradigm (MIM), which\nconsists of three key stages: Pre-training, Content-Interest-Aware Supervised\nFine-Tuning (C-SFT), and Content-Interest-Aware UBM (CiUBM). The pre-training\nstage adapts foundational models to domain-specific data, enabling the\nextraction of high-quality multi-modal embeddings. The C-SFT stage bridges the\nsemantic gap between content and user interests by leveraging user behavior\nsignals to guide the alignment of embeddings with user preferences. Finally,\nthe CiUBM stage integrates multi-modal embeddings and ID-based collaborative\nfiltering signals into a unified framework. Comprehensive offline experiments\nand online A/B tests conducted on the Taobao, one of the world's largest\ne-commerce platforms, demonstrated the effectiveness and efficiency of MIM\nmethod. The method has been successfully deployed online, achieving a\nsignificant increase of +14.14% in CTR and +4.12% in RPM, showcasing its\nindustrial applicability and substantial impact on platform performance. To\npromote further research, we have publicly released the code and dataset at\nhttps://pan.quark.cn/s/8fc8ec3e74f3.\n","authors":["Bencheng Yan","Si Chen","Shichang Jia","Jianyu Liu","Yueran Liu","Chenghan Fu","Wanxian Guan","Hui Zhao","Xiang Zhang","Kai Zhang","Wenbo Su","Pengjie Wang","Jian Xu","Bo Zheng","Baolin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.00321v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17245v2","updated":"2025-02-08T05:54:48Z","published":"2024-12-23T03:37:58Z","title":"GraphHash: Graph Clustering Enables Parameter Efficiency in Recommender\n  Systems","summary":"  Deep recommender systems rely heavily on large embedding tables to handle\nhigh-cardinality categorical features such as user/item identifiers, and face\nsignificant memory constraints at scale. To tackle this challenge, hashing\ntechniques are often employed to map multiple entities to the same embedding\nand thus reduce the size of the embedding tables. Concurrently, graph-based\ncollaborative signals have emerged as powerful tools in recommender systems,\nyet their potential for optimizing embedding table reduction remains\nunexplored. This paper introduces GraphHash, the first graph-based approach\nthat leverages modularity-based bipartite graph clustering on user-item\ninteraction graphs to reduce embedding table sizes. We demonstrate that the\nmodularity objective has a theoretical connection to message-passing, which\nprovides a foundation for our method. By employing fast clustering algorithms,\nGraphHash serves as a computationally efficient proxy for message-passing\nduring preprocessing and a plug-and-play graph-based alternative to traditional\nID hashing. Extensive experiments show that GraphHash substantially outperforms\ndiverse hashing baselines on both retrieval and click-through-rate prediction\ntasks. In particular, GraphHash achieves on average a 101.52% improvement in\nrecall when reducing the embedding table size by more than 75%, highlighting\nthe value of graph-based collaborative information for model reduction. Our\ncode is available at https://github.com/snap-research/GraphHash.\n","authors":["Xinyi Wu","Donald Loveland","Runjin Chen","Yozen Liu","Xin Chen","Leonardo Neves","Ali Jadbabaie","Clark Mingxuan Ju","Neil Shah","Tong Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.17245v2.pdf","comment":"ACM Web Conference (WWW) 2025, Oral"},{"id":"http://arxiv.org/abs/2411.13045v2","updated":"2025-02-08T02:56:02Z","published":"2024-11-20T05:30:15Z","title":"Explainable LLM-driven Multi-dimensional Distillation for E-Commerce\n  Relevance Learning","summary":"  Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience.\n","authors":["Gang Zhao","Ximing Zhang","Chenji Lu","Hui Zhao","Tianshu Wu","Pengjie Wang","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.13045v2.pdf","comment":"Accepted by WWW 2025 oral"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.05695v1","updated":"2025-02-08T21:14:28Z","published":"2025-02-08T21:14:28Z","title":"Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models\n  for Wireless Networks","summary":"  This paper proposes a novel framework for real-time adaptive-bitrate video\nstreaming by integrating latent diffusion models (LDMs) within the FFmpeg\ntechniques. This solution addresses the challenges of high bandwidth usage,\nstorage inefficiencies, and quality of experience (QoE) degradation associated\nwith traditional constant bitrate streaming (CBS) and adaptive bitrate\nstreaming (ABS). The proposed approach leverages LDMs to compress I-frames into\na latent space, offering significant storage and semantic transmission savings\nwithout sacrificing high visual quality. While it keeps B-frames and P-frames\nas adjustment metadata to ensure efficient video reconstruction at the user\nside, the proposed framework is complemented with the most state-of-the-art\ndenoising and video frame interpolation (VFI) techniques. These techniques\nmitigate semantic ambiguity and restore temporal coherence between frames, even\nin noisy wireless communication environments. Experimental results demonstrate\nthe proposed method achieves high-quality video streaming with optimized\nbandwidth usage, outperforming state-of-the-art solutions in terms of QoE and\nresource efficiency. This work opens new possibilities for scalable real-time\nvideo streaming in 5G and future post-5G networks.\n","authors":["Zijiang Yan","Jianhua Pei","Hongda Wu","Hina Tabassum","Ping Wang"],"pdf_url":"https://arxiv.org/pdf/2502.05695v1.pdf","comment":"Submission for possible publication"},{"id":"http://arxiv.org/abs/2502.03897v2","updated":"2025-02-08T09:37:13Z","published":"2025-02-06T09:18:30Z","title":"UniForm: A Unified Diffusion Transformer for Audio-Video Generation","summary":"  As a natural multimodal content, audible video delivers an immersive sensory\nexperience. Consequently, audio-video generation systems have substantial\npotential. However, existing diffusion-based studies mainly employ relatively\nindependent modules for generating each modality, which lack exploration of\nshared-weight generative modules. This approach may under-use the intrinsic\ncorrelations between audio and visual modalities, potentially resulting in\nsub-optimal generation quality. To address this, we propose UniForm, a unified\ndiffusion transformer designed to enhance cross-modal consistency. By\nconcatenating auditory and visual information, UniForm learns to generate audio\nand video simultaneously within a unified latent space, facilitating the\ncreation of high-quality and well-aligned audio-visual pairs. Extensive\nexperiments demonstrate the superior performance of our method in joint\naudio-video generation, audio-guided video generation, and video-guided audio\ngeneration tasks. Our demos are available at https://uniform-t2av.github.io/.\n","authors":["Lei Zhao","Linfeng Feng","Dongxu Ge","Fangqiu Yi","Chi Zhang","Xiao-Lei Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2502.03897v2.pdf","comment":"Our demos are available at https://uniform-t2av.github.io/"},{"id":"http://arxiv.org/abs/2403.08505v5","updated":"2025-02-08T09:33:17Z","published":"2024-03-13T13:12:57Z","title":"CAMSIC: Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed. Code\nis available at https://github.com/Xinjie-Q/CAMSIC.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Jiawei Shao","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v5.pdf","comment":"Accepted by AAAI 2025"}]},"2025-02-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.02749v3","updated":"2025-02-11T18:59:47Z","published":"2024-10-03T17:57:22Z","title":"Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis","summary":"  Software engineers mainly write code by editing existing programs. In\ncontrast, language models (LMs) autoregressively synthesize programs in a\nsingle pass. One explanation for this is the scarcity of sequential edit data.\nWhile high-quality instruction data for code synthesis is scarce, edit data for\nsynthesis is even scarcer. To fill this gap, we develop a synthetic data\ngeneration algorithm called LintSeq. This algorithm refactors programs into\nsequences of synthetic edits by using a linter to procedurally sample across\ninterdependent lines of source code. Synthetic edits sampled with LintSeq\nreflect the syntax and semantics of their programming language. To test the\nalgorithm, we use it to refactor a dataset of instruction + program pairs into\ninstruction + program-diff-sequence tuples. Then, we fine-tune a series of\nsmaller LMs ranging from 2.6B to 14B parameters on both the re-factored and\noriginal versions of this dataset. We perform comprehensive evaluations\ncomparing edit sequence code LMs against baselines on HumanEval, MBPP(+),\nCodeContests, DS-1000, and BigCodeBench. We show that models fine-tuned to\niteratively synthesize code match or outperform baselines on pass@1, and\nexhibit better scaling across higher pass@k as a function of total test-time\nFLOPs. Finally, we also pretrain our own tiny LMs for code understanding. We\nshow that fine-tuning these models to synthesize code edit-by-edit results in\nstrong performance on HumanEval and MBPP(+) compared to existing code language\nmodels of similar scale such as CodeT5+, AlphaCode, and Codex.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2410.02749v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07780v1","updated":"2025-02-11T18:59:35Z","published":"2025-02-11T18:59:35Z","title":"DarwinLM: Evolutionary Structured Pruning of Large Language Models","summary":"  Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training.\n","authors":["Shengkun Tang","Oliver Sieberling","Eldar Kurtic","Zhiqiang Shen","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2502.07780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08446v2","updated":"2025-02-11T18:59:26Z","published":"2024-06-12T17:37:09Z","title":"OLMES: A Standard for Language Model Evaluations","summary":"  Progress in AI is often demonstrated by new models claiming improved\nperformance on tasks measuring model capabilities. Evaluating language models\ncan be particularly challenging, as choices of how a model is evaluated on a\ntask can lead to large changes in measured performance. There is no common\nstandard setup, so different models are evaluated on the same tasks in\ndifferent ways, leading to claims about which models perform best not being\nreproducible. We propose OLMES, a completely documented, practical, open\nstandard for reproducible LLM evaluations. In developing this standard, we\nidentify and review the varying factors in evaluation practices adopted by the\ncommunity - such as details of prompt formatting, choice of in-context\nexamples, probability normalizations, and task formulation. In particular,\nOLMES supports meaningful comparisons between smaller base models that require\nthe unnatural \"cloze\" formulation of multiple-choice questions against larger\nmodels that can utilize the original formulation. OLMES includes\nwell-considered, documented recommendations guided by results from existing\nliterature as well as new experiments resolving open questions.\n","authors":["Yuling Gu","Oyvind Tafjord","Bailey Kuehl","Dany Haddad","Jesse Dodge","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2406.08446v2.pdf","comment":"Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07776v1","updated":"2025-02-11T18:58:04Z","published":"2025-02-11T18:58:04Z","title":"Auditing Prompt Caching in Language Model APIs","summary":"  Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.\n","authors":["Chenchen Gu","Xiang Lisa Li","Rohith Kuditipudi","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2502.07776v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.07771v1","updated":"2025-02-11T18:55:57Z","published":"2025-02-11T18:55:57Z","title":"Breaking Down Bias: On The Limits of Generalizable Pruning Strategies","summary":"  We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case.\n","authors":["Sibo Ma","Alejandro Salinas","Peter Henderson","Julian Nyarko"],"pdf_url":"https://arxiv.org/pdf/2502.07771v1.pdf","comment":"28 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.01130v3","updated":"2025-02-11T18:55:04Z","published":"2024-07-01T09:51:48Z","title":"Cross-Lingual Transfer Learning for Speech Translation","summary":"  There has been increasing interest in building multilingual foundation models\nfor NLP and speech research. This paper examines how to expand the speech\ntranslation capability of these models with restricted data. Whisper, a speech\nfoundation model with strong performance on speech recognition and English\ntranslation, is used as the example model. Using speech-to-speech retrieval to\nanalyse the audio representations generated by the encoder, we show that\nutterances from different languages are mapped to a shared semantic space. This\nshared embedding space can then be leveraged for zero-shot cross-lingual\ntransfer in speech translation. By fine-tuning the Whisper decoder with only\nEnglish-to-Chinese speech translation data, improved performance for\ntranslation to Chinese can be obtained for multiple languages, in addition to\nEnglish. Furthermore, for languages related to those seen in training it is\npossible to perform speech translation, despite the model never seeing the\nlanguage in training, or being able to perform transcription.\n","authors":["Rao Ma","Mengjie Qian","Yassir Fathullah","Siyuan Tang","Mark Gales","Kate Knill"],"pdf_url":"https://arxiv.org/pdf/2407.01130v3.pdf","comment":"Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2406.08598v3","updated":"2025-02-11T18:42:44Z","published":"2024-06-12T19:05:43Z","title":"Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks","summary":"  As Large Language Models (LLMs) continue to evolve, evaluating them remains a\npersistent challenge. Many recent evaluations use LLMs as judges to score\noutputs from other LLMs, often relying on a single large model like GPT-4o.\nHowever, using a single LLM judge is prone to intra-model bias, and many tasks\n- such as those related to emotional intelligence, creative writing, and\npersuasiveness - may be too subjective for a single model to judge fairly. We\nintroduce the Language Model Council (LMC), where a group of LLMs collaborate\nto create tests, respond to them, and evaluate each other's responses to\nproduce a ranking in a democratic fashion. Unlike previous approaches that\nfocus on reducing cost or bias by using a panel of smaller models, our work\nexamines the benefits and nuances of a fully inclusive LLM evaluation system.\nIn a detailed case study on emotional intelligence, we deploy a council of 20\nrecent LLMs to rank each other on open-ended responses to interpersonal\nconflicts. Our results show that the LMC produces rankings that are more\nseparable and more robust, and through a user study, we show that they are more\nconsistent with human evaluations than any individual LLM judge. Using all LLMs\nfor judging can be costly, however, so we use Monte Carlo simulations and\nhand-curated sub-councils to study hypothetical council compositions and\ndiscuss the value of the incremental LLM judge.\n","authors":["Justin Zhao","Flor Miriam Plaza-del-Arco","Benjie Genchel","Amanda Cercas Curry"],"pdf_url":"https://arxiv.org/pdf/2406.08598v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07755v1","updated":"2025-02-11T18:32:24Z","published":"2025-02-11T18:32:24Z","title":"An Advanced NLP Framework for Automated Medical Diagnosis with DeBERTa\n  and Dynamic Contextual Positional Gating","summary":"  This paper presents a novel Natural Language Processing (NLP) framework for\nenhancing medical diagnosis through the integration of advanced techniques in\ndata augmentation, feature extraction, and classification. The proposed\napproach employs back-translation to generate diverse paraphrased datasets,\nimproving robustness and mitigating overfitting in classification tasks.\nLeveraging Decoding-enhanced BERT with Disentangled Attention (DeBERTa) with\nDynamic Contextual Positional Gating (DCPG), the model captures fine-grained\ncontextual and positional relationships, dynamically adjusting the influence of\npositional information based on semantic context to produce high-quality text\nembeddings. For classification, an Attention-Based Feedforward Neural Network\n(ABFNN) is utilized, effectively focusing on the most relevant features to\nimprove decision-making accuracy. Applied to the classification of symptoms,\nclinical notes, and other medical texts, this architecture demonstrates its\nability to address the complexities of medical data. The combination of data\naugmentation, contextual embedding generation, and advanced classification\nmechanisms offers a robust and accurate diagnostic tool, with potential\napplications in automated medical diagnosis and clinical decision support. This\nmethod demonstrates the effectiveness of the proposed NLP framework for medical\ndiagnosis, achieving remarkable results with an accuracy of 99.78%, recall of\n99.72%, precision of 99.79%, and an F1-score of 99.75%. These metrics not only\nunderscore the model's robust performance in classifying medical texts with\nexceptional precision and reliability but also highlight its superiority over\nexisting methods, making it a highly promising tool for automated diagnostic\nsystems.\n","authors":["Mohammad Ali Labbaf Khaniki","Sahabeh Saadati","Mohammad Manthouri"],"pdf_url":"https://arxiv.org/pdf/2502.07755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09401v2","updated":"2025-02-11T18:18:59Z","published":"2024-02-14T18:58:40Z","title":"Reinforcement Learning from Human Feedback with Active Queries","summary":"  Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method.\n","authors":["Kaixuan Ji","Jiafan He","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2402.09401v2.pdf","comment":"28 pages, 1 figure, 4 table"},{"id":"http://arxiv.org/abs/2407.05502v3","updated":"2025-02-11T18:17:53Z","published":"2024-07-07T21:26:36Z","title":"Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models","summary":"  Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views.\n","authors":["Nikhil Sharma","Kenton Murray","Ziang Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.05502v3.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07747v1","updated":"2025-02-11T18:14:44Z","published":"2025-02-11T18:14:44Z","title":"WHODUNIT: Evaluation benchmark for culprit detection in mystery stories","summary":"  We present a novel data set, WhoDunIt, to assess the deductive reasoning\ncapabilities of large language models (LLM) within narrative contexts.\nConstructed from open domain mystery novels and short stories, the dataset\nchallenges LLMs to identify the perpetrator after reading and comprehending the\nstory. To evaluate model robustness, we apply a range of character-level name\naugmentations, including original names, name swaps, and substitutions with\nwell-known real and/or fictional entities from popular discourse. We further\nuse various prompting styles to investigate the influence of prompting on\ndeductive reasoning accuracy.\n  We conduct evaluation study with state-of-the-art models, specifically\nGPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with\nmajority response selection to ensure reliability. The results demonstrate that\nwhile LLMs perform reliably on unaltered texts, accuracy diminishes with\ncertain name substitutions, particularly those with wide recognition. This\ndataset is publicly available here.\n","authors":["Kshitij Gupta"],"pdf_url":"https://arxiv.org/pdf/2502.07747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04463v2","updated":"2025-02-11T18:06:02Z","published":"2025-02-06T19:18:16Z","title":"Training Language Models to Reason Efficiently","summary":"  Scaling model size and training data has led to great advances in the\nperformance of Large Language Models (LLMs). However, the diminishing returns\nof this approach necessitate alternative methods to improve model capabilities,\nparticularly in tasks requiring advanced reasoning. Large reasoning models,\nwhich leverage long chain-of-thoughts, bring unprecedented breakthroughs in\nproblem-solving capabilities but at a substantial deployment cost associated to\nlonger generations. Reducing inference costs is crucial for the economic\nfeasibility, user experience, and environmental sustainability of these models.\n  In this work, we propose to train large reasoning models to reason\nefficiently. More precisely, we use reinforcement learning (RL) to train\nreasoning models to dynamically allocate inference-time compute based on task\ncomplexity. Our method incentivizes models to minimize unnecessary\ncomputational overhead while maintaining accuracy, thereby achieving\nsubstantial efficiency gains. It enables the derivation of a family of\nreasoning models with varying efficiency levels, controlled via a single\nhyperparameter. Experiments on two open-weight large reasoning models\ndemonstrate significant reductions in inference cost while preserving most of\nthe accuracy.\n","authors":["Daman Arora","Andrea Zanette"],"pdf_url":"https://arxiv.org/pdf/2502.04463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07732v1","updated":"2025-02-11T17:51:52Z","published":"2025-02-11T17:51:52Z","title":"Economics of Sourcing Human Data","summary":"  Progress in AI has relied on human-generated data, from annotator\nmarketplaces to the wider Internet. However, the widespread use of large\nlanguage models now threatens the quality and integrity of human-generated data\non these very platforms. We argue that this issue goes beyond the immediate\nchallenge of filtering AI-generated content--it reveals deeper flaws in how\ndata collection systems are designed. Existing systems often prioritize speed,\nscale, and efficiency at the cost of intrinsic human motivation, leading to\ndeclining engagement and data quality. We propose that rethinking data\ncollection systems to align with contributors' intrinsic motivations--rather\nthan relying solely on external incentives--can help sustain high-quality data\nsourcing at scale while maintaining contributor trust and long-term\nparticipation.\n","authors":["Sebastin Santy","Prasanta Bhattacharya","Manoel Horta Ribeiro","Kelsey Allen","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2502.07732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16527v2","updated":"2025-02-11T17:40:41Z","published":"2024-03-25T08:11:02Z","title":"Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art","summary":"  Autonomous systems are soon to be ubiquitous, spanning manufacturing,\nagriculture, healthcare, entertainment, and other industries. Most of these\nsystems are developed with modular sub-components for decision-making,\nplanning, and control that may be hand-engineered or learning-based. While\nthese approaches perform well under the situations they were specifically\ndesigned for, they can perform especially poorly in out-of-distribution\nscenarios that will undoubtedly arise at test-time. The rise of foundation\nmodels trained on multiple tasks with impressively large datasets has led\nresearchers to believe that these models may provide \"common sense\" reasoning\nthat existing planners are missing, bridging the gap between algorithm\ndevelopment and deployment. While researchers have shown promising results in\ndeploying foundation models to decision-making tasks, these models are known to\nhallucinate and generate decisions that may sound reasonable, but are in fact\npoor. We argue there is a need to step back and simultaneously design systems\nthat can quantify the certainty of a model's decision, and detect when it may\nbe hallucinating. In this work, we discuss the current use cases of foundation\nmodels for decision-making tasks, provide a general definition for\nhallucinations with examples, discuss existing approaches to hallucination\ndetection and mitigation with a focus on decision problems, present guidelines,\nand explore areas for further research in this exciting field.\n","authors":["Neeloy Chakraborty","Melkior Ornik","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2403.16527v2.pdf","comment":"Accepted to ACM Computing Surveys; 55 pages, 5 tables, 3 figures"},{"id":"http://arxiv.org/abs/2410.10868v2","updated":"2025-02-11T17:31:55Z","published":"2024-10-08T11:24:59Z","title":"Large Continual Instruction Assistant","summary":"  Continual Instruction Tuning (CIT) is adopted to continually instruct Large\nModels to follow human intent data by data. It is observed that existing\ngradient update would heavily destroy the performance on previous datasets\nduring CIT process. Instead, Exponential Moving Average (EMA), owns the ability\nto trace previous parameters, which can aid in decreasing forgetting.\nNonetheless, its stable balance weight fails to deal with the ever-changing\ndatasets, leading to the out-of-balance between plasticity and stability. In\nthis paper, we propose a general continual instruction tuning framework to\naddress the challenge. Starting from the trade-off prerequisite and EMA update,\nwe propose the plasticity and stability ideal condition. Based on Taylor\nexpansion in the loss function, we find the optimal balance weight can be\nautomatically determined by the gradients and learned parameters. Therefore, we\npropose a stable-plasticity balanced coefficient to avoid knowledge confusion.\nBased on the semantic similarity of the instructions, we can determine whether\nto retrain or expand the training parameters and allocate the most suitable\nparameters for the testing instances. Extensive experiments across multiple\ncontinual instruction tuning benchmarks demonstrate that our approach not only\nenhances anti-forgetting capabilities but also significantly improves overall\ncontinual tuning performance. For example, based on LLaVA-7B, the forgetting is\nreduced from 5.42 to 1.93. Our code will be made publicly available soon.\n","authors":["Jingyang Qiao","Zhizhong Zhang","Xin Tan","Yanyun Qu","Shouhong Ding","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2410.10868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18922v3","updated":"2025-02-11T17:23:13Z","published":"2024-04-29T17:58:30Z","title":"DPO Meets PPO: Reinforced Token Optimization for RLHF","summary":"  In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nlarge language models, its open-source implementation is still largely\nsub-optimal. To address these issues, we introduce a framework that models RLHF\nproblems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Under this framework, we introduce an\nalgorithm Reinforced Token Optimization (\\texttt{RTO}), which learns the\ntoken-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive experiments\ndemonstrate that \\texttt{RTO} performs better than PPO and other direct\npreference learning algorithms. In particular, RTO outperforms PPO by 7.5\npoints on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code\nand models are available at\n\\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.\n","authors":["Han Zhong","Zikang Shan","Guhao Feng","Wei Xiong","Xinle Cheng","Li Zhao","Di He","Jiang Bian","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.18922v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07717v1","updated":"2025-02-11T17:18:47Z","published":"2025-02-11T17:18:47Z","title":"Making Language Models Robust Against Negation","summary":"  Negation has been a long-standing challenge for language models. Previous\nstudies have shown that they struggle with negation in many natural language\nunderstanding tasks. In this work, we propose a self-supervised method to make\nlanguage models more robust against negation. We introduce a novel task, Next\nSentence Polarity Prediction (NSPP), and a variation of the Next Sentence\nPrediction (NSP) task. We show that BERT and RoBERTa further pre-trained on our\ntasks outperform the off-the-shelf versions on nine negation-related\nbenchmarks. Most notably, our pre-training tasks yield between 1.8% and 9.1%\nimprovement on CondaQA, a large question-answering corpus requiring reasoning\nover negation.\n","authors":["MohammadHossein Rezaei","Eduardo Blanco"],"pdf_url":"https://arxiv.org/pdf/2502.07717v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2408.06663v4","updated":"2025-02-11T16:57:29Z","published":"2024-08-13T06:28:43Z","title":"Amuro and Char: Analyzing the Relationship between Pre-Training and\n  Fine-Tuning of Large Language Models","summary":"  The development of large language models leads to the formation of a\npre-train-then-align paradigm, in which the model is typically pre-trained on a\nlarge text corpus and undergoes a tuning stage to align the model with human\npreference or downstream tasks. In this work, we investigate the relationship\nbetween pre-training and fine-tuning by fine-tuning multiple intermediate\npre-trained model checkpoints. Our results on 18 datasets suggest that i)\ncontinual pre-training improves the model in a latent way that unveils after\nfine-tuning; ii) with extra fine-tuning, the datasets that the model does not\ndemonstrate capability gain much more than those that the model performs well\nduring the pre-training stage; iii) although model benefits significantly\nthrough supervised fine-tuning, it may forget previously known domain knowledge\nand the tasks that are not seen during fine-tuning; iv) the model resembles\nhigh sensitivity to evaluation prompts after supervised fine-tuning, but this\nsensitivity can be alleviated by more pre-training.\n","authors":["Kaiser Sun","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2408.06663v4.pdf","comment":"Updated Draft"},{"id":"http://arxiv.org/abs/2406.12009v3","updated":"2025-02-11T16:49:17Z","published":"2024-06-17T18:25:02Z","title":"FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial\n  Information Disclosure","summary":"  Accurate and transparent financial information disclosure is essential in\naccounting and finance, fostering trust and enabling informed investment\ndecisions that drive economic development. Among many information disclosure\nplatforms, the Chinese stock exchanges' investor interactive platform provides\na novel and interactive way for listed firms to disclose information of\ninterest to investors through an online question-and-answer (Q&A) format.\nHowever, it is common for listed firms to respond to questions with limited or\nno substantive information, and automatically evaluating the quality of\nfinancial information disclosure on large amounts of Q&A pairs is challenging.\nIn this study, our interdisciplinary team of AI and finance professionals\nproposed FinTruthQA, a benchmark designed to evaluate advanced natural language\nprocessing (NLP) techniques for the automatic quality assessment of information\ndisclosure in financial Q&A data. It comprises 6,000 real-world financial Q&A\nentries and each Q&A was manually annotated based on four key evaluation\ncriteria. We benchmarked various NLP techniques on FinTruthQA, including large\nlanguage models(LLMs). Experiments showed that existing NLP models have strong\npredictive ability for question identification and question relevance tasks,\nbut are suboptimal for answer readability and answer relevance tasks. By\nestablishing this benchmark, we provide a robust foundation for the automatic\nevaluation of information disclosure, demonstrating how AI can be leveraged for\nsocial good by promoting transparency, fairness, and investor protection in\nfinancial disclosure practices. FinTruthQA can be used by auditors, regulators,\nand financial analysts for real-time monitoring and data-driven\ndecision-making, as well as by researchers for advanced studies in accounting\nand finance, ultimately fostering greater trust and efficiency in the financial\nmarkets.\n","authors":["Ziyue Xu","Peilin Zhou","Xinyu Shi","Jiageng Wu","Yikang Jiang","Dading Chong","Bin Ke","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2406.12009v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13921v3","updated":"2025-02-11T16:48:15Z","published":"2025-01-23T18:59:02Z","title":"The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities","summary":"  Llama-Breeze2 (hereinafter referred to as Breeze2) is a suite of advanced\nmulti-modal language models, available in 3B and 8B parameter configurations,\nspecifically designed to enhance Traditional Chinese language representation.\nBuilding upon the Llama 3.2 model family, we continue the pre-training of\nBreeze2 on an extensive corpus to enhance the linguistic and cultural heritage\nof Traditional Chinese. In addition to language modeling capabilities, we\nsignificantly augment the models with function calling and vision understanding\ncapabilities. At the time of this publication, as far as we are aware, absent\nreasoning-inducing prompts, Breeze2 are the strongest performing models in\nTraditional Chinese function calling and image understanding in its size class.\nThe effectiveness of Breeze2 is benchmarked across various tasks, including\nTaiwan general knowledge, instruction-following, long context, function\ncalling, and vision understanding. We are publicly releasing all Breeze2 models\nunder the Llama 3.2 Community License. We also showcase the capabilities of the\nmodel running on mobile platform with a mobile application which we also open\nsource.\n","authors":["MediaTek Research"," :","Chan-Jan Hsu","Chia-Sheng Liu","Meng-Hsi Chen","Muxi Chen","Po-Chun Hsu","Yi-Chang Chen","Da-Shan Shiu"],"pdf_url":"https://arxiv.org/pdf/2501.13921v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00507v2","updated":"2025-02-11T16:39:55Z","published":"2025-02-01T17:55:58Z","title":"A statistically consistent measure of Semantic Variability using\n  Language Models","summary":"  To address the issue of variability in the output generated by a language\nmodel, we present a measure of semantic variability that is statistically\nconsistent under mild assumptions. This measure, denoted as semantic spectral\nentropy, is a easy to implement algorithm that requires just off the shelf\nlanguage models. We put very few restrictions on the language models and we\nhave shown in a clear simulation studies that such method can generate accurate\nmetric despite randomness that arise from the language models.\n","authors":["Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2502.00507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07687v1","updated":"2025-02-11T16:38:16Z","published":"2025-02-11T16:38:16Z","title":"Large Language Models as Proxies for Theories of Human Linguistic\n  Cognition","summary":"  We consider the possible role of current large language models (LLMs) in the\nstudy of human linguistic cognition. We focus on the use of such models as\nproxies for theories of cognition that are relatively linguistically-neutral in\ntheir representations and learning but differ from current LLMs in key ways. We\nillustrate this potential use of LLMs as proxies for theories of cognition in\nthe context of two kinds of questions: (a) whether the target theory accounts\nfor the acquisition of a given pattern from a given corpus; and (b) whether the\ntarget theory makes a given typologically-attested pattern easier to acquire\nthan another, typologically-unattested pattern. For each of the two questions\nwe show, building on recent literature, how current LLMs can potentially be of\nhelp, but we note that at present this help is quite limited.\n","authors":["Imry Ziv","Nur Lan","Emmanuel Chemla","Roni Katzir"],"pdf_url":"https://arxiv.org/pdf/2502.07687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12645v3","updated":"2025-02-11T16:36:32Z","published":"2024-06-18T14:13:13Z","title":"Evaluating Evidence Attribution in Generated Fact Checking Explanations","summary":"  Automated fact-checking systems often struggle with trustworthiness, as their\ngenerated explanations can include hallucinations. In this work, we explore\nevidence attribution for fact-checking explanation generation. We introduce a\nnovel evaluation protocol -- citation masking and recovery -- to assess\nattribution quality in generated explanations. We implement our protocol using\nboth human annotators and automatic annotators, and find that LLM annotation\ncorrelates with human annotation, suggesting that attribution assessment can be\nautomated. Finally, our experiments reveal that: (1) the best-performing LLMs\nstill generate explanations with inaccurate attributions; and (2) human-curated\nevidence is essential for generating better explanations. Code and data are\navailable here: https://github.com/ruixing76/Transparent-FCExp.\n","authors":["Rui Xing","Timothy Baldwin","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2406.12645v3.pdf","comment":"Accepted to NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2502.07683v1","updated":"2025-02-11T16:35:04Z","published":"2025-02-11T16:35:04Z","title":"exHarmony: Authorship and Citations for Benchmarking the Reviewer\n  Assignment Problem","summary":"  The peer review process is crucial for ensuring the quality and reliability\nof scholarly work, yet assigning suitable reviewers remains a significant\nchallenge. Traditional manual methods are labor-intensive and often\nineffective, leading to nonconstructive or biased reviews. This paper\nintroduces the exHarmony (eHarmony but for connecting experts to manuscripts)\nbenchmark, designed to address these challenges by re-imagining the Reviewer\nAssignment Problem (RAP) as a retrieval task. Utilizing the extensive data from\nOpenAlex, we propose a novel approach that considers a host of signals from the\nauthors, most similar experts, and the citation relations as potential\nindicators for a suitable reviewer for a manuscript. This approach allows us to\ndevelop a standard benchmark dataset for evaluating the reviewer assignment\nproblem without needing explicit labels. We benchmark various methods,\nincluding traditional lexical matching, static neural embeddings, and\ncontextualized neural embeddings, and introduce evaluation metrics that assess\nboth relevance and diversity in the context of RAP. Our results indicate that\nwhile traditional methods perform reasonably well, contextualized embeddings\ntrained on scholarly literature show the best performance. The findings\nunderscore the importance of further research to enhance the diversity and\neffectiveness of reviewer assignments.\n","authors":["Sajad Ebrahimi","Sara Salamat","Negar Arabzadeh","Mahdi Bashari","Ebrahim Bagheri"],"pdf_url":"https://arxiv.org/pdf/2502.07683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07677v1","updated":"2025-02-11T16:27:28Z","published":"2025-02-11T16:27:28Z","title":"Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered\n  LLM Approach","summary":"  Achieving a delicate balance between fostering trust in law en- forcement and\nprotecting the rights of both officers and civilians continues to emerge as a\npressing research and product challenge in the world today. In the pursuit of\nfairness and transparency, this study presents an innovative AI-driven system\ndesigned to generate police report drafts from complex, noisy, and multi-role\ndialogue data. Our approach intelligently extracts key elements of law\nenforcement interactions and includes them in the draft, producing structured\nnarratives that are not only high in quality but also reinforce accountability\nand procedural clarity. This frame- work holds the potential to transform the\nreporting process, ensur- ing greater oversight, consistency, and fairness in\nfuture policing practices. A demonstration video of our system can be accessed\nat https://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-\nY-kpCHNO/view?usp=sharing\n","authors":["Param Kulkarni","Yingchi Liu","Hao-Ming Fu","Shaohua Yang","Isuru Gunasekara","Matt Peloquin","Noah Spitzer-Williams","Xiaotian Zhou","Xiaozhong Liu","Zhengping Ji","Yasser Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2502.07677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05907v2","updated":"2025-02-11T16:22:45Z","published":"2024-09-06T15:47:40Z","title":"Programming Refusal with Conditional Activation Steering","summary":"  LLMs have shown remarkable capabilities, but precisely controlling their\nresponse behavior remains challenging. Existing activation steering methods\nalter LLM behavior indiscriminately, limiting their practical applicability in\nsettings where selective responses are essential, such as content moderation or\ndomain-specific assistants. In this paper, we propose Conditional Activation\nSteering (CAST), which analyzes LLM activation patterns during inference to\nselectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts\nactivate distinct patterns in the model's hidden states. Using CAST, one can\nsystematically control LLM behavior with rules like \"if input is about hate\nspeech or adult content, then refuse\" or \"if input is not about legal advice,\nthen refuse.\" This allows for selective modification of responses to specific\ncontent while maintaining normal responses to other content, all without\nrequiring weight optimization. We release an open-source implementation of our\nframework at <github.com/IBM/activation-steering>.\n","authors":["Bruce W. Lee","Inkit Padhi","Karthikeyan Natesan Ramamurthy","Erik Miehling","Pierre Dognin","Manish Nagireddy","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2409.05907v2.pdf","comment":"ICLR 2025, Spotlight"},{"id":"http://arxiv.org/abs/2402.11355v5","updated":"2025-02-11T16:03:35Z","published":"2024-02-17T18:12:02Z","title":"A Practical Method for Generating String Counterfactuals","summary":"  Interventions targeting the representation space of language models (LMs)\nhave emerged as an effective means to influence model behavior. Such methods\nare employed, for example, to eliminate or alter the encoding of demographic\ninformation such as gender within the model's representations and, in so doing,\ncreate a counterfactual representation. However, because the intervention\noperates within the representation space, understanding precisely what aspects\nof the text it modifies poses a challenge. In this paper, we give a method to\nconvert representation counterfactuals into string counterfactuals. We\ndemonstrate that this approach enables us to analyze the linguistic alterations\ncorresponding to a given representation space intervention and to interpret the\nfeatures utilized to encode a specific concept. Moreover, the resulting\ncounterfactuals can be used to mitigate bias in classification through data\naugmentation.\n","authors":["Matan Avitan","Ryan Cotterell","Yoav Goldberg","Shauli Ravfogel"],"pdf_url":"https://arxiv.org/pdf/2402.11355v5.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.05670v2","updated":"2025-02-11T16:02:57Z","published":"2025-02-08T19:13:40Z","title":"Language Models Largely Exhibit Human-like Constituent Ordering\n  Preferences","summary":"  Though English sentences are typically inflexible vis-\\`a-vis word order,\nconstituents often show far more variability in ordering. One prominent theory\npresents the notion that constituent ordering is directly correlated with\nconstituent weight: a measure of the constituent's length or complexity. Such\ntheories are interesting in the context of natural language processing (NLP),\nbecause while recent advances in NLP have led to significant gains in the\nperformance of large language models (LLMs), much remains unclear about how\nthese models process language, and how this compares to human language\nprocessing. In particular, the question remains whether LLMs display the same\npatterns with constituent movement, and may provide insights into existing\ntheories on when and how the shift occurs in human language. We compare a\nvariety of LLMs with diverse properties to evaluate broad LLM performance on\nfour types of constituent movement: heavy NP shift, particle movement, dative\nalternation, and multiple PPs. Despite performing unexpectedly around particle\nmovement, LLMs generally align with human preferences around constituent\nordering.\n","authors":["Ada Defne Tur","Gaurav Kamath","Siva Reddy"],"pdf_url":"https://arxiv.org/pdf/2502.05670v2.pdf","comment":"NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.07663v1","updated":"2025-02-11T15:56:22Z","published":"2025-02-11T15:56:22Z","title":"Human Decision-making is Susceptible to AI-driven Manipulation","summary":"  Artificial Intelligence (AI) systems are increasingly intertwined with daily\nlife, assisting users in executing various tasks and providing guidance on\ndecision-making. This integration introduces risks of AI-driven manipulation,\nwhere such systems may exploit users' cognitive biases and emotional\nvulnerabilities to steer them toward harmful outcomes. Through a randomized\ncontrolled trial with 233 participants, we examined human susceptibility to\nsuch manipulation in financial (e.g., purchases) and emotional (e.g., conflict\nresolution) decision-making contexts. Participants interacted with one of three\nAI agents: a neutral agent (NA) optimizing for user benefit without explicit\ninfluence, a manipulative agent (MA) designed to covertly influence beliefs and\nbehaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit\npsychological tactics to reach its hidden objectives. By analyzing\nparticipants' decision patterns and shifts in their preference ratings\npost-interaction, we found significant susceptibility to AI-driven\nmanipulation. Particularly, across both decision-making domains, participants\ninteracting with the manipulative agents shifted toward harmful options at\nsubstantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA:\n42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional,\n12.8%). Notably, our findings reveal that even subtle manipulative objectives\n(MA) can be as effective as employing explicit psychological strategies (SEMA)\nin swaying human decision-making. By revealing the potential for covert AI\ninfluence, this study highlights a critical vulnerability in human-AI\ninteractions, emphasizing the need for ethical safeguards and regulatory\nframeworks to ensure responsible deployment of AI technologies and protect\nhuman autonomy.\n","authors":["Sahand Sabour","June M. Liu","Siyang Liu","Chris Z. Yao","Shiyao Cui","Xuanming Zhang","Wen Zhang","Yaru Cao","Advait Bhat","Jian Guan","Wei Wu","Rada Mihalcea","Tim Althoff","Tatia M. C. Lee","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07663v1.pdf","comment":"Work in progress. Code and data will be made available via\n  https://github.com/Sahandfer/Manipulation-Susceptibility"},{"id":"http://arxiv.org/abs/2502.06556v2","updated":"2025-02-11T15:48:42Z","published":"2025-02-10T15:24:30Z","title":"ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms","summary":"  Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant simple errors,\nincluding compilation and cascade errors. Motivated by this observation, we\nfurther evaluate all frontier LLMs under manual error-fixing and\nself-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms.\n","authors":["Yibo Wang","Congying Xia","Wenting Zhao","Jiangshu Du","Chunyu Miao","Zhongfen Deng","Philip S. Yu","Chen Xing"],"pdf_url":"https://arxiv.org/pdf/2502.06556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05878v2","updated":"2025-02-11T15:45:52Z","published":"2025-02-09T12:26:05Z","title":"Enhancing Financial Time-Series Forecasting with Retrieval-Augmented\n  Large Language Models","summary":"  Stock movement prediction, a critical task in financial time-series\nforecasting, relies on identifying and retrieving key influencing factors from\nvast and complex datasets. However, traditional text-trained or numeric\nsimilarity-based retrieval methods often struggle to handle the intricacies of\nfinancial data. To address this, we propose the first retrieval-augmented\ngeneration (RAG) framework specifically designed for financial time-series\nforecasting. Our framework incorporates three key innovations: a fine-tuned 1B\nlarge language model (StockLLM) as its backbone, a novel candidate selection\nmethod enhanced by LLM feedback, and a training objective that maximizes the\nsimilarity between queries and historically significant sequences. These\nadvancements enable our retriever, FinSeer, to uncover meaningful patterns\nwhile effectively minimizing noise in complex financial datasets. To support\nrobust evaluation, we also construct new datasets that integrate financial\nindicators and historical stock prices. Experimental results demonstrate that\nour RAG framework outperforms both the baseline StockLLM and random retrieval\nmethods, showcasing its effectiveness. FinSeer, as the retriever, achieves an\n8% higher accuracy on the BIGDATA22 benchmark and retrieves more impactful\nsequences compared to existing retrieval methods. This work highlights the\nimportance of tailored retrieval models in financial forecasting and provides a\nnovel, scalable framework for future research in the field.\n","authors":["Mengxi Xiao","Zihao Jiang","Lingfei Qian","Zhengyu Chen","Yueru He","Yijing Xu","Yuecheng Jiang","Dong Li","Ruey-Ling Weng","Min Peng","Jimin Huang","Sophia Ananiadou","Qianqian Xie"],"pdf_url":"https://arxiv.org/pdf/2502.05878v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.03736v3","updated":"2025-02-11T15:42:19Z","published":"2024-06-06T04:22:11Z","title":"Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data","summary":"  Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.\n","authors":["Jingyang Ou","Shen Nie","Kaiwen Xue","Fengqi Zhu","Jiacheng Sun","Zhenguo Li","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2406.03736v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01192v3","updated":"2025-02-11T15:36:41Z","published":"2024-11-02T09:39:49Z","title":"Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and\n  Cross-Cultural Embedding Models and Benchmarks","summary":"  We introduce {\\bf Swan}, a family of embedding models centred around the\nArabic language, addressing both small-scale and large-scale use cases. Swan\nincludes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on\nArMistral, a pretrained Arabic large language model. To evaluate these models,\nwe propose ArabicMTEB, a comprehensive benchmark suite that assesses\ncross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text\nembedding performance, covering eight diverse tasks and spanning 94 datasets.\nSwan-Large achieves state-of-the-art results, outperforming\nMultilingual-E5-large in most Arabic tasks, while the Swan-Small consistently\nsurpasses Multilingual-E5-base. Our extensive evaluations demonstrate that Swan\nmodels are both dialectally and culturally aware, excelling across various\nArabic domains while offering significant monetary efficiency. This work\nsignificantly advances the field of Arabic language modelling and provides\nvaluable resources for future research and applications in Arabic natural\nlanguage processing. Our models and benchmark are available at our GitHub page:\n\\href{https://github.com/UBC-NLP/swan}{https://github.com/UBC-NLP/swan}\n","authors":["Gagan Bhatia","El Moatez Billah Nagoudi","Abdellah El Mekki","Fakhraddin Alwajih","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2411.01192v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07642v1","updated":"2025-02-11T15:33:17Z","published":"2025-02-11T15:33:17Z","title":"FoQA: A Faroese Question-Answering Dataset","summary":"  We present FoQA, a Faroese extractive question-answering (QA) dataset with\n2,000 samples, created using a semi-automated approach combining Large Language\nModels (LLMs) and human validation. The dataset was generated from Faroese\nWikipedia articles using GPT-4-turbo for initial QA generation, followed by\nquestion rephrasing to increase complexity and native speaker validation to\nensure quality. We provide baseline performance metrics for FoQA across\nmultiple models, including LLMs and BERT, demonstrating its effectiveness in\nevaluating Faroese QA performance. The dataset is released in three versions: a\nvalidated set of 2,000 samples, a complete set of all 10,001 generated samples,\nand a set of 2,395 rejected samples for error analysis.\n","authors":["Annika Simonsen","Dan Saattrup Nielsen","Hafsteinn Einarsson"],"pdf_url":"https://arxiv.org/pdf/2502.07642v1.pdf","comment":"Camera-ready version for RESOURCEFUL workshop, 2025"},{"id":"http://arxiv.org/abs/2502.07637v1","updated":"2025-02-11T15:25:10Z","published":"2025-02-11T15:25:10Z","title":"BiaSWE: An Expert Annotated Dataset for Misogyny Detection in Swedish","summary":"  In this study, we introduce the process for creating BiaSWE, an\nexpert-annotated dataset tailored for misogyny detection in the Swedish\nlanguage. To address the cultural and linguistic specificity of misogyny in\nSwedish, we collaborated with experts from the social sciences and humanities.\nOur interdisciplinary team developed a rigorous annotation process,\nincorporating both domain knowledge and language expertise, to capture the\nnuances of misogyny in a Swedish context. This methodology ensures that the\ndataset is not only culturally relevant but also aligned with broader efforts\nin bias detection for low-resource languages. The dataset, along with the\nannotation guidelines, is publicly available for further research.\n","authors":["Kätriin Kukk","Danila Petrelli","Judit Casademont","Eric J. W. Orlowski","Michał Dzieliński","Maria Jacobson"],"pdf_url":"https://arxiv.org/pdf/2502.07637v1.pdf","comment":"To appear at NoDaLiDa 2025"},{"id":"http://arxiv.org/abs/2502.07629v1","updated":"2025-02-11T15:17:00Z","published":"2025-02-11T15:17:00Z","title":"Exploring Mobile Touch Interaction with Large Language Models","summary":"  Interacting with Large Language Models (LLMs) for text editing on mobile\ndevices currently requires users to break out of their writing environment and\nswitch to a conversational AI interface. In this paper, we propose to control\nthe LLM via touch gestures performed directly on the text. We first chart a\ndesign space that covers fundamental touch input and text transformations. In\nthis space, we then concretely explore two control mappings: spread-to-generate\nand pinch-to-shorten, with visual feedback loops. We evaluate this concept in a\nuser study (N=14) that compares three feedback designs: no visualisation, text\nlength indicator, and length + word indicator. The results demonstrate that\ntouch-based control of LLMs is both feasible and user-friendly, with the length\n+ word indicator proving most effective for managing text generation. This work\nlays the foundation for further research into gesture-based interaction with\nLLMs on touch devices.\n","authors":["Tim Zindulka","Jannek Sekowski","Florian Lehmann","Daniel Buschek"],"pdf_url":"https://arxiv.org/pdf/2502.07629v1.pdf","comment":"21 pages, 16 figures, 3 tables, ACM CHI 2025"},{"id":"http://arxiv.org/abs/2502.07623v1","updated":"2025-02-11T15:10:23Z","published":"2025-02-11T15:10:23Z","title":"Lexical categories of stem-forming roots in Mapudüngun verb forms","summary":"  After developing a computational system for morphological analysis of the\nMapuche language, and evaluating it with texts from various authors and styles,\nit became necessary to verify the linguistic assumptions of the source used as\nthe basis for implementing this tool.\n  In the present work, the primary focus is on the lexical category\nclassification of Mapud\\\"ungun roots recognised as verbal in the source\nutilised for the development of the morphological analysis system.\n  The results of this lexical category revision directly benefit the\ncomputational analyser, as they are implemented as soon as they are verified.\nAdditionally, it is hoped that these results will help clarify some\nuncertainties about lexical categories in the Mapuche language.\n  This work addresses a preliminary task to identify the valency of true verbal\nroots, the results of which will be presented in a subsequent work that\ncomplements this article.\n","authors":["Andrés Chandía"],"pdf_url":"https://arxiv.org/pdf/2502.07623v1.pdf","comment":"22 pages, 2 large tables, 2 sample tables"},{"id":"http://arxiv.org/abs/2502.07616v1","updated":"2025-02-11T15:05:26Z","published":"2025-02-11T15:05:26Z","title":"Tractable Transformers for Flexible Conditional Generation","summary":"  Non-autoregressive (NAR) generative models are valuable because they can\nhandle diverse conditional generation tasks in a more principled way than their\nautoregressive (AR) counterparts, which are constrained by sequential\ndependency requirements. Recent advancements in NAR models, such as diffusion\nlanguage models, have demonstrated superior performance in unconditional\ngeneration compared to AR models (e.g., GPTs) of similar sizes. However, such\nimprovements do not always lead to improved conditional generation performance.\nWe show that a key reason for this gap is the difficulty in generalizing to\nconditional probability queries unseen during training. As a result, strong\nunconditional generation performance does not guarantee high-quality\nconditional generation. This paper proposes Tractable Transformers\n(Tracformer), a Transformer-based generative model that is more robust to\ndifferent conditional generation tasks. Unlike existing models that rely solely\non global contextual features derived from full inputs, Tracformers incorporate\na sparse Transformer encoder to capture both local and global contextual\ninformation. This information is routed through a decoder for conditional\ngeneration. Empirical results demonstrate that Tracformers achieve\nstate-of-the-art conditional generation performance on text modeling compared\nto recent diffusion and AR model baselines.\n","authors":["Anji Liu","Xuejie Liu","Dayuan Zhao","Mathias Niepert","Yitao Liang","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2502.07616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14317v2","updated":"2025-02-11T14:51:08Z","published":"2024-08-26T14:45:03Z","title":"Claim Verification in the Age of Large Language Models: A Survey","summary":"  The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.\n","authors":["Alphaeus Dmonte","Roland Oruche","Marcos Zampieri","Prasad Calyam","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2408.14317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07601v1","updated":"2025-02-11T14:50:43Z","published":"2025-02-11T14:50:43Z","title":"Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large\n  Language Models","summary":"  Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the\ntraditional unsupervised AD setting that requires a large number of normal\nsamples to train a model, ZSAD is more practical for handling data-restricted\nreal-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have\nshown revolutionary reasoning capabilities in various vision tasks. However,\nthe reasoning of image abnormalities remains underexplored due to the lack of\ncorresponding datasets and benchmarks. To facilitate research in AD &\nreasoning, we establish the first visual instruction tuning dataset,\nAnomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through\ninvestigation with our benchmark, we reveal that current MLLMs like GPT-4o\ncannot accurately detect and describe fine-grained anomalous details in images.\nTo address this, we propose Anomaly-OneVision (Anomaly-OV), the first\nspecialist visual assistant for ZSAD and reasoning. Inspired by human behavior\nin visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM)\nmechanism to adaptively select and emphasize abnormal visual tokens. Extensive\nexperiments demonstrate that Anomaly-OV achieves significant improvements over\nadvanced generalist models in both detection and reasoning. Extensions to\nmedical and 3D AD are provided for future study. The link to our project page:\nhttps://xujiacong.github.io/Anomaly-OV/\n","authors":["Jiacong Xu","Shao-Yuan Lo","Bardia Safaei","Vishal M. Patel","Isht Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2502.07601v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.07599v1","updated":"2025-02-11T14:49:44Z","published":"2025-02-11T14:49:44Z","title":"DPO-Shift: Shifting the Distribution of Direct Preference Optimization","summary":"  Direct Preference Optimization (DPO) and its variants have become\nincreasingly popular for aligning language models with human preferences. These\nmethods aim to teach models to better distinguish between chosen (or preferred)\nand rejected (or dispreferred) responses. However, prior research has\nidentified that the probability of chosen responses often decreases during\ntraining, and this phenomenon is known as likelihood displacement. To tackle\nthis challenge, in this work we introduce \\method to controllably shift the\ndistribution of the chosen probability. Then, we show that \\method exhibits a\nfundamental trade-off between improving the chosen probability and sacrificing\nthe reward margin, as supported by both theoretical analysis and experimental\nvalidation. Furthermore, we demonstrate the superiority of \\method over DPO on\ndownstream tasks such as MT-Bench and a designed win rate experiment. We\nbelieve this study shows that the likelihood displacement issue of DPO can be\neffectively mitigated with a simple, theoretically grounded solution. Our code\nis available at https://github.com/Meaquadddd/DPO-Shift.\n","authors":["Xiliang Yang","Feng Jiang","Qianen Zhang","Lei Zhao","Xiao Li"],"pdf_url":"https://arxiv.org/pdf/2502.07599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07586v1","updated":"2025-02-11T14:34:05Z","published":"2025-02-11T14:34:05Z","title":"We Can't Understand AI Using our Existing Vocabulary","summary":"  This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better.\n","authors":["John Hewitt","Robert Geirhos","Been Kim"],"pdf_url":"https://arxiv.org/pdf/2502.07586v1.pdf","comment":"Position paper"},{"id":"http://arxiv.org/abs/2502.04964v2","updated":"2025-02-11T14:32:15Z","published":"2025-02-07T14:30:12Z","title":"CoCoA: A Generalized Approach to Uncertainty Quantification by\n  Integrating Confidence and Consistency of LLM Outputs","summary":"  Uncertainty quantification (UQ) methods for Large Language Models (LLMs)\nencompasses a variety of approaches, with two major types being particularly\nprominent: information-based, which focus on model confidence expressed as\ntoken probabilities, and consistency-based, which assess the semantic\nrelationship between multiple outputs generated using repeated sampling.\nSeveral recent methods have combined these two approaches and shown impressive\nperformance in various applications. However, they sometimes fail to outperform\nmuch simpler baseline methods. Our investigation reveals distinctive\ncharacteristics of LLMs as probabilistic models, which help to explain why\nthese UQ methods underperform in certain tasks. Based on these findings, we\npropose a new way of synthesizing model confidence and output consistency that\nleads to a family of efficient and robust UQ methods. We evaluate our approach\nacross a variety of tasks such as question answering, abstractive\nsummarization, and machine translation, demonstrating sizable improvements over\nstate-of-the-art UQ approaches.\n","authors":["Roman Vashurin","Maiya Goloburda","Preslav Nakov","Artem Shelmanov","Maxim Panov"],"pdf_url":"https://arxiv.org/pdf/2502.04964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07577v1","updated":"2025-02-11T14:23:13Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15633v3","updated":"2025-02-11T14:18:17Z","published":"2024-10-21T04:30:53Z","title":"GATEAU: Selecting Influential Samples for Long Context Alignment","summary":"  Aligning large language models to handle instructions with extremely long\ncontexts has yet to be fully investigated. Previous studies attempt to scale up\nthe available data volume by synthesizing long instruction-following samples,\nas constructing such a dataset tends to be challenging for annotators. However,\na lack of a well-defined strategy for ensuring data quality may introduce\nlow-quality samples and restrict the model performance. Thus, we propose\nGATEAU, a novel framework to address the unique challenge of long context\nalignment by identifying the influential samples enriched with long-range\ndependency relations. Specifically, GATEAU measures the long-range dependencies\nfrom two essential aspects: the difficulty of generating target responses due\nto the long-range dependencies, and the difficulty of understanding long inputs\ndue to such dependencies. Comprehensive experiments indicate that GATEAU\neffectively identifies influential samples and the model trained on these\nselected samples exhibits better instruction-following and long-context\nunderstanding capabilities.\n","authors":["Shuzheng Si","Haozhe Zhao","Gang Chen","Yunshui Li","Kangyang Luo","Chuancheng Lv","Kaikai An","Fanchao Qi","Baobao Chang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.15633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07575v1","updated":"2025-02-11T14:17:29Z","published":"2025-02-11T14:17:29Z","title":"Towards Efficient and Multifaceted Computer-assisted Pronunciation\n  Training Leveraging Hierarchical Selective State Space Model and Decoupled\n  Cross-entropy Loss","summary":"  Prior efforts in building computer-assisted pronunciation training (CAPT)\nsystems often treat automatic pronunciation assessment (APA) and\nmispronunciation detection and diagnosis (MDD) as separate fronts: the former\naims to provide multiple pronunciation aspect scores across diverse linguistic\nlevels, while the latter focuses instead on pinpointing the precise phonetic\npronunciation errors made by non-native language learners. However, it is\ngenerally expected that a full-fledged CAPT system should perform both\nfunctionalities simultaneously and efficiently. In response to this surging\ndemand, we in this work first propose HMamba, a novel CAPT approach that\nseamlessly integrates APA and MDD tasks in parallel. In addition, we introduce\na novel loss function, decoupled cross-entropy loss (deXent), specifically\ntailored for MDD to facilitate better-supervised learning for detecting\nmispronounced phones, thereby enhancing overall performance. A comprehensive\nset of empirical results on the speechocean762 benchmark dataset demonstrates\nthe effectiveness of our approach on APA. Notably, our proposed approach also\nyields a considerable improvement in MDD performance over a strong baseline,\nachieving an F1-score of 63.85%. Our codes are made available at\nhttps://github.com/Fuann/hmamba\n","authors":["Fu-An Chao","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07575v1.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.04315v3","updated":"2025-02-11T14:01:39Z","published":"2025-02-06T18:57:06Z","title":"ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters","summary":"  Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/\n","authors":["Kamer Ali Yuksel","Hassan Sawaf"],"pdf_url":"https://arxiv.org/pdf/2502.04315v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07563v1","updated":"2025-02-11T14:01:39Z","published":"2025-02-11T14:01:39Z","title":"LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid","summary":"  Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE.\n","authors":["Weigao Sun","Disen Lan","Yiran Zhong","Xiaoye Qu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07563v1.pdf","comment":"Technical report, 17 pages"},{"id":"http://arxiv.org/abs/2502.07555v1","updated":"2025-02-11T13:48:10Z","published":"2025-02-11T13:48:10Z","title":"O1 Embedder: Let Retrievers Think Before Action","summary":"  The growing power of large language models (LLMs) has revolutionized how\npeople access and utilize information. Notably, the LLMs excel at performing\nfine-grained data representation, which facilitates precise retrieval of\ninformation. They also generate high-quality answers based on external\nreferences, enabling the production of useful knowledge. The recent\nintroduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another\nleap forward, highlighting LLMs' ability to think progressively before\ndelivering final answers. This breakthrough significantly improves the ability\nto address complex tasks, e.g., coding and math proofs.\n  Inspired by this progress, we aim to develop similar capabilities for\nretrieval models, which hold great promise for tackling critical challenges in\nthe field, including multi-task retrieval, zero-shot retrieval, and tasks\nrequiring intensive reasoning of complex relationships. With this motivation,\nwe propose a novel approach called O1 Embedder, which generates useful thoughts\nfor the input query before making retrieval for the target documents. To\nrealize this objective, we conquer two technical difficulties. First, we design\na data synthesis workflow, creating training signals for O1 Embedder by\ngenerating initial thoughts from an LLM-expert and subsequently refining them\nusing a retrieval committee. Second, we optimize the training process, enabling\na pre-trained model to be jointly fine-tuned to generate retrieval thoughts via\nbehavior cloning and perform dense retrieval through contrastive learning. Our\napproach is evaluated by comprehensive experiments, where substantial\nimprovements are achieved across 12 popular datasets, spanning both in-domain\nand out-of-domain scenarios. These results highlight O1 Embedder's remarkable\naccuracy and generalizability, paving the way for the development of\nnext-generation IR foundation models.\n","authors":["Ruin Yan","Zheng Liu","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2502.07555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07552v1","updated":"2025-02-11T13:41:06Z","published":"2025-02-11T13:41:06Z","title":"Unsupervised Translation of Emergent Communication","summary":"  Emergent Communication (EC) provides a unique window into the language\nsystems that emerge autonomously when agents are trained to jointly achieve\nshared goals. However, it is difficult to interpret EC and evaluate its\nrelationship with natural languages (NL). This study employs unsupervised\nneural machine translation (UNMT) techniques to decipher ECs formed during\nreferential games with varying task complexities, influenced by the semantic\ndiversity of the environment. Our findings demonstrate UNMT's potential to\ntranslate EC, illustrating that task complexity characterized by semantic\ndiversity enhances EC translatability, while higher task complexity with\nconstrained semantic variability exhibits pragmatic EC, which, although\nchallenging to interpret, remains suitable for translation. This research marks\nthe first attempt, to our knowledge, to translate EC without the aid of\nparallel data.\n","authors":["Ido Levy","Orr Paradise","Boaz Carmeli","Ron Meir","Shafi Goldwasser","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2502.07552v1.pdf","comment":"19 pages (including appendix and bibliography), Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2502.07544v1","updated":"2025-02-11T13:30:41Z","published":"2025-02-11T13:30:41Z","title":"Grammar Control in Dialogue Response Generation for Language Learning\n  Chatbots","summary":"  Chatbots based on large language models offer cheap conversation practice\nopportunities for language learners. However, they are hard to control for\nlinguistic forms that correspond to learners' current needs, such as grammar.\nWe control grammar in chatbot conversation practice by grounding a dialogue\nresponse generation model in a pedagogical repository of grammar skills. We\nalso explore how this control helps learners to produce specific grammar. We\ncomprehensively evaluate prompting, fine-tuning, and decoding strategies for\ngrammar-controlled dialogue response generation. Strategically decoding Llama3\noutperforms GPT-3.5 when tolerating minor response quality losses. Our\nsimulation predicts grammar-controlled responses to support grammar acquisition\nadapted to learner proficiency. Existing language learning chatbots and\nresearch on second language acquisition benefit from these affordances. Code\navailable on GitHub.\n","authors":["Dominik Glandorf","Peng Cui","Detmar Meurers","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2502.07544v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07541v1","updated":"2025-02-11T13:28:56Z","published":"2025-02-11T13:28:56Z","title":"Corporate Greenwashing Detection in Text - a Survey","summary":"  Greenwashing is an effort to mislead the public about the environmental\nimpact of an entity, such as a state or company. We provide a comprehensive\nsurvey of the scientific literature addressing natural language processing\nmethods to identify potentially misleading climate-related corporate\ncommunications, indicative of greenwashing. We break the detection of\ngreenwashing into intermediate tasks, and review the state-of-the-art\napproaches for each of them. We discuss datasets, methods, and results, as well\nas limitations and open challenges. We also provide an overview of how far the\nfield has come as a whole, and point out future research directions.\n","authors":["Tom Calamai","Oana Balalau","Théo Le Guenedal","Fabian M. Suchanek"],"pdf_url":"https://arxiv.org/pdf/2502.07541v1.pdf","comment":"35 pages, 1 figure, 21 pages (appendix), working paper"},{"id":"http://arxiv.org/abs/2502.00641v2","updated":"2025-02-11T13:12:16Z","published":"2025-02-02T03:07:45Z","title":"Evaluating Small Language Models for News Summarization: Implications\n  and Factors Influencing Performance","summary":"  The increasing demand for efficient summarization tools in\nresource-constrained environments highlights the need for effective solutions.\nWhile large language models (LLMs) deliver superior summarization quality,\ntheir high computational resource requirements limit practical use\napplications. In contrast, small language models (SLMs) present a more\naccessible alternative, capable of real-time summarization on edge devices.\nHowever, their summarization capabilities and comparative performance against\nLLMs remain underexplored. This paper addresses this gap by presenting a\ncomprehensive evaluation of 19 SLMs for news summarization across 2,000 news\nsamples, focusing on relevance, coherence, factual consistency, and summary\nlength. Our findings reveal significant variations in SLM performance, with\ntop-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results\ncomparable to those of 70B LLMs while generating more concise summaries.\nNotably, SLMs are better suited for simple prompts, as overly complex prompts\nmay lead to a decline in summary quality. Additionally, our analysis indicates\nthat instruction tuning does not consistently enhance the news summarization\ncapabilities of SLMs. This research not only contributes to the understanding\nof SLMs but also provides practical insights for researchers seeking efficient\nsummarization solutions that balance performance and resource use.\n","authors":["Borui Xu","Yao Chen","Zeyi Wen","Weiguo Liu","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2502.00641v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13381v2","updated":"2025-02-11T12:44:39Z","published":"2025-01-23T04:50:03Z","title":"Do as We Do, Not as You Think: the Conformity of Large Language Models","summary":"  Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm.\n","authors":["Zhiyuan Weng","Guikun Chen","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2501.13381v2.pdf","comment":"ICLR 2025 (Oral). Code: https://github.com/Zhiyuan-Weng/BenchForm"},{"id":"http://arxiv.org/abs/2412.19018v4","updated":"2025-02-11T12:39:22Z","published":"2024-12-26T01:56:42Z","title":"Let the Fuzzy Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability","summary":"  Large language models (LLMs) often struggle with balanced class accuracy in\ntext classification tasks using in-context learning (ICL), hindering some\npractical uses due to user dissatisfaction or safety risks caused by\nmisclassifications. Retraining LLMs to address root causes in data or model\npriors is neither easy nor cost-effective. This paper delves deeper into the\nclass accuracy imbalance issue, identifying that it arises because certain\nclasses consistently receive disproportionately high ICL probabilities, causing\nunder-prediction and lower accuracy for others. More importantly, probability\nranges affect the imbalance differently, allowing for precise, range-specific\ncorrections. We introduce FuRud (Fuzzy Rule Optimization-based Debiasing), a\nmethod for sample-level class probability correction. FuRud tackles\ninterpretability challenges by determining why certain classes need corrections\nand tailoring adjustments for each instance's class probabilities which is\npowered by fuzzy sets with triangular membership functions, transforming a\nclass probability based on the range it belongs to. By solving a nonlinear\ninteger programming problem with a labeled set of ICL class probabilities to\nminimize class accuracy bias (COBias) and maximize overall accuracy, each class\nselects an optimal correction function from 19 triangular membership functions\nwithout updating an LLM, and the selected functions correct test instances at\ninference. Across seven benchmark datasets, FuRud reduces COBias by over half\n(56%) and improves overall accuracy by 21% relatively, outperforming\nstate-of-the-art debiasing methods.\n","authors":["Ruixi Lin","Yang You"],"pdf_url":"https://arxiv.org/pdf/2412.19018v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00233v2","updated":"2025-02-11T12:33:13Z","published":"2024-12-31T02:53:27Z","title":"Zero-Shot Strategies for Length-Controllable Summarization","summary":"  Large language models (LLMs) struggle with precise length control,\nparticularly in zero-shot settings. We conduct a comprehensive study evaluating\nLLMs' length control capabilities across multiple measures and propose\npractical methods to improve controllability. Our experiments with LLaMA 3\nreveal stark differences in length adherence across measures and highlight\ninherent biases of the model. To address these challenges, we introduce a set\nof methods: length approximation, target adjustment, sample filtering, and\nautomated revisions. By combining these methods, we demonstrate substantial\nimprovements in length compliance while maintaining or enhancing summary\nquality, providing highly effective zero-shot strategies for precise length\ncontrol without the need for model fine-tuning or architectural changes. With\nour work, we not only advance our understanding of LLM behavior in controlled\ntext generation but also pave the way for more reliable and adaptable\nsummarization systems in real-world applications.\n","authors":["Fabian Retkowski","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2501.00233v2.pdf","comment":"Accepted to NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2412.14872v2","updated":"2025-02-11T12:25:11Z","published":"2024-12-19T14:11:15Z","title":"Theoretical Proof that Generated Text in the Corpus Leads to the\n  Collapse of Auto-regressive Language Models","summary":"  Auto-regressive language models (LMs) have been widely used to generate text\non the World Wide Web. The generated text is often collected into the training\ncorpus of the next generations of LMs. Previous work experimentally found that\nLMs collapse when trained on recursively generated text. This paper presents\ntheoretical proof that once a corpus (such as the World Wide Web) begins to\nincorporate generated text, and the training text of each LM is sampled from\nthis corpus, then no matter how small the amount of text generated by each LM\nthat enters the corpus is, after a sufficient amount of time, LM collapse is\nbound to occur. Our proof is validated by a series of experiments showing that\nthe collapsed LMs perform no better than an untrained LM with randomly\ninitialized parameters. By proving the existence of LM collapse, we express our\nconcerns about the current situation in which an increasing amount of generated\ntext may be used in LM training. The source code is available in the online\ndata warehouse: https://github.com/wanglc02/generated-data\n","authors":["Lecheng Wang","Xianjie Shi","Ge Li","Jia Li","Xuanming Zhang","Yihong Dong","Wenpin Jiao","Hong Mei"],"pdf_url":"https://arxiv.org/pdf/2412.14872v2.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2409.00696v3","updated":"2025-02-11T12:21:13Z","published":"2024-09-01T11:24:54Z","title":"Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM\n  Evaluation","summary":"  Rating-based human evaluation has become an essential tool to accurately\nevaluate the impressive performance of large language models (LLMs). However,\ncurrent rating systems suffer from several important limitations: first, they\nfail to account for biases that significantly influence evaluation results,\nsecond, they require large and expensive preference datasets to obtain accurate\nratings, and third, they do not facilitate meaningful comparisons of model\nratings across different tasks. To address these issues, we introduce\nPolyrating, an expressive and flexible rating system based on maximum a\nposteriori estimation that enables a more nuanced and thorough analysis of\nmodel performance at lower costs. Polyrating can detect and quantify biases\naffecting human preferences, ensuring fairer model comparisons. Further,\nPolyrating can reduce the cost of human evaluations by up to $41\\%$ for new\nmodels and up to $77\\%$ for new tasks by leveraging existing benchmark scores.\nLastly, Polyrating enables direct comparisons of ratings across different\ntasks, providing a comprehensive understanding of an LLMs' strengths,\nweaknesses, and relative performance across different applications.\n","authors":["Jasper Dekoninck","Maximilian Baader","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2409.00696v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04411v2","updated":"2025-02-11T12:09:51Z","published":"2025-02-06T11:26:30Z","title":"Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and\n  Uncertainty Based Routing","summary":"  Model merging aggregates Large Language Models (LLMs) finetuned on different\ntasks into a stronger one. However, parameter conflicts between models leads to\nperformance degradation in averaging. While model routing addresses this issue\nby selecting individual models during inference, it imposes excessive storage\nand compute costs, and fails to leverage the common knowledge from different\nmodels. In this work, we observe that different layers exhibit varying levels\nof parameter conflicts. Building on this insight, we average layers with\nminimal parameter conflicts and use a novel task-level expert routing for\nlayers with significant conflicts. To further reduce storage costs, inspired by\ntask arithmetic sparsity, we decouple multiple fine-tuned experts into a dense\nexpert and several sparse experts. Considering the out-of-distribution samples,\nwe select and merge appropriate experts based on the task uncertainty of the\ninput data. We conduct extensive experiments on both LLaMA and Qwen with\nvarying parameter scales, and evaluate on real-world reasoning tasks. Results\ndemonstrate that our method consistently achieves significant performance\nimprovements while requiring less system cost compared to existing methods.\n","authors":["Kunfeng Lai","Zhenheng Tang","Xinglin Pan","Peijie Dong","Xiang Liu","Haolan Chen","Li Shen","Bo Li","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2502.04411v2.pdf","comment":"work in progress. arXiv admin note: text overlap with\n  arXiv:2405.09673 by other authors"},{"id":"http://arxiv.org/abs/2411.04585v2","updated":"2025-02-11T12:00:39Z","published":"2024-11-07T10:11:38Z","title":"The State and Fate of Summarization Datasets: A Survey","summary":"  Automatic summarization has consistently attracted attention due to its\nversatility and wide application in various downstream tasks. Despite its\npopularity, we find that annotation efforts have largely been disjointed, and\nhave lacked common terminology. Consequently, it is challenging to discover\nexisting resources or identify coherent research directions. To address this,\nwe survey a large body of work spanning 133 datasets in over 100 languages,\ncreating a novel ontology covering sample properties, collection methods and\ndistribution. With this ontology we make key observations, including the lack\nin accessible high-quality datasets for low-resource languages, and the field's\nover-reliance on the news domain and on automatically collected distant\nsupervision. Finally, we make available a web interface that allows users to\ninteract and explore our ontology and dataset collection, as well as a template\nfor a summarization data card, which can be used to streamline future research\ninto a more coherent body of work.\n","authors":["Noam Dahan","Gabriel Stanovsky"],"pdf_url":"https://arxiv.org/pdf/2411.04585v2.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07490v1","updated":"2025-02-11T11:49:03Z","published":"2025-02-11T11:49:03Z","title":"Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More","summary":"  Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models.\n","authors":["Xialie Zhuang","Zhikai Jia","Jianjin Li","Zhenyu Zhang","Li Shen","Zheng Cao","Shiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07490v1.pdf","comment":"15 pages,7 figures"},{"id":"http://arxiv.org/abs/2502.07487v1","updated":"2025-02-11T11:46:38Z","published":"2025-02-11T11:46:38Z","title":"Multi-Agent Collaboration for Multilingual Code Instruction Tuning","summary":"  Recent advancement in code understanding and generation demonstrates that\ncode LLMs fine-tuned on a high-quality instruction dataset can gain powerful\ncapabilities to address wide-ranging code-related tasks. However, most previous\nexisting methods mainly view each programming language in isolation and ignore\nthe knowledge transfer among different programming languages. To bridge the gap\namong different programming languages, we introduce a novel multi-agent\ncollaboration framework to enhance multilingual instruction tuning for code\nLLMs, where multiple language-specific intelligent agent components with\ngeneration memory work together to transfer knowledge from one language to\nanother efficiently and effectively. Specifically, we first generate the\nlanguage-specific instruction data from the code snippets and then provide the\ngenerated data as the seed data for language-specific agents. Multiple\nlanguage-specific agents discuss and collaborate to formulate a new instruction\nand its corresponding solution (A new programming language or existing\nprogramming language), To further encourage the cross-lingual transfer, each\nagent stores its generation history as memory and then summarizes its merits\nand faults. Finally, the high-quality multilingual instruction data is used to\nencourage knowledge transfer among different programming languages to train\nQwen2.5-xCoder. Experimental results on multilingual programming benchmarks\ndemonstrate the superior performance of Qwen2.5-xCoder in sharing common\nknowledge, highlighting its potential to reduce the cross-lingual gap.\n","authors":["Jian Yang","Wei Zhang","Jiaxi Yang","Yibo Miao","Shanghaoran Quan","Zhenhe Wu","Qiyao Peng","Liqun Yang","Tianyu Liu","Zeyu Cui","Binyuan Hui","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.07487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07459v1","updated":"2025-02-11T11:07:44Z","published":"2025-02-11T11:07:44Z","title":"PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian","summary":"  Large language models predominantly reflect Western cultures, largely due to\nthe dominance of English-centric training data. This imbalance presents a\nsignificant challenge, as LLMs are increasingly used across diverse contexts\nwithout adequate evaluation of their cultural competence in non-English\nlanguages, including Persian. To address this gap, we introduce PerCul, a\ncarefully constructed dataset designed to assess the sensitivity of LLMs toward\nPersian culture. PerCul features story-based, multiple-choice questions that\ncapture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is\ncurated with input from native Persian annotators to ensure authenticity and to\nprevent the use of translation as a shortcut. We evaluate several\nstate-of-the-art multilingual and Persian-specific LLMs, establishing a\nfoundation for future research in cross-cultural NLP evaluation. Our\nexperiments demonstrate a 11.3% gap between best closed source model and\nlayperson baseline while the gap increases to 21.3% by using the best\nopen-weight model. You can access the dataset from here:\nhttps://huggingface.co/datasets/teias-ai/percul\n","authors":["Erfan Moosavi Monazzah","Vahid Rahimzadeh","Yadollah Yaghoobzadeh","Azadeh Shakery","Mohammad Taher Pilehvar"],"pdf_url":"https://arxiv.org/pdf/2502.07459v1.pdf","comment":"Accepted at NAACL 2025 Main Conference, the dataset is available on\n  HuggingFace (see https://huggingface.co/datasets/teias-ai/percul)"},{"id":"http://arxiv.org/abs/2406.07222v2","updated":"2025-02-11T11:02:10Z","published":"2024-06-11T13:01:50Z","title":"Improving Autoformalization using Type Checking","summary":"  Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics.\n","authors":["Auguste Poiroux","Gail Weiss","Viktor Kunčak","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2406.07222v2.pdf","comment":"New benchmarks released, see\n  https://github.com/augustepoiroux/RLMEval ,\n  https://huggingface.co/datasets/PAug/ProofNetSharp , and\n  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see\n  https://github.com/augustepoiroux/LeanInteract"},{"id":"http://arxiv.org/abs/2502.07455v1","updated":"2025-02-11T10:57:12Z","published":"2025-02-11T10:57:12Z","title":"RusCode: Russian Cultural Code Benchmark for Text-to-Image Generation","summary":"  Text-to-image generation models have gained popularity among users around the\nworld. However, many of these models exhibit a strong bias toward\nEnglish-speaking cultures, ignoring or misrepresenting the unique\ncharacteristics of other language groups, countries, and nationalities. The\nlack of cultural awareness can reduce the generation quality and lead to\nundesirable consequences such as unintentional insult, and the spread of\nprejudice. In contrast to the field of natural language processing, cultural\nawareness in computer vision has not been explored as extensively. In this\npaper, we strive to reduce this gap. We propose a RusCode benchmark for\nevaluating the quality of text-to-image generation containing elements of the\nRussian cultural code. To do this, we form a list of 19 categories that best\nrepresent the features of Russian visual culture. Our final dataset consists of\n1250 text prompts in Russian and their translations into English. The prompts\ncover a wide range of topics, including complex concepts from art, popular\nculture, folk traditions, famous people's names, natural objects, scientific\nachievements, etc. We present the results of a human evaluation of the\nside-by-side comparison of Russian visual concepts representations using\npopular generative models.\n","authors":["Viacheslav Vasilev","Julia Agafonova","Nikolai Gerasimenko","Alexander Kapitanov","Polina Mikhailova","Evelina Mironova","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2502.07455v1.pdf","comment":"Accepted for NAACL 2025 Findings, GitHub:\n  https://github.com/ai-forever/RusCode"},{"id":"http://arxiv.org/abs/2502.07445v1","updated":"2025-02-11T10:43:36Z","published":"2025-02-11T10:43:36Z","title":"Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon","summary":"  Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation.\n","authors":["Nurit Cohen-Inger","Yehonatan Elisha","Bracha Shapira","Lior Rokach","Seffi Cohen"],"pdf_url":"https://arxiv.org/pdf/2502.07445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07442v1","updated":"2025-02-11T10:37:01Z","published":"2025-02-11T10:37:01Z","title":"Hierarchical Document Parsing via Large Margin Feature Matching and\n  Heuristics","summary":"  We present our solution to the AAAI-25 VRD-IU challenge, achieving first\nplace in the competition. Our approach integrates large margin loss for\nimproved feature discrimination and employs heuristic rules to refine\nhierarchical relationships. By combining a deep learning-based matching\nstrategy with greedy algorithms, we achieve a significant boost in accuracy\nwhile maintaining computational efficiency. Our method attains an accuracy of\n0.98904 on the private leaderboard, demonstrating its effectiveness in document\nstructure parsing. Source codes are publicly available at\nhttps://github.com/ffyyytt/VRUID-AAAI-DAKiet\n","authors":["Duong Anh Kiet"],"pdf_url":"https://arxiv.org/pdf/2502.07442v1.pdf","comment":"DocUI@AAAI-25, 2 pages, technical report"},{"id":"http://arxiv.org/abs/2410.18850v2","updated":"2025-02-11T10:36:13Z","published":"2024-10-24T15:32:52Z","title":"kNN For Whisper And Its Effect On Bias And Speaker Adaptation","summary":"  Speech recognition performance varies by language, domain, and speaker\ncharacteristics such as accent, but fine-tuning a model on any of these\ncategories may lead to catastrophic forgetting. Token-level $k$ nearest\nneighbor search ($k$NN), first proposed for neural sequence decoders for\nnatural language generation (NLG) and machine translation (MT), is a\nnon-parametric method that instead adapts using inference-time search in an\nexternal datastore, without training the underlying model. We show that\nWhisper, a transformer end-to-end speech model, benefits from $k$NN. We\ninvestigate the differences between the speech and text setups. We discuss\nimplications for speaker adaptation, and analyze improvements by gender,\naccent, and age.\n","authors":["Maya K. Nachesa","Vlad Niculae"],"pdf_url":"https://arxiv.org/pdf/2410.18850v2.pdf","comment":"Accepted to Findings of NAACL 2025. 7 pages incl. appendix, 2\n  figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.07424v1","updated":"2025-02-11T10:10:26Z","published":"2025-02-11T10:10:26Z","title":"RomanLens: Latent Romanization and its role in Multilinguality in LLMs","summary":"  Large Language Models (LLMs) exhibit remarkable multilingual generalization\ndespite being predominantly trained on English-centric corpora. A fundamental\nquestion arises: how do LLMs achieve such robust multilingual capabilities? For\nnon-Latin script languages, we investigate the role of romanization - the\nrepresentation of non-Latin scripts using Latin characters - as a bridge in\nmultilingual processing. Using mechanistic interpretability techniques, we\nanalyze next-token generation and find that intermediate layers frequently\nrepresent target words in romanized form before transitioning to native script,\na phenomenon we term Latent Romanization. Further, through activation patching\nexperiments, we demonstrate that LLMs encode semantic concepts similarly across\nnative and romanized scripts, suggesting a shared underlying representation.\nAdditionally in translation towards non Latin languages, our findings reveal\nthat when the target language is in romanized form, its representations emerge\nearlier in the model's layers compared to native script. These insights\ncontribute to a deeper understanding of multilingual representation in LLMs and\nhighlight the implicit role of romanization in facilitating language transfer.\nOur work provides new directions for potentially improving multilingual\nlanguage modeling and interpretability.\n","authors":["Alan Saji","Jaavid Aktar Husain","Thanmay Jayakumar","Raj Dabre","Anoop Kunchukuttan","Mitesh M. Khapra","Ratish Puduppully"],"pdf_url":"https://arxiv.org/pdf/2502.07424v1.pdf","comment":"18 pages, 18 figures"},{"id":"http://arxiv.org/abs/2502.02577v2","updated":"2025-02-11T10:06:14Z","published":"2025-02-04T18:53:42Z","title":"A comparison of translation performance between DeepL and Supertext","summary":"  As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext.\n","authors":["Alex Flückiger","Chantal Amrhein","Tim Graf","Frédéric Odermatt","Martin Pömsl","Philippe Schläpfer","Florian Schottmann","Samuel Läubli"],"pdf_url":"https://arxiv.org/pdf/2502.02577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00560v2","updated":"2025-02-11T10:02:55Z","published":"2024-12-31T17:46:51Z","title":"Re-evaluating Automatic LLM System Ranking for Alignment with Human\n  Preference","summary":"  Evaluating and ranking the capabilities of different LLMs is crucial for\nunderstanding their performance and alignment with human preferences. Due to\nthe high cost and time-consuming nature of human evaluations, an automatic LLM\nbencher (i.e., an automatic evaluation framework that aims to rank LLMs based\non their alignment with human preferences) is indispensable. An automatic LLM\nbencher consists of four components: the input set (e.g., a user instruction),\nthe evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise\ncomparison), and the aggregation method (e.g., the ELO rating system). However,\nprevious work has not thoroughly explored how to select these components or how\ntheir different combinations influence the results. In this work, through\ncontrolled experiments, we provide a series of recommendations on how to choose\neach component to better automate the evaluation of LLMs. Furthermore, we\ndiscovered that when evaluating LLMs with similar performance, the performance\nof the automatic LLM bencher declines sharply, underscoring the limitations of\ncurrent benchers and calling for future work. Lastly, we found that the\nevaluation models' performance at the instance level (e.g., the accuracy of\nselecting the best output) does not always align with their effectiveness when\nused as a component of a bencher, highlighting the importance of dedicated\nsystem-level evaluation of benchers.\n","authors":["Mingqi Gao","Yixin Liu","Xinyu Hu","Xiaojun Wan","Jonathan Bragg","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2501.00560v2.pdf","comment":"Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07418v1","updated":"2025-02-11T09:54:39Z","published":"2025-02-11T09:54:39Z","title":"Entity Linking using LLMs for Automated Product Carbon Footprint\n  Estimation","summary":"  Growing concerns about climate change and sustainability are driving\nmanufacturers to take significant steps toward reducing their carbon\nfootprints. For these manufacturers, a first step towards this goal is to\nidentify the environmental impact of the individual components of their\nproducts. We propose a system leveraging large language models (LLMs) to\nautomatically map components from manufacturer Bills of Materials (BOMs) to\nLife Cycle Assessment (LCA) database entries by using LLMs to expand on\navailable component information. Our approach reduces the need for manual data\nprocessing, paving the way for more accessible sustainability practices.\n","authors":["Steffen Castle","Julian Moreno Schneider","Leonhard Hennig","Georg Rehm"],"pdf_url":"https://arxiv.org/pdf/2502.07418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07391v1","updated":"2025-02-11T09:19:46Z","published":"2025-02-11T09:19:46Z","title":"Target-Augmented Shared Fusion-based Multimodal Sarcasm Explanation\n  Generation","summary":"  Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g.,\nentity, event, or person) in an inherent way. Multimodal Sarcasm Explanation\n(MuSE) aims at revealing the intended irony in a sarcastic post using a natural\nlanguage explanation. Though important, existing systems overlooked the\nsignificance of the target of sarcasm in generating explanations. In this\npaper, we propose a Target-aUgmented shaRed fusion-Based sarcasm explanatiOn\nmodel, aka. TURBO. We design a novel shared-fusion mechanism to leverage the\ninter-modality relationships between an image and its caption. TURBO assumes\nthe target of the sarcasm and guides the multimodal shared fusion mechanism in\nlearning intricacies of the intended irony for explanations. We evaluate our\nproposed TURBO model on the MORE+ dataset. Comparison against multiple\nbaselines and state-of-the-art models signifies the performance improvement of\nTURBO by an average margin of $+3.3\\%$. Moreover, we explore LLMs in zero and\none-shot settings for our task and observe that LLM-generated explanation,\nthough remarkable, often fails to capture the critical nuances of the sarcasm.\nFurthermore, we supplement our study with extensive human evaluation on TURBO's\ngenerated explanations and find them out to be comparatively better than other\nsystems.\n","authors":["Palaash Goel","Dushyant Singh Chauhan","Md Shad Akhtar"],"pdf_url":"https://arxiv.org/pdf/2502.07391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07386v1","updated":"2025-02-11T09:12:05Z","published":"2025-02-11T09:12:05Z","title":"Parametric type design in the era of variable and color fonts","summary":"  Parametric fonts are programatically defined fonts with variable parameters,\npioneered by Donald Kunth with his MetaFont technology in the 1980s. While\nDonald Knuth's ideas in MetaFont and subsequently in MetaPost are often seen as\nlegacy techniques from the pre-graphical user interface (GUI) era of type\ndesign, recent trends like variable fonts suggest a resurgence of certain\nprinciples. This paper explores a modern type design process built on\nparametric design principles, specifically using MetaPost. The author created\ntwo variable fonts with this method and released them under a free, open-source\nlicense. The paper details the methodology, workflow, and insights gained from\nthis process.\n","authors":["Santhosh Thottingal"],"pdf_url":"https://arxiv.org/pdf/2502.07386v1.pdf","comment":"Conference: Grapholinguistics in the 21st century - From graphemes to\n  knowledge"},{"id":"http://arxiv.org/abs/2502.07373v1","updated":"2025-02-11T08:48:46Z","published":"2025-02-11T08:48:46Z","title":"EvoFlow: Evolving Diverse Agentic Workflows On The Fly","summary":"  The past two years have witnessed the evolution of large language model\n(LLM)-based multi-agent systems from labor-intensive manual design to partial\nautomation (\\textit{e.g.}, prompt engineering, communication topology) and\neventually to fully automated design. However, existing agentic automation\npipelines often lack LLM heterogeneity and focus on single-objective\nperformance optimization, limiting their potential to combine weaker models for\nmore customized and cost-effective solutions. To address this challenge, we\npropose EvoFlow, a niching evolutionary algorithm-based framework to\nautomatically search a population of heterogeneous and complexity-adaptive\nagentic workflows, rather than a single homogeneous, complex workflow.\nTechnically, EvoFlow performs \\textit{(1) tag-based retrieval} to extract\nparent workflows from an agentic population, evolves new workflows through\n\\textit{(2) crossover} and \\textit{(3) mutation}, and employs \\textit{(4)\nniching-based selection} to maintain population diversity and quality.\nExtensive evaluations across seven benchmarks demonstrate that EvoFlow is:\n\\textbf{(I) diverse}, evolving a population of workflows ranging from simple\nI/O tasks to complex multi-turn interactions; \\textbf{(II) high-performing},\noutperforming previous handcrafted and automated workflows by\n$1.23\\%\\sim29.86\\%$; \\textbf{(III) economical}, surpassing powerful\n\\llmname{o1-preview} at $12.4\\%$ of its inference cost using weaker open-source\nmodels.\n","authors":["Guibin Zhang","Kaijie Chen","Guancheng Wan","Heng Chang","Hong Cheng","Kun Wang","Shuyue Hu","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2502.07373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07365v1","updated":"2025-02-11T08:37:16Z","published":"2025-02-11T08:37:16Z","title":"LongReD: Mitigating Short-Text Degradation of Long-Context Large\n  Language Models via Restoration Distillation","summary":"  Large language models (LLMs) have gained extended context windows through\nscaling positional encodings and lightweight continual pre-training. However,\nthis often leads to degraded performance on short-text tasks, while the reasons\nfor this degradation remain insufficiently explored. In this work, we identify\ntwo primary factors contributing to this issue: distribution drift in hidden\nstates and attention scores, and catastrophic forgetting during continual\npre-training. To address these challenges, we propose Long Context Pre-training\nwith Restoration Distillation (LongReD), a novel approach designed to mitigate\nshort-text performance degradation through minimizing the distribution\ndiscrepancy between the extended and original models. Besides training on long\ntexts, LongReD distills the hidden state of selected layers from the original\nmodel on short texts. Additionally, LongReD also introduces a short-to-long\ndistillation, aligning the output distribution on short texts with that on long\ntexts by leveraging skipped positional indices. Experiments on common text\nbenchmarks demonstrate that LongReD effectively preserves the model's\nshort-text performance while maintaining comparable or even better capacity to\nhandle long texts than baselines.\n","authors":["Zican Dong","Junyi Li","Jinhao Jiang","Mingyu Xu","Wayne Xin Zhao","Bingning Wang","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07352v1","updated":"2025-02-11T08:23:56Z","published":"2025-02-11T08:23:56Z","title":"Bridging the Evaluation Gap: Leveraging Large Language Models for Topic\n  Model Evaluation","summary":"  This study presents a framework for automated evaluation of dynamically\nevolving topic taxonomies in scientific literature using Large Language Models\n(LLMs). In digital library systems, topic modeling plays a crucial role in\nefficiently organizing and retrieving scholarly content, guiding researchers\nthrough complex knowledge landscapes. As research domains proliferate and\nshift, traditional human centric and static evaluation methods struggle to\nmaintain relevance. The proposed approach harnesses LLMs to measure key quality\ndimensions, such as coherence, repetitiveness, diversity, and topic-document\nalignment, without heavy reliance on expert annotators or narrow statistical\nmetrics. Tailored prompts guide LLM assessments, ensuring consistent and\ninterpretable evaluations across various datasets and modeling techniques.\nExperiments on benchmark corpora demonstrate the method's robustness,\nscalability, and adaptability, underscoring its value as a more holistic and\ndynamic alternative to conventional evaluation strategies.\n","authors":["Zhiyin Tan","Jennifer D'Souza"],"pdf_url":"https://arxiv.org/pdf/2502.07352v1.pdf","comment":"accepted by IRCDL 2025"},{"id":"http://arxiv.org/abs/2406.12109v2","updated":"2025-02-11T08:22:35Z","published":"2024-06-17T21:37:09Z","title":"Can LLMs Learn Macroeconomic Narratives from Social Media?","summary":"  This study empirically tests the $\\textit{Narrative Economics}$ hypothesis,\nwhich posits that narratives (ideas that are spread virally and affect public\nbeliefs) can influence economic fluctuations. We introduce two curated datasets\ncontaining posts from X (formerly Twitter) which capture economy-related\nnarratives (Data will be shared upon paper acceptance). Employing Natural\nLanguage Processing (NLP) methods, we extract and summarize narratives from the\ntweets. We test their predictive power for $\\textit{macroeconomic}$ forecasting\nby incorporating the tweets' or the extracted narratives' representations in\ndownstream financial prediction tasks. Our work highlights the challenges in\nimproving macroeconomic models with narrative data, paving the way for the\nresearch community to realistically address this important challenge. From a\nscientific perspective, our investigation offers valuable insights and NLP\ntools for narrative extraction and summarization using Large Language Models\n(LLMs), contributing to future research on the role of narratives in economics.\n","authors":["Almog Gueta","Amir Feder","Zorik Gekhman","Ariel Goldstein","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2406.12109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07346v1","updated":"2025-02-11T08:17:19Z","published":"2025-02-11T08:17:19Z","title":"BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large\n  Language Models","summary":"  Previous multilingual benchmarks focus primarily on simple understanding\ntasks, but for large language models(LLMs), we emphasize proficiency in\ninstruction following, reasoning, long context understanding, code generation,\nand so on. However, measuring these advanced capabilities across languages is\nunderexplored. To address the disparity, we introduce BenchMAX, a multi-way\nmultilingual evaluation benchmark that allows for fair comparisons of these\nimportant abilities across languages. To maintain high quality, three distinct\nnative-speaking annotators independently annotate each sample within all tasks\nafter the data was machine-translated from English into 16 other languages.\nAdditionally, we present a novel translation challenge stemming from dataset\nconstruction. Extensive experiments on BenchMAX reveal varying effectiveness of\ncore capabilities across languages, highlighting performance gaps that cannot\nbe bridged by simply scaling up model size. BenchMAX serves as a comprehensive\nmultilingual evaluation platform, providing a promising test bed to promote the\ndevelopment of multilingual language models. The dataset and code are publicly\naccessible.\n","authors":["Xu Huang","Wenhao Zhu","Hanxu Hu","Conghui He","Lei Li","Shujian Huang","Fei Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.07346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07340v1","updated":"2025-02-11T08:05:56Z","published":"2025-02-11T08:05:56Z","title":"Aligning Large Language Models to Follow Instructions and Hallucinate\n  Less via Effective Data Filtering","summary":"  Training LLMs on data that contains unfamiliar knowledge during the\ninstruction tuning stage can make LLMs overconfident and encourage\nhallucinations. To address this challenge, we introduce a novel framework,\nNOVA, which identifies high-quality data that aligns well with the LLM's\nlearned knowledge to reduce hallucinations. NOVA includes Internal Consistency\nProbing (ICP) and Semantic Equivalence Identification (SEI) to measure how\nfamiliar the LLM is with instruction data. Specifically, ICP evaluates the\nLLM's understanding of the given instruction by calculating the tailored\nconsistency among multiple self-generated responses. SEI further assesses the\nfamiliarity of the LLM with the target response by comparing it to the\ngenerated responses, using the proposed semantic clustering and well-designed\nvoting strategy. Finally, we introduce an expert-aligned reward model,\nconsidering characteristics beyond just familiarity to enhance data quality. By\nconsidering data quality and avoiding unfamiliar data, we can utilize the\nselected data to effectively align LLMs to follow instructions and hallucinate\nless. Extensive experiments and analysis show that NOVA significantly reduces\nhallucinations and allows LLMs to maintain a strong ability to follow\ninstructions.\n","authors":["Shuzheng Si","Haozhe Zhao","Gang Chen","Cheng Gao","Yuzhuo Bai","Zhitong Wang","Kaikai An","Kangyang Luo","Chen Qian","Fanchao Qi","Baobao Chang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.07340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07328v1","updated":"2025-02-11T07:46:29Z","published":"2025-02-11T07:46:29Z","title":"Music for All: Exploring Multicultural Representations in Music\n  Generation Models (Camera Ready)","summary":"  The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.\n","authors":["Atharva Mehta","Shivam Chauhan","Amirbek Djanibekov","Atharva Kulkarni","Gus Xia","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07328v1.pdf","comment":"17 pages, 5 figures, accepted to NAACL'25"},{"id":"http://arxiv.org/abs/2502.07322v1","updated":"2025-02-11T07:42:09Z","published":"2025-02-11T07:42:09Z","title":"MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject\n  Batch Editing for LLMs","summary":"  As large language models continue to scale up, knowledge editing techniques\nthat modify models' internal knowledge without full retraining have gained\nsignificant attention. MEMIT, a prominent batch editing algorithm, stands out\nfor its capability to perform mass knowledge modifications. However, we uncover\na critical limitation that MEMIT's editing efficacy significantly deteriorates\nwhen processing batches containing multiple edits sharing the same subject. Our\nanalysis reveals that the root cause lies in MEMIT's key value modeling\nframework: When multiple facts with the same subject in a batch are modeled\nthrough MEMIT's key value mechanism, identical keys (derived from the shared\nsubject) are forced to represent different values (corresponding to different\nknowledge), resulting in updates conflicts during editing. Addressing this\nissue, we propose MEMIT-Merge, an enhanced approach that merges value\ncomputation processes for facts sharing the same subject, effectively resolving\nthe performance degradation in same-subject batch editing scenarios.\nExperimental results demonstrate that when MEMIT's edit success rate drops to\naround 50% at larger batch sizes, MEMIT-Merge maintains a success rate\nexceeding 90%, showcasing remarkable robustness to subject entity collisions.\n","authors":["Zilu Dong","Xiangqing Shen","Rui Xia"],"pdf_url":"https://arxiv.org/pdf/2502.07322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20163v2","updated":"2025-02-11T07:35:58Z","published":"2024-10-26T12:34:07Z","title":"UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers","summary":"  Existing information retrieval (IR) models often assume a homogeneous\nstructure for knowledge sources and user queries, limiting their applicability\nin real-world settings where retrieval is inherently heterogeneous and diverse.\nIn this paper, we introduce UniHGKR, a unified instruction-aware heterogeneous\nknowledge retriever that (1) builds a unified retrieval space for heterogeneous\nknowledge and (2) follows diverse user instructions to retrieve knowledge of\nspecified types. UniHGKR consists of three principal stages: heterogeneous\nself-supervised pretraining, text-anchored embedding alignment, and\ninstruction-aware retriever fine-tuning, enabling it to generalize across\nvaried retrieval contexts. This framework is highly scalable, with a BERT-based\nversion and a UniHGKR-7B version trained on large language models. Also, we\nintroduce CompMix-IR, the first native heterogeneous knowledge retrieval\nbenchmark. It includes two retrieval scenarios with various instructions, over\n9,400 question-answer (QA) pairs, and a corpus of 10 million entries, covering\nfour different types of data. Extensive experiments show that UniHGKR\nconsistently outperforms state-of-the-art methods on CompMix-IR, achieving up\nto 6.36% and 54.23% relative improvements in two scenarios, respectively.\nFinally, by equipping our retriever for open-domain heterogeneous QA systems,\nwe achieve a new state-of-the-art result on the popular ConvMix task, with an\nabsolute improvement of up to 5.90 points.\n","authors":["Dehai Min","Zhiyang Xu","Guilin Qi","Lifu Huang","Chenyu You"],"pdf_url":"https://arxiv.org/pdf/2410.20163v2.pdf","comment":"NAACL 2025, Main, Long Paper"},{"id":"http://arxiv.org/abs/2408.00662v2","updated":"2025-02-11T07:32:30Z","published":"2024-08-01T15:58:05Z","title":"Aligning Multiple Knowledge Graphs in a Single Pass","summary":"  Entity alignment (EA) is to identify equivalent entities across different\nknowledge graphs (KGs), which can help fuse these KGs into a more comprehensive\none. Previous EA methods mainly focus on aligning a pair of KGs, and to the\nbest of our knowledge, no existing EA method considers aligning multiple (more\nthan two) KGs. To fill this research gap, in this work, we study a novel\nproblem of aligning multiple KGs and propose an effective framework named\nMultiEA to solve the problem. First, we embed the entities of all the candidate\nKGs into a common feature space by a shared KG encoder. Then, we explore three\nalignment strategies to minimize the distances among pre-aligned entities. In\nparticular, we propose an innovative inference enhancement technique to improve\nthe alignment performance by incorporating high-order similarities. Finally, to\nverify the effectiveness of MultiEA, we construct two new real-world benchmark\ndatasets and conduct extensive experiments on them. The results show that our\nMultiEA can effectively and efficiently align multiple KGs in a single pass. We\nrelease the source codes of MultiEA at: https://github.com/kepsail/MultiEA.\n","authors":["Yaming Yang","Zhe Wang","Ziyu Guan","Wei Zhao","Weigang Lu","Xinyan Huang","Jiangtao Cui","Xiaofei He"],"pdf_url":"https://arxiv.org/pdf/2408.00662v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07316v1","updated":"2025-02-11T07:26:50Z","published":"2025-02-11T07:26:50Z","title":"CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction","summary":"  Reasoning is a fundamental capability of Large Language Models. While prior\nresearch predominantly focuses on enhancing narrow skills like math or code\ngeneration, improving performance on many other reasoning tasks remains\nchallenging due to sparse and fragmented training data. To address this issue,\nwe propose CodeI/O, a novel approach that systematically condenses diverse\nreasoning patterns inherently embedded in contextually-grounded codes, through\ntransforming the original code into a code input-output prediction format. By\ntraining models to predict inputs/outputs given code and test cases entirely in\nnatural language as Chain-of-Thought (CoT) rationales, we expose them to\nuniversal reasoning primitives -- like logic flow planning, state-space\nsearching, decision tree traversal, and modular decomposition -- while\ndecoupling structured reasoning from code-specific syntax and preserving\nprocedural rigor. Experimental results demonstrate CodeI/O leads to consistent\nimprovements across symbolic, scientific, logic, math & numerical, and\ncommonsense reasoning tasks. By matching the existing ground-truth outputs or\nre-executing the code with predicted inputs, we can verify each prediction and\nfurther enhance the CoTs through multi-turn revision, resulting in CodeI/O++\nand achieving higher performance. Our data and models are available at\nhttps://github.com/hkust-nlp/CodeIO.\n","authors":["Junlong Li","Daya Guo","Dejian Yang","Runxin Xu","Yu Wu","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2502.07316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05343v2","updated":"2025-02-11T07:17:37Z","published":"2024-10-07T07:19:50Z","title":"EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos\n  Referring to Procedural Texts","summary":"  Mistake action detection is crucial for developing intelligent archives that\ndetect workers' errors and provide feedback. Existing studies have focused on\nvisually apparent mistakes in free-style activities, resulting in video-only\napproaches to mistake detection. However, in text-following activities, models\ncannot determine the correctness of some actions without referring to the\ntexts. Additionally, current mistake datasets rarely use procedural texts for\nvideo recording except for cooking. To fill these gaps, this paper proposes the\nEgoOops dataset, where egocentric videos record erroneous activities when\nfollowing procedural texts across diverse domains. It features three types of\nannotations: video-text alignment, mistake labels, and descriptions for\nmistakes. We also propose a mistake detection approach, combining video-text\nalignment and mistake label classification to leverage the texts. Our\nexperimental results show that incorporating procedural texts is essential for\nmistake detection. Data is available through\nhttps://y-haneji.github.io/EgoOops-project-page/.\n","authors":["Yuto Haneji","Taichi Nishimura","Hirotaka Kameko","Keisuke Shirai","Tomoya Yoshida","Keiya Kajimura","Koki Yamamoto","Taiyu Cui","Tomohiro Nishimoto","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2410.05343v2.pdf","comment":"Main 6 pages, supplementary 13 pages"},{"id":"http://arxiv.org/abs/2502.07306v1","updated":"2025-02-11T07:09:37Z","published":"2025-02-11T07:09:37Z","title":"TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language\n  Navigation","summary":"  In this work, we propose a modular approach for the Vision-Language\nNavigation (VLN) task by decomposing the problem into four sub-modules that use\nstate-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs)\nin a zero-shot setting. Given navigation instruction in natural language, we\nfirst prompt LLM to extract the landmarks and the order in which they are\nvisited. Assuming the known model of the environment, we retrieve the top-k\nlocations of the last landmark and generate $k$ path hypotheses from the\nstarting location to the last landmark using the shortest path algorithm on the\ntopological map of the environment. Each path hypothesis is represented by a\nsequence of panoramas. We then use dynamic programming to compute the alignment\nscore between the sequence of panoramas and the sequence of landmark names,\nwhich match scores obtained from VLM. Finally, we compute the nDTW metric\nbetween the hypothesis that yields the highest alignment score to evaluate the\npath fidelity. We demonstrate superior performance compared to other approaches\nthat use joint semantic maps like VLMaps \\cite{vlmaps} on the complex\nR2R-Habitat \\cite{r2r} instruction dataset and quantify in detail the effect of\nvisual grounding on navigation performance.\n","authors":["Navid Rajabi","Jana Kosecka"],"pdf_url":"https://arxiv.org/pdf/2502.07306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10704v2","updated":"2025-02-11T07:05:58Z","published":"2024-12-14T06:24:55Z","title":"VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal\n  Retrieval-Augmented Generation","summary":"  Understanding information from a collection of multiple documents,\nparticularly those with visually rich elements, is important for\ndocument-grounded question answering. This paper introduces VisDoMBench, the\nfirst comprehensive benchmark designed to evaluate QA systems in multi-document\nsettings with rich multimodal content, including tables, charts, and\npresentation slides. We propose VisDoMRAG, a novel multimodal Retrieval\nAugmented Generation (RAG) approach that simultaneously utilizes visual and\ntextual RAG, combining robust visual retrieval capabilities with sophisticated\nlinguistic reasoning. VisDoMRAG employs a multi-step reasoning process\nencompassing evidence curation and chain-of-thought reasoning for concurrent\ntextual and visual RAG pipelines. A key novelty of VisDoMRAG is its\nconsistency-constrained modality fusion mechanism, which aligns the reasoning\nprocesses across modalities at inference time to produce a coherent final\nanswer. This leads to enhanced accuracy in scenarios where critical information\nis distributed across modalities and improved answer verifiability through\nimplicit context attribution. Through extensive experiments involving\nopen-source and proprietary large language models, we benchmark\nstate-of-the-art document QA methods on VisDoMBench. Extensive results show\nthat VisDoMRAG outperforms unimodal and long-context LLM baselines for\nend-to-end multimodal document QA by 12-20%.\n","authors":["Manan Suri","Puneet Mathur","Franck Dernoncourt","Kanika Goswami","Ryan A. Rossi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2412.10704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11507v3","updated":"2025-02-11T07:03:51Z","published":"2024-10-15T11:20:42Z","title":"Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs","summary":"  While various vertical domain large language models (LLMs) have been\ndeveloped, automatically evaluating their performance across different domains\nremains a critical challenge. Current benchmark-based methods often rely on\nstatic and costly datasets, are misaligned with practical user needs, and lack\nflexibility across domains. To address these limitations, we revisit the\nevaluation process and introduce two key concepts: Benchmark+, which extends\nthe traditional question-answer benchmark into a more flexible\n``strategy-criterion'' format; and Assessment+, which enhances the interaction\nprocess, enabling deeper exploration and supporting analysis from broader\nperspectives. We propose TestAgent, an agent-based evaluation framework that\nimplements these concepts using retrieval-augmented generation and\nreinforcement learning. TestAgent enables automatic dynamic benchmark\ngeneration and in-depth assessment across diverse vertical domain scenarios.\nExperiments on tasks ranging from constructing multiple vertical domain\nevaluations to converting static benchmarks into dynamic forms demonstrate the\neffectiveness of TestAgent. This work offers an interesting perspective on\nautomatic evaluation for LLMs and highlights a pathway for dynamic and\ndomain-adaptive assessments.\n","authors":["Wanying Wang","Zeyu Ma","Pengfei Liu","Mingang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.11507v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16751v3","updated":"2025-02-11T07:00:28Z","published":"2024-02-26T17:16:28Z","title":"Value Preferences Estimation and Disambiguation in Hybrid Participatory\n  Systems","summary":"  Understanding citizens' values in participatory systems is crucial for\ncitizen-centric policy-making. We envision a hybrid participatory system where\nparticipants make choices and provide motivations for those choices, and AI\nagents estimate their value preferences by interacting with them. We focus on\nsituations where a conflict is detected between participants' choices and\nmotivations, and propose methods for estimating value preferences while\naddressing detected inconsistencies by interacting with the participants. We\noperationalize the philosophical stance that \"valuing is deliberatively\nconsequential.\" That is, if a participant's choice is based on a deliberation\nof value preferences, the value preferences can be observed in the motivation\nthe participant provides for the choice. Thus, we propose and compare value\npreferences estimation methods that prioritize the values estimated from\nmotivations over the values estimated from choices alone. Then, we introduce a\ndisambiguation strategy that combines Natural Language Processing and Active\nLearning to address the detected inconsistencies between choices and\nmotivations. We evaluate the proposed methods on a dataset of a large-scale\nsurvey on energy transition. The results show that explicitly addressing\ninconsistencies between choices and motivations improves the estimation of an\nindividual's value preferences. The disambiguation strategy does not show\nsubstantial improvements when compared to similar baselines--however, we\ndiscuss how the novelty of the approach can open new research avenues and\npropose improvements to address the current limitations.\n","authors":["Enrico Liscio","Luciano C. Siebert","Catholijn M. Jonker","Pradeep K. Murukannaiah"],"pdf_url":"https://arxiv.org/pdf/2402.16751v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07299v1","updated":"2025-02-11T06:53:59Z","published":"2025-02-11T06:53:59Z","title":"Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification","summary":"  The interactions between DNA, RNA, and proteins are fundamental to biological\nprocesses, as illustrated by the central dogma of molecular biology. While\nmodern biological pre-trained models have achieved great success in analyzing\nthese macromolecules individually, their interconnected nature remains\nunder-explored. In this paper, we follow the guidance of the central dogma to\nredesign both the data and model pipeline and offer a comprehensive framework,\nLife-Code, that spans different biological functions. As for data flow, we\npropose a unified pipeline to integrate multi-omics data by\nreverse-transcribing RNA and reverse-translating amino acids into\nnucleotide-based sequences. As for the model, we design a codon tokenizer and a\nhybrid long-sequence architecture to encode the interactions of both coding and\nnon-coding regions with masked modeling pre-training. To model the translation\nand folding process with coding sequences, Life-Code learns protein structures\nof the corresponding amino acids by knowledge distillation from off-the-shelf\nprotein language models. Such designs enable Life-Code to capture complex\ninteractions within genetic sequences, providing a more comprehensive\nunderstanding of multi-omics with the central dogma. Extensive Experiments show\nthat Life-Code achieves state-of-the-art performance on various tasks across\nthree omics, highlighting its potential for advancing multi-omics analysis and\ninterpretation.\n","authors":["Zicheng Liu","Siyuan Li","Zhiyuan Chen","Lei Xin","Fang Wu","Chang Yu","Qirong Yang","Yucheng Guo","Yujie Yang","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2502.07299v1.pdf","comment":"12 pages main text with 6 pages Appendix"},{"id":"http://arxiv.org/abs/2501.07890v2","updated":"2025-02-11T06:47:01Z","published":"2025-01-14T06:59:51Z","title":"GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via\n  Introducing Self-Rethinking Mechanism","summary":"  Traditional Mixture-of-Experts (MoE) networks benefit from utilizing multiple\nsmaller expert models as opposed to a single large network. However, these\nexperts typically operate independently, leaving a question open about whether\ninterconnecting these models could enhance the performance of MoE networks. In\nresponse, we introduce GRAPHMOE, a novel method aimed at augmenting the\ncognitive depth of language models via a self-rethinking mechanism constructed\non Pseudo GraphMoE networks. GRAPHMOE employs a recurrent routing strategy to\nsimulate iterative thinking steps, thereby facilitating the flow of information\namong expert nodes. We implement the GRAPHMOE architecture using Low-Rank\nAdaptation techniques (LoRA) and conduct extensive experiments on various\nbenchmark datasets. The experimental results reveal that GRAPHMOE outperforms\nother LoRA based models, achieving state-of-the-art (SOTA) performance.\nAdditionally, this study explores a novel recurrent routing strategy that may\ninspire further advancements in enhancing the reasoning capabilities of\nlanguage models.\n","authors":["Chen Tang","Bo Lv","Zifan Zheng","Bohao Yang","Kun Zhao","Ning Liao","Xiaoxing Wang","Feiyu Xiong","Zhiyu Li","Nayu Liu","Jingchi Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.07890v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2405.19846v7","updated":"2025-02-11T06:22:30Z","published":"2024-05-30T08:50:55Z","title":"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model","summary":"  Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.\n","authors":["Chaochen Gao","Xing Wu","Qi Fu","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2405.19846v7.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2408.09366v2","updated":"2025-02-11T06:15:47Z","published":"2024-08-18T05:41:36Z","title":"Improving and Assessing the Fidelity of Large Language Models Alignment\n  to Online Communities","summary":"  Large language models (LLMs) have shown promise in representing individuals\nand communities, offering new ways to study complex social dynamics. However,\neffectively aligning LLMs with specific human groups and systematically\nassessing the fidelity of the alignment remains a challenge. This paper\npresents a robust framework for aligning LLMs with online communities via\ninstruction-tuning and comprehensively evaluating alignment across various\naspects of language, including authenticity, emotional tone, toxicity, and\nharm. We demonstrate the utility of our approach by applying it to online\ncommunities centered on dieting and body image. We administer an eating\ndisorder psychometric test to the aligned LLMs to reveal unhealthy beliefs and\nsuccessfully differentiate communities with varying levels of eating disorder\nrisk. Our results highlight the potential of LLMs in automated moderation and\nbroader applications in public health and social science research.\n","authors":["Minh Duc Chu","Zihao He","Rebecca Dorn","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2408.09366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02904v2","updated":"2025-02-11T06:14:31Z","published":"2025-02-05T05:57:37Z","title":"ScholaWrite: A Dataset of End-to-End Scholarly Writing Process","summary":"  Writing is a cognitively demanding task involving continuous decision-making,\nheavy use of working memory, and frequent switching between multiple\nactivities. Scholarly writing is particularly complex as it requires authors to\ncoordinate many pieces of multiform knowledge. To fully understand writers'\ncognitive thought process, one should fully decode the end-to-end writing data\n(from individual ideas to final manuscript) and understand their complex\ncognitive mechanisms in scholarly writing. We introduce ScholaWrite dataset,\nthe first-of-its-kind keystroke logs of an end-to-end scholarly writing process\nfor complete manuscripts, with thorough annotations of cognitive writing\nintentions behind each keystroke. Our dataset includes LaTeX-based keystroke\ndata from five preprints with nearly 62K total text changes and annotations\nacross 4 months of paper writing. ScholaWrite shows promising usability and\napplications (e.g., iterative self-writing) for the future development of AI\nwriting assistants for academic research, which necessitate complex methods\nbeyond LLM prompting. Our experiments clearly demonstrated the importance of\ncollection of end-to-end writing data, rather than the final manuscript, for\nthe development of future writing assistants to support the cognitive thinking\nprocess of scientists. Our de-identified dataset, demo, and code repository are\navailable on our project page.\n","authors":["Linghe Wang","Minhwa Lee","Ross Volkov","Luan Tuyen Chau","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2502.02904v2.pdf","comment":"Equal contribution: Linghe Wang, Minhwa Lee | project page:\n  https://minnesotanlp.github.io/scholawrite/"},{"id":"http://arxiv.org/abs/2502.07286v1","updated":"2025-02-11T06:06:25Z","published":"2025-02-11T06:06:25Z","title":"Small Language Model Makes an Effective Long Text Extractor","summary":"  Named Entity Recognition (NER) is a fundamental problem in natural language\nprocessing (NLP). However, the task of extracting longer entity spans (e.g.,\nawards) from extended texts (e.g., homepages) is barely explored. Current NER\nmethods predominantly fall into two categories: span-based methods and\ngeneration-based methods. Span-based methods require the enumeration of all\npossible token-pair spans, followed by classification on each span, resulting\nin substantial redundant computations and excessive GPU memory usage. In\ncontrast, generation-based methods involve prompting or fine-tuning large\nlanguage models (LLMs) to adapt to downstream NER tasks. However, these methods\nstruggle with the accurate generation of longer spans and often incur\nsignificant time costs for effective fine-tuning. To address these challenges,\nthis paper introduces a lightweight span-based NER method called SeNER, which\nincorporates a bidirectional arrow attention mechanism coupled with\nLogN-Scaling on the [CLS] token to embed long texts effectively, and comprises\na novel bidirectional sliding-window plus-shaped attention (BiSPA) mechanism to\nreduce redundant candidate token-pair spans significantly and model\ninteractions between token-pair spans simultaneously. Extensive experiments\ndemonstrate that our method achieves state-of-the-art extraction accuracy on\nthree long NER datasets and is capable of extracting entities from long texts\nin a GPU-memory-friendly manner. Code:\nhttps://github.com/THUDM/scholar-profiling/tree/main/sener\n","authors":["Yelin Chen","Fanjin Zhang","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2502.07286v1.pdf","comment":"AAAI'25, 9 pages, 1 appendix pages"},{"id":"http://arxiv.org/abs/2502.06101v2","updated":"2025-02-11T05:53:00Z","published":"2025-02-10T02:15:12Z","title":"RALLRec: Improving Retrieval Augmented Large Language Model\n  Recommendation with Representation Learning","summary":"  Large Language Models (LLMs) have been integrated into recommendation systems\nto enhance user behavior comprehension. The Retrieval Augmented Generation\n(RAG) technique is further incorporated into these systems to retrieve more\nrelevant items and improve system performance. However, existing RAG methods\nrely primarily on textual semantics and often fail to incorporate the most\nrelevant items, limiting the effectiveness of the systems.\n  In this paper, we propose Representation learning for retrieval-Augmented\nLarge Language model Recommendation (RALLRec). Specifically, we enhance textual\nsemantics by prompting LLMs to generate more detailed item descriptions,\nfollowed by joint representation learning of textual and collaborative\nsemantics, which are extracted by the LLM and recommendation models,\nrespectively. Considering the potential time-varying characteristics of user\ninterest, a simple yet effective reranking method is further introduced to\ncapture the dynamics of user preference. We conducted extensive experiments on\nthree real-world datasets, and the evaluation results validated the\neffectiveness of our method. Code is made public at\nhttps://github.com/JianXu95/RALLRec.\n","authors":["Jian Xu","Sichun Luo","Xiangyu Chen","Haoming Huang","Hanxu Hou","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2502.06101v2.pdf","comment":"Accepted by TheWebConf'25 (WWW'25) as a Short Paper"},{"id":"http://arxiv.org/abs/2410.11287v2","updated":"2025-02-11T05:41:41Z","published":"2024-10-15T05:10:34Z","title":"Process Reward Model with Q-Value Rankings","summary":"  Process Reward Modeling (PRM) is critical for complex reasoning and\ndecision-making tasks where the accuracy of intermediate steps significantly\ninfluences the overall outcome. Existing PRM approaches, primarily framed as\nclassification problems, employ cross-entropy loss to independently evaluate\neach step's correctness. This method can lead to suboptimal reward distribution\nand does not adequately address the interdependencies among steps. To address\nthese limitations, we introduce the Process Q-value Model (PQM), a novel\nframework that redefines PRM in the context of a Markov Decision Process. PQM\noptimizes Q-value rankings based on a novel comparative loss function,\nenhancing the model's ability to capture the intricate dynamics among\nsequential decisions. This approach provides a more granular and theoretically\ngrounded methodology for process rewards. Our extensive empirical evaluations\nacross various sampling policies, language model backbones, and multi-step\nreasoning benchmarks show that PQM outperforms classification-based PRMs. The\neffectiveness of the comparative loss function is highlighted in our\ncomprehensive ablation studies, confirming PQM's practical efficacy and\ntheoretical advantage.\n","authors":["Wendi Li","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.11287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07272v1","updated":"2025-02-11T05:39:49Z","published":"2025-02-11T05:39:49Z","title":"GENERator: A Long-Context Generative Genomic Foundation Model","summary":"  Advancements in DNA sequencing technologies have significantly improved our\nability to decode genomic sequences. However, the prediction and interpretation\nof these sequences remain challenging due to the intricate nature of genetic\nmaterial. Large language models (LLMs) have introduced new opportunities for\nbiological sequence analysis. Recent developments in genomic language models\nhave underscored the potential of LLMs in deciphering DNA sequences.\nNonetheless, existing models often face limitations in robustness and\napplication scope, primarily due to constraints in model structure and training\ndata scale. To address these limitations, we present GENERator, a generative\ngenomic foundation model featuring a context length of 98k base pairs (bp) and\n1.2B parameters. Trained on an expansive dataset comprising 386B bp of\neukaryotic DNA, the GENERator demonstrates state-of-the-art performance across\nboth established and newly proposed benchmarks. The model adheres to the\ncentral dogma of molecular biology, accurately generating protein-coding\nsequences that translate into proteins structurally analogous to known\nfamilies. It also shows significant promise in sequence optimization,\nparticularly through the prompt-responsive generation of promoter sequences\nwith specific activity profiles. These capabilities position the GENERator as a\npivotal tool for genomic research and biotechnological advancement, enhancing\nour ability to interpret and predict complex biological systems and enabling\nprecise genomic interventions.\n","authors":["Wei Wu","Qiuyi Li","Mingyang Li","Kun Fu","Fuli Feng","Jieping Ye","Hui Xiong","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04674v2","updated":"2025-02-11T05:36:24Z","published":"2025-02-07T05:39:55Z","title":"AdParaphrase: Paraphrase Dataset for Analyzing Linguistic Features\n  toward Generating Attractive Ad Texts","summary":"  Effective linguistic choices that attract potential customers play crucial\nroles in advertising success. This study aims to explore the linguistic\nfeatures of ad texts that influence human preferences. Although the creation of\nattractive ad texts is an active area of research, progress in understanding\nthe specific linguistic features that affect attractiveness is hindered by\nseveral obstacles. First, human preferences are complex and influenced by\nmultiple factors, including their content, such as brand names, and their\nlinguistic styles, making analysis challenging. Second, publicly available ad\ntext datasets that include human preferences are lacking, such as ad\nperformance metrics and human feedback, which reflect people's interests. To\naddress these problems, we present AdParaphrase, a paraphrase dataset that\ncontains human preferences for pairs of ad texts that are semantically\nequivalent but differ in terms of wording and style. This dataset allows for\npreference analysis that focuses on the differences in linguistic features. Our\nanalysis revealed that ad texts preferred by human judges have higher fluency,\nlonger length, more nouns, and use of bracket symbols. Furthermore, we\ndemonstrate that an ad text-generation model that considers these findings\nsignificantly improves the attractiveness of a given text. The dataset is\npublicly available at: https://github.com/CyberAgentAILab/AdParaphrase.\n","authors":["Soichiro Murakami","Peinan Zhang","Hidetaka Kamigaito","Hiroya Takamura","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2502.04674v2.pdf","comment":"Accepted to NAACL2025 Findings"},{"id":"http://arxiv.org/abs/2502.07266v1","updated":"2025-02-11T05:28:59Z","published":"2025-02-11T05:28:59Z","title":"When More is Less: Understanding Chain-of-Thought Length in LLMs","summary":"  Chain-of-thought (CoT) reasoning enhances the multi-step reasoning\ncapabilities of large language models (LLMs) by breaking complex tasks into\nsmaller, manageable sub-tasks. Researchers have been exploring ways to guide\nmodels to generate more complex CoT processes to improve the reasoning ability\nof LLMs, such as long CoT and the test-time scaling law. However, for most\nmodels and tasks, does an increase in CoT length consistently lead to improved\nreasoning accuracy? In this paper, we observe a nuanced relationship: as the\nnumber of reasoning steps increases, performance initially improves but\neventually decreases. To understand this phenomenon, we provide a piece of\nevidence that longer reasoning processes are increasingly susceptible to noise.\nWe theoretically prove the existence of an optimal CoT length and derive a\nscaling law for this optimal length based on model capability and task\ndifficulty. Inspired by our theory, we conduct experiments on both synthetic\nand real world datasets and propose Length-filtered Vote to alleviate the\neffects of excessively long or short CoTs. Our findings highlight the critical\nneed to calibrate CoT length to align with model capabilities and task demands,\noffering a principled framework for optimizing multi-step reasoning in LLMs.\n","authors":["Yuyang Wu","Yifei Wang","Tianqi Du","Stefanie Jegelka","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00290v2","updated":"2025-02-11T05:26:22Z","published":"2025-02-01T03:18:02Z","title":"Estimating LLM Uncertainty with Logits","summary":"  In recent years, Large Language Models (LLMs) have seen remarkable\nadvancements and have been extensively integrated across various fields.\nDespite their progress, LLMs are prone to hallucinations, producing responses\nthat may not be dependable if the models lack sufficient grounding knowledge.\nTo mitigate this issue, methods for estimating uncertainty have been adopted,\nwith a focus on critical tokens as indicators of reliability. Nevertheless,\nprobability-based approaches have shown limitations in assessing token-level\nreliability due to the erosion of evidence strength information acquired during\ntraining. In this paper, we introduce Logits-induced Token Uncertainty (LogU),\na novel framework designed to estimate token-specific uncertainty in LLMs in\nreal time, without the need for multiple sampling rounds. By leveraging\nevidence modeling for the implementation of LogU, we utilize the derived\nuncertainty measures to steer downstream tasks. Our experimental findings\nhighlight the substantial effectiveness and potential of LogU, marking a\nsignificant advancement in addressing the challenge of model hallucinations.\n","authors":["Huan Ma","Jingdong Chen","Guangyu Wang","Changqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00290v2.pdf","comment":"Fixed some data errors in Table 1"},{"id":"http://arxiv.org/abs/2407.06249v2","updated":"2025-02-11T05:23:45Z","published":"2024-07-08T17:55:04Z","title":"CodeUpdateArena: Benchmarking Knowledge Editing on API Updates","summary":"  Large language models (LLMs) are increasingly being used to synthesize and\nreason about source code. However, the static nature of these models' knowledge\ndoes not reflect the fact that libraries and API functions they invoke are\ncontinuously evolving, with functionality being added or changing. While\nnumerous benchmarks evaluate how LLMs can generate code, no prior work has\nstudied how an LLMs' knowledge about code API functions can be updated. To fill\nthis gap, we present CodeUpdateArena, a benchmark for knowledge editing in the\ncode domain. An instance in our benchmark consists of a synthetic API function\nupdate paired with a program synthesis example that uses the updated\nfunctionality; our goal is to update an LLM to be able to solve this program\nsynthesis example without providing documentation of the update at inference\ntime. Compared to knowledge editing for facts encoded in text, success here is\nmore challenging: a code LLM must correctly reason about the semantics of the\nmodified function rather than just reproduce its syntax. Our dataset is\nconstructed by first prompting GPT-4 to generate atomic and executable function\nupdates. Then, for each update, we generate program synthesis examples whose\ncode solutions are prone to use the update. Our benchmark covers updates of\nvarious types to 54 functions from seven diverse Python packages, with a total\nof 670 program synthesis examples. Our experiments show that prepending\ndocumentation of the update to open-source code LLMs (i.e., DeepSeek,\nCodeLlama) does not allow them to incorporate changes for problem solving, and\nexisting knowledge editing techniques also have substantial room for\nimprovement. We hope our benchmark will inspire new methods for knowledge\nupdating in code LLMs.\n","authors":["Zeyu Leo Liu","Shrey Pandit","Xi Ye","Eunsol Choi","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2407.06249v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.11107v3","updated":"2025-02-11T05:08:56Z","published":"2024-06-17T00:17:11Z","title":"Exploring Safety-Utility Trade-Offs in Personalized Language Models","summary":"  As large language models (LLMs) become increasingly integrated into daily\napplications, it is essential to ensure they operate fairly across diverse user\ndemographics. In this work, we show that LLMs suffer from personalization bias,\nwhere their performance is impacted when they are personalized to a user's\nidentity. We quantify personalization bias by evaluating the performance of\nLLMs along two axes - safety and utility. We measure safety by examining how\nbenign LLM responses are to unsafe prompts with and without personalization. We\nmeasure utility by evaluating the LLM's performance on various tasks, including\ngeneral knowledge, mathematical abilities, programming, and reasoning skills.\nWe find that various LLMs, ranging from open-source models like Llama (Touvron\net al., 2023) and Mistral (Jiang et al., 2023) to API-based ones like GPT-3.5\nand GPT-4o (Ouyang et al., 2022), exhibit significant variance in performance\nin terms of safety-utility trade-offs depending on the user's identity.\nFinally, we discuss several strategies to mitigate personalization bias using\npreference tuning and prompt-based defenses.\n","authors":["Anvesh Rao Vijjini","Somnath Basu Roy Chowdhury","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2406.11107v3.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07263v1","updated":"2025-02-11T05:07:36Z","published":"2025-02-11T05:07:36Z","title":"Hidden Division of Labor in Scientific Teams Revealed Through 1.6\n  Million LaTeX Files","summary":"  Recognition of individual contributions is fundamental to the scientific\nreward system, yet coauthored papers obscure who did what. Traditional\nproxies-author order and career stage-reinforce biases, while contribution\nstatements remain self-reported and limited to select journals. We construct\nthe first large-scale dataset on writing contributions by analyzing\nauthor-specific macros in LaTeX files from 1.6 million papers (1991-2023) by 2\nmillion scientists. Validation against self-reported statements (precision =\n0.87), author order patterns, field-specific norms, and Overleaf records\n(Spearman's rho = 0.6, p < 0.05) confirms the reliability of the created data.\nUsing explicit section information, we reveal a hidden division of labor within\nscientific teams: some authors primarily contribute to conceptual sections\n(e.g., Introduction and Discussion), while others focus on technical sections\n(e.g., Methods and Experiments). These findings provide the first large-scale\nevidence of implicit labor division in scientific teams, challenging\nconventional authorship practices and informing institutional policies on\ncredit allocation.\n","authors":["Jiaxin Pei","Lulin Yang","Lingfei Wu"],"pdf_url":"https://arxiv.org/pdf/2502.07263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04057v2","updated":"2025-02-11T04:12:21Z","published":"2024-09-06T06:57:04Z","title":"Self-Harmonized Chain of Thought","summary":"  Chain-of-thought (CoT) prompting has demonstrated the capacity of large\nlanguage models to perform complex reasoning through intermediate steps. While\neffective, current CoT methods face challenges: Zero-shot-CoT can lead to\nreasoning errors, and Few-shot-CoT requires labor-intensive manual\ndemonstrations. Auto-CoT attempts to address these issues by automatically\ngenerating diverse demonstrations, but this diversity can lead to inconsistent\nreasoning patterns. We propose ECHO (Self-Harmonized Chain of Thought), a novel\nmethod that unifies diverse solution paths into a consistent and effective\nreasoning pattern. ECHO employs an iterative process to refine and harmonize\nautomatically generated demonstrations, mitigating the limitations of existing\napproaches. Our comprehensive experiments across arithmetic, commonsense, and\nsymbolic reasoning tasks demonstrate that ECHO outperforms Auto-CoT by an\naverage of 2.8%. These findings suggest that ECHO represents a significant step\ntowards more robust and generalizable automated reasoning in large language\nmodels.\n","authors":["Ziqi Jin","Wei Lu"],"pdf_url":"https://arxiv.org/pdf/2409.04057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07237v1","updated":"2025-02-11T04:00:21Z","published":"2025-02-11T04:00:21Z","title":"DrugImproverGPT: A Large Language Model for Drug Optimization with\n  Fine-Tuning via Structured Policy Optimization","summary":"  Finetuning a Large Language Model (LLM) is crucial for generating results\ntowards specific objectives. This research delves into the realm of drug\noptimization and introduce a novel reinforcement learning algorithm to finetune\na drug optimization LLM-based generative model, enhancing the original drug\nacross target objectives, while retains the beneficial chemical properties of\nthe original drug. This work is comprised of two primary components: (1)\nDrugImprover: A framework tailored for improving robustness and efficiency in\ndrug optimization. It includes a LLM designed for drug optimization and a novel\nStructured Policy Optimization (SPO) algorithm, which is theoretically\ngrounded. This algorithm offers a unique perspective for fine-tuning the\nLLM-based generative model by aligning the improvement of the generated\nmolecule with the input molecule under desired objectives. (2) A dataset of 1\nmillion compounds, each with OEDOCK docking scores on 5 human proteins\nassociated with cancer cells and 24 binding sites from SARS-CoV-2 virus. We\nconduct a comprehensive evaluation of SPO and demonstrate its effectiveness in\nimproving the original drug across target properties. Our code and dataset will\nbe publicly available at: https://github.com/xuefeng-cs/DrugImproverGPT.\n","authors":["Xuefeng Liu","Songhao Jiang","Siyu Chen","Zhuoran Yang","Yuxin Chen","Ian Foster","Rick Stevens"],"pdf_url":"https://arxiv.org/pdf/2502.07237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06087v2","updated":"2025-02-11T03:39:36Z","published":"2025-02-10T01:04:36Z","title":"ConMeC: A Dataset for Metonymy Resolution with Common Nouns","summary":"  Metonymy plays an important role in our daily communication. People naturally\nthink about things using their most salient properties or commonly related\nconcepts. For example, by saying \"The bus decided to skip our stop today,\" we\nactually mean that the bus driver made the decision, not the bus. Prior work on\nmetonymy resolution has mainly focused on named entities. However, metonymy\ninvolving common nouns (such as desk, baby, and school) is also a frequent and\nchallenging phenomenon. We argue that NLP systems should be capable of\nidentifying the metonymic use of common nouns in context. We create a new\nmetonymy dataset ConMeC, which consists of 6,000 sentences, where each sentence\nis paired with a target common noun and annotated by humans to indicate whether\nthat common noun is used metonymically or not in that context. We also\nintroduce a chain-of-thought based prompting method for detecting metonymy\nusing large language models (LLMs). We evaluate our LLM-based pipeline, as well\nas a supervised BERT model on our dataset and three other metonymy datasets.\nOur experimental results demonstrate that LLMs could achieve performance\ncomparable to the supervised BERT model on well-defined metonymy categories,\nwhile still struggling with instances requiring nuanced semantic understanding.\nOur dataset is publicly available at: https://github.com/SaptGhosh/ConMeC.\n","authors":["Saptarshi Ghosh","Tianyu Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.06087v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07223v1","updated":"2025-02-11T03:32:34Z","published":"2025-02-11T03:32:34Z","title":"Graph RAG-Tool Fusion","summary":"  Recent developments in retrieval-augmented generation (RAG) for selecting\nrelevant tools from a tool knowledge base enable LLM agents to scale their\ncomplex tool calling capabilities to hundreds or thousands of external tools,\nAPIs, or agents-as-tools. However, traditional RAG-based tool retrieval fails\nto capture structured dependencies between tools, limiting the retrieval\naccuracy of a retrieved tool's dependencies. For example, among a vector\ndatabase of tools, a \"get stock price\" API requires a \"stock ticker\" parameter\nfrom a \"get stock ticker\" API, and both depend on OS-level internet\nconnectivity tools. In this paper, we address this limitation by introducing\nGraph RAG-Tool Fusion, a novel plug-and-play approach that combines the\nstrengths of vector-based retrieval with efficient graph traversal to capture\nall relevant tools (nodes) along with any nested dependencies (edges) within\nthe predefined tool knowledge graph. We also present ToolLinkOS, a new tool\nselection benchmark of 573 fictional tools, spanning over 15 industries, each\nwith an average of 6.3 tool dependencies. We demonstrate that Graph RAG-Tool\nFusion achieves absolute improvements of 71.7% and 22.1% over na\\\"ive RAG on\nToolLinkOS and ToolSandbox benchmarks, respectively (mAP@10). ToolLinkOS\ndataset is available at\nhttps://github.com/EliasLumer/Graph-RAG-Tool-Fusion-ToolLinkOS\n","authors":["Elias Lumer","Pradeep Honaganahalli Basavaraju","Myles Mason","James A. Burke","Vamse Kumar Subbiah"],"pdf_url":"https://arxiv.org/pdf/2502.07223v1.pdf","comment":"25 pages, 14 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.11235v2","updated":"2025-02-11T03:32:09Z","published":"2024-10-15T03:40:20Z","title":"GT2Vec: Large Language Models as Multi-Modal Encoders for Text and\n  Graph-Structured Data","summary":"  Graph-structured information offers rich contextual information that can\nenhance language models by providing structured relationships and hierarchies,\nleading to more expressive embeddings for various applications such as\nretrieval, question answering, and classification. However, existing methods\nfor integrating graph and text embeddings, often based on Multi-layer\nPerceptrons (MLPs) or shallow transformers, are limited in their ability to\nfully exploit the heterogeneous nature of these modalities. To overcome this,\nwe propose GT2Vec, a simple yet effective framework that leverages Large\nLanguage Models (LLMs) to jointly encode text and graph data. Specifically,\nGT2Vec employs an MLP adapter to project graph embeddings into the same space\nas text embeddings, allowing the LLM to process both modalities jointly. Unlike\nprior work, we also introduce contrastive learning to align the graph and text\nspaces more effectively, thereby improving the quality of learned joint\nembeddings. Empirical results across six datasets spanning three tasks,\nknowledge graph-contextualized question answering, graph-text pair\nclassification, and retrieval, demonstrate that GT2Vec consistently outperforms\nexisting baselines, achieving significant improvements across multiple\ndatasets. These results highlight GT2Vec's effectiveness in integrating graph\nand text data. Ablation studies further validate the effectiveness of our\nmethod.\n","authors":["Jiacheng Lin","Kun Qian","Haoyu Han","Nurendra Choudhary","Tianxin Wei","Zhongruo Wang","Sahika Genc","Edward W Huang","Sheng Wang","Karthik Subbian","Danai Koutra","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2410.11235v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13276v3","updated":"2025-02-11T03:11:48Z","published":"2024-10-17T07:07:09Z","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs","summary":"  Attention is the cornerstone of modern Large Language Models (LLMs). Yet its\nquadratic complexity hinders efficiency and scalability, especially for\nlong-context processing. A promising approach is to leverage sparsity in\nattention. However, existing sparsity-based solutions predominantly rely on\npredefined patterns or heuristics at the attention head level, struggling to\nadapt dynamically to different contexts efficiently.\n  We propose SeerAttention, a simple yet effective attention mechanism that\ndirectly learns the block-level attention sparsity from the LLM itself.\nInspired by the gating mechanism in Mixture of Experts (MoE), SeerAttention\naugments the conventional attention with a learnable gate that selectively\nactivates important blocks within the attention map. Specifically, the gate\nfirst pools the query (Q) and key (K) tensors along the sequence dimension and\nprocesses them through learnable linear layers. The resulting matrices are then\nmultiplied together to produce the gating scores, which are used to predict\nblock-level attention sparsity. Combined with our block-sparse FlashAttention\nkernel, SeerAttention can achieve significant speedup on GPUs. When applied to\npre-trained LLMs, SeerAttention only requires training the gate parameters in a\nlightweight self-distillation manner, allowing rapid convergence. Our\nevaluation results demonstrate that SeerAttention achieves better model\naccuracy and lower latency for long-context pre-filling compared to prior\nmethods.\n","authors":["Yizhao Gao","Zhichen Zeng","Dayou Du","Shijie Cao","Peiyuan Zhou","Jiaxing Qi","Junjie Lai","Hayden Kwok-Hay So","Ting Cao","Fan Yang","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13276v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07188v1","updated":"2025-02-11T02:30:21Z","published":"2025-02-11T02:30:21Z","title":"A Large-Scale Benchmark for Vietnamese Sentence Paraphrases","summary":"  This paper presents ViSP, a high-quality Vietnamese dataset for sentence\nparaphrasing, consisting of 1.2M original-paraphrase pairs collected from\nvarious domains. The dataset was constructed using a hybrid approach that\ncombines automatic paraphrase generation with manual evaluation to ensure high\nquality. We conducted experiments using methods such as back-translation, EDA,\nand baseline models like BART and T5, as well as large language models (LLMs),\nincluding GPT-4o, Gemini-1.5, Aya, Qwen-2.5, and Meta-Llama-3.1 variants. To\nthe best of our knowledge, this is the first large-scale study on Vietnamese\nparaphrasing. We hope that our dataset and findings will serve as a valuable\nfoundation for future research and applications in Vietnamese paraphrase tasks.\n","authors":["Sang Quang Nguyen","Kiet Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.07188v1.pdf","comment":"Accepted in NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.07186v1","updated":"2025-02-11T02:25:44Z","published":"2025-02-11T02:25:44Z","title":"Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs","summary":"  Zero-shot LLMs are now also used for textual classification tasks, e.g.,\nsentiment/emotion detection of a given input as a sentence/article. However,\ntheir performance can be suboptimal in such data annotation tasks. We introduce\na novel technique Perceived Confidence Scoring (PCS) that evaluates LLM's\nconfidence for its classification of an input by leveraging Metamorphic\nRelations (MRs). The MRs generate semantically equivalent yet textually mutated\nversions of the input. Following the principles of Metamorphic Testing (MT),\nthe mutated versions are expected to have annotation labels similar to the\ninput. By analyzing the consistency of LLM responses across these variations,\nPCS computes a confidence score based on the frequency of predicted labels. PCS\ncan be used both for single LLM and multiple LLM settings (e.g., majority\nvoting). We introduce an algorithm Perceived Differential Evolution (PDE) that\ndetermines the optimal weights assigned to the MRs and the LLMs for a\nclassification task. Empirical evaluation shows PCS significantly improves\nzero-shot accuracy for Llama-3-8B-Instruct (4.96%) and Mistral-7B-Instruct-v0.3\n(10.52%), with Gemma-2-9b-it showing a 9.39% gain. When combining all three\nmodels, PCS significantly outperforms majority voting by 7.75%.\n","authors":["Sina Salimian","Gias Uddin","Most Husne Jahan","Shaina Raza"],"pdf_url":"https://arxiv.org/pdf/2502.07186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07184v1","updated":"2025-02-11T02:19:13Z","published":"2025-02-11T02:19:13Z","title":"Refine Knowledge of Large Language Models via Adaptive Contrastive\n  Learning","summary":"  How to alleviate the hallucinations of Large Language Models (LLMs) has\nalways been the fundamental goal pursued by the LLMs research community.\nLooking through numerous hallucination-related studies, a mainstream category\nof methods is to reduce hallucinations by optimizing the knowledge\nrepresentation of LLMs to change their output. Considering that the core focus\nof these works is the knowledge acquired by models, and knowledge has long been\na central theme in human societal progress, we believe that the process of\nmodels refining knowledge can greatly benefit from the way humans learn. In our\nwork, by imitating the human learning process, we design an Adaptive\nContrastive Learning strategy. Our method flexibly constructs different\npositive and negative samples for contrastive learning based on LLMs' actual\nmastery of knowledge. This strategy helps LLMs consolidate the correct\nknowledge they already possess, deepen their understanding of the correct\nknowledge they have encountered but not fully grasped, forget the incorrect\nknowledge they previously learned, and honestly acknowledge the knowledge they\nlack. Extensive experiments and detailed analyses on widely used datasets\ndemonstrate the effectiveness of our method.\n","authors":["Yinghui Li","Haojing Huang","Jiayi Kuang","Yangning Li","Shu-Yu Guo","Chao Qu","Xiaoyu Tan","Hai-Tao Zheng","Ying Shen","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.07184v1.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2406.11230v2","updated":"2025-02-11T02:17:24Z","published":"2024-06-17T05:54:06Z","title":"Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of\n  Multimodal Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) have shown significant promise in\nvarious applications, leading to broad interest from researchers and\npractitioners alike. However, a comprehensive evaluation of their long-context\ncapabilities remains underexplored. To address these gaps, we introduce the\nMultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to\nassess the long-context capabilities of MLLMs. Besides multi-image input, we\nemploy image stitching to further increase the input context length, and\ndevelop a protocol to automatically generate labels for sub-image level\nretrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their\ncapability to locate a target sub-image (needle) within a set of images\n(haystack) based on textual instructions and descriptions of image contents.\nThis setup necessitates an advanced understanding of extensive visual contexts\nand effective information retrieval within long-context image inputs. With this\nbenchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and\nopen-source models. The findings reveal that GPT-4o consistently surpasses\nother models in long-context scenarios, but suffers from hallucination problems\nin negative samples, i.e., when needles are not in the haystacks. Our\ncomprehensive long-context evaluation of MLLMs also sheds lights on the\nconsiderable performance gap between API-based and open-source models. All the\ncode, data, and instructions required to reproduce the main results are\navailable at https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.\n","authors":["Hengyi Wang","Haizhou Shi","Shiwei Tan","Weiyi Qin","Wenyuan Wang","Tunyu Zhang","Akshay Nambi","Tanuja Ganu","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.11230v2.pdf","comment":"Accepted at NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2406.12665v3","updated":"2025-02-11T02:09:38Z","published":"2024-06-18T14:35:12Z","title":"CollabStory: Multi-LLM Collaborative Story Generation and Authorship\n  Analysis","summary":"  The rise of unifying frameworks that enable seamless interoperability of\nLarge Language Models (LLMs) has made LLM-LLM collaboration for open-ended\ntasks a possibility. Despite this, there have not been efforts to explore such\ncollaborative writing. We take the next step beyond human-LLM collaboration to\nexplore this multi-LLM scenario by generating the first exclusively\nLLM-generated collaborative stories dataset called CollabStory. We focus on\nsingle-author to multi-author (up to 5 LLMs) scenarios, where multiple LLMs\nco-author stories. We generate over 32k stories using open-source\ninstruction-tuned LLMs. Further, we take inspiration from the PAN tasks that\nhave set the standard for human-human multi-author writing tasks and analysis.\nWe extend their authorship-related tasks for multi-LLM settings and present\nbaselines for LLM-LLM collaboration. We find that current baselines are not\nable to handle this emerging scenario. Thus, CollabStory is a resource that\ncould help propel an understanding as well as the development of new techniques\nto discern the use of multiple LLMs. This is crucial to study in the context of\nwriting tasks since LLM-LLM collaboration could potentially overwhelm ongoing\nchallenges related to plagiarism detection, credit assignment, maintaining\nacademic integrity in educational settings, and addressing copyright\ninfringement concerns. We make our dataset and code available at\nhttps://github.com/saranya-venkatraman/CollabStory.\n","authors":["Saranya Venkatraman","Nafis Irtiza Tripto","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2406.12665v3.pdf","comment":"Accepted to NAACL Findings 2025"},{"id":"http://arxiv.org/abs/2502.04564v2","updated":"2025-02-11T01:56:16Z","published":"2025-02-06T23:38:29Z","title":"My LLM might Mimic AAE -- But When Should it?","summary":"  We examine the representation of African American English (AAE) in large\nlanguage models (LLMs), exploring (a) the perceptions Black Americans have of\nhow effective these technologies are at producing authentic AAE, and (b) in\nwhat contexts Black Americans find this desirable. Through both a survey of\nBlack Americans ($n=$ 104) and annotation of LLM-produced AAE by Black\nAmericans ($n=$ 228), we find that Black Americans favor choice and autonomy in\ndetermining when AAE is appropriate in LLM output. They tend to prefer that\nLLMs default to communicating in Mainstream U.S. English in formal settings,\nwith greater interest in AAE production in less formal settings. When LLMs were\nappropriately prompted and provided in context examples, our participants found\ntheir outputs to have a level of AAE authenticity on par with transcripts of\nBlack American speech. Select code and data for our project can be found here:\nhttps://github.com/smelliecat/AAEMime.git\n","authors":["Sandra C. Sandoval","Christabel Acquaye","Kwesi Cobbina","Mohammad Nayeem Teli","Hal Daumé III"],"pdf_url":"https://arxiv.org/pdf/2502.04564v2.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2410.09302v2","updated":"2025-02-11T01:46:35Z","published":"2024-10-11T23:29:20Z","title":"Enhancing Multi-Step Reasoning Abilities of Language Models through\n  Direct Q-Function Optimization","summary":"  Reinforcement Learning (RL) plays a crucial role in aligning large language\nmodels (LLMs) with human preferences and improving their ability to perform\ncomplex tasks. However, current approaches either require significant\ncomputational resources due to the use of multiple models and extensive online\nsampling for training (e.g., PPO) or are framed as bandit problems (e.g., DPO,\nDRO), which often struggle with multi-step reasoning tasks, such as math\nproblem solving and complex reasoning that involve long chains of thought. To\novercome these limitations, we introduce Direct Q-function Optimization (DQO),\nwhich formulates the response generation process as a Markov Decision Process\n(MDP) and utilizes the soft actor-critic (SAC) framework to optimize a\nQ-function directly parameterized by the language model. The MDP formulation of\nDQO offers structural advantages over bandit-based methods, enabling more\neffective process supervision. Experimental results on two math problem-solving\ndatasets, GSM8K and MATH, demonstrate that DQO outperforms previous methods,\nestablishing it as a promising offline reinforcement learning approach for\naligning language models.\n","authors":["Kaixuan Ji","Guanlin Liu","Ning Dai","Qingping Yang","Renjie Zheng","Zheng Wu","Chen Dun","Quanquan Gu","Lin Yan"],"pdf_url":"https://arxiv.org/pdf/2410.09302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10952v3","updated":"2025-02-11T01:46:13Z","published":"2024-06-16T14:12:37Z","title":"Avoiding Copyright Infringement via Large Language Model Unlearning","summary":"  Pre-trained Large Language Models (LLMs) have demonstrated remarkable\ncapabilities but also pose risks by learning and generating copyrighted\nmaterial, leading to significant legal and ethical concerns. In real-world\nscenarios, model owners need to continuously address copyright infringement as\nnew requests for content removal emerge at different time points. This leads to\nthe need for sequential unlearning, where copyrighted content is removed\nsequentially as new requests arise. Despite its practical relevance, sequential\nunlearning in the context of copyright infringement has not been rigorously\nexplored in existing literature. To address this gap, we propose Stable\nSequential Unlearning (SSU), a novel framework designed to unlearn copyrighted\ncontent from LLMs over multiple time steps. Our approach works by identifying\nand removing specific weight updates in the model's parameters that correspond\nto copyrighted content. We improve unlearning efficacy by introducing random\nlabeling loss and ensuring the model retains its general-purpose knowledge by\nadjusting targeted parameters. Experimental results show that SSU achieves an\neffective trade-off between unlearning efficacy and general-purpose language\nabilities, outperforming existing baselines.\n","authors":["Guangyao Dou","Zheyuan Liu","Qing Lyu","Kaize Ding","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2406.10952v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07165v1","updated":"2025-02-11T01:10:13Z","published":"2025-02-11T01:10:13Z","title":"Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent\n  Prompting Strategy for Text Classification","summary":"  We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent\nprompting strategy for text classification. It first asks multiple LLM agents\nto independently generate candidate principles based on analysis of\ndemonstration samples with or without labels, consolidates them into final\nprinciples via a finalizer agent, and then sends them to a classifier agent to\nperform downstream classification tasks. Extensive experiments on binary and\nmulti-class classification datasets with different sizes of LLMs show that our\napproach not only achieves substantial performance gains (1.55% - 19.37%) over\nzero-shot prompting on macro-F1 score but also outperforms other strong\nbaselines (CoT and stepback prompting). Principles generated by our approach\nhelp LLMs perform better on classification tasks than human crafted principles\non two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach\nalso shows on-par or better performance compared to demonstration-based\nfew-shot prompting approaches, yet with substantially lower inference costs.\nAblation studies show that label information and the multi-agent cooperative\nLLM framework play an important role in generating high-quality principles to\nfacilitate downstream classification tasks.\n","authors":["Peipei Wei","Dimitris Dimitriadis","Yan Xu","Mingwei Shen"],"pdf_url":"https://arxiv.org/pdf/2502.07165v1.pdf","comment":"To be published in AAAI 2025 Workshop on Advancing LLM-Based\n  Multi-Agent Collaboration"},{"id":"http://arxiv.org/abs/2502.07164v1","updated":"2025-02-11T01:03:33Z","published":"2025-02-11T01:03:33Z","title":"Does Training on Synthetic Data Make Models Less Robust?","summary":"  An increasingly common practice is to train large language models (LLMs)\nusing synthetic data. Often this synthetic data is produced by the same or\nsimilar LLMs as those it is being used to train. This raises the question of\nwhether the synthetic data might in fact exacerbate certain \"blindspots\" by\nreinforcing heuristics that the LLM already encodes. In this paper, we conduct\nsimulated experiments on the natural language inference (NLI) task with\nLlama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted\nevaluation set designed to measure the presence of specific heuristic\nstrategies for NLI, as our \"blindspot\" task. Our goal is to determine whether\nperformance disparities between the general and blind spot tasks emerge. Our\nresults indicate that synthetic data does not reinforce blindspots in the way\nwe expected. Specifically, we see that, while fine-tuning with synthetic data\ndoesn't necessarily reduce the use of the heuristic, it also does not make it\nworse as we hypothesized.\n","authors":["Lingze Zhang","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2502.07164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03161v2","updated":"2025-02-11T00:45:46Z","published":"2024-04-04T02:22:37Z","title":"BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro\n  QR Codes","summary":"  This paper introduces BioVL-QR, a biochemical vision-and-language dataset\ncomprising 23 egocentric experiment videos, corresponding protocols, and\nvision-and-language alignments. A major challenge in understanding biochemical\nvideos is detecting equipment, reagents, and containers because of the\ncluttered environment and indistinguishable objects. Previous studies assumed\nmanual object annotation, which is costly and time-consuming. To address the\nissue, we focus on Micro QR Codes. However, detecting objects using only Micro\nQR Codes is still difficult due to blur and occlusion caused by object\nmanipulation. To overcome this, we propose an object labeling method combining\na Micro QR Code detector with an off-the-shelf hand object detector. As an\napplication of the method and BioVL-QR, we tackled the task of localizing the\nprocedural steps in an instructional video. The experimental results show that\nusing Micro QR Codes and our method improves biochemical video understanding.\nData and code are available through https://nishi10mo.github.io/BioVL-QR/\n","authors":["Tomohiro Nishimoto","Taichi Nishimura","Koki Yamamoto","Keisuke Shirai","Hirotaka Kameko","Yuto Haneji","Tomoya Yoshida","Keiya Kajimura","Taiyu Cui","Chihiro Nishiwaki","Eriko Daikoku","Natsuko Okuda","Fumihito Ono","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2404.03161v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2502.07143v1","updated":"2025-02-11T00:13:52Z","published":"2025-02-11T00:13:52Z","title":"Ask Patients with Patience: Enabling LLMs for Human-Centric Medical\n  Dialogue with Grounded Reasoning","summary":"  Accurate and efficient diagnosis in online medical consultations remains a\nchallenge for current large language models. These models often rely on\nsingle-turn interactions and lack the ability to refine their predictions\nthrough follow-up questions. Additionally, their responses frequently contain\ncomplex medical terminology, making them less accessible to non-medical users\nand creating barriers to effective communication. In this paper, we introduce\nAsk Patients with Patience (APP), the first multi-turn dialogue that enables\nLLMs to iteratively refine diagnoses based on grounded reasoning. By\nintegrating medical guidelines and entropy minimization, APP improves both\ndiagnostic accuracy and efficiency. Furthermore, it features human-centric\ncommunication that bridges the gap between user comprehension and medical\nterminology, significantly enhancing user accessibility and engagement. We\nevaluated APP using a subset of the ReMeDi dataset, comparing it with\nsingle-turn and traditional multi-turn LLM baselines. APP achieved higher\nsimilarity scores in diagnosis predictions, demonstrating better alignment with\nground truth diagnoses. Entropy analysis showed that APP reduces diagnostic\nuncertainty more rapidly across iterations, increasing confidence in its\npredictions. APP also excels in user accessibility and empathy, further\nbridging the gap between complex medical language and user understanding. Code\nwill be released at: https://github.com/SuperMedIntel/AskPatients.\n","authors":["Jiayuan Zhu","Junde Wu"],"pdf_url":"https://arxiv.org/pdf/2502.07143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07139v1","updated":"2025-02-11T00:09:45Z","published":"2025-02-11T00:09:45Z","title":"Language-TPP: Integrating Temporal Point Processes with Language Models\n  for Event Analysis","summary":"  Temporal Point Processes (TPPs) have been widely used for event sequence\nmodeling, but they often struggle to incorporate rich textual event\ndescriptions effectively. Conversely, while Large Language Models (LLMs) have\nbeen shown remarkable capabilities in processing textual data, they lack\nmechanisms for handling temporal dynamics. To bridge this gap, we introduce\nLanguage-TPP, a unified framework that integrates TPPs with LLMs for enhanced\nevent sequence modeling. Language-TPP introduces a novel temporal encoding\nmechanism that converts continuous time intervals into specialized byte-tokens,\nenabling seamless integration with standard LLM architectures. This approach\nallows Language-TPP to achieve state-of-the-art performance across multiple TPP\ntasks, including event time prediction, type prediction, and intensity\nestimation, on five datasets. Additionally, we demonstrate that incorporating\ntemporal information significantly improves the quality of generated event\ndescriptions.\n","authors":["Quyu Kong","Yixuan Zhang","Yang Liu","Panrong Tong","Enqi Liu","Feng Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07138v1","updated":"2025-02-11T00:07:40Z","published":"2025-02-11T00:07:40Z","title":"Towards a Robust Framework for Multimodal Hate Detection: A Study on\n  Video vs. Image-based Content","summary":"  Social media platforms enable the propagation of hateful content across\ndifferent modalities such as textual, auditory, and visual, necessitating\neffective detection methods. While recent approaches have shown promise in\nhandling individual modalities, their effectiveness across different modality\ncombinations remains unexplored. This paper presents a systematic analysis of\nfusion-based approaches for multimodal hate detection, focusing on their\nperformance across video and image-based content. Our comprehensive evaluation\nreveals significant modality-specific limitations: while simple embedding\nfusion achieves state-of-the-art performance on video content (HateMM dataset)\nwith a 9.9% points F1-score improvement, it struggles with complex image-text\nrelationships in memes (Hateful Memes dataset). Through detailed ablation\nstudies and error analysis, we demonstrate how current fusion approaches fail\nto capture nuanced cross-modal interactions, particularly in cases involving\nbenign confounders. Our findings provide crucial insights for developing more\nrobust hate detection systems and highlight the need for modality-specific\narchitectural considerations. The code is available at\nhttps://github.com/gak97/Video-vs-Meme-Hate.\n","authors":["Girish A. Koushik","Diptesh Kanojia","Helen Treharne"],"pdf_url":"https://arxiv.org/pdf/2502.07138v1.pdf","comment":"Accepted to the MM4SG Workshop at the WebConf 2025"},{"id":"http://arxiv.org/abs/2502.08020v1","updated":"2025-02-11T23:40:53Z","published":"2025-02-11T23:40:53Z","title":"Speculate, then Collaborate: Fusing Knowledge of Language Models during\n  Decoding","summary":"  Large Language Models (LLMs) often excel in specific domains but fall short\nin others due to the limitations of their training. Thus, enabling LLMs to\nsolve problems collaboratively by integrating their complementary knowledge\npromises to improve their performance across domains. To realize this\npotential, we introduce a novel Collaborative Speculative Decoding (CoSD)\nalgorithm that enables efficient LLM knowledge fusion at test time without\nrequiring additional model training. CoSD employs a draft model to generate\ninitial sequences and an easy-to-learn rule or decision tree to decide when to\ninvoke an assistant model to improve these drafts. CoSD not only enhances\nknowledge fusion but also improves inference efficiency, is transferable across\ndomains and models, and offers greater explainability. Experimental results\ndemonstrate that CoSD improves accuracy by up to 10\\% across benchmarks\ncompared to existing methods, providing a scalable and effective solution for\nLLM-based applications\n","authors":["Ziyao Wang","Muneeza Azmart","Ang Li","Raya Horesh","Mikhail Yurochkin"],"pdf_url":"https://arxiv.org/pdf/2502.08020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08009v1","updated":"2025-02-11T23:09:50Z","published":"2025-02-11T23:09:50Z","title":"The Geometry of Prompting: Unveiling Distinct Mechanisms of Task\n  Adaptation in Language Models","summary":"  Decoder-only language models have the ability to dynamically switch between\nvarious computational tasks based on input prompts. Despite many successful\napplications of prompting, there is very limited understanding of the internal\nmechanism behind such flexibility. In this work, we investigate how different\nprompting methods affect the geometry of representations in these models.\nEmploying a framework grounded in statistical physics, we reveal that various\nprompting techniques, while achieving similar performance, operate through\ndistinct representational mechanisms for task adaptation. Our analysis\nhighlights the critical role of input distribution samples and label semantics\nin few-shot in-context learning. We also demonstrate evidence of synergistic\nand interfering interactions between different tasks on the representational\nlevel. Our work contributes to the theoretical understanding of large language\nmodels and lays the groundwork for developing more effective,\nrepresentation-aware prompting strategies.\n","authors":["Artem Kirsanov","Chi-Ning Chou","Kyunghyun Cho","SueYeon Chung"],"pdf_url":"https://arxiv.org/pdf/2502.08009v1.pdf","comment":"To appear in NAACL Findings 2025"},{"id":"http://arxiv.org/abs/2410.24155v2","updated":"2025-02-11T22:53:31Z","published":"2024-10-31T17:12:14Z","title":"Blind Spot Navigation in LLM Reasoning with Thought Space Explorer","summary":"  Recent advances in large language models (LLMs) have demonstrated their\npotential in handling complex reasoning tasks, which are usually achieved by\nconstructing a thought chain to guide the model to solve the problem with\nmulti-step thinking. However, existing methods often remain confined to\npreviously explored solution spaces and thus overlook the critical blind spot\nwithin LLMs' cognitive range. To address these issues, we design the Thought\nSpace Explorer (TSE), a novel framework to expand and optimize thought\nstructures to guide LLMs to explore their blind spots of thinking. By\ngenerating new reasoning steps and branches based on the original thought\nstructure with various designed strategies, TSE broadens the thought space and\nalleviates the impact of blind spots for LLM reasoning. Experimental results on\nmultiple levels of reasoning tasks demonstrate the efficacy of TSE. We also\nconduct extensive analysis to understand how structured and expansive thought\ncan contribute to unleashing the potential of LLM reasoning capabilities.\n","authors":["Jinghan Zhang","Fengran Mo","Xiting Wang","Kunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07985v1","updated":"2025-02-11T22:06:25Z","published":"2025-02-11T22:06:25Z","title":"MetaSC: Test-Time Safety Specification Optimization for Language Models","summary":"  We propose a novel dynamic safety framework that optimizes language model\n(LM) safety reasoning at inference time without modifying model weights.\nBuilding on recent advances in self-critique methods, our approach leverages a\nmeta-critique mechanism that iteratively updates safety prompts-termed\nspecifications-to drive the critique and revision process adaptively. This\ntest-time optimization not only improves performance against adversarial\njailbreak requests but also in diverse general safety-related tasks, such as\navoiding moral harm or pursuing honest responses. Our empirical evaluations\nacross several language models demonstrate that dynamically optimized safety\nprompts yield significantly higher safety scores compared to fixed system\nprompts and static self-critique defenses. Code to be released at\nhttps://github.com/vicgalle/meta-self-critique.git .\n","authors":["Víctor Gallego"],"pdf_url":"https://arxiv.org/pdf/2502.07985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07972v1","updated":"2025-02-11T21:36:31Z","published":"2025-02-11T21:36:31Z","title":"Training Sparse Mixture Of Experts Text Embedding Models","summary":"  Transformer-based text embedding models have improved their performance on\nbenchmarks like MIRACL and BEIR by increasing their parameter counts. However,\nthis scaling approach introduces significant deployment challenges, including\nincreased inference latency and memory usage. These challenges are particularly\nsevere in retrieval-augmented generation (RAG) applications, where large\nmodels' increased memory requirements constrain dataset ingestion capacity, and\ntheir higher latency directly impacts query-time performance. While causal\nlanguage models have addressed similar efficiency challenges using Mixture of\nExperts (MoE) architectures, this approach hasn't been successfully adapted to\nthe general text embedding setting. In this paper, we introduce Nomic Embed v2,\nthe first general purpose MoE text embedding model. Our model outperforms\nmodels in the same parameter class on both monolingual and multilingual\nbenchmarks while also maintaining competitive performance with models twice its\nsize. We open-source all code, models, and evaluation data to ensure full\nreproducibility of our training pipeline.\n","authors":["Zach Nussbaum","Brandon Duderstadt"],"pdf_url":"https://arxiv.org/pdf/2502.07972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05400v2","updated":"2025-02-11T21:30:16Z","published":"2025-02-08T01:20:09Z","title":"Dynamic Noise Preference Optimization for LLM Self-Improvement via\n  Synthetic Data","summary":"  Although LLMs have achieved significant success, their reliance on large\nvolumes of human-annotated data has limited their potential for further\nscaling. In this situation, utilizing self-generated synthetic data has become\ncrucial for fine-tuning LLMs without extensive human annotation. However,\ncurrent methods often fail to ensure consistent improvements across iterations,\nwith performance stagnating after only minimal updates. To overcome these\nchallenges, we introduce Dynamic Noise Preference Optimization (DNPO). DNPO\nemploys a dynamic sample labeling mechanism to construct preference pairs for\ntraining and introduces controlled, trainable noise into the preference\noptimization process. Our approach effectively prevents stagnation and enables\ncontinuous improvement. In experiments with Zephyr-7B, DNPO consistently\noutperforms existing methods, showing an average performance boost of 2.6%\nacross multiple benchmarks. Additionally, DNPO shows a significant improvement\nin model-generated data quality, with a 29.4% win-loss rate gap compared to the\nbaseline in GPT-4 evaluations. This highlights its effectiveness in enhancing\nmodel performance through iterative refinement.\n","authors":["Haoyan Yang","Ting Hua","Shangqian Gao","Binfeng Xu","Zheng Tang","Jie Xu","Hongxia Jin","Vijay Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2502.05400v2.pdf","comment":"Due to an update in the company's publication approval process, a\n  newly appointed manager has been added to the review workflow. As a result,\n  we need to resubmit the application for approval under the revised process.\n  Therefore, we are temporarily withdrawing this submission until the new\n  approval workflow is completed"},{"id":"http://arxiv.org/abs/2502.07963v1","updated":"2025-02-11T21:21:05Z","published":"2025-02-11T21:21:05Z","title":"Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?","summary":"  Medical research faces well-documented challenges in translating novel\ntreatments into clinical practice. Publishing incentives encourage researchers\nto present \"positive\" findings, even when empirical results are equivocal.\nConsequently, it is well-documented that authors often spin study results,\nespecially in article abstracts. Such spin can influence clinician\ninterpretation of evidence and may affect patient care decisions. In this\nstudy, we ask whether the interpretation of trial results offered by Large\nLanguage Models (LLMs) is similarly affected by spin. This is important since\nLLMs are increasingly being used to trawl through and synthesize published\nmedical evidence. We evaluated 22 LLMs and found that they are across the board\nmore susceptible to spin than humans. They might also propagate spin into their\noutputs: We find evidence, e.g., that LLMs implicitly incorporate spin into\nplain language summaries that they generate. We also find, however, that LLMs\nare generally capable of recognizing spin, and can be prompted in a way to\nmitigate spin's impact on LLM outputs.\n","authors":["Hye Sun Yun","Karen Y. C. Zhang","Ramez Kouzy","Iain J. Marshall","Junyi Jessy Li","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2502.07963v1.pdf","comment":"20 pages, 10 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.07938v1","updated":"2025-02-11T20:35:29Z","published":"2025-02-11T20:35:29Z","title":"Adapting Multilingual Embedding Models to Historical Luxembourgish","summary":"  The growing volume of digitized historical texts requires effective semantic\nsearch using text embeddings. However, pre-trained multilingual models,\ntypically evaluated on contemporary texts, face challenges with historical\ndigitized content due to OCR noise and outdated spellings. We explore the use\nof multilingual embeddings for cross-lingual semantic search on historical\nLuxembourgish, a low-resource language. We collect historical Luxembourgish\nnews articles spanning various time periods and use GPT-4o to segment and\ntranslate them into closely related languages, creating 20,000 parallel\ntraining sentences per language pair. We further create a historical bitext\nmining evaluation set and find that these models struggle to perform\ncross-lingual search on historical Luxembourgish. To address this, we propose a\nsimple adaptation method using in-domain training data, achieving up to 98\\%\naccuracy in cross-lingual evaluations. We release our adapted models and\nhistorical Luxembourgish-German/French bitexts to support further research.\n","authors":["Andrianos Michail","Corina Julia Raclé","Juri Opitz","Simon Clematide"],"pdf_url":"https://arxiv.org/pdf/2502.07938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11295v4","updated":"2025-02-11T19:42:26Z","published":"2024-09-17T15:49:44Z","title":"EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage","summary":"  Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies.\n","authors":["Zeyi Liao","Lingbo Mo","Chejian Xu","Mintong Kang","Jiawei Zhang","Chaowei Xiao","Yuan Tian","Bo Li","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2409.11295v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2404.14394v2","updated":"2025-02-11T19:35:42Z","published":"2024-04-22T17:55:11Z","title":"A Multimodal Automated Interpretability Agent","summary":"  This paper describes MAIA, a Multimodal Automated Interpretability Agent.\nMAIA is a system that uses neural models to automate neural model understanding\ntasks like feature interpretation and failure mode discovery. It equips a\npre-trained vision-language model with a set of tools that support iterative\nexperimentation on subcomponents of other models to explain their behavior.\nThese include tools commonly used by human interpretability researchers: for\nsynthesizing and editing inputs, computing maximally activating exemplars from\nreal-world datasets, and summarizing and describing experimental results.\nInterpretability experiments proposed by MAIA compose these tools to describe\nand explain system behavior. We evaluate applications of MAIA to computer\nvision models. We first characterize MAIA's ability to describe (neuron-level)\nfeatures in learned representations of images. Across several trained models\nand a novel dataset of synthetic vision neurons with paired ground-truth\ndescriptions, MAIA produces descriptions comparable to those generated by\nexpert human experimenters. We then show that MAIA can aid in two additional\ninterpretability tasks: reducing sensitivity to spurious features, and\nautomatically identifying inputs likely to be mis-classified.\n","authors":["Tamar Rott Shaham","Sarah Schwettmann","Franklin Wang","Achyuta Rajaram","Evan Hernandez","Jacob Andreas","Antonio Torralba"],"pdf_url":"https://arxiv.org/pdf/2404.14394v2.pdf","comment":"25 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.07912v1","updated":"2025-02-11T19:33:07Z","published":"2025-02-11T19:33:07Z","title":"Elevating Legal LLM Responses: Harnessing Trainable Logical Structures\n  and Semantic Knowledge with Legal Reasoning","summary":"  Large Language Models (LLMs) have achieved impressive results across numerous\ndomains, yet they experience notable deficiencies in legal question-answering\ntasks. LLMs often generate generalized responses that lack the logical\nspecificity required for expert legal advice and are prone to hallucination,\nproviding answers that appear correct but are unreliable. Retrieval-Augmented\nGeneration (RAG) techniques offer partial solutions to address this challenge,\nbut existing approaches typically focus only on semantic similarity, neglecting\nthe logical structure essential to legal reasoning. In this paper, we propose\nthe Logical-Semantic Integration Model (LSIM), a novel supervised framework\nthat bridges semantic and logical coherence. LSIM comprises three components:\nreinforcement learning predicts a structured fact-rule chain for each question,\na trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant\ncandidate questions by integrating semantic and logical features, and\nin-context learning generates the final answer using the retrieved content. Our\nexperiments on a real-world legal QA dataset-validated through both automated\nmetrics and human evaluation-demonstrate that LSIM significantly enhances\naccuracy and reliability compared to existing methods.\n","authors":["Rujing Yao","Yang Wu","Chenghao Wang","Jingwei Xiong","Fang Wang","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17892v2","updated":"2025-02-11T19:24:09Z","published":"2024-09-26T14:40:45Z","title":"EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language\n  Models","summary":"  In this work, we introduce EMMA-500, a large-scale multilingual language\nmodel continue-trained on texts across 546 languages designed for enhanced\nmultilingual performance, focusing on improving language coverage for\nlow-resource languages. To facilitate continual pre-training, we compile the\nMaLA corpus, a comprehensive multilingual dataset enriched with curated\ndatasets across diverse domains. Leveraging this corpus, we conduct extensive\ncontinual pre-training of the Llama 2 7B model, resulting in EMMA-500, which\ndemonstrates robust performance across a wide collection of benchmarks,\nincluding a comprehensive set of multilingual tasks. Our results highlight the\neffectiveness of continual pre-training in expanding large language models'\nlanguage capacity, particularly for underrepresented languages, demonstrating\nsignificant gains in cross-lingual transfer, task generalization, and language\nadaptability. We release the MaLA corpus, EMMA-500 model weights, scripts, and\nmodel generations.\n","authors":["Shaoxiong Ji","Zihao Li","Indraneil Paul","Jaakko Paavola","Peiqin Lin","Pinzhen Chen","Dayyán O'Brien","Hengyu Luo","Hinrich Schütze","Jörg Tiedemann","Barry Haddow"],"pdf_url":"https://arxiv.org/pdf/2409.17892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09763v2","updated":"2025-02-11T19:21:52Z","published":"2023-11-16T10:38:43Z","title":"Test-time Backdoor Mitigation for Black-Box Large Language Models with\n  Defensive Demonstrations","summary":"  Existing studies in backdoor defense have predominantly focused on the\ntraining phase, overlooking the critical aspect of testing time defense. This\ngap becomes pronounced in the context of LLMs deployed as Web Services, which\ntypically offer only black-box access, rendering training-time defenses\nimpractical. To bridge this gap, this study critically examines the use of\ndemonstrations as a defense mechanism against backdoor attacks in black-box\nLLMs. We retrieve task-relevant demonstrations from a clean data pool and\nintegrate them with user queries during testing. This approach does not\nnecessitate modifications or tuning of the model, nor does it require insight\ninto the model's internal architecture. The alignment properties inherent in\nin-context learning play a pivotal role in mitigating the impact of backdoor\ntriggers, effectively recalibrating the behavior of compromised models. Our\nexperimental analysis demonstrates that this method robustly defends against\nboth instance-level and instruction-level backdoor attacks, outperforming\nexisting defense baselines across most evaluation scenarios.\n","authors":["Wenjie Mo","Jiashu Xu","Qin Liu","Jiongxiao Wang","Jun Yan","Hadi Askari","Chaowei Xiao","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2311.09763v2.pdf","comment":"Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07904v1","updated":"2025-02-11T19:19:08Z","published":"2025-02-11T19:19:08Z","title":"Intelligent Legal Assistant: An Interactive Clarification System for\n  Legal Question Answering","summary":"  The rise of large language models has opened new avenues for users seeking\nlegal advice. However, users often lack professional legal knowledge, which can\nlead to questions that omit critical information. This deficiency makes it\nchallenging for traditional legal question-answering systems to accurately\nidentify users' actual needs, often resulting in imprecise or generalized\nadvice. In this work, we develop a legal question-answering system called\nIntelligent Legal Assistant, which interacts with users to precisely capture\ntheir needs. When a user poses a question, the system requests that the user\nselect their geographical location to pinpoint the applicable laws. It then\ngenerates clarifying questions and options based on the key information missing\nfrom the user's initial question. This allows the user to select and provide\nthe necessary details. Once all necessary information is provided, the system\nproduces an in-depth legal analysis encompassing three aspects: overall\nconclusion, jurisprudential analysis, and resolution suggestions.\n","authors":["Rujing Yao","Yiquan Wu","Tong Zhang","Xuhui Zhang","Yuting Huang","Yang Wu","Jiayin Yang","Changlong Sun","Fang Wang","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00075v3","updated":"2025-02-11T19:08:08Z","published":"2024-06-21T19:18:16Z","title":"Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference","summary":"  We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.\n","authors":["Anton Xue","Avishree Khare","Rajeev Alur","Surbhi Goel","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2407.00075v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.07784v1","updated":"2025-02-11T18:59:59Z","published":"2025-02-11T18:59:59Z","title":"MatSwap: Light-aware material transfers in images","summary":"  We present MatSwap, a method to transfer materials to designated surfaces in\nan image photorealistically. Such a task is non-trivial due to the large\nentanglement of material appearance, geometry, and lighting in a photograph. In\nthe literature, material editing methods typically rely on either cumbersome\ntext engineering or extensive manual annotations requiring artist knowledge and\n3D scene properties that are impractical to obtain. In contrast, we propose to\ndirectly learn the relationship between the input material -- as observed on a\nflat surface -- and its appearance within the scene, without the need for\nexplicit UV mapping. To achieve this, we rely on a custom light- and\ngeometry-aware diffusion model. We fine-tune a large-scale pre-trained\ntext-to-image model for material transfer using our synthetic dataset,\npreserving its strong priors to ensure effective generalization to real images.\nAs a result, our method seamlessly integrates a desired material into the\ntarget location in the photograph while retaining the identity of the scene. We\nevaluate our method on synthetic and real images and show that it compares\nfavorably to recent work both qualitatively and quantitatively. We will release\nour code and data upon publication.\n","authors":["Ivan Lopes","Valentin Deschaintre","Yannick Hold-Geoffroy","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2502.07784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07785v1","updated":"2025-02-11T18:59:59Z","published":"2025-02-11T18:59:59Z","title":"Pippo: High-Resolution Multi-View Humans from a Single Image","summary":"  We present Pippo, a generative model capable of producing 1K resolution dense\nturnaround videos of a person from a single casually clicked photo. Pippo is a\nmulti-view diffusion transformer and does not require any additional inputs -\ne.g., a fitted parametric model or camera parameters of the input image. We\npre-train Pippo on 3B human images without captions, and conduct multi-view\nmid-training and post-training on studio captured humans. During mid-training,\nto quickly absorb the studio dataset, we denoise several (up to 48) views at\nlow-resolution, and encode target cameras coarsely using a shallow MLP. During\npost-training, we denoise fewer views at high-resolution and use pixel-aligned\ncontrols (e.g., Spatial anchor and Plucker rays) to enable 3D consistent\ngenerations. At inference, we propose an attention biasing technique that\nallows Pippo to simultaneously generate greater than 5 times as many views as\nseen during training. Finally, we also introduce an improved metric to evaluate\n3D consistency of multi-view generations, and show that Pippo outperforms\nexisting works on multi-view human generation from a single image.\n","authors":["Yash Kant","Ethan Weber","Jin Kyu Kim","Rawal Khirodkar","Su Zhaoen","Julieta Martinez","Igor Gilitschenski","Shunsuke Saito","Timur Bagautdinov"],"pdf_url":"https://arxiv.org/pdf/2502.07785v1.pdf","comment":"Project Page - http://yashkant.github.io/pippo"},{"id":"http://arxiv.org/abs/2502.07782v1","updated":"2025-02-11T18:59:52Z","published":"2025-02-11T18:59:52Z","title":"A Flag Decomposition for Hierarchical Datasets","summary":"  Flag manifolds encode hierarchical nested sequences of subspaces and serve as\npowerful structures for various computer vision and machine learning\napplications. Despite their utility in tasks such as dimensionality reduction,\nmotion averaging, and subspace clustering, current applications are often\nrestricted to extracting flags using common matrix decomposition methods like\nthe singular value decomposition. Here, we address the need for a general\nalgorithm to factorize and work with hierarchical datasets. In particular, we\npropose a novel, flag-based method that decomposes arbitrary hierarchical\nreal-valued data into a hierarchy-preserving flag representation in Stiefel\ncoordinates. Our work harnesses the potential of flag manifolds in applications\nincluding denoising, clustering, and few-shot learning.\n","authors":["Nathan Mankovich","Ignacio Santamaria","Gustau Camps-Valls","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2502.07782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07778v1","updated":"2025-02-11T18:59:10Z","published":"2025-02-11T18:59:10Z","title":"Stay-Positive: A Case for Ignoring Real Image Features in Fake Image\n  Detection","summary":"  Detecting AI generated images is a challenging yet essential task. A primary\ndifficulty arises from the detectors tendency to rely on spurious patterns,\nsuch as compression artifacts, which can influence its decisions. These issues\noften stem from specific patterns that the detector associates with the real\ndata distribution, making it difficult to isolate the actual generative traces.\nWe argue that an image should be classified as fake if and only if it contains\nartifacts introduced by the generative model. Based on this premise, we propose\nStay Positive, an algorithm designed to constrain the detectors focus to\ngenerative artifacts while disregarding those associated with real data.\nExperimental results demonstrate that detectors trained with Stay Positive\nexhibit reduced susceptibility to spurious correlations, leading to improved\ngeneralization and robustness to post processing. Additionally, unlike\ndetectors that associate artifacts with real images, those that focus purely on\nfake artifacts are better at detecting inpainted real images.\n","authors":["Anirudh Sundara Rajan","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2502.07778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01846v2","updated":"2025-02-11T18:44:48Z","published":"2025-02-03T21:46:30Z","title":"UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping","summary":"  3D Gaussian Splatting (3DGS) has demonstrated superior quality in modeling 3D\nobjects and scenes. However, generating 3DGS remains challenging due to their\ndiscrete, unstructured, and permutation-invariant nature. In this work, we\npresent a simple yet effective method to overcome these challenges. We utilize\nspherical mapping to transform 3DGS into a structured 2D representation, termed\nUVGS. UVGS can be viewed as multi-channel images, with feature dimensions as a\nconcatenation of Gaussian attributes such as position, scale, color, opacity,\nand rotation. We further find that these heterogeneous features can be\ncompressed into a lower-dimensional (e.g., 3-channel) shared feature space\nusing a carefully designed multi-branch network. The compressed UVGS can be\ntreated as typical RGB images. Remarkably, we discover that typical VAEs\ntrained with latent diffusion models can directly generalize to this new\nrepresentation without additional training. Our novel representation makes it\neffortless to leverage foundational 2D models, such as diffusion models, to\ndirectly model 3DGS. Additionally, one can simply increase the 2D UV resolution\nto accommodate more Gaussians, making UVGS a scalable solution compared to\ntypical 3D backbones. This approach immediately unlocks various novel\ngeneration applications of 3DGS by inherently utilizing the already developed\nsuperior 2D generation capabilities. In our experiments, we demonstrate various\nunconditional, conditional generation, and inpainting applications of 3DGS\nbased on diffusion models, which were previously non-trivial.\n","authors":["Aashish Rai","Dilin Wang","Mihir Jain","Nikolaos Sarafianos","Kefan Chen","Srinath Sridhar","Aayush Prakash"],"pdf_url":"https://arxiv.org/pdf/2502.01846v2.pdf","comment":"https://aashishrai3799.github.io/uvgs"},{"id":"http://arxiv.org/abs/2407.10366v2","updated":"2025-02-11T18:44:46Z","published":"2024-07-15T00:13:53Z","title":"Accessing Vision Foundation Models via ImageNet-1K","summary":"  Vision foundation models are renowned for the generalization ability due to\nmassive training data. Nevertheless, they demand tremendous training resources,\nand the training data is often inaccessible, e.g., CLIP, DINOv2, posing great\nchallenges to developing derivatives that could facilitate the research. In\nthis work, we offer a very simple and general solution, named \\textit{Proteus},\nto distill foundation models into smaller equivalents on ImageNet-1K without\naccess to the original training data. Specifically, we remove the designs from\nconventional knowledge distillation settings that result in dataset bias and\npresent three levels of training objectives, i.e., token, patch, and feature,\nto maximize the efficacy of knowledge transfer. In this manner, Proteus is\ntrained at ImageNet-level costs with surprising ability, facilitating the\naccessibility of training foundation models for the broader research community.\nWhen leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the\nperformance of the Oracle method DINOv2-L/14 (142M training data) across 19\nbenchmarks and outperforms other vision foundation models including CLIP-L/14\n(400M), OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M) with a significantly\nsmaller training set of 1.2M images.\n","authors":["Yitian Zhang","Xu Ma","Yue Bai","Huan Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2407.10366v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.07758v1","updated":"2025-02-11T18:38:02Z","published":"2025-02-11T18:38:02Z","title":"Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras","summary":"  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n","authors":["Nektarios A. Valous","Eckhard Hitzer","Dragoş Duşe","Rodrigo Rojas Moraleda","Ferdinand Popp","Meggy Suarez-Carmona","Anna Berthel","Ismini Papageorgiou","Carlo Fremd","Alexander Rölle","Christina C. Westhoff","Bénédicte Lenoir","Niels Halama","Inka Zörnig","Dirk Jäger"],"pdf_url":"https://arxiv.org/pdf/2502.07758v1.pdf","comment":"24 pages, 18 figures, 14 tables"},{"id":"http://arxiv.org/abs/2502.02027v3","updated":"2025-02-11T18:33:27Z","published":"2025-02-04T05:24:44Z","title":"From Fog to Failure: How Dehazing Can Harm Clear Image Object Detection","summary":"  This study explores the challenges of integrating human visual cue-based\ndehazing into object detection, given the selective nature of human perception.\nWhile human vision adapts dynamically to environmental conditions,\ncomputational dehazing does not always enhance detection uniformly. We propose\na multi-stage framework where a lightweight detector identifies regions of\ninterest (RoIs), which are then enhanced via spatial attention-based dehazing\nbefore final detection by a heavier model. Though effective in foggy\nconditions, this approach unexpectedly degrades the performance on clear\nimages. We analyze this phenomenon, investigate possible causes, and offer\ninsights for designing hybrid pipelines that balance enhancement and detection.\nOur findings highlight the need for selective preprocessing and challenge\nassumptions about universal benefits from cascading transformations.\n","authors":["Ashutosh Kumar","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2502.02027v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07754v1","updated":"2025-02-11T18:27:39Z","published":"2025-02-11T18:27:39Z","title":"MeshSplats: Mesh-Based Rendering with Gaussian Splatting Initialization","summary":"  Gaussian Splatting (GS) is a recent and pivotal technique in 3D computer\ngraphics. GS-based algorithms almost always bypass classical methods such as\nray tracing, which offers numerous inherent advantages for rendering. For\nexample, ray tracing is able to handle incoherent rays for advanced lighting\neffects, including shadows and reflections. To address this limitation, we\nintroduce MeshSplats, a method which converts GS to a mesh-like format.\nFollowing the completion of training, MeshSplats transforms Gaussian elements\ninto mesh faces, enabling rendering using ray tracing methods with all their\nassociated benefits. Our model can be utilized immediately following\ntransformation, yielding a mesh of slightly reduced quality without additional\ntraining. Furthermore, we can enhance the reconstruction quality through the\napplication of a dedicated optimization algorithm that operates on mesh faces\nrather than Gaussian components. The efficacy of our method is substantiated by\nexperimental results, underscoring its extensive applications in computer\ngraphics and image processing.\n","authors":["Rafał Tobiasz","Grzegorz Wilczyński","Marcin Mazur","Sławomir Tadeja","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2502.07754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07753v1","updated":"2025-02-11T18:27:27Z","published":"2025-02-11T18:27:27Z","title":"Direct Ascent Synthesis: Revealing Hidden Generative Capabilities in\n  Discriminative Models","summary":"  We demonstrate that discriminative models inherently contain powerful\ngenerative capabilities, challenging the fundamental distinction between\ndiscriminative and generative architectures. Our method, Direct Ascent\nSynthesis (DAS), reveals these latent capabilities through multi-resolution\noptimization of CLIP model representations. While traditional inversion\nattempts produce adversarial patterns, DAS achieves high-quality image\nsynthesis by decomposing optimization across multiple spatial scales (1x1 to\n224x224), requiring no additional training. This approach not only enables\ndiverse applications -- from text-to-image generation to style transfer -- but\nmaintains natural image statistics ($1/f^2$ spectrum) and guides the generation\naway from non-robust adversarial patterns. Our results demonstrate that\nstandard discriminative models encode substantially richer generative knowledge\nthan previously recognized, providing new perspectives on model\ninterpretability and the relationship between adversarial examples and natural\nimage synthesis.\n","authors":["Stanislav Fort","Jonathan Whitaker"],"pdf_url":"https://arxiv.org/pdf/2502.07753v1.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.07751v1","updated":"2025-02-11T18:26:22Z","published":"2025-02-11T18:26:22Z","title":"CausalGeD: Blending Causality and Diffusion for Spatial Gene Expression\n  Generation","summary":"  The integration of single-cell RNA sequencing (scRNA-seq) and spatial\ntranscriptomics (ST) data is crucial for understanding gene expression in\nspatial context. Existing methods for such integration have limited\nperformance, with structural similarity often below 60\\%, We attribute this\nlimitation to the failure to consider causal relationships between genes. We\npresent CausalGeD, which combines diffusion and autoregressive processes to\nleverage these relationships. By generalizing the Causal Attention Transformer\nfrom image generation to gene expression data, our model captures regulatory\nmechanisms without predefined relationships. Across 10 tissue datasets,\nCausalGeD outperformed state-of-the-art baselines by 5- 32\\% in key metrics,\nincluding Pearson's correlation and structural similarity, advancing both\ntechnical and biological insights.\n","authors":["Rabeya Tus Sadia","Md Atik Ahamed","Qiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07737v1","updated":"2025-02-11T17:57:53Z","published":"2025-02-11T17:57:53Z","title":"Next Block Prediction: Video Generation via Semi-Auto-Regressive\n  Modeling","summary":"  Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)\nvideo generation, but it suffers from suboptimal unidirectional dependencies\nand slow inference speed. In this work, we propose a semi-autoregressive\n(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.\nBy uniformly decomposing video content into equal-sized blocks (e.g., rows or\nframes), we shift the generation unit from individual tokens to blocks,\nallowing each token in the current block to simultaneously predict the\ncorresponding token in the next block. Unlike traditional AR modeling, our\nframework employs bidirectional attention within each block, enabling tokens to\ncapture more robust spatial dependencies. By predicting multiple tokens in\nparallel, NBP models significantly reduce the number of generation steps,\nleading to faster and more efficient inference. Our model achieves FVD scores\nof 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an\naverage of 4.4. Furthermore, thanks to the reduced number of inference steps,\nthe NBP model generates 8.89 frames (128x128 resolution) per second, achieving\nan 11x speedup. We also explored model scales ranging from 700M to 3B\nparameters, observing significant improvements in generation quality, with FVD\nscores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,\ndemonstrating the scalability of our approach.\n","authors":["Shuhuai Ren","Shuming Ma","Xu Sun","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.07737v1.pdf","comment":"project page: https://renshuhuai-andy.github.io/NBP-project/"},{"id":"http://arxiv.org/abs/2409.20562v2","updated":"2025-02-11T17:53:46Z","published":"2024-09-30T17:59:03Z","title":"SpaceMesh: A Continuous Representation for Learning Manifold Surface\n  Meshes","summary":"  Meshes are ubiquitous in visual computing and simulation, yet most existing\nmachine learning techniques represent meshes only indirectly, e.g. as the level\nset of a scalar field or deformation of a template, or as a disordered triangle\nsoup lacking local structure. This work presents a scheme to directly generate\nmanifold, polygonal meshes of complex connectivity as the output of a neural\nnetwork. Our key innovation is to define a continuous latent connectivity space\nat each mesh vertex, which implies the discrete mesh. In particular, our vertex\nembeddings generate cyclic neighbor relationships in a halfedge mesh\nrepresentation, which gives a guarantee of edge-manifoldness and the ability to\nrepresent general polygonal meshes. This representation is well-suited to\nmachine learning and stochastic optimization, without restriction on\nconnectivity or topology. We first explore the basic properties of this\nrepresentation, then use it to fit distributions of meshes from large datasets.\nThe resulting models generate diverse meshes with tessellation structure\nlearned from the dataset population, with concise details and high-quality mesh\nelements. In applications, this approach not only yields high-quality outputs\nfrom generative models, but also enables directly learning challenging geometry\nprocessing tasks such as mesh repair.\n","authors":["Tianchang Shen","Zhaoshuo Li","Marc Law","Matan Atzmon","Sanja Fidler","James Lucas","Jun Gao","Nicholas Sharp"],"pdf_url":"https://arxiv.org/pdf/2409.20562v2.pdf","comment":"published at SIGGRAPH Asia 2024"},{"id":"http://arxiv.org/abs/2502.07734v1","updated":"2025-02-11T17:53:33Z","published":"2025-02-11T17:53:33Z","title":"EdgeEar: Efficient and Accurate Ear Recognition for Edge Devices","summary":"  Ear recognition is a contactless and unobtrusive biometric technique with\napplications across various domains. However, deploying high-performing ear\nrecognition models on resource-constrained devices is challenging, limiting\ntheir applicability and widespread adoption. This paper introduces EdgeEar, a\nlightweight model based on a proposed hybrid CNN-transformer architecture to\nsolve this problem. By incorporating low-rank approximations into specific\nlinear layers, EdgeEar reduces its parameter count by a factor of 50 compared\nto the current state-of-the-art, bringing it below two million while\nmaintaining competitive accuracy. Evaluation on the Unconstrained Ear\nRecognition Challenge (UERC2023) benchmark shows that EdgeEar achieves the\nlowest EER while significantly reducing computational costs. These findings\ndemonstrate the feasibility of efficient and accurate ear recognition, which we\nbelieve will contribute to the wider adoption of ear biometrics.\n","authors":["Camile Lendering","Bernardo Perrone Ribeiro","Žiga Emeršič","Peter Peer"],"pdf_url":"https://arxiv.org/pdf/2502.07734v1.pdf","comment":"Submitted to IEEE FG 2025"},{"id":"http://arxiv.org/abs/2502.07732v1","updated":"2025-02-11T17:51:52Z","published":"2025-02-11T17:51:52Z","title":"Economics of Sourcing Human Data","summary":"  Progress in AI has relied on human-generated data, from annotator\nmarketplaces to the wider Internet. However, the widespread use of large\nlanguage models now threatens the quality and integrity of human-generated data\non these very platforms. We argue that this issue goes beyond the immediate\nchallenge of filtering AI-generated content--it reveals deeper flaws in how\ndata collection systems are designed. Existing systems often prioritize speed,\nscale, and efficiency at the cost of intrinsic human motivation, leading to\ndeclining engagement and data quality. We propose that rethinking data\ncollection systems to align with contributors' intrinsic motivations--rather\nthan relying solely on external incentives--can help sustain high-quality data\nsourcing at scale while maintaining contributor trust and long-term\nparticipation.\n","authors":["Sebastin Santy","Prasanta Bhattacharya","Manoel Horta Ribeiro","Kelsey Allen","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2502.07732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08281v3","updated":"2025-02-11T17:43:59Z","published":"2024-01-16T11:12:36Z","title":"The Faiss library","summary":"  Vector databases typically manage large collections of embedding vectors.\nCurrently, AI applications are growing rapidly, and so is the number of\nembeddings that need to be stored and indexed. The Faiss library is dedicated\nto vector similarity search, a core functionality of vector databases. Faiss is\na toolkit of indexing methods and related primitives used to search, cluster,\ncompress and transform vectors. This paper describes the trade-off space of\nvector search and the design principles of Faiss in terms of structure,\napproach to optimization and interfacing. We benchmark key features of the\nlibrary and discuss a few selected applications to highlight its broad\napplicability.\n","authors":["Matthijs Douze","Alexandr Guzhva","Chengqi Deng","Jeff Johnson","Gergely Szilvasy","Pierre-Emmanuel Mazaré","Maria Lomeli","Lucas Hosseini","Hervé Jégou"],"pdf_url":"https://arxiv.org/pdf/2401.08281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07707v1","updated":"2025-02-11T17:04:31Z","published":"2025-02-11T17:04:31Z","title":"PRVQL: Progressive Knowledge-guided Refinement for Robust Egocentric\n  Visual Query Localization","summary":"  Egocentric visual query localization (EgoVQL) focuses on localizing the\ntarget of interest in space and time from first-person videos, given a visual\nquery. Despite recent progressive, existing methods often struggle to handle\nsevere object appearance changes and cluttering background in the video due to\nlacking sufficient target cues, leading to degradation. Addressing this, we\nintroduce PRVQL, a novel Progressive knowledge-guided Refinement framework for\nEgoVQL. The core is to continuously exploit target-relevant knowledge directly\nfrom videos and utilize it as guidance to refine both query and video features\nfor improving target localization. Our PRVQL contains multiple processing\nstages. The target knowledge from one stage, comprising appearance and spatial\nknowledge extracted via two specially designed knowledge learning modules, are\nutilized as guidance to refine the query and videos features for the next\nstage, which are used to generate more accurate knowledge for further feature\nrefinement. With such a progressive process, target knowledge in PRVQL can be\ngradually improved, which, in turn, leads to better refined query and video\nfeatures for localization in the final stage. Compared to previous methods, our\nPRVQL, besides the given object cues, enjoys additional crucial target\ninformation from a video as guidance to refine features, and hence enhances\nEgoVQL in complicated scenes. In our experiments on challenging Ego4D, PRVQL\nachieves state-of-the-art result and largely surpasses other methods, showing\nits efficacy. Our code, model and results will be released at\nhttps://github.com/fb-reps/PRVQL.\n","authors":["Bing Fan","Yunhe Feng","Yapeng Tian","Yuewei Lin","Yan Huang","Heng Fan"],"pdf_url":"https://arxiv.org/pdf/2502.07707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07701v1","updated":"2025-02-11T16:58:15Z","published":"2025-02-11T16:58:15Z","title":"Magic 1-For-1: Generating One Minute Video Clips within One Minute","summary":"  In this technical report, we present Magic 1-For-1 (Magic141), an efficient\nvideo generation model with optimized memory consumption and inference latency.\nThe key idea is simple: factorize the text-to-video generation task into two\nseparate easier tasks for diffusion step distillation, namely text-to-image\ngeneration and image-to-video generation. We verify that with the same\noptimization algorithm, the image-to-video task is indeed easier to converge\nover the text-to-video task. We also explore a bag of optimization tricks to\nreduce the computational cost of training the image-to-video (I2V) models from\nthree aspects: 1) model convergence speedup by using a multi-modal prior\ncondition injection; 2) inference latency speed up by applying an adversarial\nstep distillation, and 3) inference memory cost optimization with parameter\nsparsification. With those techniques, we are able to generate 5-second video\nclips within 3 seconds. By applying a test time sliding window, we are able to\ngenerate a minute-long video within one minute with significantly improved\nvisual quality and motion dynamics, spending less than 1 second for generating\n1 second video clips on average. We conduct a series of preliminary\nexplorations to find out the optimal tradeoff between computational cost and\nvideo quality during diffusion step distillation and hope this could be a good\nfoundation model for open-source explorations. The code and the model weights\nare available at https://github.com/DA-Group-PKU/Magic-1-For-1.\n","authors":["Hongwei Yi","Shitong Shao","Tian Ye","Jiantong Zhao","Qingyu Yin","Michael Lingelbach","Li Yuan","Yonghong Tian","Enze Xie","Daquan Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16247v2","updated":"2025-02-11T16:54:45Z","published":"2024-12-20T00:01:16Z","title":"Towards scientific discovery with dictionary learning: Extracting\n  biological concepts from microscopy foundation models","summary":"  Dictionary learning (DL) has emerged as a powerful interpretability tool for\nlarge language models. By extracting known concepts (e.g., Golden-Gate Bridge)\nfrom human-interpretable data (e.g., text), sparse DL can elucidate a model's\ninner workings. In this work, we ask if DL can also be used to discover unknown\nconcepts from less human-interpretable scientific data (e.g., cell images),\nultimately enabling modern approaches to scientific discovery. As a first step,\nwe use DL algorithms to study microscopy foundation models trained on\nmulti-cell image data, where little prior knowledge exists regarding which\nhigh-level concepts should arise. We show that sparse dictionaries indeed\nextract biologically-meaningful concepts such as cell type and genetic\nperturbation type. We also propose Iterative Codebook Feature Learning~(ICFL)\nand combine it with a pre-processing step which uses PCA whitening from a\ncontrol dataset. In our experiments, we demonstrate that both ICFL and PCA\nimprove the selectivity of extracted features compared to TopK sparse\nautoencoders.\n","authors":["Konstantin Donhauser","Kristina Ulicna","Gemma Elyse Moran","Aditya Ravuri","Kian Kenyon-Dean","Cian Eastwood","Jason Hartford"],"pdf_url":"https://arxiv.org/pdf/2412.16247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10737v3","updated":"2025-02-11T16:47:17Z","published":"2024-06-15T20:47:38Z","title":"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained\nmodels to continually changing, unseen target domains. While existing CTTA\nmethods assume structured domain changes with uniform durations, real-world\nenvironments often exhibit dynamic patterns where domains recur with varying\nfrequencies and durations. Current approaches, which adapt the same parameters\nacross different domains, struggle in such dynamic conditions-they face\nconvergence issues with brief domain exposures, risk forgetting previously\nlearned knowledge, or misapplying it to irrelevant domains. To remedy this, we\npropose DPCore, a method designed for robust performance across diverse domain\nchange patterns while ensuring computational efficiency. DPCore integrates\nthree key components: Visual Prompt Adaptation for efficient domain alignment,\na Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism\nthat intelligently adjusts existing prompts for similar domains while creating\nnew ones for substantially different domains. Extensive experiments on four\nbenchmarks demonstrate that DPCore consistently outperforms various CTTA\nmethods, achieving state-of-the-art performance in both structured and dynamic\nsettings while reducing trainable parameters by 99% and computation time by 64%\ncompared to previous approaches.\n","authors":["Yunbei Zhang","Akshay Mehra","Shuaicheng Niu","Jihun Hamm"],"pdf_url":"https://arxiv.org/pdf/2406.10737v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07685v1","updated":"2025-02-11T16:36:55Z","published":"2025-02-11T16:36:55Z","title":"Matrix3D: Large Photogrammetry Model All-in-One","summary":"  We present Matrix3D, a unified model that performs several photogrammetry\nsubtasks, including pose estimation, depth prediction, and novel view synthesis\nusing just the same model. Matrix3D utilizes a multi-modal diffusion\ntransformer (DiT) to integrate transformations across several modalities, such\nas images, camera parameters, and depth maps. The key to Matrix3D's large-scale\nmulti-modal training lies in the incorporation of a mask learning strategy.\nThis enables full-modality model training even with partially complete data,\nsuch as bi-modality data of image-pose and image-depth pairs, thus\nsignificantly increases the pool of available training data. Matrix3D\ndemonstrates state-of-the-art performance in pose estimation and novel view\nsynthesis tasks. Additionally, it offers fine-grained control through\nmulti-round interactions, making it an innovative tool for 3D content creation.\nProject page: https://nju-3dv.github.io/projects/matrix3d.\n","authors":["Yuanxun Lu","Jingyang Zhang","Tian Fang","Jean-Daniel Nahmias","Yanghai Tsin","Long Quan","Xun Cao","Yao Yao","Shiwei Li"],"pdf_url":"https://arxiv.org/pdf/2502.07685v1.pdf","comment":"Project Page: https://nju-3dv.github.io/projects/matrix3d"},{"id":"http://arxiv.org/abs/2502.07680v1","updated":"2025-02-11T16:30:14Z","published":"2025-02-11T16:30:14Z","title":"Multiview Point Cloud Registration Based on Minimum Potential Energy for\n  Free-Form Blade Measurement","summary":"  Point cloud registration is an essential step for free-form blade\nreconstruction in industrial measurement. Nonetheless, measuring defects of the\n3D acquisition system unavoidably result in noisy and incomplete point cloud\ndata, which renders efficient and accurate registration challenging. In this\npaper, we propose a novel global registration method that is based on the\nminimum potential energy (MPE) method to address these problems. The basic\nstrategy is that the objective function is defined as the minimum potential\nenergy optimization function of the physical registration system. The function\ndistributes more weight to the majority of inlier points and less weight to the\nnoise and outliers, which essentially reduces the influence of perturbations in\nthe mathematical formulation. We decompose the solution into a globally optimal\napproximation procedure and a fine registration process with the trimmed\niterative closest point algorithm to boost convergence. The approximation\nprocedure consists of two main steps. First, according to the construction of\nthe force traction operator, we can simply compute the position of the\npotential energy minimum. Second, to find the MPE point, we propose a new\ntheory that employs two flags to observe the status of the registration\nprocedure. We demonstrate the performance of the proposed algorithm on four\ntypes of blades. The proposed method outperforms the other global methods in\nterms of both accuracy and noise resistance.\n","authors":["Zijie Wu","Yaonan Wang","Yang Mo","Qing Zhu","He Xie","Haotian Wu","Mingtao Feng","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2502.07680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01031v2","updated":"2025-02-11T16:23:09Z","published":"2024-02-01T21:43:27Z","title":"MRAnnotator: multi-Anatomy and many-Sequence MRI segmentation of 44\n  structures","summary":"  In this retrospective study, we annotated 44 structures on two datasets: an\ninternal dataset of 1,518 MRI sequences from 843 patients at the Mount Sinai\nHealth System, and an external dataset of 397 MRI sequences from 263 patients\nfor benchmarking. The internal dataset trained the nnU-Net model MRAnnotator,\nwhich demonstrated strong generalizability on the external dataset. MRAnnotator\noutperformed existing models such as TotalSegmentator MRI and MRSegmentator on\nboth datasets, achieving an overall average Dice score of 0.878 on the internal\ndataset and 0.875 on the external set. Model weights are available on GitHub,\nand the external test set can be shared upon request.\n","authors":["Alexander Zhou","Zelong Liu","Andrew Tieu","Nikhil Patel","Sean Sun","Anthony Yang","Peter Choi","Hao-Chih Lee","Mickael Tordjman","Louisa Deyer","Yunhao Mei","Valentin Fauveau","George Soultanidis","Bachir Taouli","Mingqian Huang","Amish Doshi","Zahi A. Fayad","Timothy Deyer","Xueyan Mei"],"pdf_url":"https://arxiv.org/pdf/2402.01031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06314v2","updated":"2025-02-11T16:04:15Z","published":"2025-02-10T10:06:46Z","title":"From Pixels to Components: Eigenvector Masking for Visual Representation\n  Learning","summary":"  Predicting masked from visible parts of an image is a powerful\nself-supervised approach for visual representation learning. However, the\ncommon practice of masking random patches of pixels exhibits certain failure\nmodes, which can prevent learning meaningful high-level features, as required\nfor downstream tasks. We propose an alternative masking strategy that operates\non a suitable transformation of the data rather than on the raw pixels.\nSpecifically, we perform principal component analysis and then randomly mask a\nsubset of components, which accounts for a fixed ratio of the data variance.\nThe learning task then amounts to reconstructing the masked components from the\nvisible ones. Compared to local patches of pixels, the principal components of\nimages carry more global information. We thus posit that predicting masked from\nvisible components involves more high-level features, allowing our masking\nstrategy to extract more useful representations. This is corroborated by our\nempirical findings which demonstrate improved image classification performance\nfor component over pixel masking. Our method thus constitutes a simple and\nrobust data-driven alternative to traditional masked image modeling approaches.\n","authors":["Alice Bizeul","Thomas Sutter","Alain Ryser","Bernhard Schölkopf","Julius von Kügelgen","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2502.06314v2.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2502.06741v2","updated":"2025-02-11T16:02:23Z","published":"2025-02-10T18:09:45Z","title":"ViSIR: Vision Transformer Single Image Reconstruction Method for Earth\n  System Models","summary":"  Purpose: Earth system models (ESMs) integrate the interactions of the\natmosphere, ocean, land, ice, and biosphere to estimate the state of regional\nand global climate under a wide variety of conditions. The ESMs are highly\ncomplex, and thus, deep neural network architectures are used to model the\ncomplexity and store the down-sampled data. In this paper, we propose the\nVision Transformer Sinusoidal Representation Networks (ViSIR) to improve the\nsingle image SR (SR) reconstruction task for the ESM data.\n  Methods: ViSIR combines the SR capability of Vision Transformers (ViT) with\nthe high-frequency detail preservation of the Sinusoidal Representation Network\n(SIREN) to address the spectral bias observed in SR tasks.\n  Results: The ViSIR outperforms ViT by 4.1 dB, SIREN by 7.5 dB, and\nSR-Generative Adversarial (SR-GANs) by 7.1dB PSNR on average for three\ndifferent measurements.\n  Conclusion: The proposed ViSIR is evaluated and compared with\nstate-of-the-art methods. The results show that the proposed algorithm is\noutperforming other methods in terms of Mean Square Error(MSE),\nPeak-Signal-to-Noise-Ratio(PSNR), and Structural Similarity Index\nMeasure(SSIM).\n","authors":["Ehsan Zeraatkar","Salah Faroughi","Jelena Tešić"],"pdf_url":"https://arxiv.org/pdf/2502.06741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12042v3","updated":"2025-02-11T15:58:10Z","published":"2024-06-17T19:22:04Z","title":"Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image\n  Diffusion Models","summary":"  Text-to-image (T2I) diffusion models have demonstrated impressive image\ngeneration capabilities. Still, their computational intensity prohibits\nresource-constrained organizations from deploying T2I models after fine-tuning\nthem on their internal target data. While pruning techniques offer a potential\nsolution to reduce the computational burden of T2I models, static pruning\nmethods use the same pruned model for all input prompts, overlooking the\nvarying capacity requirements of different prompts. Dynamic pruning addresses\nthis issue by utilizing a separate sub-network for each prompt, but it prevents\nbatch parallelism on GPUs. To overcome these limitations, we introduce Adaptive\nPrompt-Tailored Pruning (APTP), a novel prompt-based pruning method designed\nfor T2I diffusion models. Central to our approach is a prompt router model,\nwhich learns to determine the required capacity for an input text prompt and\nroutes it to an architecture code, given a total desired compute budget for\nprompts. Each architecture code represents a specialized model tailored to the\nprompts assigned to it, and the number of codes is a hyperparameter. We train\nthe prompt router and architecture codes using contrastive learning, ensuring\nthat similar prompts are mapped to nearby codes. Further, we employ optimal\ntransport to prevent the codes from collapsing into a single one. We\ndemonstrate APTP's effectiveness by pruning Stable Diffusion (SD) V2.1 using\nCC3M and COCO as target datasets. APTP outperforms the single-model pruning\nbaselines in terms of FID, CLIP, and CMMD scores. Our analysis of the clusters\nlearned by APTP reveals they are semantically meaningful. We also show that\nAPTP can automatically discover previously empirically found challenging\nprompts for SD, e.g. prompts for generating text images, assigning them to\nhigher capacity codes.\n","authors":["Alireza Ganjdanesh","Reza Shirkavand","Shangqian Gao","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2406.12042v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01547v2","updated":"2025-02-11T15:56:02Z","published":"2025-02-03T17:29:52Z","title":"mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech\n  Recognition","summary":"  Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio\nand can improve performance in noise, but most methods are trained only on\nEnglish data. One limitation is the lack of large-scale multilingual video\ndata, which makes it hard hard to train models from scratch. In this work, we\npropose mWhisper-Flamingo for multilingual AVSR which combines the strengths of\na pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable\nbetter multi-modal integration and improve the noisy multilingual performance,\nwe introduce decoder modality dropout where the model is trained both on paired\naudio-visual inputs and separate audio/visual inputs. mWhisper-Flamingo\nachieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages.\nAudio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on\nall languages in noisy conditions.\n","authors":["Andrew Rouditchenko","Samuel Thomas","Hilde Kuehne","Rogerio Feris","James Glass"],"pdf_url":"https://arxiv.org/pdf/2502.01547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11528v4","updated":"2025-02-11T15:55:25Z","published":"2023-06-20T13:31:33Z","title":"TransRef: Multi-Scale Reference Embedding Transformer for\n  Reference-Guided Image Inpainting","summary":"  Image inpainting for completing complicated semantic environments and diverse\nhole patterns of corrupted images is challenging even for state-of-the-art\nlearning-based inpainting methods trained on large-scale data. A reference\nimage capturing the same scene of a corrupted image offers informative guidance\nfor completing the corrupted image as it shares similar texture and structure\npriors to that of the holes of the corrupted image. In this work, we propose a\ntransformer-based encoder-decoder network, named TransRef, for reference-guided\nimage inpainting. Specifically, the guidance is conducted progressively through\na reference embedding procedure, in which the referencing features are\nsubsequently aligned and fused with the features of the corrupted image. For\nprecise utilization of the reference features for guidance, a reference-patch\nalignment (Ref-PA) module is proposed to align the patch features of the\nreference and corrupted images and harmonize their style differences, while a\nreference-patch transformer (Ref-PT) module is proposed to refine the embedded\nreference feature. Moreover, to facilitate the research of reference-guided\nimage restoration tasks, we construct a publicly accessible benchmark dataset\ncontaining 50K pairs of input and reference images. Both quantitative and\nqualitative evaluations demonstrate the efficacy of the reference information\nand the proposed method over the state-of-the-art methods in completing complex\nholes. Code and dataset can be accessed at https://github.com/Cameltr/TransRef.\n","authors":["Taorong Liu","Liang Liao","Delin Chen","Jing Xiao","Zheng Wang","Chia-Wen Lin","Shin'ichi Satoh"],"pdf_url":"https://arxiv.org/pdf/2306.11528v4.pdf","comment":"Neurocomputing 2025"},{"id":"http://arxiv.org/abs/2501.16065v3","updated":"2025-02-11T15:37:37Z","published":"2025-01-27T14:08:25Z","title":"CILP-FGDI: Exploiting Vision-Language Model for Generalizable Person\n  Re-Identification","summary":"  The Visual Language Model, known for its robust cross-modal capabilities, has\nbeen extensively applied in various computer vision tasks. In this paper, we\nexplore the use of CLIP (Contrastive Language-Image Pretraining), a\nvision-language model pretrained on large-scale image-text pairs to align\nvisual and textual features, for acquiring fine-grained and domain-invariant\nrepresentations in generalizable person re-identification. The adaptation of\nCLIP to the task presents two primary challenges: learning more fine-grained\nfeatures to enhance discriminative ability, and learning more domain-invariant\nfeatures to improve the model's generalization capabilities. To mitigate the\nfirst challenge thereby enhance the ability to learn fine-grained features, a\nthree-stage strategy is proposed to boost the accuracy of text descriptions.\nInitially, the image encoder is trained to effectively adapt to person\nre-identification tasks. In the second stage, the features extracted by the\nimage encoder are used to generate textual descriptions (i.e., prompts) for\neach image. Finally, the text encoder with the learned prompts is employed to\nguide the training of the final image encoder. To enhance the model's\ngeneralization capabilities to unseen domains, a bidirectional guiding method\nis introduced to learn domain-invariant image features. Specifically,\ndomain-invariant and domain-relevant prompts are generated, and both positive\n(pulling together image features and domain-invariant prompts) and negative\n(pushing apart image features and domain-relevant prompts) views are used to\ntrain the image encoder. Collectively, these strategies contribute to the\ndevelopment of an innovative CLIP-based framework for learning fine-grained\ngeneralized features in person re-identification.\n","authors":["Huazhong Zhao","Lei Qi","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2501.16065v3.pdf","comment":"Accepted by IEEE TIFS"},{"id":"http://arxiv.org/abs/2501.18855v2","updated":"2025-02-11T15:27:23Z","published":"2025-01-31T02:37:09Z","title":"FlexiCrackNet: A Flexible Pipeline for Enhanced Crack Segmentation with\n  General Features Transfered from SAM","summary":"  Automatic crack segmentation is a cornerstone technology for intelligent\nvisual perception modules in road safety maintenance and structural integrity\nsystems. Existing deep learning models and ``pre-training + fine-tuning''\nparadigms often face challenges of limited adaptability in resource-constrained\nenvironments and inadequate scalability across diverse data domains. To\novercome these limitations, we propose FlexiCrackNet, a novel pipeline that\nseamlessly integrates traditional deep learning paradigms with the strengths of\nlarge-scale pre-trained models. At its core, FlexiCrackNet employs an\nencoder-decoder architecture to extract task-specific features. The lightweight\nEdgeSAM's CNN-based encoder is exclusively used as a generic feature extractor,\ndecoupled from the fixed input size requirements of EdgeSAM. To harmonize\ngeneral and domain-specific features, we introduce the information-Interaction\ngated attention mechanism (IGAM), which adaptively fuses multi-level features\nto enhance segmentation performance while mitigating irrelevant noise. This\ndesign enables the efficient transfer of general knowledge to crack\nsegmentation tasks while ensuring adaptability to diverse input resolutions and\nresource-constrained environments. Experiments show that FlexiCrackNet\noutperforms state-of-the-art methods, excels in zero-shot generalization,\ncomputational efficiency, and segmentation robustness under challenging\nscenarios such as blurry inputs, complex backgrounds, and visually ambiguous\nartifacts. These advancements underscore the potential of FlexiCrackNet for\nreal-world applications in automated crack detection and comprehensive\nstructural health monitoring systems.\n","authors":["Xinlong Wan","Xiaoyan Jiang","Guangsheng Luo","Ferdous Sohel","Jenqneng Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.18855v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05147v2","updated":"2025-02-11T15:25:02Z","published":"2025-02-07T18:25:28Z","title":"LP-DETR: Layer-wise Progressive Relations for Object Detection","summary":"  This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach\nthat enhances DETR-based object detection through multi-scale relation\nmodeling. Our method introduces learnable spatial relationships between object\nqueries through a relation-aware self-attention mechanism, which adaptively\nlearns to balance different scales of relations (local, medium and global)\nacross decoder layers. This progressive design enables the model to effectively\ncapture evolving spatial dependencies throughout the detection pipeline.\nExtensive experiments on COCO 2017 dataset demonstrate that our method improves\nboth convergence speed and detection accuracy compared to standard\nself-attention module. The proposed method achieves competitive results,\nreaching 52.3\\% AP with 12 epochs and 52.5\\% AP with 24 epochs using ResNet-50\nbackbone, and further improving to 58.0\\% AP with Swin-L backbone. Furthermore,\nour analysis reveals an interesting pattern: the model naturally learns to\nprioritize local spatial relations in early decoder layers while gradually\nshifting attention to broader contexts in deeper layers, providing valuable\ninsights for future research in object detection.\n","authors":["Zhengjian Kang","Ye Zhang","Xiaoyu Deng","Xintao Li","Yongzhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.05147v2.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.07631v1","updated":"2025-02-11T15:21:31Z","published":"2025-02-11T15:21:31Z","title":"Divide and Merge: Motion and Semantic Learning in End-to-End Autonomous\n  Driving","summary":"  Perceiving the environment and its changes over time corresponds to two\nfundamental yet heterogeneous types of information: semantics and motion.\nPrevious end-to-end autonomous driving works represent both types of\ninformation in a single feature vector. However, including motion tasks, such\nas prediction and planning, always impairs detection and tracking performance,\na phenomenon known as negative transfer in multi-task learning. To address this\nissue, we propose Neural-Bayes motion decoding, a novel parallel detection,\ntracking, and prediction method separating semantic and motion learning,\nsimilar to the Bayes filter. Specifically, we employ a set of learned motion\nqueries that operate in parallel with the detection and tracking queries,\nsharing a unified set of recursively updated reference points. Moreover, we\nemploy interactive semantic decoding to enhance information exchange in\nsemantic tasks, promoting positive transfer. Experiments on the nuScenes\ndataset show improvements of 5% in detection and 11% in tracking. Our method\nachieves state-of-the-art collision rates in open-loop planning evaluation\nwithout any modifications to the planning module.\n","authors":["Yinzhe Shen","Ömer Şahin Taş","Kaiwen Wang","Royden Wagner","Christoph Stiller"],"pdf_url":"https://arxiv.org/pdf/2502.07631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07620v1","updated":"2025-02-11T15:09:05Z","published":"2025-02-11T15:09:05Z","title":"Causal-Informed Contrastive Learning: Towards Bias-Resilient\n  Pre-training under Concept Drift","summary":"  The evolution of large-scale contrastive pre-training propelled by top-tier\ndatasets has reached a transition point in the scaling law. Consequently,\nsustaining and enhancing a model's pre-training capabilities in drift\nenvironments have surfaced as a notable challenge. In this paper, we initially\nuncover that contrastive pre-training methods are significantly impacted by\nconcept drift wherein distributions change unpredictably, resulting in notable\nbiases in the feature space of the pre-trained model. Empowered by causal\ninference, we construct a structural causal graph to analyze the impact of\nconcept drift to contrastive pre-training systemically, and propose the causal\ninterventional contrastive objective. Upon achieving this, we devise a\nresilient contrastive pre-training approach to accommodate the data stream of\nconcept drift, with simple and scalable implementation. Extensive experiments\non various downstream tasks demonstrate our resilient contrastive pre-training\neffectively mitigates the bias stemming from the concept drift data stream.\nCodes are available at https://anonymous.4open.science/r/ResilientCL/.\n","authors":["Xiaoyu Yang","Jie Lu","En Yu"],"pdf_url":"https://arxiv.org/pdf/2502.07620v1.pdf","comment":"17pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.09754v2","updated":"2025-02-11T15:06:13Z","published":"2024-09-15T14:52:16Z","title":"Towards Single-Lens Controllable Depth-of-Field Imaging via Depth-Aware\n  Point Spread Functions","summary":"  Controllable Depth-of-Field (DoF) imaging commonly produces amazing visual\neffects based on heavy and expensive high-end lenses. However, confronted with\nthe increasing demand for mobile scenarios, it is desirable to achieve a\nlightweight solution with Minimalist Optical Systems (MOS). This work centers\naround two major limitations of MOS, i.e., the severe optical aberrations and\nuncontrollable DoF, for achieving single-lens controllable DoF imaging via\ncomputational methods. A Depth-aware Controllable DoF Imaging (DCDI) framework\nis proposed equipped with All-in-Focus (AiF) aberration correction and\nmonocular depth estimation, where the recovered image and corresponding depth\nmap are utilized to produce imaging results under diverse DoFs of any high-end\nlens via patch-wise convolution. To address the depth-varying optical\ndegradation, we introduce a Depth-aware Degradation-adaptive Training (DA2T)\nscheme. At the dataset level, a Depth-aware Aberration MOS (DAMOS) dataset is\nestablished based on the simulation of Point Spread Functions (PSFs) under\ndifferent object distances. Additionally, we design two plug-and-play\ndepth-aware mechanisms to embed depth information into the aberration image\nrecovery for better tackling depth-aware degradation. Furthermore, we propose a\nstorage-efficient Omni-Lens-Field model to represent the 4D PSF library of\nvarious lenses. With the predicted depth map, recovered image, and depth-aware\nPSF map inferred by Omni-Lens-Field, single-lens controllable DoF imaging is\nachieved. Comprehensive experimental results demonstrate that the proposed\nframework enhances the recovery performance, and attains impressive single-lens\ncontrollable DoF imaging results, providing a seminal baseline for this field.\nThe source code and the established dataset will be publicly available at\nhttps://github.com/XiaolongQian/DCDI.\n","authors":["Xiaolong Qian","Qi Jiang","Yao Gao","Shaohua Gao","Zhonghua Yi","Lei Sun","Kai Wei","Haifeng Li","Kailun Yang","Kaiwei Wang","Jian Bai"],"pdf_url":"https://arxiv.org/pdf/2409.09754v2.pdf","comment":"Accepted to IEEE Transactions on Computational Imaging (TCI). The\n  source code and the established dataset will be publicly available at\n  https://github.com/XiaolongQian/DCDI"},{"id":"http://arxiv.org/abs/2502.07617v1","updated":"2025-02-11T15:05:33Z","published":"2025-02-11T15:05:33Z","title":"Scaling Pre-training to One Hundred Billion Data for Vision Language\n  Models","summary":"  We provide an empirical investigation of the potential of pre-training\nvision-language models on an unprecedented scale: 100 billion examples. We find\nthat model performance tends to saturate at this scale on many common\nWestern-centric classification and retrieval benchmarks, such as COCO Captions.\nNevertheless, tasks of cultural diversity achieve more substantial gains from\nthe 100-billion scale web data, thanks to its coverage of long-tail concepts.\nFurthermore, we analyze the model's multilinguality and show gains in\nlow-resource languages as well. In addition, we observe that reducing the size\nof the pretraining dataset via quality filters like using CLIP, typically used\nto enhance performance, may inadvertently reduce the cultural diversity\nrepresented even in large-scale datasets. Our results highlight that while\ntraditional benchmarks may not benefit significantly from scaling noisy, raw\nweb data to 100 billion examples, this data scale is vital for building truly\ninclusive multimodal systems.\n","authors":["Xiao Wang","Ibrahim Alabdulmohsin","Daniel Salz","Zhe Li","Keran Rong","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.07617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07615v1","updated":"2025-02-11T15:05:26Z","published":"2025-02-11T15:05:26Z","title":"Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained\n  Matching Priors","summary":"  3D Gaussian Splatting (3DGS) has achieved excellent rendering quality with\nfast training and rendering speed. However, its optimization process lacks\nexplicit geometric constraints, leading to suboptimal geometric reconstruction\nin regions with sparse or no observational input views. In this work, we try to\nmitigate the issue by incorporating a pre-trained matching prior to the 3DGS\noptimization process. We introduce Flow Distillation Sampling (FDS), a\ntechnique that leverages pre-trained geometric knowledge to bolster the\naccuracy of the Gaussian radiance field. Our method employs a strategic\nsampling technique to target unobserved views adjacent to the input views,\nutilizing the optical flow calculated from the matching model (Prior Flow) to\nguide the flow analytically calculated from the 3DGS geometry (Radiance Flow).\nComprehensive experiments in depth rendering, mesh reconstruction, and novel\nview synthesis showcase the significant advantages of FDS over state-of-the-art\nmethods. Additionally, our interpretive experiments and analysis aim to shed\nlight on the effects of FDS on geometric accuracy and rendering quality,\npotentially providing readers with insights into its performance. Project page:\nhttps://nju-3dv.github.io/projects/fds\n","authors":["Lin-Zhuo Chen","Kangjie Liu","Youtian Lin","Siyu Zhu","Zhihao Li","Xun Cao","Yao Yao"],"pdf_url":"https://arxiv.org/pdf/2502.07615v1.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2412.01175v2","updated":"2025-02-11T14:59:40Z","published":"2024-12-02T06:31:28Z","title":"OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones?","summary":"  We introduce OBI-Bench, a holistic benchmark crafted to systematically\nevaluate large multi-modal models (LMMs) on whole-process oracle bone\ninscriptions (OBI) processing tasks demanding expert-level domain knowledge and\ndeliberate cognition. OBI-Bench includes 5,523 meticulously collected\ndiverse-sourced images, covering five key domain problems: recognition,\nrejoining, classification, retrieval, and deciphering. These images span\ncenturies of archaeological findings and years of research by front-line\nscholars, comprising multi-stage font appearances from excavation to synthesis,\nsuch as original oracle bone, inked rubbings, oracle bone fragments, cropped\nsingle characters, and handprinted characters. Unlike existing benchmarks,\nOBI-Bench focuses on advanced visual perception and reasoning with OBI-specific\nknowledge, challenging LMMs to perform tasks akin to those faced by experts.\nThe evaluation of 6 proprietary LMMs as well as 17 open-source LMMs highlights\nthe substantial challenges and demands posed by OBI-Bench. Even the latest\nversions of GPT-4o, Gemini 1.5 Pro, and Qwen-VL-Max are still far from\npublic-level humans in some fine-grained perception tasks. However, they\nperform at a level comparable to untrained humans in deciphering tasks,\nindicating remarkable capabilities in offering new interpretative perspectives\nand generating creative guesses. We hope OBI-Bench can facilitate the community\nto develop domain-specific multi-modal foundation models towards ancient\nlanguage research and delve deeper to discover and enhance these untapped\npotentials of LMMs.\n","authors":["Zijian Chen","Tingzhu Chen","Wenjun Zhang","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2412.01175v2.pdf","comment":"Accepted by ICLR 2025 as a Poster. 31 pages, 18 figures"},{"id":"http://arxiv.org/abs/2502.06428v2","updated":"2025-02-11T14:59:25Z","published":"2025-02-10T13:03:05Z","title":"CoS: Chain-of-Shot Prompting for Long Video Understanding","summary":"  Multi-modal Large Language Models (MLLMs) struggle with long videos due to\nthe need for excessive visual tokens. These tokens exceed massively the context\nlength of MLLMs, resulting in filled by redundant task-irrelevant shots. How to\nselect shots is an unsolved critical problem: sparse sampling risks missing key\ndetails, while exhaustive sampling overwhelms the model with irrelevant\ncontent, leading to video misunderstanding. To solve this problem, we propose\nChain-of-Shot prompting (CoS). The key idea is to frame shot selection as\ntest-time visual prompt optimisation, choosing shots adaptive to video\nunderstanding semantic task by optimising shots-task alignment. CoS has two key\nparts: (1) a binary video summary mechanism that performs pseudo temporal\ngrounding, discovering a binary coding to identify task-relevant shots, and (2)\na video co-reasoning module that deploys the binary coding to pair (learning to\nalign) task-relevant positive shots with irrelevant negative shots. It embeds\nthe optimised shot selections into the original video, facilitating a focus on\nrelevant context to optimize long video understanding. Experiments across three\nbaselines and five datasets demonstrate the effectiveness and adaptability of\nCoS. Code given in https://lwpyh.github.io/CoS.\n","authors":["Jian Hu","Zixu Cheng","Chenyang Si","Wei Li","Shaogang Gong"],"pdf_url":"https://arxiv.org/pdf/2502.06428v2.pdf","comment":"A training-free test-time optimisation approach for long video\n  understanding"},{"id":"http://arxiv.org/abs/2412.07041v3","updated":"2025-02-11T14:57:40Z","published":"2024-12-09T23:01:04Z","title":"Generalized Least Squares Kernelized Tensor Factorization","summary":"  Completing multidimensional tensor-structured data with missing entries is a\nfundamental task for many real-world applications involving incomplete or\ncorrupted datasets. For data with spatial or temporal side information,\nlow-rank factorization models with smoothness constraints have demonstrated\nstrong performance. Although effective at capturing global and long-range\ncorrelations, these models often struggle to capture short-scale,\nhigh-frequency variations in the data. To address this limitation, we propose\nthe Generalized Least Squares Kernelized Tensor Factorization (GLSKF) framework\nfor tensor completion. GLSKF integrates smoothness-constrained low-rank\nfactorization with a locally correlated residual process; the resulting\nadditive structure enables effective characterization of both global\ndependencies and local variations. Specifically, we define the covariance norm\nto enforce the smoothness of factor matrices in the global low-rank\nfactorization, and use structured covariance/kernel functions to model the\nlocal processes. For model estimation, we develop an alternating least squares\n(ALS) procedure with closed-form solutions for each subproblem. GLSKF utilizes\nzero-padding and slicing operations based on projection matrices which preserve\nthe Kronecker structure of covariances, facilitating efficient computations\nthrough the conjugate gradient (CG) method. The proposed framework is evaluated\non four real-world datasets across diverse tasks. Experimental results\ndemonstrate that GLSKF achieves superior performance and scalability,\nestablishing it as a novel solution for multidimensional tensor completion.\n","authors":["Mengying Lei","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2412.07041v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07602v1","updated":"2025-02-11T14:52:11Z","published":"2025-02-11T14:52:11Z","title":"An Improved Optimal Proximal Gradient Algorithm for Non-Blind Image\n  Deblurring","summary":"  Image deblurring remains a central research area within image processing,\ncritical for its role in enhancing image quality and facilitating clearer\nvisual representations across diverse applications. This paper tackles the\noptimization problem of image deblurring, assuming a known blurring kernel. We\nintroduce an improved optimal proximal gradient algorithm (IOptISTA), which\nbuilds upon the optimal gradient method and a weighting matrix, to efficiently\naddress the non-blind image deblurring problem. Based on two regularization\ncases, namely the $l_1$ norm and total variation norm, we perform numerical\nexperiments to assess the performance of our proposed algorithm. The results\nindicate that our algorithm yields enhanced PSNR and SSIM values, as well as a\nreduced tolerance, compared to existing methods.\n","authors":["Qingsong Wang","Shengze Xu","Xiaojiao Tong","Tieyong Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.07602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07601v1","updated":"2025-02-11T14:50:43Z","published":"2025-02-11T14:50:43Z","title":"Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large\n  Language Models","summary":"  Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the\ntraditional unsupervised AD setting that requires a large number of normal\nsamples to train a model, ZSAD is more practical for handling data-restricted\nreal-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have\nshown revolutionary reasoning capabilities in various vision tasks. However,\nthe reasoning of image abnormalities remains underexplored due to the lack of\ncorresponding datasets and benchmarks. To facilitate research in AD &\nreasoning, we establish the first visual instruction tuning dataset,\nAnomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through\ninvestigation with our benchmark, we reveal that current MLLMs like GPT-4o\ncannot accurately detect and describe fine-grained anomalous details in images.\nTo address this, we propose Anomaly-OneVision (Anomaly-OV), the first\nspecialist visual assistant for ZSAD and reasoning. Inspired by human behavior\nin visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM)\nmechanism to adaptively select and emphasize abnormal visual tokens. Extensive\nexperiments demonstrate that Anomaly-OV achieves significant improvements over\nadvanced generalist models in both detection and reasoning. Extensions to\nmedical and 3D AD are provided for future study. The link to our project page:\nhttps://xujiacong.github.io/Anomaly-OV/\n","authors":["Jiacong Xu","Shao-Yuan Lo","Bardia Safaei","Vishal M. Patel","Isht Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2502.07601v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.07600v1","updated":"2025-02-11T14:50:10Z","published":"2025-02-11T14:50:10Z","title":"PlaySlot: Learning Inverse Latent Dynamics for Controllable\n  Object-Centric Video Prediction and Planning","summary":"  Predicting future scene representations is a crucial task for enabling robots\nto understand and interact with the environment. However, most existing methods\nrely on video sequences and simulations with precise action annotations,\nlimiting their ability to leverage the large amount of available unlabeled\nvideo data. To address this challenge, we propose PlaySlot, an object-centric\nvideo prediction model that infers object representations and latent actions\nfrom unlabeled video sequences. It then uses these representations to forecast\nfuture object states and video frames. PlaySlot allows to generate multiple\npossible futures conditioned on latent actions, which can be inferred from\nvideo dynamics, provided by a user, or generated by a learned action policy,\nthus enabling versatile and interpretable world modeling. Our results show that\nPlaySlot outperforms both stochastic and object-centric baselines for video\nprediction across different environments. Furthermore, we show that our\ninferred latent actions can be used to learn robot behaviors sample-efficiently\nfrom unlabeled video demonstrations. Videos and code are available at\nhttps://play-slot.github.io/PlaySlot/.\n","authors":["Angel Villar-Corrales","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2502.07600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07592v1","updated":"2025-02-11T14:41:30Z","published":"2025-02-11T14:41:30Z","title":"YOLO Network For Defect Detection In Optical lenses","summary":"  Mass-produced optical lenses often exhibit defects that alter their\nscattering properties and compromise quality standards. Manual inspection is\nusually adopted to detect defects, but it is not recommended due to low\naccuracy, high error rate and limited scalability. To address these challenges,\nthis study presents an automated defect detection system based on the YOLOv8\ndeep learning model. A custom dataset of optical lenses, annotated with defect\nand lens regions, was created to train the model. Experimental results obtained\nin this study reveal that the system can be used to efficiently and accurately\ndetect defects in optical lenses. The proposed system can be utilized in\nreal-time industrial environments to enhance quality control processes by\nenabling reliable and scalable defect detection in optical lens manufacturing.\n","authors":["Habib Yaseen"],"pdf_url":"https://arxiv.org/pdf/2502.07592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07590v1","updated":"2025-02-11T14:39:59Z","published":"2025-02-11T14:39:59Z","title":"DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT\n  Training","summary":"  Diffusion Transformers (DiTs) have shown remarkable performance in modeling\nand generating high-quality videos. However, the quadratic computational\ncomplexity of 3D full attention mechanism presents significant challenges in\nscaling video DiT training, especially for high-definition and lengthy videos,\nwhere attention can dominate up to 95% of the end-to-end time and necessitate\nspecialized communication paradigms to handle large input sizes.\n  This paper introduces DSV, a novel framework designed to accelerate and scale\nthe training of video DiTs by leveraging the inherent dynamic attention\nsparsity throughout the training process. DSV employs a two-stage training\nalgorithm that exploits sparsity patterns, focusing on critical elements\nsupported by efficient, tailored kernels. To accommodate the new sparsity\ndimension, we develop a hybrid sparsity-aware context parallelism that\neffectively scales to large inputs by addressing the heterogeneity of sparsity\nacross attention heads and blocks, resulting in optimized sparse computation\nand communication. Extensive evaluations demonstrate that DSV achieves up to\n3.02x gain in training throughput with nearly no quality degradation.\n","authors":["Xin Tan","Yuetao Chen","Yimin Jiang","Xing Chen","Kun Yan","Nan Duan","Yibo Zhu","Daxin Jiang","Hong Xu"],"pdf_url":"https://arxiv.org/pdf/2502.07590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08502v2","updated":"2025-02-11T14:38:01Z","published":"2024-08-16T03:01:07Z","title":"Efficient Image-to-Image Diffusion Classifier for Adversarial Robustness","summary":"  Diffusion models (DMs) have demonstrated great potential in the field of\nadversarial robustness, where DM-based defense methods can achieve superior\ndefense capability without adversarial training. However, they all require huge\ncomputational costs due to the usage of large-scale pre-trained DMs, making it\ndifficult to conduct full evaluation under strong attacks and compare with\ntraditional CNN-based methods. Simply reducing the network size and timesteps\nin DMs could significantly harm the image generation quality, which invalidates\nprevious frameworks. To alleviate this issue, we redesign the diffusion\nframework from generating high-quality images to predicting distinguishable\nimage labels. Specifically, we employ an image translation framework to learn\nmany-to-one mapping from input samples to designed orthogonal image labels.\nBased on this framework, we introduce an efficient Image-to-Image diffusion\nclassifier with a pruned U-Net structure and reduced diffusion timesteps.\nBesides the framework, we redesign the optimization objective of DMs to fit the\ntarget of image classification, where a new classification loss is incorporated\nin the DM-based image translation framework to distinguish the generated label\nfrom those of other classes. We conduct sufficient evaluations of the proposed\nclassifier under various attacks on popular benchmarks. Extensive experiments\nshow that our method achieves better adversarial robustness with fewer\ncomputational costs than DM-based and CNN-based methods. The code is available\nat https://github.com/hfmei/IDC\n","authors":["Hefei Mei","Minjing Dong","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2408.08502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11054v2","updated":"2025-02-11T14:15:13Z","published":"2024-08-20T17:58:59Z","title":"Near, far: Patch-ordering enhances vision foundation models' scene\n  understanding","summary":"  We introduce NeCo: Patch Neighbor Consistency, a novel self-supervised\ntraining loss that enforces patch-level nearest neighbor consistency across a\nstudent and teacher model. Compared to contrastive approaches that only yield\nbinary learning signals, i.e., 'attract' and 'repel', this approach benefits\nfrom the more fine-grained learning signal of sorting spatially dense features\nrelative to reference patches. Our method leverages differentiable sorting\napplied on top of pretrained representations, such as DINOv2-registers to\nbootstrap the learning signal and further improve upon them. This dense\npost-pretraining leads to superior performance across various models and\ndatasets, despite requiring only 19 hours on a single GPU. This method\ngenerates high-quality dense feature encoders and establishes several new\nstate-of-the-art results such as +5.5% and +6% for non-parametric in-context\nsemantic segmentation on ADE20k and Pascal VOC, +7.2% and +5.7% for linear\nsegmentation evaluations on COCO-Things and -Stuff and improvements in the 3D\nunderstanding of multi-view consistency on SPair-71k, by more than 1.5%.\n","authors":["Valentinos Pariza","Mohammadreza Salehi","Gertjan Burghouts","Francesco Locatello","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2408.11054v2.pdf","comment":"Accepted at ICLR25. The webpage is accessible at:\n  https://vpariza.github.io/NeCo/"},{"id":"http://arxiv.org/abs/2409.09369v4","updated":"2025-02-11T14:11:14Z","published":"2024-09-14T08:47:45Z","title":"Interpretable Vision-Language Survival Analysis with Ordinal Inductive\n  Bias for Computational Pathology","summary":"  Histopathology Whole-Slide Images (WSIs) provide an important tool to assess\ncancer prognosis in computational pathology (CPATH). While existing survival\nanalysis (SA) approaches have made exciting progress, they are generally\nlimited to adopting highly-expressive network architectures and only\ncoarse-grained patient-level labels to learn visual prognostic representations\nfrom gigapixel WSIs. Such learning paradigm suffers from critical performance\nbottlenecks, when facing present scarce training data and standard\nmulti-instance learning (MIL) framework in CPATH. To overcome it, this paper,\nfor the first time, proposes a new Vision-Language-based SA (VLSA) paradigm.\nConcretely, (1) VLSA is driven by pathology VL foundation models. It no longer\nrelies on high-capability networks and shows the advantage of data efficiency.\n(2) In vision-end, VLSA encodes textual prognostic prior and then employs it as\nauxiliary signals to guide the aggregating of visual prognostic features at\ninstance level, thereby compensating for the weak supervision in MIL. Moreover,\ngiven the characteristics of SA, we propose i) ordinal survival prompt learning\nto transform continuous survival labels into textual prompts; and ii) ordinal\nincidence function as prediction target to make SA compatible with VL-based\nprediction. Notably, VLSA's predictions can be interpreted intuitively by our\nShapley values-based method. The extensive experiments on five datasets confirm\nthe effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH\nby offering weakly-supervised MIL an effective means to learn valuable\nprognostic clues from gigapixel WSIs. Our source code is available at\nhttps://github.com/liupei101/VLSA.\n","authors":["Pei Liu","Luping Ji","Jiaxiang Gou","Bo Fu","Mao Ye"],"pdf_url":"https://arxiv.org/pdf/2409.09369v4.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2404.07664v2","updated":"2025-02-11T14:05:29Z","published":"2024-04-11T11:55:42Z","title":"Finding Dino: A Plug-and-Play Framework for Zero-Shot Detection of\n  Out-of-Distribution Objects Using Prototypes","summary":"  Detecting and localising unknown or out-of-distribution (OOD) objects in any\nscene can be a challenging task in vision, particularly in safety-critical\ncases involving autonomous systems like automated vehicles or trains.\nSupervised anomaly segmentation or open-world object detection models depend on\ntraining on exhaustively annotated datasets for every domain and still struggle\nin distinguishing between background and OOD objects. In this work, we present\na plug-and-play framework - PRototype-based OOD detection Without Labels\n(PROWL). It is an inference-based method that does not require training on the\ndomain dataset and relies on extracting relevant features from self-supervised\npre-trained models. PROWL can be easily adapted to detect in-domain objects in\nany operational design domain (ODD) in a zero-shot manner by specifying a list\nof known classes from this domain. PROWL, as a first zero-shot unsupervised\nmethod, achieves state-of-the-art results on the RoadAnomaly and RoadObstacle\ndatasets provided in road driving benchmarks - SegmentMeIfYouCan (SMIYC) and\nFishyscapes, as well as comparable performance against existing supervised\nmethods trained without auxiliary OOD data. We also demonstrate its\ngeneralisability to other domains such as rail and maritime.\n","authors":["Poulami Sinhamahapatra","Franziska Schwaiger","Shirsha Bose","Huiyu Wang","Karsten Roscher","Stephan Guennemann"],"pdf_url":"https://arxiv.org/pdf/2404.07664v2.pdf","comment":"Accepted in IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2502.07564v1","updated":"2025-02-11T14:03:39Z","published":"2025-02-11T14:03:39Z","title":"An Elliptic Curve Based Solution to the Perspective-Three-Point Problem","summary":"  The Perspective-Three-Point Problem (P3P) is solved by first focusing on\ndetermining the directions of the lines through pairs of control points,\nrelative to the camera, rather than the distances from the camera to the\ncontrol points. The analysis of this produces an efficient, accurate and\nreasonably simple P3P solver, which is compared with a state-of-the-art P3P\nsolver, \"Lambda Twist.\" Both methods depend on the accurate computation of a\nsingle root of a cubic polynomial. They have been implemented and tested for a\nwide range of control-point triangles, and under certain reasonable\nrestrictions, the new method is noticably more accurate than Lambda Twist,\nthough it is slower. However, the principal value of the present work is not in\nintroducing yet another P3P solver, but lies rather in the discovery of an\nintimate connection between the P3P problem and a special family of elliptic\ncurves that includes curves utilized in cryptography. This holds the potential\nfor further advances in a number of directions. To make this connection, an\ninteresting spherical analogue of an ancient \"sliding\" problem is stated and\nsolved.\n","authors":["Michael Q. Rieck"],"pdf_url":"https://arxiv.org/pdf/2502.07564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07560v1","updated":"2025-02-11T13:57:30Z","published":"2025-02-11T13:57:30Z","title":"Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning","summary":"  Class-incremental learning (CIL) seeks to enable a model to sequentially\nlearn new classes while retaining knowledge of previously learned ones.\nBalancing flexibility and stability remains a significant challenge,\nparticularly when the task ID is unknown. To address this, our study reveals\nthat the gap in feature distribution between novel and existing tasks is\nprimarily driven by differences in mean and covariance moments. Building on\nthis insight, we propose a novel semantic drift calibration method that\nincorporates mean shift compensation and covariance calibration. Specifically,\nwe calculate each class's mean by averaging its sample embeddings and estimate\ntask shifts using weighted embedding changes based on their proximity to the\nprevious mean, effectively capturing mean shifts for all learned classes with\neach new task. We also apply Mahalanobis distance constraint for covariance\ncalibration, aligning class-specific embedding covariances between old and\ncurrent networks to mitigate the covariance shift. Additionally, we integrate a\nfeature-level self-distillation approach to enhance generalization.\nComprehensive experiments on commonly used datasets demonstrate the\neffectiveness of our approach. The source code is available at\n\\href{https://github.com/fwu11/MACIL.git}{https://github.com/fwu11/MACIL.git}.\n","authors":["Fangwen Wu","Lechao Cheng","Shengeng Tang","Xiaofeng Zhu","Chaowei Fang","Dingwen Zhang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07560v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2501.08962v2","updated":"2025-02-11T13:55:01Z","published":"2025-01-15T17:18:46Z","title":"An analysis of data variation and bias in image-based dermatological\n  datasets for machine learning classification","summary":"  AI algorithms have become valuable in aiding professionals in healthcare. The\nincreasing confidence obtained by these models is helpful in critical decision\ndemands. In clinical dermatology, classification models can detect malignant\nlesions on patients' skin using only RGB images as input. However, most\nlearning-based methods employ data acquired from dermoscopic datasets on\ntraining, which are large and validated by a gold standard. Clinical models aim\nto deal with classification on users' smartphone cameras that do not contain\nthe corresponding resolution provided by dermoscopy. Also, clinical\napplications bring new challenges. It can contain captures from uncontrolled\nenvironments, skin tone variations, viewpoint changes, noises in data and\nlabels, and unbalanced classes. A possible alternative would be to use transfer\nlearning to deal with the clinical images. However, as the number of samples is\nlow, it can cause degradations on the model's performance; the source\ndistribution used in training differs from the test set. This work aims to\nevaluate the gap between dermoscopic and clinical samples and understand how\nthe dataset variations impact training. It assesses the main differences\nbetween distributions that disturb the model's prediction. Finally, from\nexperiments on different architectures, we argue how to combine the data from\ndivergent distributions, decreasing the impact on the model's final accuracy.\n","authors":["Francisco Filho","Emanoel Santos","Rodrigo Mota","Kelvin Cunha","Fabio Papais","Amanda Arruda","Mateus Baltazar","Camila Vieira","José Gabriel Tavares","Rafael Barros","Othon Souza","Thales Bezerra","Natalia Lopes","Érico Moutinho","Jéssica Guido","Shirley Cruz","Paulo Borba","Tsang Ing Ren"],"pdf_url":"https://arxiv.org/pdf/2501.08962v2.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.07556v1","updated":"2025-02-11T13:48:11Z","published":"2025-02-11T13:48:11Z","title":"SketchFlex: Facilitating Spatial-Semantic Coherence in Text-to-Image\n  Generation with Region-Based Sketches","summary":"  Text-to-image models can generate visually appealing images from text\ndescriptions. Efforts have been devoted to improving model controls with prompt\ntuning and spatial conditioning. However, our formative study highlights the\nchallenges for non-expert users in crafting appropriate prompts and specifying\nfine-grained spatial conditions (e.g., depth or canny references) to generate\nsemantically cohesive images, especially when multiple objects are involved. In\nresponse, we introduce SketchFlex, an interactive system designed to improve\nthe flexibility of spatially conditioned image generation using rough region\nsketches. The system automatically infers user prompts with rational\ndescriptions within a semantic space enriched by crowd-sourced object\nattributes and relationships. Additionally, SketchFlex refines users' rough\nsketches into canny-based shape anchors, ensuring the generation quality and\nalignment of user intentions. Experimental results demonstrate that SketchFlex\nachieves more cohesive image generations than end-to-end models, meanwhile\nsignificantly reducing cognitive load and better matching user intentions\ncompared to region-based generation baseline.\n","authors":["Haichuan Lin","Yilin Ye","Jiazhi Xia","Wei Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.07556v1.pdf","comment":"conference: CHI2025"},{"id":"http://arxiv.org/abs/2409.12886v2","updated":"2025-02-11T13:46:32Z","published":"2024-09-19T16:28:45Z","title":"EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting","summary":"  With their meaningful geometry and their omnipresence in the 3D world, edges\nare extremely useful primitives in computer vision. 3D edges comprise of lines\nand curves, and methods to reconstruct them use either multi-view images or\npoint clouds as input. State-of-the-art image-based methods first learn a 3D\nedge point cloud then fit 3D edges to it. The edge point cloud is obtained by\nlearning a 3D neural implicit edge field from which the 3D edge points are\nsampled on a specific level set (0 or 1). However, such methods present two\nimportant drawbacks: i) it is not realistic to sample points on exact level\nsets due to float imprecision and training inaccuracies. Instead, they are\nsampled within a range of levels so the points do not lie accurately on the 3D\nedges and require further processing. ii) Such implicit representations are\ncomputationally expensive and require long training times. In this paper, we\naddress these two limitations and propose a 3D edge mapping that is simpler,\nmore efficient, and preserves accuracy. Our method learns explicitly the 3D\nedge points and their edge direction hence bypassing the need for point\nsampling. It casts a 3D edge point as the center of a 3D Gaussian and the edge\ndirection as the principal axis of the Gaussian. Such a representation has the\nadvantage of being not only geometrically meaningful but also compatible with\nthe efficient training optimization defined in Gaussian Splatting. Results show\nthat the proposed method produces edges as accurate and complete as the\nstate-of-the-art while being an order of magnitude faster. Code is released at\nhttps://github.com/kunalchelani/EdgeGaussians.\n","authors":["Kunal Chelani","Assia Benbihi","Torsten Sattler","Fredrik Kahl"],"pdf_url":"https://arxiv.org/pdf/2409.12886v2.pdf","comment":"To appear in the proceedings of WACV 2025"},{"id":"http://arxiv.org/abs/2409.11536v2","updated":"2025-02-11T13:20:42Z","published":"2024-09-17T20:13:54Z","title":"Obfuscation Based Privacy Preserving Representations are Recoverable\n  Using Neighborhood Information","summary":"  Rapid growth in the popularity of AR/VR/MR applications and cloud-based\nvisual localization systems has given rise to an increased focus on the privacy\nof user content in the localization process. This privacy concern has been\nfurther escalated by the ability of deep neural networks to recover detailed\nimages of a scene from a sparse set of 3D or 2D points and their descriptors -\nthe so-called inversion attacks. Research on privacy-preserving localization\nhas therefore focused on preventing these inversion attacks on both the query\nimage keypoints and the 3D points of the scene map. To this end, several\ngeometry obfuscation techniques that lift points to higher-dimensional spaces,\ni.e., lines or planes, or that swap coordinates between points % have been\nproposed. In this paper, we point to a common weakness of these obfuscations\nthat allows to recover approximations of the original point positions under the\nassumption of known neighborhoods. We further show that these neighborhoods can\nbe computed by learning to identify descriptors that co-occur in neighborhoods.\nExtensive experiments show that our approach for point recovery is practically\napplicable to all existing geometric obfuscation schemes. Our results show that\nthese schemes should not be considered privacy-preserving, even though they are\nclaimed to be privacy-preserving. Code will be available at\nhttps://github.com/kunalchelani/RecoverPointsNeighborhood.\n","authors":["Kunal Chelani","Assia Benbihi","Fredrik Kahl","Torsten Sattler","Zuzana Kukelova"],"pdf_url":"https://arxiv.org/pdf/2409.11536v2.pdf","comment":"To appear in the proceedings of 3DV 2025"},{"id":"http://arxiv.org/abs/2502.07531v1","updated":"2025-02-11T13:11:59Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07526v1","updated":"2025-02-11T13:05:42Z","published":"2025-02-11T13:05:42Z","title":"CodePhys: Robust Video-based Remote Physiological Measurement through\n  Latent Codebook Querying","summary":"  Remote photoplethysmography (rPPG) aims to measure non-contact physiological\nsignals from facial videos, which has shown great potential in many\napplications. Most existing methods directly extract video-based rPPG features\nby designing neural networks for heart rate estimation. Although they can\nachieve acceptable results, the recovery of rPPG signal faces intractable\nchallenges when interference from real-world scenarios takes place on facial\nvideo. Specifically, facial videos are inevitably affected by non-physiological\nfactors (e.g., camera device noise, defocus, and motion blur), leading to the\ndistortion of extracted rPPG signals. Recent rPPG extraction methods are easily\naffected by interference and degradation, resulting in noisy rPPG signals. In\nthis paper, we propose a novel method named CodePhys, which innovatively treats\nrPPG measurement as a code query task in a noise-free proxy space (i.e.,\ncodebook) constructed by ground-truth PPG signals. We consider noisy rPPG\nfeatures as queries and generate high-fidelity rPPG features by matching them\nwith noise-free PPG features from the codebook. Our approach also incorporates\na spatial-aware encoder network with a spatial attention mechanism to highlight\nphysiologically active areas and uses a distillation loss to reduce the\ninfluence of non-periodic visual interference. Experimental results on four\nbenchmark datasets demonstrate that CodePhys outperforms state-of-the-art\nmethods in both intra-dataset and cross-dataset settings.\n","authors":["Shuyang Chu","Menghan Xia","Mengyao Yuan","Xin Liu","Tapio Seppanen","Guoying Zhao","Jingang Shi"],"pdf_url":"https://arxiv.org/pdf/2502.07526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12255v4","updated":"2025-02-11T13:03:27Z","published":"2025-01-21T16:23:05Z","title":"HAC++: Towards 100X Compression of 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel\nview synthesis, boasting rapid rendering speed with high fidelity. However, the\nsubstantial Gaussians and their associated attributes necessitate effective\ncompression techniques. Nevertheless, the sparse and unorganized nature of the\npoint cloud of Gaussians (or anchors in our paper) presents challenges for\ncompression. To achieve a compact size, we propose HAC++, which leverages the\nrelationships between unorganized anchors and a structured hash grid, utilizing\ntheir mutual information for context modeling. Additionally, HAC++ captures\nintra-anchor contextual relationships to further enhance compression\nperformance. To facilitate entropy coding, we utilize Gaussian distributions to\nprecisely estimate the probability of each quantized attribute, where an\nadaptive quantization module is proposed to enable high-precision quantization\nof these attributes for improved fidelity restoration. Moreover, we incorporate\nan adaptive masking strategy to eliminate invalid Gaussians and anchors.\nOverall, HAC++ achieves a remarkable size reduction of over 100X compared to\nvanilla 3DGS when averaged on all datasets, while simultaneously improving\nfidelity. It also delivers more than 20X size reduction compared to\nScaffold-GS. Our code is available at\nhttps://github.com/YihangChen-ee/HAC-plus.\n","authors":["Yihang Chen","Qianyi Wu","Weiyao Lin","Mehrtash Harandi","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2501.12255v4.pdf","comment":"Project Page: https://yihangchen-ee.github.io/project_hac++/ Code:\n  https://github.com/YihangChen-ee/HAC-plus. This paper is a journal extension\n  of HAC at arXiv:2403.14530 (ECCV 2024)"},{"id":"http://arxiv.org/abs/2406.16502v3","updated":"2025-02-11T12:57:22Z","published":"2024-06-24T10:12:03Z","title":"LOGCAN++: Adaptive Local-global class-aware network for semantic\n  segmentation of remote sensing imagery","summary":"  Remote sensing images usually characterized by complex backgrounds, scale and\norientation variations, and large intra-class variance. General semantic\nsegmentation methods usually fail to fully investigate the above issues, and\nthus their performances on remote sensing image segmentation are limited. In\nthis paper, we propose our LOGCAN++, a semantic segmentation model customized\nfor remote sensing images, which is made up of a Global Class Awareness (GCA)\nmodule and several Local Class Awareness (LCA) modules. The GCA module captures\nglobal representations for class-level context modeling to reduce the\ninterference of background noise. The LCA module generates local class\nrepresentations as intermediate perceptual elements to indirectly associate\npixels with the global class representations, targeting at dealing with the\nlarge intra-class variance problem. In particular, we introduce affine\ntransformations in the LCA module for adaptive extraction of local class\nrepresentations to effectively tolerate scale and orientation variations in\nremotely sensed images. Extensive experiments on three benchmark datasets show\nthat our LOGCAN++ outperforms current mainstream general and remote sensing\nsemantic segmentation methods and achieves a better trade-off between speed and\naccuracy. Code is available at https://github.com/xwmaxwma/rssegmentation.\n","authors":["Xiaowen Ma","Rongrong Lian","Zhenkai Wu","Hongbo Guo","Mengting Ma","Sensen Wu","Zhenhong Du","Siyang Song","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16502v3.pdf","comment":"Accepted by TGRS2025"},{"id":"http://arxiv.org/abs/2405.19458v4","updated":"2025-02-11T12:41:08Z","published":"2024-05-29T19:12:08Z","title":"MemControl: Mitigating Memorization in Diffusion Models via Automated\n  Parameter Selection","summary":"  Diffusion models excel in generating images that closely resemble their\ntraining data but are also susceptible to data memorization, raising privacy,\nethical, and legal concerns, particularly in sensitive domains such as medical\nimaging. We hypothesize that this memorization stems from the\noverparameterization of deep models and propose that regularizing model\ncapacity during fine-tuning can mitigate this issue. Firstly, we empirically\nshow that regulating the model capacity via Parameter-efficient fine-tuning\n(PEFT) mitigates memorization to some extent, however, it further requires the\nidentification of the exact parameter subsets to be fine-tuned for high-quality\ngeneration. To identify these subsets, we introduce a bi-level optimization\nframework, MemControl, that automates parameter selection using memorization\nand generation quality metrics as rewards during fine-tuning. The parameter\nsubsets discovered through MemControl achieve a superior tradeoff between\ngeneration quality and memorization. For the task of medical image generation,\nour approach outperforms existing state-of-the-art memorization mitigation\nstrategies by fine-tuning as few as 0.019% of model parameters. Moreover, we\ndemonstrate that the discovered parameter subsets are transferable to\nnon-medical domains. Our framework is scalable to large datasets, agnostic to\nreward functions, and can be integrated with existing approaches for further\nmemorization mitigation. To the best of our knowledge, this is the first study\nto empirically evaluate memorization in medical images and propose a targeted\nyet universal mitigation strategy. The code is available at\nhttps://github.com/Raman1121/Diffusion_Memorization_HPO.\n","authors":["Raman Dutt","Ondrej Bohdal","Pedro Sanchez","Sotirios A. Tsaftaris","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2405.19458v4.pdf","comment":"Accepted into WACV 2025 (Applications Track)"},{"id":"http://arxiv.org/abs/2501.02737v2","updated":"2025-02-11T12:38:35Z","published":"2025-01-06T03:11:12Z","title":"Holistic Semantic Representation for Navigational Trajectory Generation","summary":"  Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation.\n","authors":["Ji Cao","Tongya Zheng","Qinghong Guo","Yu Wang","Junshu Dai","Shunyu Liu","Jie Yang","Jie Song","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2501.02737v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.07516v1","updated":"2025-02-11T12:36:00Z","published":"2025-02-11T12:36:00Z","title":"The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation","summary":"  Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study adopts a data-driven approach and presents\nthe first systematic attempt to identify prompts and text tokens in MIMIC-CXR\nthat contribute the most to training data memorization. Our analysis reveals an\nunexpected finding: prompts containing traces of de-identification procedures\nare among the most memorized, with de-identification markers contributing the\nmost. Furthermore, we also find existing inference-time memorization mitigation\nstrategies are ineffective and fail to sufficiently reduce the model's reliance\non memorized text tokens highlighting a broader issue in T2I synthesis with\nMIMIC-CXR. On this front, we propose actionable strategies to enhance privacy\nand improve the reliability of generative models in medical imaging. Finally,\nour results provide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset.\n","authors":["Raman Dutt"],"pdf_url":"https://arxiv.org/pdf/2502.07516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07511v1","updated":"2025-02-11T12:28:50Z","published":"2025-02-11T12:28:50Z","title":"Quantitative evaluation of unsupervised clustering algorithms for\n  dynamic total-body PET image analysis","summary":"  Background. Recently, dynamic total-body positron emission tomography (PET)\nimaging has become possible due to new scanner devices. While clustering\nalgorithms have been proposed for PET analysis already earlier, there is still\nlittle research systematically evaluating these algorithms for processing of\ndynamic total-body PET images. Materials and methods. Here, we compare the\nperformance of 15 unsupervised clustering methods, including K-means either by\nitself or after principal component analysis (PCA) or independent component\nanalysis (ICA), Gaussian mixture model (GMM), fuzzy c-means (FCM),\nagglomerative clustering, spectral clustering, and several newer clustering\nalgorithms, for classifying time activity curves (TACs) in dynamic PET images.\nWe use dynamic total-body $^{15}$O-water PET images collected from 30 patients\nwith suspected or confirmed coronary artery disease. To evaluate the clustering\nalgorithms in a quantitative way, we use them to classify 5000 TACs from each\nimage based on whether the curve is taken from brain, right heart ventricle,\nright kidney, lower right lung lobe, or urinary bladder. Results. According to\nour results, the best methods are GMM, FCM, and ICA combined with mini batch\nK-means, which classified the TACs with a median accuracies of 89\\%, 83\\%, and\n81\\%, respectively, in a processing time of half a second or less on average\nfor each image. Conclusion. GMM, FCM, and ICA with mini batch K-means show\npromise for dynamic total-body PET analysis.\n","authors":["Oona Rainio","Maria K. Jaakkola","Riku Klén"],"pdf_url":"https://arxiv.org/pdf/2502.07511v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2301.00524v3","updated":"2025-02-11T12:25:17Z","published":"2023-01-02T04:27:25Z","title":"Learning Confident Classifiers in the Presence of Label Noise","summary":"  The success of Deep Neural Network (DNN) models significantly depends on the\nquality of provided annotations. In medical image segmentation, for example,\nhaving multiple expert annotations for each data point is common to minimize\nsubjective annotation bias. Then, the goal of estimation is to filter out the\nlabel noise and recover the ground-truth masks, which are not explicitly given.\nThis paper proposes a probabilistic model for noisy observations that allows us\nto build a confident classification and segmentation models. To accomplish it,\nwe explicitly model label noise and introduce a new information-based\nregularization that pushes the network to recover the ground-truth labels. In\naddition, for segmentation task we adjust the loss function by prioritizing\nlearning in high-confidence regions where all the annotators agree on labeling.\nWe evaluate the proposed method on a series of classification tasks such as\nnoisy versions of MNIST, CIFAR-10, Fashion-MNIST datasets as well as CIFAR-10N,\nwhich is real-world dataset with noisy human annotations. Additionally, for\nsegmentation task, we consider several medical imaging datasets, such as, LIDC\nand RIGA that reflect real-world inter-variability among multiple annotators.\nOur experiments show that our algorithm outperforms state-of-the-art solutions\nfor the considered classification and segmentation problems.\n","authors":["Asma Ahmed Hashmi","Aigerim Zhumabayeva","Nikita Kotelevskii","Artem Agafonov","Mohammad Yaqub","Maxim Panov","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2301.00524v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07508v1","updated":"2025-02-11T12:22:35Z","published":"2025-02-11T12:22:35Z","title":"Enhance-A-Video: Better Generated Video for Free","summary":"  DiT-based video generation has achieved remarkable results, but research into\nenhancing existing models remains relatively unexplored. In this work, we\nintroduce a training-free approach to enhance the coherence and quality of\nDiT-based generated videos, named Enhance-A-Video. The core idea is enhancing\nthe cross-frame correlations based on non-diagonal temporal attention\ndistributions. Thanks to its simple design, our approach can be easily applied\nto most DiT-based video generation frameworks without any retraining or\nfine-tuning. Across various DiT-based video generation models, our approach\ndemonstrates promising improvements in both temporal consistency and visual\nquality. We hope this research can inspire future explorations in video\ngeneration enhancement.\n","authors":["Yang Luo","Xuanlei Zhao","Mengzhao Chen","Kaipeng Zhang","Wenqi Shao","Kai Wang","Zhangyang Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2502.07508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07505v1","updated":"2025-02-11T12:15:56Z","published":"2025-02-11T12:15:56Z","title":"Efficient Continuous Group Convolutions for Local SE(3) Equivariance in\n  3D Point Clouds","summary":"  Extending the translation equivariance property of convolutional neural\nnetworks to larger symmetry groups has been shown to reduce sample complexity\nand enable more discriminative feature learning. Further, exploiting additional\nsymmetries facilitates greater weight sharing than standard convolutions,\nleading to an enhanced network expressivity without an increase in parameter\ncount. However, extending the equivariant properties of a convolution layer\ncomes at a computational cost. In particular, for 3D data, expanding\nequivariance to the SE(3) group (rotation and translation) results in a 6D\nconvolution operation, which is not tractable for larger data samples such as\n3D scene scans. While efforts have been made to develop efficient SE(3)\nequivariant networks, existing approaches rely on discretization or only\nintroduce global rotation equivariance. This limits their applicability to\npoint clouds representing a scene composed of multiple objects. This work\npresents an efficient, continuous, and local SE(3) equivariant convolution\nlayer for point cloud processing based on general group convolution and local\nreference frames. Our experiments show that our approach achieves competitive\nor superior performance across a range of datasets and tasks, including object\nclassification and semantic segmentation, with negligible computational\noverhead.\n","authors":["Lisa Weijler","Pedro Hermosilla"],"pdf_url":"https://arxiv.org/pdf/2502.07505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07492v1","updated":"2025-02-11T11:51:12Z","published":"2025-02-11T11:51:12Z","title":"RoMA: Robust Malware Attribution via Byte-level Adversarial Training\n  with Global Perturbations and Adversarial Consistency Regularization","summary":"  Attributing APT (Advanced Persistent Threat) malware to their respective\ngroups is crucial for threat intelligence and cybersecurity. However, APT\nadversaries often conceal their identities, rendering attribution inherently\nadversarial. Existing machine learning-based attribution models, while\neffective, remain highly vulnerable to adversarial attacks. For example, the\nstate-of-the-art byte-level model MalConv sees its accuracy drop from over 90%\nto below 2% under PGD (projected gradient descent) attacks. Existing\ngradient-based adversarial training techniques for malware detection or image\nprocessing were applied to malware attribution in this study, revealing that\nboth robustness and training efficiency require significant improvement. To\naddress this, we propose RoMA, a novel single-step adversarial training\napproach that integrates global perturbations to generate enhanced adversarial\nsamples and employs adversarial consistency regularization to improve\nrepresentation quality and resilience. A novel APT malware dataset named AMG18,\nwith diverse samples and realistic class imbalances, is introduced for\nevaluation. Extensive experiments show that RoMA significantly outperforms\nseven competing methods in both adversarial robustness (e.g., achieving over\n80% robust accuracy-more than twice that of the next-best method under PGD\nattacks) and training efficiency (e.g., more than twice as fast as the\nsecond-best method in terms of accuracy), while maintaining superior standard\naccuracy in non-adversarial scenarios.\n","authors":["Yuxia Sun","Huihong Chen","Jingcai Guo","Aoxiang Sun","Zhetao Li","Haolin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07492v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.07486v1","updated":"2025-02-11T11:45:52Z","published":"2025-02-11T11:45:52Z","title":"Automated Road Extraction and Centreline Fitting in LiDAR Point Clouds","summary":"  Road information extraction from 3D point clouds is useful for urban planning\nand traffic management. Existing methods often rely on local features and the\nrefraction angle of lasers from kerbs, which makes them sensitive to variable\nkerb designs and issues in high-density areas due to data homogeneity. We\npropose an approach for extracting road points and fitting centrelines using a\ntop-down view of LiDAR based ground-collected point clouds. This prospective\nview reduces reliance on specific kerb design and results in better road\nextraction. We first perform statistical outlier removal and density-based\nclustering to reduce noise from 3D point cloud data. Next, we perform ground\npoint filtering using a grid-based segmentation method that adapts to diverse\nroad scenarios and terrain characteristics. The filtered points are then\nprojected onto a 2D plane, and the road is extracted by a skeletonisation\nalgorithm. The skeleton is back-projected onto the 3D point cloud with\ncalculated normals, which guide a region growing algorithm to find nearby road\npoints. The extracted road points are then smoothed with the Savitzky-Golay\nfilter to produce the final centreline. Our initial approach without\npost-processing of road skeleton achieved 67% in IoU by testing on the Perth\nCBD dataset with different road types. Incorporating the post-processing of the\nroad skeleton improved the extraction of road points around the smoothed\nskeleton. The refined approach achieved a higher IoU value of 73% and with 23%\nreduction in the processing time. Our approach offers a generalised and\ncomputationally efficient solution that combines 3D and 2D processing\ntechniques, laying the groundwork for future road reconstruction and 3D-to-2D\npoint cloud alignment.\n","authors":["Xinyu Wang","Muhammad Ibrahim","Atif Mansoor","Hasnein Tareque","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2502.07486v1.pdf","comment":"8 pages, 10 figures, accepted in DICTA 2024"},{"id":"http://arxiv.org/abs/2502.06288v2","updated":"2025-02-11T11:25:19Z","published":"2025-02-10T09:31:12Z","title":"Enhancing Ground-to-Aerial Image Matching for Visual Misinformation\n  Detection Using Semantic Segmentation","summary":"  The recent advancements in generative AI techniques, which have significantly\nincreased the online dissemination of altered images and videos, have raised\nserious concerns about the credibility of digital media available on the\nInternet and distributed through information channels and social networks. This\nissue particularly affects domains that rely heavily on trustworthy data, such\nas journalism, forensic analysis, and Earth observation. To address these\nconcerns, the ability to geolocate a non-geo-tagged ground-view image without\nexternal information, such as GPS coordinates, has become increasingly\ncritical. This study tackles the challenge of linking a ground-view image,\npotentially exhibiting varying fields of view (FoV), to its corresponding\nsatellite image without the aid of GPS data. To achieve this, we propose a\nnovel four-stream Siamese-like architecture, the Quadruple Semantic Align Net\n(SAN-QUAD), which extends previous state-of-the-art (SOTA) approaches by\nleveraging semantic segmentation applied to both ground and satellite imagery.\nExperimental results on a subset of the CVUSA dataset demonstrate significant\nimprovements of up to 9.8\\% over prior methods across various FoV settings.\n","authors":["Emanuele Mule","Matteo Pannacci","Ali Ghasemi Goudarzi","Francesco Pro","Lorenzo Papa","Luca Maiano","Irene Amerini"],"pdf_url":"https://arxiv.org/pdf/2502.06288v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.01322v3","updated":"2025-02-11T11:18:26Z","published":"2024-08-02T15:20:34Z","title":"A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty\n  and Semantic Object Cues for Gaze Guidance in Dynamic Scenes","summary":"  The objects we perceive guide our eye movements when observing real-world\ndynamic scenes. Yet, gaze shifts and selective attention are critical for\nperceiving details and refining object boundaries. Object segmentation and gaze\nbehavior are, however, typically treated as two independent processes. Here, we\npresent a computational model that simulates these processes in an\ninterconnected manner and allows for hypothesis-driven investigations of\ndistinct attentional mechanisms. Drawing on an information processing pattern\nfrom robotics, we use a Bayesian filter to recursively segment the scene, which\nalso provides an uncertainty estimate for the object boundaries that we use to\nguide active scene exploration. We demonstrate that this model closely\nresembles observers' free viewing behavior on a dataset of dynamic real-world\nscenes, measured by scanpath statistics, including foveation duration and\nsaccade amplitude distributions used for parameter fitting and higher-level\nstatistics not used for fitting. These include how object detections,\ninspections, and returns are balanced and a delay of returning saccades without\nan explicit implementation of such temporal inhibition of return. Extensive\nsimulations and ablation studies show that uncertainty promotes balanced\nexploration and that semantic object cues are crucial to forming the perceptual\nunits used in object-based attention. Moreover, we show how our model's modular\ndesign allows for extensions, such as incorporating saccadic momentum or\npre-saccadic attention, to further align its output with human scanpaths.\n","authors":["Vito Mengers","Nicolas Roth","Oliver Brock","Klaus Obermayer","Martin Rolfs"],"pdf_url":"https://arxiv.org/pdf/2408.01322v3.pdf","comment":"40+25 pages, 8+7 figures"},{"id":"http://arxiv.org/abs/2502.07466v1","updated":"2025-02-11T11:17:39Z","published":"2025-02-11T11:17:39Z","title":"Less is More: Masking Elements in Image Condition Features Avoids\n  Content Leakages in Style Transfer Diffusion Models","summary":"  Given a style-reference image as the additional image condition,\ntext-to-image diffusion models have demonstrated impressive capabilities in\ngenerating images that possess the content of text prompts while adopting the\nvisual style of the reference image. However, current state-of-the-art methods\noften struggle to disentangle content and style from style-reference images,\nleading to issues such as content leakages. To address this issue, we propose a\nmasking-based method that efficiently decouples content from style without the\nneed of tuning any model parameters. By simply masking specific elements in the\nstyle reference's image features, we uncover a critical yet under-explored\nprinciple: guiding with appropriately-selected fewer conditions (e.g., dropping\nseveral image feature elements) can efficiently avoid unwanted content flowing\ninto the diffusion models, enhancing the style transfer performances of\ntext-to-image diffusion models. In this paper, we validate this finding both\ntheoretically and experimentally. Extensive experiments across various styles\ndemonstrate the effectiveness of our masking-based method and support our\ntheoretical results.\n","authors":["Lin Zhu","Xinbing Wang","Chenghu Zhou","Qinying Gu","Nanyang Ye"],"pdf_url":"https://arxiv.org/pdf/2502.07466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17438v2","updated":"2025-02-11T11:10:32Z","published":"2024-11-26T13:54:24Z","title":"Object-centric proto-symbolic behavioural reasoning from pixels","summary":"  Autonomous intelligent agents must bridge computational challenges at\ndisparate levels of abstraction, from the low-level spaces of sensory input and\nmotor commands to the high-level domain of abstract reasoning and planning. A\nkey question in designing such agents is how best to instantiate the\nrepresentational space that will interface between these two levels -- ideally\nwithout requiring supervision in the form of expensive data annotations. These\nobjectives can be efficiently achieved by representing the world in terms of\nobjects (grounded in perception and action). In this work, we present a novel,\nbrain-inspired, deep-learning architecture that learns from pixels to\ninterpret, control, and reason about its environment, using object-centric\nrepresentations. We show the utility of our approach through tasks in synthetic\nenvironments that require a combination of (high-level) logical reasoning and\n(low-level) continuous control. Results show that the agent can learn emergent\nconditional behavioural reasoning, such as $(A \\to B) \\land (\\neg A \\to C)$, as\nwell as logical composition $(A \\to B) \\land (A \\to C) \\vdash A \\to (B \\land\nC)$ and XOR operations, and successfully controls its environment to satisfy\nobjectives deduced from these logical rules. The agent can adapt online to\nunexpected changes in its environment and is robust to mild violations of its\nworld model, thanks to dynamic internal desired goal generation. While the\npresent results are limited to synthetic settings (2D and 3D activated versions\nof dSprites), which fall short of real-world levels of complexity, the proposed\narchitecture shows how to manipulate grounded object representations, as a key\ninductive bias for unsupervised learning, to enable behavioral reasoning.\n","authors":["Ruben van Bergen","Justus Hübotter","Pablo Lanillos"],"pdf_url":"https://arxiv.org/pdf/2411.17438v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07457v1","updated":"2025-02-11T11:03:09Z","published":"2025-02-11T11:03:09Z","title":"Bidirectional Uncertainty-Aware Region Learning for Semi-Supervised\n  Medical Image Segmentation","summary":"  In semi-supervised medical image segmentation, the poor quality of unlabeled\ndata and the uncertainty in the model's predictions lead to models that\ninevitably produce erroneous pseudo-labels. These errors accumulate throughout\nmodel training, thereby weakening the model's performance. We found that these\nerroneous pseudo-labels are typically concentrated in high-uncertainty regions.\nTraditional methods improve performance by directly discarding pseudo-labels in\nthese regions, but this can also result in neglecting potentially valuable\ntraining data. To alleviate this problem, we propose a bidirectional\nuncertainty-aware region learning strategy. In training labeled data, we focus\non high-uncertainty regions, using precise label information to guide the\nmodel's learning in potentially uncontrollable areas. Meanwhile, in the\ntraining of unlabeled data, we concentrate on low-uncertainty regions to reduce\nthe interference of erroneous pseudo-labels on the model. Through this\nbidirectional learning strategy, the model's overall performance has\nsignificantly improved. Extensive experiments show that our proposed method\nachieves significant performance improvement on different medical image\nsegmentation tasks.\n","authors":["Shiwei Zhou","Haifeng Zhao","Dengdi Sun"],"pdf_url":"https://arxiv.org/pdf/2502.07457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07456v1","updated":"2025-02-11T11:00:58Z","published":"2025-02-11T11:00:58Z","title":"FedAPA: Server-side Gradient-Based Adaptive Personalized Aggregation for\n  Federated Learning on Heterogeneous Data","summary":"  Personalized federated learning (PFL) tailors models to clients' unique data\ndistributions while preserving privacy. However, existing\naggregation-weight-based PFL methods often struggle with heterogeneous data,\nfacing challenges in accuracy, computational efficiency, and communication\noverhead. We propose FedAPA, a novel PFL method featuring a server-side,\ngradient-based adaptive aggregation strategy to generate personalized models,\nby updating aggregation weights based on gradients of client-parameter changes\nwith respect to the aggregation weights in a centralized manner. FedAPA\nguarantees theoretical convergence and achieves superior accuracy and\ncomputational efficiency compared to 10 PFL competitors across three datasets,\nwith competitive communication overhead.\n","authors":["Yuxia Sun","Aoxiang Sun","Siyi Pan","Zhixiao Fu","Jingcai Guo"],"pdf_url":"https://arxiv.org/pdf/2502.07456v1.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.07209v2","updated":"2025-02-11T10:58:09Z","published":"2024-06-11T12:32:53Z","title":"MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout\n  Guidance","summary":"  Recent advancements in text-to-image generation models have dramatically\nenhanced the generation of photorealistic images from textual prompts, leading\nto an increased interest in personalized text-to-image applications,\nparticularly in multi-subject scenarios. However, these advances are hindered\nby two main challenges: firstly, the need to accurately maintain the details of\neach referenced subject in accordance with the textual descriptions; and\nsecondly, the difficulty in achieving a cohesive representation of multiple\nsubjects in a single image without introducing inconsistencies. To address\nthese concerns, our research introduces the MS-Diffusion framework for\nlayout-guided zero-shot image personalization with multi-subjects. This\ninnovative approach integrates grounding tokens with the feature resampler to\nmaintain detail fidelity among subjects. With the layout guidance, MS-Diffusion\nfurther improves the cross-attention to adapt to the multi-subject inputs,\nensuring that each subject condition acts on specific areas. The proposed\nmulti-subject cross-attention orchestrates harmonious inter-subject\ncompositions while preserving the control of texts. Comprehensive quantitative\nand qualitative experiments affirm that this method surpasses existing models\nin both image and text fidelity, promoting the development of personalized\ntext-to-image generation. The project page is https://MS-Diffusion.github.io.\n","authors":["X. Wang","Siming Fu","Qihan Huang","Wanggui He","Hao Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.07209v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07455v1","updated":"2025-02-11T10:57:12Z","published":"2025-02-11T10:57:12Z","title":"RusCode: Russian Cultural Code Benchmark for Text-to-Image Generation","summary":"  Text-to-image generation models have gained popularity among users around the\nworld. However, many of these models exhibit a strong bias toward\nEnglish-speaking cultures, ignoring or misrepresenting the unique\ncharacteristics of other language groups, countries, and nationalities. The\nlack of cultural awareness can reduce the generation quality and lead to\nundesirable consequences such as unintentional insult, and the spread of\nprejudice. In contrast to the field of natural language processing, cultural\nawareness in computer vision has not been explored as extensively. In this\npaper, we strive to reduce this gap. We propose a RusCode benchmark for\nevaluating the quality of text-to-image generation containing elements of the\nRussian cultural code. To do this, we form a list of 19 categories that best\nrepresent the features of Russian visual culture. Our final dataset consists of\n1250 text prompts in Russian and their translations into English. The prompts\ncover a wide range of topics, including complex concepts from art, popular\nculture, folk traditions, famous people's names, natural objects, scientific\nachievements, etc. We present the results of a human evaluation of the\nside-by-side comparison of Russian visual concepts representations using\npopular generative models.\n","authors":["Viacheslav Vasilev","Julia Agafonova","Nikolai Gerasimenko","Alexander Kapitanov","Polina Mikhailova","Evelina Mironova","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2502.07455v1.pdf","comment":"Accepted for NAACL 2025 Findings, GitHub:\n  https://github.com/ai-forever/RusCode"},{"id":"http://arxiv.org/abs/2502.04843v2","updated":"2025-02-11T10:48:23Z","published":"2025-02-07T11:24:23Z","title":"PoI: Pixel of Interest for Novel View Synthesis Assisted Scene\n  Coordinate Regression","summary":"  The task of estimating camera poses can be enhanced through novel view\nsynthesis techniques such as NeRF and Gaussian Splatting to increase the\ndiversity and extension of training data. However, these techniques often\nproduce rendered images with issues like blurring and ghosting, which\ncompromise their reliability. These issues become particularly pronounced for\nScene Coordinate Regression (SCR) methods, which estimate 3D coordinates at the\npixel level. To mitigate the problems associated with unreliable rendered\nimages, we introduce a novel filtering approach, which selectively extracts\nwell-rendered pixels while discarding the inferior ones. This filter\nsimultaneously measures the SCR model's real-time reprojection loss and\ngradient during training. Building on this filtering technique, we also develop\na new strategy to improve scene coordinate regression using sparse inputs,\ndrawing on successful applications of sparse input techniques in novel view\nsynthesis. Our experimental results validate the effectiveness of our method,\ndemonstrating state-of-the-art performance on indoor and outdoor datasets.\n","authors":["Feifei Li","Qi Song","Chi Zhang","Hui Shuai","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2502.04843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08508v3","updated":"2025-02-11T10:38:48Z","published":"2024-11-13T10:43:39Z","title":"BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel\n  View Synthesis","summary":"  We present billboard Splatting (BBSplat) - a novel approach for 3D scene\nrepresentation based on textured geometric primitives. BBSplat represents the\nscene as a set of optimizable textured planar primitives with learnable RGB\ntextures and alpha-maps to control their shape. BBSplat primitives can be used\nin any Gaussian Splatting pipeline as drop-in replacements for Gaussians. The\nproposed primitives close the rendering quality gap between 2D and 3D Gaussian\nSplatting (GS), preserving the accurate mesh extraction ability of 2D\nprimitives. Our novel regularization term encourages textures to have a sparser\nstructure, unlocking an efficient compression that leads to a reduction in the\nstorage space of the model. Our experiments show the efficiency of BBSplat on\nstandard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,\nand Mip-NeRF-360.\n","authors":["David Svitov","Pietro Morerio","Lourdes Agapito","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2411.08508v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07442v1","updated":"2025-02-11T10:37:01Z","published":"2025-02-11T10:37:01Z","title":"Hierarchical Document Parsing via Large Margin Feature Matching and\n  Heuristics","summary":"  We present our solution to the AAAI-25 VRD-IU challenge, achieving first\nplace in the competition. Our approach integrates large margin loss for\nimproved feature discrimination and employs heuristic rules to refine\nhierarchical relationships. By combining a deep learning-based matching\nstrategy with greedy algorithms, we achieve a significant boost in accuracy\nwhile maintaining computational efficiency. Our method attains an accuracy of\n0.98904 on the private leaderboard, demonstrating its effectiveness in document\nstructure parsing. Source codes are publicly available at\nhttps://github.com/ffyyytt/VRUID-AAAI-DAKiet\n","authors":["Duong Anh Kiet"],"pdf_url":"https://arxiv.org/pdf/2502.07442v1.pdf","comment":"DocUI@AAAI-25, 2 pages, technical report"},{"id":"http://arxiv.org/abs/2502.01216v2","updated":"2025-02-11T10:27:01Z","published":"2025-02-03T10:13:34Z","title":"Exploring Few-Shot Defect Segmentation in General Industrial Scenarios\n  with Metric Learning and Vision Foundation Models","summary":"  Industrial defect segmentation is critical for manufacturing quality control.\nDue to the scarcity of training defect samples, few-shot semantic segmentation\n(FSS) holds significant value in this field. However, existing studies mostly\napply FSS to tackle defects on simple textures, without considering more\ndiverse scenarios. This paper aims to address this gap by exploring FSS in\nbroader industrial products with various defect types. To this end, we\ncontribute a new real-world dataset and reorganize some existing datasets to\nbuild a more comprehensive few-shot defect segmentation (FDS) benchmark. On\nthis benchmark, we thoroughly investigate metric learning-based FSS methods,\nincluding those based on meta-learning and those based on Vision Foundation\nModels (VFMs). We observe that existing meta-learning-based methods are\ngenerally not well-suited for this task, while VFMs hold great potential. We\nfurther systematically study the applicability of various VFMs in this task,\ninvolving two paradigms: feature matching and the use of Segment Anything (SAM)\nmodels. We propose a novel efficient FDS method based on feature matching.\nMeanwhile, we find that SAM2 is particularly effective for addressing FDS\nthrough its video track mode. The contributed dataset and code will be\navailable at: https://github.com/liutongkun/GFDS.\n","authors":["Tongkun Liu","Bing Li","Xiao Jin","Yupeng Shi","Qiuying Li","Xiang Wei"],"pdf_url":"https://arxiv.org/pdf/2502.01216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07436v1","updated":"2025-02-11T10:24:57Z","published":"2025-02-11T10:24:57Z","title":"Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head\n  Attention without Alignment Barriers","summary":"  Knowledge distillation (KD) in transformers often faces challenges due to\nmisalignment in the number of attention heads between teacher and student\nmodels. Existing methods either require identical head counts or introduce\nprojectors to bridge dimensional gaps, limiting flexibility and efficiency. We\npropose Squeezing-Heads Distillation (SHD), a novel approach that enables\nseamless knowledge transfer between models with varying head counts by\ncompressing multi-head attention maps via efficient linear approximation.\nUnlike prior work, SHD eliminates alignment barriers without additional\nparameters or architectural modifications. Our method dynamically approximates\nthe combined effect of multiple teacher heads into fewer student heads,\npreserving fine-grained attention patterns while reducing redundancy.\nExperiments across language (LLaMA, GPT) and vision (DiT, MDT) generative and\nvision (DeiT) discriminative tasks demonstrate SHD's effectiveness: it\noutperforms logit-based and feature-alignment KD baselines, achieving\nstate-of-the-art results in image classification, image generation language\nfine-tuning, and language pre-training. The key innovations of flexible head\ncompression, projector-free design, and linear-time complexity make SHD a\nversatile and scalable solution for distilling modern transformers. This work\nbridges a critical gap in KD, enabling efficient deployment of compact models\nwithout compromising performance.\n","authors":["Zhaodong Bing","Linze Li","Jiajun Liang"],"pdf_url":"https://arxiv.org/pdf/2502.07436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07431v1","updated":"2025-02-11T10:19:50Z","published":"2025-02-11T10:19:50Z","title":"ArthroPhase: A Novel Dataset and Method for Phase Recognition in\n  Arthroscopic Video","summary":"  This study aims to advance surgical phase recognition in arthroscopic\nprocedures, specifically Anterior Cruciate Ligament (ACL) reconstruction, by\nintroducing the first arthroscopy dataset and developing a novel\ntransformer-based model. We aim to establish a benchmark for arthroscopic\nsurgical phase recognition by leveraging spatio-temporal features to address\nthe specific challenges of arthroscopic videos including limited field of view,\nocclusions, and visual distortions. We developed the ACL27 dataset, comprising\n27 videos of ACL surgeries, each labeled with surgical phases. Our model\nemploys a transformer-based architecture, utilizing temporal-aware frame-wise\nfeature extraction through a ResNet-50 and transformer layers. This approach\nintegrates spatio-temporal features and introduces a Surgical Progress Index\n(SPI) to quantify surgery progression. The model's performance was evaluated\nusing accuracy, precision, recall, and Jaccard Index on the ACL27 and Cholec80\ndatasets. The proposed model achieved an overall accuracy of 72.91% on the\nACL27 dataset. On the Cholec80 dataset, the model achieved a comparable\nperformance with the state-of-the-art methods with an accuracy of 92.4%. The\nSPI demonstrated an output error of 10.6% and 9.86% on ACL27 and Cholec80\ndatasets respectively, indicating reliable surgery progression estimation. This\nstudy introduces a significant advancement in surgical phase recognition for\narthroscopy, providing a comprehensive dataset and a robust transformer-based\nmodel. The results validate the model's effectiveness and generalizability,\nhighlighting its potential to improve surgical training, real-time assistance,\nand operational efficiency in orthopedic surgery. The publicly available\ndataset and code will facilitate future research and development in this\ncritical field.\n","authors":["Ali Bahari Malayeri","Matthias Seibold","Nicola Cavalcanti","Jonas Hein","Sascha Jecklin","Lazaros Vlachopoulos","Sandro Fucentese","Sandro Hodel","Philipp Furnstahl"],"pdf_url":"https://arxiv.org/pdf/2502.07431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07422v1","updated":"2025-02-11T10:02:43Z","published":"2025-02-11T10:02:43Z","title":"MoENAS: Mixture-of-Expert based Neural Architecture Search for jointly\n  Accurate, Fair, and Robust Edge Deep Neural Networks","summary":"  There has been a surge in optimizing edge Deep Neural Networks (DNNs) for\naccuracy and efficiency using traditional optimization techniques such as\npruning, and more recently, employing automatic design methodologies. However,\nthe focus of these design techniques has often overlooked critical metrics such\nas fairness, robustness, and generalization. As a result, when evaluating SOTA\nedge DNNs' performance in image classification using the FACET dataset, we\nfound that they exhibit significant accuracy disparities (14.09%) across 10\ndifferent skin tones, alongside issues of non-robustness and poor\ngeneralizability. In response to these observations, we introduce\nMixture-of-Experts-based Neural Architecture Search (MoENAS), an automatic\ndesign technique that navigates through a space of mixture of experts to\ndiscover accurate, fair, robust, and general edge DNNs. MoENAS improves the\naccuracy by 4.02% compared to SOTA edge DNNs and reduces the skin tone accuracy\ndisparities from 14.09% to 5.60%, while enhancing robustness by 3.80% and\nminimizing overfitting to 0.21%, all while keeping model size close to\nstate-of-the-art models average size (+0.4M). With these improvements, MoENAS\nestablishes a new benchmark for edge DNN design, paving the way for the\ndevelopment of more inclusive and robust edge DNNs.\n","authors":["Lotfi Abdelkrim Mecharbat","Alberto Marchisio","Muhammad Shafique","Mohammad M. Ghassemi","Tuka Alhanai"],"pdf_url":"https://arxiv.org/pdf/2502.07422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07417v1","updated":"2025-02-11T09:54:09Z","published":"2025-02-11T09:54:09Z","title":"Fast-COS: A Fast One-Stage Object Detector Based on Reparameterized\n  Attention Vision Transformer for Autonomous Driving","summary":"  The perception system is a a critical role of an autonomous driving system\nfor ensuring safety. The driving scene perception system fundamentally\nrepresents an object detection task that requires achieving a balance between\naccuracy and processing speed. Many contemporary methods focus on improving\ndetection accuracy but often overlook the importance of real-time detection\ncapabilities when computational resources are limited. Thus, it is vital to\ninvestigate efficient object detection strategies for driving scenes. This\npaper introduces Fast-COS, a novel single-stage object detection framework\ncrafted specifically for driving scene applications. The research initiates\nwith an analysis of the backbone, considering both macro and micro\narchitectural designs, yielding the Reparameterized Attention Vision\nTransformer (RAViT). RAViT utilizes Reparameterized Multi-Scale Depth-Wise\nConvolution (RepMSDW) and Reparameterized Self-Attention (RepSA) to enhance\ncomputational efficiency and feature extraction. In extensive tests across GPU,\nedge, and mobile platforms, RAViT achieves 81.4% Top-1 accuracy on the\nImageNet-1K dataset, demonstrating significant throughput improvements over\ncomparable backbone models such as ResNet, FastViT, RepViT, and\nEfficientFormer. Additionally, integrating RepMSDW into a feature pyramid\nnetwork forms RepFPN, enabling fast and multi-scale feature fusion. Fast-COS\nenhances object detection in driving scenes, attaining an AP50 score of 57.2%\non the BDD100K dataset and 80.0% on the TJU-DHD Traffic dataset. It surpasses\nleading models in efficiency, delivering up to 75.9% faster GPU inference and\n1.38 higher throughput on edge devices compared to FCOS, YOLOF, and RetinaNet.\nThese findings establish Fast-COS as a highly scalable and reliable solution\nsuitable for real-time applications, especially in resource-limited\nenvironments like autonomous driving systems\n","authors":["Novendra Setyawan","Ghufron Wahyu Kurniawan","Chi-Chia Sun","Wen-Kai Kuo","Jun-Wei Hsieh"],"pdf_url":"https://arxiv.org/pdf/2502.07417v1.pdf","comment":"Under Review on IEEE Transactions on Intelligent Transportation\n  Systems"},{"id":"http://arxiv.org/abs/2502.07411v1","updated":"2025-02-11T09:45:06Z","published":"2025-02-11T09:45:06Z","title":"EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering","summary":"  We introduce EgoTextVQA, a novel and rigorously constructed benchmark for\negocentric QA assistance involving scene text. EgoTextVQA contains 1.5K\nego-view videos and 7K scene-text aware questions that reflect real-user needs\nin outdoor driving and indoor house-keeping activities. The questions are\ndesigned to elicit identification and reasoning on scene text in an egocentric\nand dynamic environment. With EgoTextVQA, we comprehensively evaluate 10\nprominent multimodal large language models. Currently, all models struggle, and\nthe best results (Gemini 1.5 Pro) are around 33% accuracy, highlighting the\nsevere deficiency of these techniques in egocentric QA assistance. Our further\ninvestigations suggest that precise temporal grounding and multi-frame\nreasoning, along with high resolution and auxiliary scene-text inputs, are key\nfor better performance. With thorough analyses and heuristic suggestions, we\nhope EgoTextVQA can serve as a solid testbed for research in egocentric\nscene-text QA assistance.\n","authors":["Sheng Zhou","Junbin Xiao","Qingyun Li","Yicong Li","Xun Yang","Dan Guo","Meng Wang","Tat-Seng Chua","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2502.07411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07409v1","updated":"2025-02-11T09:42:13Z","published":"2025-02-11T09:42:13Z","title":"MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for\n  Few-Shot WSI Classification","summary":"  Whole slide pathology image classification presents challenges due to\ngigapixel image sizes and limited annotation labels, hindering model\ngeneralization. This paper introduces a prompt learning method to adapt large\nvision-language models for few-shot pathology classification. We first extend\nthe Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology\nimage tiles, into a vision-language model by adding adaptors and aligning it\nwith medical text encoders via contrastive learning on 923K image-text pairs.\nThe model is then used to extract visual features and text embeddings from\nfew-shot annotations and fine-tunes with learnable prompt embeddings. Unlike\nprior methods that combine prompts with frozen features using prefix embeddings\nor self-attention, we propose multi-granular attention that compares\ninteractions between learnable prompts with individual image patches and groups\nof them. This approach improves the model's ability to capture both\nfine-grained details and broader context, enhancing its recognition of complex\npatterns across sub-regions. To further improve accuracy, we leverage\n(unbalanced) optimal transport-based visual-text distance to secure model\nrobustness by mitigating perturbations that might occur during the data\naugmentation process. Empirical experiments on lung, kidney, and breast\npathology modalities validate the effectiveness of our approach; thereby, we\nsurpass several of the latest competitors and consistently improve performance\nacross diverse architectures, including CLIP, PLIP, and Prov-GigaPath\nintegrated PLIP. We release our implementations and pre-trained models at this\nMGPATH.\n","authors":["Anh-Tien Nguyen","Duy Minh Ho Nguyen","Nghiem Tuong Diep","Trung Quoc Nguyen","Nhat Ho","Jacqueline Michelle Metsch","Miriam Cindy Maurer","Daniel Sonntag","Hanibal Bohnenberger","Anne-Christin Hauschild"],"pdf_url":"https://arxiv.org/pdf/2502.07409v1.pdf","comment":"first version"},{"id":"http://arxiv.org/abs/2502.07408v1","updated":"2025-02-11T09:40:45Z","published":"2025-02-11T09:40:45Z","title":"No Data, No Optimization: A Lightweight Method To Disrupt Neural\n  Networks With Sign-Flips","summary":"  Deep Neural Networks (DNNs) can be catastrophically disrupted by flipping\nonly a handful of sign bits in their parameters. We introduce Deep Neural\nLesion (DNL), a data-free, lightweight method that locates these critical\nparameters and triggers massive accuracy drops. We validate its efficacy on a\nwide variety of computer vision models and datasets. The method requires no\ntraining data or optimization and can be carried out via common exploits\nsoftware, firmware or hardware based attack vectors. An enhanced variant that\nuses a single forward and backward pass further amplifies the damage beyond\nDNL's zero-pass approach. Flipping just two sign bits in ResNet50 on ImageNet\nreduces accuracy by 99.8\\%. We also show that selectively protecting a small\nfraction of vulnerable sign bits provides a practical defense against such\nattacks.\n","authors":["Ido Galil","Moshe Kimhi","Ran El-Yaniv"],"pdf_url":"https://arxiv.org/pdf/2502.07408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07404v1","updated":"2025-02-11T09:37:10Z","published":"2025-02-11T09:37:10Z","title":"Human-in-the-Loop Annotation for Image-Based Engagement Estimation:\n  Assessing the Impact of Model Reliability on Annotation Accuracy","summary":"  Human-in-the-loop (HITL) frameworks are increasingly recognized for their\npotential to improve annotation accuracy in emotion estimation systems by\ncombining machine predictions with human expertise. This study focuses on\nintegrating a high-performing image-based emotion model into a HITL annotation\nframework to evaluate the collaborative potential of human-machine interaction\nand identify the psychological and practical factors critical to successful\ncollaboration. Specifically, we investigate how varying model reliability and\ncognitive framing influence human trust, cognitive load, and annotation\nbehavior in HITL systems. We demonstrate that model reliability and\npsychological framing significantly impact annotators' trust, engagement, and\nconsistency, offering insights into optimizing HITL frameworks. Through three\nexperimental scenarios with 29 participants--baseline model reliability (S1),\nfabricated errors (S2), and cognitive bias introduced by negative framing\n(S3)--we analyzed behavioral and qualitative data. Reliable predictions in S1\nyielded high trust and annotation consistency, while unreliable outputs in S2\nled to increased critical evaluations but also heightened frustration and\nresponse variability. Negative framing in S3 revealed how cognitive bias\ninfluenced participants to perceive the model as more relatable and accurate,\ndespite misinformation regarding its reliability. These findings highlight the\nimportance of both reliable machine outputs and psychological factors in\nshaping effective human-machine collaboration. By leveraging the strengths of\nboth human oversight and automated systems, this study establishes a scalable\nHITL framework for emotion annotation and lays the foundation for broader\napplications in adaptive learning and human-computer interaction.\n","authors":["Sahana Yadnakudige Subramanya","Ko Watanabe","Andreas Dengel","Shoya Ishimaru"],"pdf_url":"https://arxiv.org/pdf/2502.07404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07403v1","updated":"2025-02-11T09:32:31Z","published":"2025-02-11T09:32:31Z","title":"Extended monocular 3D imaging","summary":"  3D vision is of paramount importance for numerous applications ranging from\nmachine intelligence to precision metrology. Despite much recent progress, the\nmajority of 3D imaging hardware remains bulky and complicated and provides much\nlower image resolution compared to their 2D counterparts. Moreover, there are\nmany well-known scenarios that existing 3D imaging solutions frequently fail.\nHere, we introduce an extended monocular 3D imaging (EM3D) framework that fully\nexploits the vectorial wave nature of light. Via the multi-stage fusion of\ndiffraction- and polarization-based depth cues, using a compact monocular\ncamera equipped with a diffractive-refractive hybrid lens, we experimentally\ndemonstrate the snapshot acquisition of a million-pixel and accurate 3D point\ncloud for extended scenes that are traditionally challenging, including those\nwith low texture, being highly reflective, or nearly transparent, without a\ndata prior. Furthermore, we discover that the combination of depth and\npolarization information can unlock unique new opportunities in material\nidentification, which may further expand machine intelligence for applications\nlike target recognition and face anti-spoofing. The straightforward yet\npowerful architecture thus opens up a new path for a higher-dimensional machine\nvision in a minimal form factor, facilitating the deployment of monocular\ncameras for applications in much more diverse scenarios.\n","authors":["Zicheng Shen","Feng Zhao","Yibo Ni","Yuanmu Yang"],"pdf_url":"https://arxiv.org/pdf/2502.07403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03631v2","updated":"2025-02-11T09:26:43Z","published":"2025-01-07T09:00:36Z","title":"Exploring Iterative Manifold Constraint for Zero-shot Image Editing","summary":"  Editability and fidelity are two essential demands for text-driven image\nediting, which expects that the editing area should align with the target\nprompt and the rest remain unchanged separately. The current cutting-edge\nediting methods usually obey an \"inversion-then-editing\" pipeline, where the\ninput image is inverted to an approximate Gaussian noise ${z}_T$, based on\nwhich a sampling process is conducted using the target prompt. Nevertheless, we\nargue that it is not a good choice to use a near-Gaussian noise as a pivot for\nfurther editing since it would bring plentiful fidelity errors. We verify this\nby a pilot analysis, discovering that intermediate-inverted latents can achieve\na better trade-off between editability and fidelity than the fully-inverted\n${z}_T$. Based on this, we propose a novel zero-shot editing paradigm dubbed\nZZEdit, which first locates a qualified intermediate-inverted latent marked as\n${z}_p$ as a better editing pivot, which is sufficient-for-editing while\nstructure-preserving. Then, a ZigZag process is designed to execute denoising\nand inversion alternately, which progressively inject target guidance to\n${z}_p$ while preserving the structure information of $p$ step. Afterwards, to\nachieve the same step number of inversion and denoising, we execute a pure\nsampling process under the target prompt. Essentially, our ZZEdit performs\niterative manifold constraint between the manifold of $M_{p}$ and $M_{p-1}$,\nleading to fewer fidelity errors. Extensive experiments highlight the\neffectiveness of ZZEdit in diverse image editing scenarios compared with the\n\"inversion-then-editing\" pipeline.\n","authors":["Maomao Li","Yu Li","Yunfei Liu","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2501.03631v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2410.03782v2","updated":"2025-02-11T09:21:41Z","published":"2024-10-03T16:25:35Z","title":"DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation","summary":"  Adapting a pre-trained foundation model on downstream tasks should ensure\nrobustness against distribution shifts without the need to retrain the whole\nmodel. Although existing weight interpolation methods are simple yet effective,\nwe argue their static nature limits downstream performance while achieving\nefficiency. In this work, we propose DaWin, a training-free dynamic weight\ninterpolation method that leverages the entropy of individual models over each\nunlabeled test sample to assess model expertise, and compute per-sample\ninterpolation coefficients dynamically. Unlike previous works that typically\nrely on additional training to learn such coefficients, our approach requires\nno training. Then, we propose a mixture modeling approach that greatly reduces\ninference overhead raised by dynamic interpolation. We validate DaWin on the\nlarge-scale visual recognition benchmarks, spanning 14 tasks across robust\nfine-tuning -- ImageNet and derived five distribution shift benchmarks -- and\nmulti-task learning with eight classification tasks. Results demonstrate that\nDaWin achieves significant performance gain in considered settings, with\nminimal computational overhead. We further discuss DaWin's analytic behavior to\nexplain its empirical success.\n","authors":["Changdae Oh","Yixuan Li","Kyungwoo Song","Sangdoo Yun","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2410.03782v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07389v1","updated":"2025-02-11T09:19:39Z","published":"2025-02-11T09:19:39Z","title":"FADE: Forecasting for Anomaly Detection on ECG","summary":"  Cardiovascular diseases, a leading cause of noncommunicable disease-related\ndeaths, require early and accurate detection to improve patient outcomes.\nTaking advantage of advances in machine learning and deep learning, multiple\napproaches have been proposed in the literature to address the challenge of\ndetecting ECG anomalies. Typically, these methods are based on the manual\ninterpretation of ECG signals, which is time consuming and depends on the\nexpertise of healthcare professionals. The objective of this work is to propose\na deep learning system, FADE, designed for normal ECG forecasting and anomaly\ndetection, which reduces the need for extensive labeled datasets and manual\ninterpretation. FADE has been trained in a self-supervised manner with a novel\nmorphological inspired loss function. Unlike conventional models that learn\nfrom labeled anomalous ECG waveforms, our approach predicts the future of\nnormal ECG signals, thus avoiding the need for extensive labeled datasets.\nUsing a novel distance function to compare forecasted ECG signals with actual\nsensor data, our method effectively identifies cardiac anomalies. Additionally,\nthis approach can be adapted to new contexts through domain adaptation\ntechniques. To evaluate our proposal, we performed a set of experiments using\ntwo publicly available datasets: MIT-BIH NSR and MIT-BIH Arrythmia. The results\ndemonstrate that our system achieves an average accuracy of 83.84% in anomaly\ndetection, while correctly classifying normal ECG signals with an accuracy of\n85.46%. Our proposed approach exhibited superior performance in the early\ndetection of cardiac anomalies in ECG signals, surpassing previous methods that\npredominantly identify a limited range of anomalies. FADE effectively detects\nboth abnormal heartbeats and arrhythmias, offering significant advantages in\nhealthcare through cost reduction or processing of large-scale ECG data.\n","authors":["Paula Ruiz-Barroso","Francisco M. Castro","José Miranda","Denisa-Andreea Constantinescu","David Atienza","Nicolás Guil"],"pdf_url":"https://arxiv.org/pdf/2502.07389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18987v2","updated":"2025-02-11T09:03:09Z","published":"2024-10-09T17:19:22Z","title":"Point Cloud Synthesis Using Inner Product Transforms","summary":"  Point-cloud synthesis, i.e. the generation of novel point clouds from an\ninput distribution, remains a challenging task, for which numerous complex\nmachine-learning models have been devised. We develop a novel method that\nencodes geometrical-topological characteristics of point clouds using inner\nproducts, leading to a highly-efficient point cloud representation with\nprovable expressivity properties. Integrated into deep learning models, our\nencoding exhibits high quality in typical tasks like reconstruction,\ngeneration, and interpolation, with inference times orders of magnitude faster\nthan existing methods.\n","authors":["Ernst Röell","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2410.18987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07381v1","updated":"2025-02-11T08:57:45Z","published":"2025-02-11T08:57:45Z","title":"Spatial Degradation-Aware and Temporal Consistent Diffusion Model for\n  Compressed Video Super-Resolution","summary":"  Due to limitations of storage and bandwidth, videos stored and transmitted on\nthe Internet are usually low-quality with low-resolution and compression noise.\nAlthough video super-resolution (VSR) is an efficient technique to enhance\nvideo resolution, relatively VSR methods focus on compressed videos. Directly\napplying general VSR approaches leads to the failure of improving practical\nvideos, especially when frames are highly compressed at a low bit rate.\nRecently, diffusion models have achieved superior performance in low-level\nvisual tasks, and their high-realism generation capability enables them to be\napplied in VSR. To synthesize more compression-lost details and refine temporal\nconsistency, we propose a novel Spatial Degradation-Aware and Temporal\nConsistent (SDATC) diffusion model for compressed VSR. Specifically, we\nintroduce a distortion Control module (DCM) to modulate diffusion model inputs\nand guide the generation. Next, the diffusion model executes the denoising\nprocess for texture generation with fine-tuned spatial prompt-based\ncompression-aware module (PCAM) and spatio-temporal attention module (STAM).\nPCAM extracts features to encode specific compression information dynamically.\nSTAM extends the spatial attention mechanism to a spatio-temporal dimension for\ncapturing temporal correlation. Extensive experimental results on benchmark\ndatasets demonstrate the effectiveness of the proposed modules in enhancing\ncompressed videos.\n","authors":["Hongyu An","Xinfeng Zhang","Shijie Zhao","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07372v1","updated":"2025-02-11T08:47:58Z","published":"2025-02-11T08:47:58Z","title":"USRNet: Unified Scene Recovery Network for Enhancing Traffic Imaging\n  under Multiple Adverse Weather Conditions","summary":"  Advancements in computer vision technology have facilitated the extensive\ndeployment of intelligent transportation systems and visual surveillance\nsystems across various applications, including autonomous driving, public\nsafety, and environmental monitoring. However, adverse weather conditions such\nas haze, rain, snow, and more complex mixed degradation can significantly\ndegrade image quality. The degradation compromises the accuracy and reliability\nof these systems across various scenarios. To tackle the challenge of\ndeveloping adaptable models for scene restoration, we introduce the unified\nscene recovery network (USRNet), capable of handling multiple types of image\ndegradation. The USRNet features a sophisticated architecture consisting of a\nscene encoder, an attention-driven node independent learning mechanism (NILM),\nan edge decoder, and a scene restoration module. The scene encoder, powered by\nadvanced residual blocks, extracts deep features from degraded images in a\nprogressive manner, ensuring thorough encoding of degradation information. To\nenhance the USRNet's adaptability in diverse weather conditions, we introduce\nNILM, which enables the network to learn and respond to different scenarios\nwith precision, thereby increasing its robustness. The edge decoder is designed\nto extract edge features with precision, which is essential for maintaining\nimage sharpness. Experimental results demonstrate that USRNet surpasses\nexisting methods in handling complex imaging degradations, thereby improving\nthe accuracy and reliability of visual systems across diverse scenarios. The\ncode resources for this work can be accessed in\nhttps://github.com/LouisYxLu/USRNet.\n","authors":["Yuxu Lu","Ai Chen","Dong Yang","Ryan Wen Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09225v2","updated":"2025-02-11T08:38:29Z","published":"2023-09-27T23:31:30Z","title":"Autonomous Driving using Spiking Neural Networks on Dynamic Vision\n  Sensor Data: A Case Study of Traffic Light Change Detection","summary":"  Autonomous driving is a challenging task that has gained broad attention from\nboth academia and industry. Current solutions using convolutional neural\nnetworks require large amounts of computational resources, leading to high\npower consumption. Spiking neural networks (SNNs) provide an alternative\ncomputational model to process information and make decisions. This\nbiologically plausible model has the advantage of low latency and energy\nefficiency. Recent work using SNNs for autonomous driving mostly focused on\nsimple tasks like lane keeping in simplified simulation environments. This\npaper studies SNNs on photo-realistic driving scenes in the CARLA simulator,\nwhich is an important step toward using SNNs on real vehicles. The efficacy and\ngeneralizability of the method will be investigated.\n","authors":["Xuelei Chen","Sotirios Spanogianopoulos"],"pdf_url":"https://arxiv.org/pdf/2311.09225v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05749v2","updated":"2025-02-11T08:33:03Z","published":"2025-02-09T02:43:57Z","title":"UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal\n  Control","summary":"  Recent advances in diffusion bridge models leverage Doob's $h$-transform to\nestablish fixed endpoints between distributions, demonstrating promising\nresults in image translation and restoration tasks. However, these approaches\nfrequently produce blurred or excessively smoothed image details and lack a\ncomprehensive theoretical foundation to explain these shortcomings. To address\nthese limitations, we propose UniDB, a unified framework for diffusion bridges\nbased on Stochastic Optimal Control (SOC). UniDB formulates the problem through\nan SOC-based optimization and derives a closed-form solution for the optimal\ncontroller, thereby unifying and generalizing existing diffusion bridge models.\nWe demonstrate that existing diffusion bridges employing Doob's $h$-transform\nconstitute a special case of our framework, emerging when the terminal penalty\ncoefficient in the SOC cost function tends to infinity. By incorporating a\ntunable terminal penalty coefficient, UniDB achieves an optimal balance between\ncontrol costs and terminal penalties, substantially improving detail\npreservation and output quality. Notably, UniDB seamlessly integrates with\nexisting diffusion bridge models, requiring only minimal code modifications.\nExtensive experiments across diverse image restoration tasks validate the\nsuperiority and adaptability of the proposed framework. Our code is available\nat https://github.com/UniDB-SOC/UniDB/.\n","authors":["Kaizhen Zhu","Mokai Pan","Yuexin Ma","Yanwei Fu","Jingyi Yu","Jingya Wang","Ye Shi"],"pdf_url":"https://arxiv.org/pdf/2502.05749v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07360v1","updated":"2025-02-11T08:30:25Z","published":"2025-02-11T08:30:25Z","title":"Supervised contrastive learning for cell stage classification of animal\n  embryos","summary":"  Video microscopy, when combined with machine learning, offers a promising\napproach for studying the early development of in vitro produced (IVP) embryos.\nHowever, manually annotating developmental events, and more specifically cell\ndivisions, is time-consuming for a biologist and cannot scale up for practical\napplications. We aim to automatically classify the cell stages of embryos from\n2D time-lapse microscopy videos with a deep learning approach. We focus on the\nanalysis of bovine embryonic development using video microscopy, as we are\nprimarily interested in the application of cattle breeding, and we have created\na Bovine Embryos Cell Stages (ECS) dataset. The challenges are three-fold: (1)\nlow-quality images and bovine dark cells that make the identification of cell\nstages difficult, (2) class ambiguity at the boundaries of developmental\nstages, and (3) imbalanced data distribution. To address these challenges, we\nintroduce CLEmbryo, a novel method that leverages supervised contrastive\nlearning combined with focal loss for training, and the lightweight 3D neural\nnetwork CSN-50 as an encoder. We also show that our method generalizes well.\nCLEmbryo outperforms state-of-the-art methods on both our Bovine ECS dataset\nand the publicly available NYU Mouse Embryos dataset.\n","authors":["Yasmine Hachani","Patrick Bouthemy","Elisa Fromont","Sylvie Ruffini","Ludivine Laffont","Alline de Paula Reis"],"pdf_url":"https://arxiv.org/pdf/2502.07360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07351v1","updated":"2025-02-11T08:22:21Z","published":"2025-02-11T08:22:21Z","title":"Multi-Task-oriented Nighttime Haze Imaging Enhancer for Vision-driven\n  Measurement Systems","summary":"  Salient object detection (SOD) plays a critical role in vision-driven\nmeasurement systems (VMS), facilitating the detection and segmentation of key\nvisual elements in an image. However, adverse imaging conditions such as haze\nduring the day, low light, and haze at night severely degrade image quality,\nand complicating the SOD process. To address these challenges, we propose a\nmulti-task-oriented nighttime haze imaging enhancer (MToIE), which integrates\nthree tasks: daytime dehazing, low-light enhancement, and nighttime dehazing.\nThe MToIE incorporates two key innovative components: First, the network\nemploys a task-oriented node learning mechanism to handle three specific\ndegradation types: day-time haze, low light, and night-time haze conditions,\nwith an embedded self-attention module enhancing its performance in nighttime\nimaging. In addition, multi-receptive field enhancement module that efficiently\nextracts multi-scale features through three parallel depthwise separable\nconvolution branches with different dilation rates, capturing comprehensive\nspatial information with minimal computational overhead. To ensure optimal\nimage reconstruction quality and visual characteristics, we suggest a hybrid\nloss function. Extensive experiments on different types of weather/imaging\nconditions illustrate that MToIE surpasses existing methods, significantly\nenhancing the accuracy and reliability of vision systems across diverse imaging\nscenarios. The code is available at https://github.com/Ai-Chen-Lab/MToIE.\n","authors":["Ai Chen","Yuxu Lu","Dong Yang","Junlin Zhou","Yan Fu","Duanbing Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02929v2","updated":"2025-02-11T08:09:50Z","published":"2024-06-05T04:37:06Z","title":"ZeroDiff: Solidified Visual-Semantic Correlation in Zero-Shot Learning","summary":"  Zero-shot Learning (ZSL) aims to enable classifiers to identify unseen\nclasses. This is typically achieved by generating visual features for unseen\nclasses based on learned visual-semantic correlations from seen classes.\nHowever, most current generative approaches heavily rely on having a sufficient\nnumber of samples from seen classes. Our study reveals that a scarcity of seen\nclass samples results in a marked decrease in performance across many\ngenerative ZSL techniques. We argue, quantify, and empirically demonstrate that\nthis decline is largely attributable to spurious visual-semantic correlations.\nTo address this issue, we introduce ZeroDiff, an innovative generative\nframework for ZSL that incorporates diffusion mechanisms and contrastive\nrepresentations to enhance visual-semantic correlations. ZeroDiff comprises\nthree key components: (1) Diffusion augmentation, which naturally transforms\nlimited data into an expanded set of noised data to mitigate generative model\noverfitting; (2) Supervised-contrastive (SC)-based representations that\ndynamically characterize each limited sample to support visual feature\ngeneration; and (3) Multiple feature discriminators employing a\nWasserstein-distance-based mutual learning approach, evaluating generated\nfeatures from various perspectives, including pre-defined semantics, SC-based\nrepresentations, and the diffusion process. Extensive experiments on three\npopular ZSL benchmarks demonstrate that ZeroDiff not only achieves significant\nimprovements over existing ZSL methods but also maintains robust performance\neven with scarce training data. Our codes are available at\nhttps://github.com/FouriYe/ZeroDiff_ICLR25.\n","authors":["Zihan Ye","Shreyank N. Gowda","Xiaowei Huang","Haotian Xu","Yaochu Jin","Kaizhu Huang","Xiaobo Jin"],"pdf_url":"https://arxiv.org/pdf/2406.02929v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07331v1","updated":"2025-02-11T07:49:31Z","published":"2025-02-11T07:49:31Z","title":"ERANet: Edge Replacement Augmentation for Semi-Supervised Meniscus\n  Segmentation with Prototype Consistency Alignment and Conditional\n  Self-Training","summary":"  Manual segmentation is labor-intensive, and automatic segmentation remains\nchallenging due to the inherent variability in meniscal morphology, partial\nvolume effects, and low contrast between the meniscus and surrounding tissues.\nTo address these challenges, we propose ERANet, an innovative semi-supervised\nframework for meniscus segmentation that effectively leverages both labeled and\nunlabeled images through advanced augmentation and learning strategies. ERANet\nintegrates three key components: edge replacement augmentation (ERA), prototype\nconsistency alignment (PCA), and a conditional self-training (CST) strategy\nwithin a mean teacher architecture. ERA introduces anatomically relevant\nperturbations by simulating meniscal variations, ensuring that augmentations\nalign with the structural context. PCA enhances segmentation performance by\naligning intra-class features and promoting compact, discriminative feature\nrepresentations, particularly in scenarios with limited labeled data. CST\nimproves segmentation robustness by iteratively refining pseudo-labels and\nmitigating the impact of label noise during training. Together, these\ninnovations establish ERANet as a robust and scalable solution for meniscus\nsegmentation, effectively addressing key barriers to practical implementation.\nWe validated ERANet comprehensively on 3D Double Echo Steady State (DESS) and\n3D Fast/Turbo Spin Echo (FSE/TSE) MRI sequences. The results demonstrate the\nsuperior performance of ERANet compared to state-of-the-art methods. The\nproposed framework achieves reliable and accurate segmentation of meniscus\nstructures, even when trained on minimal labeled data. Extensive ablation\nstudies further highlight the synergistic contributions of ERA, PCA, and CST,\nsolidifying ERANet as a transformative solution for semi-supervised meniscus\nsegmentation in medical imaging.\n","authors":["Siyue Li","Yongcheng Yao","Junru Zhong","Shutian Zhao","Yudong Zhang","Shuihua Wang","Jin Hong","Weitian Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07327v1","updated":"2025-02-11T07:43:47Z","published":"2025-02-11T07:43:47Z","title":"Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated\n  Videos","summary":"  With the rapid development of AI-generated content (AIGC), the creation of\nhigh-quality AI-generated videos has become faster and easier, resulting in the\nInternet being flooded with all kinds of video content. However, the impact of\nthese videos on the content ecosystem remains largely unexplored. Video\ninformation retrieval remains a fundamental approach for accessing video\ncontent. Building on the observation that retrieval models often favor\nAI-generated content in ad-hoc and image retrieval tasks, we investigate\nwhether similar biases emerge in the context of challenging video retrieval,\nwhere temporal and visual factors may further influence model behavior. To\nexplore this, we first construct a comprehensive benchmark dataset containing\nboth real and AI-generated videos, along with a set of fair and rigorous\nmetrics to assess bias. This benchmark consists of 13,000 videos generated by\ntwo state-of-the-art open-source video generation models. We meticulously\ndesign a suite of rigorous metrics to accurately measure this preference,\naccounting for potential biases arising from the limited frame rate and\nsuboptimal quality of AIGC videos. We then applied three off-the-shelf video\nretrieval models to perform retrieval tasks on this hybrid dataset. Our\nfindings reveal a clear preference for AI-generated videos in retrieval.\nFurther investigation shows that incorporating AI-generated videos into the\ntraining set of retrieval models exacerbates this bias. Unlike the preference\nobserved in image modalities, we find that video retrieval bias arises from\nboth unseen visual and temporal information, making the root causes of video\nbias a complex interplay of these two factors. To mitigate this bias, we\nfine-tune the retrieval models using a contrastive learning approach. The\nresults of this study highlight the potential implications of AI-generated\nvideos on retrieval systems.\n","authors":["Haowen Gao","Liang Pang","Shicheng Xu","Leigang Qu","Tat-Seng Chua","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07323v1","updated":"2025-02-11T07:42:44Z","published":"2025-02-11T07:42:44Z","title":"Semantic to Structure: Learning Structural Representations for\n  Infringement Detection","summary":"  Structural information in images is crucial for aesthetic assessment, and it\nis widely recognized in the artistic field that imitating the structure of\nother works significantly infringes on creators' rights. The advancement of\ndiffusion models has led to AI-generated content imitating artists' structural\ncreations, yet effective detection methods are still lacking. In this paper, we\ndefine this phenomenon as \"structural infringement\" and propose a corresponding\ndetection method. Additionally, we develop quantitative metrics and create\nmanually annotated datasets for evaluation: the SIA dataset of synthesized\ndata, and the SIR dataset of real data. Due to the current lack of datasets for\nstructural infringement detection, we propose a new data synthesis strategy\nbased on diffusion models and LLM, successfully training a structural\ninfringement detection model. Experimental results show that our method can\nsuccessfully detect structural infringements and achieve notable improvements\non annotated test sets.\n","authors":["Chuanwei Huang","Zexi Jia","Hongyan Fei","Yeshuang Zhu","Zhiqiang Yuan","Jinchao Zhang","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11394v3","updated":"2025-02-11T07:34:37Z","published":"2024-07-16T05:26:14Z","title":"DreamCatalyst: Fast and High-Quality 3D Editing via Controlling\n  Editability and Identity Preservation","summary":"  Score distillation sampling (SDS) has emerged as an effective framework in\ntext-driven 3D editing tasks, leveraging diffusion models for 3D-consistent\nediting. However, existing SDS-based 3D editing methods suffer from long\ntraining times and produce low-quality results. We identify that the root cause\nof this performance degradation is \\textit{their conflict with the sampling\ndynamics of diffusion models}. Addressing this conflict allows us to treat SDS\nas a diffusion reverse process for 3D editing via sampling from data space. In\ncontrast, existing methods naively distill the score function using diffusion\nmodels. From these insights, we propose DreamCatalyst, a novel framework that\nconsiders these sampling dynamics in the SDS framework. Specifically, we devise\nthe optimization process of our DreamCatalyst to approximate the diffusion\nreverse process in editing tasks, thereby aligning with diffusion sampling\ndynamics. As a result, DreamCatalyst successfully reduces training time and\nimproves editing quality. Our method offers two modes: (1) a fast mode that\nedits Neural Radiance Fields (NeRF) scenes approximately 23 times faster than\ncurrent state-of-the-art NeRF editing methods, and (2) a high-quality mode that\nproduces superior results about 8 times faster than these methods. Notably, our\nhigh-quality mode outperforms current state-of-the-art NeRF editing methods in\nterms of both speed and quality. DreamCatalyst also surpasses the\nstate-of-the-art 3D Gaussian Splatting (3DGS) editing methods, establishing\nitself as an effective and model-agnostic 3D editing solution. See more\nextensive results on our project page: https://dream-catalyst.github.io.\n","authors":["Jiwook Kim","Seonho Lee","Jaeyo Shin","Jiho Choi","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2407.11394v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2406.18538v2","updated":"2025-02-11T07:31:10Z","published":"2024-05-17T06:11:10Z","title":"VideoQA-SC: Adaptive Semantic Communication for Video Question Answering","summary":"  Although semantic communication (SC) has shown its potential in efficiently\ntransmitting multimodal data such as texts, speeches and images, SC for videos\nhas focused primarily on pixel-level reconstruction. However, these SC systems\nmay be suboptimal for downstream intelligent tasks. Moreover, SC systems\nwithout pixel-level video reconstruction present advantages by achieving higher\nbandwidth efficiency and real-time performance of various intelligent tasks.\nThe difficulty in such system design lies in the extraction of task-related\ncompact semantic representations and their accurate delivery over noisy\nchannels. In this paper, we propose an end-to-end SC system, named VideoQA-SC\nfor video question answering (VideoQA) tasks. Our goal is to accomplish VideoQA\ntasks directly based on video semantics over noisy or fading wireless channels,\nbypassing the need for video reconstruction at the receiver. To this end, we\ndevelop a spatiotemporal semantic encoder for effective video semantic\nextraction, and a learning-based bandwidth-adaptive deep joint source-channel\ncoding (DJSCC) scheme for efficient and robust video semantic transmission.\nExperiments demonstrate that VideoQA-SC outperforms traditional and advanced\nDJSCC-based SC systems that rely on video reconstruction at the receiver under\na wide range of channel conditions and bandwidth constraints. In particular,\nwhen the signal-to-noise ratio is low, VideoQA-SC can improve the answer\naccuracy by 5.17% while saving almost 99.5\\% of the bandwidth at the same time,\ncompared with the advanced DJSCC-based SC system. Our results show the great\npotential of SC system design for video applications.\n","authors":["Jiangyuan Guo","Wei Chen","Yuxuan Sun","Jialong Xu","Bo Ai"],"pdf_url":"https://arxiv.org/pdf/2406.18538v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20759v3","updated":"2025-02-11T07:27:41Z","published":"2024-05-31T12:20:02Z","title":"Information Theoretic Text-to-Image Alignment","summary":"  Diffusion models for Text-to-Image (T2I) conditional generation have recently\nachieved tremendous success. Yet, aligning these models with user's intentions\nstill involves a laborious trial-and-error process, and this challenging\nalignment problem has attracted considerable attention from the research\ncommunity. In this work, instead of relying on fine-grained linguistic analyses\nof prompts, human annotation, or auxiliary vision-language models, we use\nMutual Information (MI) to guide model alignment. In brief, our method uses\nself-supervised fine-tuning and relies on a point-wise (MI) estimation between\nprompts and images to create a synthetic fine-tuning set for improving model\nalignment. Our analysis indicates that our method is superior to the\nstate-of-the-art, yet it only requires the pre-trained denoising network of the\nT2I model itself to estimate MI, and a simple fine-tuning strategy that\nimproves alignment while maintaining image quality. Code available at\nhttps://github.com/Chao0511/mitune.\n","authors":["Chao Wang","Giulio Franzese","Alessandro Finamore","Massimo Gallo","Pietro Michiardi"],"pdf_url":"https://arxiv.org/pdf/2405.20759v3.pdf","comment":"to appear at ICLR25"},{"id":"http://arxiv.org/abs/2403.10348v3","updated":"2025-02-11T07:25:00Z","published":"2024-03-15T14:34:34Z","title":"Denoising Task Difficulty-based Curriculum for Training Diffusion Models","summary":"  Diffusion-based generative models have emerged as powerful tools in the realm\nof generative modeling. Despite extensive research on denoising across various\ntimesteps and noise levels, a conflict persists regarding the relative\ndifficulties of the denoising tasks. While various studies argue that lower\ntimesteps present more challenging tasks, others contend that higher timesteps\nare more difficult. To address this conflict, our study undertakes a\ncomprehensive examination of task difficulties, focusing on convergence\nbehavior and changes in relative entropy between consecutive probability\ndistributions across timesteps. Our observational study reveals that denoising\nat earlier timesteps poses challenges characterized by slower convergence and\nhigher relative entropy, indicating increased task difficulty at these lower\ntimesteps. Building on these observations, we introduce an easy-to-hard\nlearning scheme, drawing from curriculum learning, to enhance the training\nprocess of diffusion models. By organizing timesteps or noise levels into\nclusters and training models with ascending orders of difficulty, we facilitate\nan order-aware training regime, progressing from easier to harder denoising\ntasks, thereby deviating from the conventional approach of training diffusion\nmodels simultaneously across all timesteps. Our approach leads to improved\nperformance and faster convergence by leveraging benefits of curriculum\nlearning, while maintaining orthogonality with existing improvements in\ndiffusion training techniques. We validate these advantages through\ncomprehensive experiments in image generation tasks, including unconditional,\nclass-conditional, and text-to-image generation.\n","authors":["Jin-Young Kim","Hyojun Go","Soonwoo Kwon","Hyun-Gyoon Kim"],"pdf_url":"https://arxiv.org/pdf/2403.10348v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05343v2","updated":"2025-02-11T07:17:37Z","published":"2024-10-07T07:19:50Z","title":"EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos\n  Referring to Procedural Texts","summary":"  Mistake action detection is crucial for developing intelligent archives that\ndetect workers' errors and provide feedback. Existing studies have focused on\nvisually apparent mistakes in free-style activities, resulting in video-only\napproaches to mistake detection. However, in text-following activities, models\ncannot determine the correctness of some actions without referring to the\ntexts. Additionally, current mistake datasets rarely use procedural texts for\nvideo recording except for cooking. To fill these gaps, this paper proposes the\nEgoOops dataset, where egocentric videos record erroneous activities when\nfollowing procedural texts across diverse domains. It features three types of\nannotations: video-text alignment, mistake labels, and descriptions for\nmistakes. We also propose a mistake detection approach, combining video-text\nalignment and mistake label classification to leverage the texts. Our\nexperimental results show that incorporating procedural texts is essential for\nmistake detection. Data is available through\nhttps://y-haneji.github.io/EgoOops-project-page/.\n","authors":["Yuto Haneji","Taichi Nishimura","Hirotaka Kameko","Keisuke Shirai","Tomoya Yoshida","Keiya Kajimura","Koki Yamamoto","Taiyu Cui","Tomohiro Nishimoto","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2410.05343v2.pdf","comment":"Main 6 pages, supplementary 13 pages"},{"id":"http://arxiv.org/abs/2502.07309v1","updated":"2025-02-11T07:12:26Z","published":"2025-02-11T07:12:26Z","title":"Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous\n  Driving","summary":"  Understanding world dynamics is crucial for planning in autonomous driving.\nRecent methods attempt to achieve this by learning a 3D occupancy world model\nthat forecasts future surrounding scenes based on current observation. However,\n3D occupancy labels are still required to produce promising results.\nConsidering the high annotation cost for 3D outdoor scenes, we propose a\nsemi-supervised vision-centric 3D occupancy world model, PreWorld, to leverage\nthe potential of 2D labels through a novel two-stage training paradigm: the\nself-supervised pre-training stage and the fully-supervised fine-tuning stage.\nSpecifically, during the pre-training stage, we utilize an attribute projection\nhead to generate different attribute fields of a scene (e.g., RGB, density,\nsemantic), thus enabling temporal supervision from 2D labels via volume\nrendering techniques. Furthermore, we introduce a simple yet effective\nstate-conditioned forecasting module to recursively forecast future occupancy\nand ego trajectory in a direct manner. Extensive experiments on the nuScenes\ndataset validate the effectiveness and scalability of our method, and\ndemonstrate that PreWorld achieves competitive performance across 3D occupancy\nprediction, 4D occupancy forecasting and motion planning tasks.\n","authors":["Xiang Li","Pengfei Li","Yupeng Zheng","Wei Sun","Yan Wang","Yilun Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07309v1.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07306v1","updated":"2025-02-11T07:09:37Z","published":"2025-02-11T07:09:37Z","title":"TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language\n  Navigation","summary":"  In this work, we propose a modular approach for the Vision-Language\nNavigation (VLN) task by decomposing the problem into four sub-modules that use\nstate-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs)\nin a zero-shot setting. Given navigation instruction in natural language, we\nfirst prompt LLM to extract the landmarks and the order in which they are\nvisited. Assuming the known model of the environment, we retrieve the top-k\nlocations of the last landmark and generate $k$ path hypotheses from the\nstarting location to the last landmark using the shortest path algorithm on the\ntopological map of the environment. Each path hypothesis is represented by a\nsequence of panoramas. We then use dynamic programming to compute the alignment\nscore between the sequence of panoramas and the sequence of landmark names,\nwhich match scores obtained from VLM. Finally, we compute the nDTW metric\nbetween the hypothesis that yields the highest alignment score to evaluate the\npath fidelity. We demonstrate superior performance compared to other approaches\nthat use joint semantic maps like VLMaps \\cite{vlmaps} on the complex\nR2R-Habitat \\cite{r2r} instruction dataset and quantify in detail the effect of\nvisual grounding on navigation performance.\n","authors":["Navid Rajabi","Jana Kosecka"],"pdf_url":"https://arxiv.org/pdf/2502.07306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10857v2","updated":"2025-02-11T07:01:26Z","published":"2024-12-14T15:11:42Z","title":"Robust Persian Digit Recognition in Noisy Environments Using Hybrid\n  CNN-BiGRU Model","summary":"  Artificial intelligence (AI) has significantly advanced speech recognition\napplications. However, many existing neural network-based methods struggle with\nnoise, reducing accuracy in real-world environments. This study addresses\nisolated spoken Persian digit recognition (zero to nine) under noisy\nconditions, particularly for phonetically similar numbers. A hybrid model\ncombining residual convolutional neural networks and bidirectional gated\nrecurrent units (BiGRU) is proposed, utilizing word units instead of phoneme\nunits for speaker-independent recognition. The FARSDIGIT1 dataset, augmented\nwith various approaches, is processed using Mel-Frequency Cepstral Coefficients\n(MFCC) for feature extraction. Experimental results demonstrate the model's\neffectiveness, achieving 98.53%, 96.10%, and 95.92% accuracy on training,\nvalidation, and test sets, respectively. In noisy conditions, the proposed\napproach improves recognition by 26.88% over phoneme unit-based LSTM models and\nsurpasses the Mel-scale Two Dimension Root Cepstrum Coefficients (MTDRCC)\nfeature extraction technique along with MLP model (MTDRCC+MLP) by 7.61%.\n","authors":["Ali Nasr-Esfahani","Mehdi Bekrani","Roozbeh Rajabi"],"pdf_url":"https://arxiv.org/pdf/2412.10857v2.pdf","comment":"6 pages, two columns, submitted to Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2502.07302v1","updated":"2025-02-11T06:58:50Z","published":"2025-02-11T06:58:50Z","title":"CASC-AI: Consensus-aware Self-corrective AI Agents for Noise Cell\n  Segmentation","summary":"  Multi-class cell segmentation in high-resolution gigapixel whole slide images\n(WSI) is crucial for various clinical applications. However, training such\nmodels typically requires labor-intensive, pixel-wise annotations by domain\nexperts. Recent efforts have democratized this process by involving lay\nannotators without medical expertise. However, conventional non-agent-based\napproaches struggle to handle annotation noise adaptively, as they lack\nmechanisms to mitigate false positives (FP) and false negatives (FN) at both\nthe image-feature and pixel levels. In this paper, we propose a consensus-aware\nself-corrective AI agent that leverages the Consensus Matrix to guide its\nlearning process. The Consensus Matrix defines regions where both the AI and\nannotators agree on cell and non-cell annotations, which are prioritized with\nstronger supervision. Conversely, areas of disagreement are adaptively weighted\nbased on their feature similarity to high-confidence agreement regions, with\nmore similar regions receiving greater attention. Additionally, contrastive\nlearning is employed to separate features of noisy regions from those of\nreliable agreement regions by maximizing their dissimilarity. This paradigm\nenables the AI to iteratively refine noisy labels, enhancing its robustness.\nValidated on one real-world lay-annotated cell dataset and two simulated noisy\ndatasets, our method demonstrates improved segmentation performance,\neffectively correcting FP and FN errors and showcasing its potential for\ntraining robust models on noisy datasets. The official implementation and cell\nannotations are publicly available at https://github.com/ddrrnn123/CASC-AI.\n","authors":["Ruining Deng","Yihe Yang","David J. Pisapia","Benjamin Liechty","Junchao Zhu","Juming Xiong","Junlin Guo","Zhengyi Lu","Jiacheng Wang","Xing Yao","Runxuan Yu","Rendong Zhang","Gaurav Rudravaram","Mengmeng Yin","Pinaki Sarder","Haichun Yang","Yuankai Huo","Mert R. Sabuncu"],"pdf_url":"https://arxiv.org/pdf/2502.07302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07289v1","updated":"2025-02-11T06:21:42Z","published":"2025-02-11T06:21:42Z","title":"Learning Inverse Laplacian Pyramid for Progressive Depth Completion","summary":"  Depth completion endeavors to reconstruct a dense depth map from sparse depth\nmeasurements, leveraging the information provided by a corresponding color\nimage. Existing approaches mostly hinge on single-scale propagation strategies\nthat iteratively ameliorate initial coarse depth estimates through pixel-level\nmessage passing. Despite their commendable outcomes, these techniques are\nfrequently hampered by computational inefficiencies and a limited grasp of\nscene context. To circumvent these challenges, we introduce LP-Net, an\ninnovative framework that implements a multi-scale, progressive prediction\nparadigm based on Laplacian Pyramid decomposition. Diverging from\npropagation-based approaches, LP-Net initiates with a rudimentary,\nlow-resolution depth prediction to encapsulate the global scene context,\nsubsequently refining this through successive upsampling and the reinstatement\nof high-frequency details at incremental scales. We have developed two novel\nmodules to bolster this strategy: 1) the Multi-path Feature Pyramid module,\nwhich segregates feature maps into discrete pathways, employing multi-scale\ntransformations to amalgamate comprehensive spatial information, and 2) the\nSelective Depth Filtering module, which dynamically learns to apply both\nsmoothness and sharpness filters to judiciously mitigate noise while\naccentuating intricate details. By integrating these advancements, LP-Net not\nonly secures state-of-the-art (SOTA) performance across both outdoor and indoor\nbenchmarks such as KITTI, NYUv2, and TOFDC, but also demonstrates superior\ncomputational efficiency. At the time of submission, LP-Net ranks 1st among all\npeer-reviewed methods on the official KITTI leaderboard.\n","authors":["Kun Wang","Zhiqiang Yan","Junkai Fan","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2502.07289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07288v1","updated":"2025-02-11T06:20:28Z","published":"2025-02-11T06:20:28Z","title":"KPIs 2024 Challenge: Advancing Glomerular Segmentation from Patch- to\n  Slide-Level","summary":"  Chronic kidney disease (CKD) is a major global health issue, affecting over\n10% of the population and causing significant mortality. While kidney biopsy\nremains the gold standard for CKD diagnosis and treatment, the lack of\ncomprehensive benchmarks for kidney pathology segmentation hinders progress in\nthe field. To address this, we organized the Kidney Pathology Image\nSegmentation (KPIs) Challenge, introducing a dataset that incorporates\npreclinical rodent models of CKD with over 10,000 annotated glomeruli from 60+\nPeriodic Acid Schiff (PAS)-stained whole slide images. The challenge includes\ntwo tasks, patch-level segmentation and whole slide image segmentation and\ndetection, evaluated using the Dice Similarity Coefficient (DSC) and F1-score.\nBy encouraging innovative segmentation methods that adapt to diverse CKD models\nand tissue conditions, the KPIs Challenge aims to advance kidney pathology\nanalysis, establish new benchmarks, and enable precise, large-scale\nquantification for disease research and diagnosis.\n","authors":["Ruining Deng","Tianyuan Yao","Yucheng Tang","Junlin Guo","Siqi Lu","Juming Xiong","Lining Yu","Quan Huu Cap","Pengzhou Cai","Libin Lan","Ze Zhao","Adrian Galdran","Amit Kumar","Gunjan Deotale","Dev Kumar Das","Inyoung Paik","Joonho Lee","Geongyu Lee","Yujia Chen","Wangkai Li","Zhaoyang Li","Xuege Hou","Zeyuan Wu","Shengjin Wang","Maximilian Fischer","Lars Kramer","Anghong Du","Le Zhang","Maria Sanchez Sanchez","Helena Sanchez Ulloa","David Ribalta Heredia","Carlos Perez de Arenaza Garcia","Shuoyu Xu","Bingdou He","Xinping Cheng","Tao Wang","Noemie Moreau","Katarzyna Bozek","Shubham Innani","Ujjwal Baid","Kaura Solomon Kefas","Bennett A. Landman","Yu Wang","Shilin Zhao","Mengmeng Yin","Haichun Yang","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2502.07288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07278v1","updated":"2025-02-11T05:47:16Z","published":"2025-02-11T05:47:16Z","title":"Articulate That Object Part (ATOP): 3D Part Articulation from Text and\n  Motion Personalization","summary":"  We present ATOP (Articulate That Object Part), a novel method based on motion\npersonalization to articulate a 3D object with respect to a part and its motion\nas prescribed in a text prompt. Specifically, the text input allows us to tap\ninto the power of modern-day video diffusion to generate plausible motion\nsamples for the right object category and part. In turn, the input 3D object\nprovides image prompting to personalize the generated video to that very object\nwe wish to articulate. Our method starts with a few-shot finetuning for\ncategory-specific motion generation, a key first step to compensate for the\nlack of articulation awareness by current video diffusion models. For this, we\nfinetune a pre-trained multi-view image generation model for controllable\nmulti-view video generation, using a small collection of video samples obtained\nfor the target object category. This is followed by motion video\npersonalization that is realized by multi-view rendered images of the target 3D\nobject. At last, we transfer the personalized video motion to the target 3D\nobject via differentiable rendering to optimize part motion parameters by a\nscore distillation sampling loss. We show that our method is capable of\ngenerating realistic motion videos and predict 3D motion parameters in a more\naccurate and generalizable way, compared to prior works.\n","authors":["Aditya Vora","Sauradip Nag","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07278v1.pdf","comment":"Technical Report, 16 pages"},{"id":"http://arxiv.org/abs/2502.05979v2","updated":"2025-02-11T05:45:45Z","published":"2025-02-09T18:12:25Z","title":"VFX Creator: Animated Visual Effect Generation with Controllable\n  Diffusion Transformer","summary":"  Crafting magic and illusions is one of the most thrilling aspects of\nfilmmaking, with visual effects (VFX) serving as the powerhouse behind\nunforgettable cinematic experiences. While recent advances in generative\nartificial intelligence have driven progress in generic image and video\nsynthesis, the domain of controllable VFX generation remains relatively\nunderexplored. In this work, we propose a novel paradigm for animated VFX\ngeneration as image animation, where dynamic effects are generated from\nuser-friendly textual descriptions and static reference images. Our work makes\ntwo primary contributions: (i) Open-VFX, the first high-quality VFX video\ndataset spanning 15 diverse effect categories, annotated with textual\ndescriptions, instance segmentation masks for spatial conditioning, and\nstart-end timestamps for temporal control. (ii) VFX Creator, a simple yet\neffective controllable VFX generation framework based on a Video Diffusion\nTransformer. The model incorporates a spatial and temporal controllable LoRA\nadapter, requiring minimal training videos. Specifically, a plug-and-play mask\ncontrol module enables instance-level spatial manipulation, while tokenized\nstart-end motion timestamps embedded in the diffusion process, alongside the\ntext encoder, allow precise temporal control over effect timing and pace.\nExtensive experiments on the Open-VFX test set demonstrate the superiority of\nthe proposed system in generating realistic and dynamic effects, achieving\nstate-of-the-art performance and generalization ability in both spatial and\ntemporal controllability. Furthermore, we introduce a specialized metric to\nevaluate the precision of temporal control. By bridging traditional VFX\ntechniques with generative approaches, VFX Creator unlocks new possibilities\nfor efficient and high-quality video effect generation, making advanced VFX\naccessible to a broader audience.\n","authors":["Xinyu Liu","Ailing Zeng","Wei Xue","Harry Yang","Wenhan Luo","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2502.05979v2.pdf","comment":"Project page: https://vfx-creator0.github.io/"},{"id":"http://arxiv.org/abs/2502.07277v1","updated":"2025-02-11T05:44:50Z","published":"2025-02-11T05:44:50Z","title":"Enhancing Video Understanding: Deep Neural Networks for Spatiotemporal\n  Analysis","summary":"  It's no secret that video has become the primary way we share information\nonline. That's why there's been a surge in demand for algorithms that can\nanalyze and understand video content. It's a trend going to continue as video\ncontinues to dominate the digital landscape. These algorithms will extract and\nclassify related features from the video and will use them to describe the\nevents and objects in the video. Deep neural networks have displayed\nencouraging outcomes in the realm of feature extraction and video description.\nThis paper will explore the spatiotemporal features found in videos and recent\nadvancements in deep neural networks in video understanding. We will review\nsome of the main trends in video understanding models and their structural\ndesign, the main problems, and some offered solutions in this topic. We will\nalso review and compare significant video understanding and action recognition\ndatasets.\n","authors":["Amir Hosein Fadaei","Mohammad-Reza A. Dehaqani"],"pdf_url":"https://arxiv.org/pdf/2502.07277v1.pdf","comment":"29 pages, 25 figures"},{"id":"http://arxiv.org/abs/2502.07276v1","updated":"2025-02-11T05:42:21Z","published":"2025-02-11T05:42:21Z","title":"Dataset Ownership Verification in Contrastive Pre-trained Models","summary":"  High-quality open-source datasets, which necessitate substantial efforts for\ncuration, has become the primary catalyst for the swift progress of deep\nlearning. Concurrently, protecting these datasets is paramount for the\nwell-being of the data owner. Dataset ownership verification emerges as a\ncrucial method in this domain, but existing approaches are often limited to\nsupervised models and cannot be directly extended to increasingly popular\nunsupervised pre-trained models. In this work, we propose the first dataset\nownership verification method tailored specifically for self-supervised\npre-trained models by contrastive learning. Its primary objective is to\nascertain whether a suspicious black-box backbone has been pre-trained on a\nspecific unlabeled dataset, aiding dataset owners in upholding their rights.\nThe proposed approach is motivated by our empirical insights that when models\nare trained with the target dataset, the unary and binary instance\nrelationships within the embedding space exhibit significant variations\ncompared to models trained without the target dataset. We validate the efficacy\nof this approach across multiple contrastive pre-trained models including\nSimCLR, BYOL, SimSiam, MOCO v3, and DINO. The results demonstrate that our\nmethod rejects the null hypothesis with a $p$-value markedly below $0.05$,\nsurpassing all previous methodologies. Our code is available at\nhttps://github.com/xieyc99/DOV4CL.\n","authors":["Yuechen Xie","Jie Song","Mengqi Xue","Haofei Zhang","Xingen Wang","Bingde Hu","Genlang Chen","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2502.07276v1.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.07269v1","updated":"2025-02-11T05:35:36Z","published":"2025-02-11T05:35:36Z","title":"Exploring Active Data Selection Strategies for Continuous Training in\n  Deepfake Detection","summary":"  In deepfake detection, it is essential to maintain high performance by\nadjusting the parameters of the detector as new deepfake methods emerge. In\nthis paper, we propose a method to automatically and actively select the small\namount of additional data required for the continuous training of deepfake\ndetection models in situations where deepfake detection models are regularly\nupdated. The proposed method automatically selects new training data from a\n\\textit{redundant} pool set containing a large number of images generated by\nnew deepfake methods and real images, using the confidence score of the\ndeepfake detection model as a metric. Experimental results show that the\ndeepfake detection model, continuously trained with a small amount of\nadditional data automatically selected and added to the original training set,\nsignificantly and efficiently improved the detection performance, achieving an\nEER of 2.5% with only 15% of the amount of data in the pool set.\n","authors":["Yoshihiko Furuhashi","Junichi Yamagishi","Xin Wang","Huy H. Nguyen","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2502.07269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06619v2","updated":"2025-02-11T05:29:42Z","published":"2025-02-10T16:16:34Z","title":"Unleashing the Potential of Pre-Trained Diffusion Models for\n  Generalizable Person Re-Identification","summary":"  Domain-generalizable re-identification (DG Re-ID) aims to train a model on\none or more source domains and evaluate its performance on unseen target\ndomains, a task that has attracted growing attention due to its practical\nrelevance. While numerous methods have been proposed, most rely on\ndiscriminative or contrastive learning frameworks to learn generalizable\nfeature representations. However, these approaches often fail to mitigate\nshortcut learning, leading to suboptimal performance. In this work, we propose\na novel method called diffusion model-assisted representation learning with a\ncorrelation-aware conditioning scheme (DCAC) to enhance DG Re-ID. Our method\nintegrates a discriminative and contrastive Re-ID model with a pre-trained\ndiffusion model through a correlation-aware conditioning scheme. By\nincorporating ID classification probabilities generated from the Re-ID model\nwith a set of learnable ID-wise prompts, the conditioning scheme injects dark\nknowledge that captures ID correlations to guide the diffusion process.\nSimultaneously, feedback from the diffusion model is back-propagated through\nthe conditioning scheme to the Re-ID model, effectively improving the\ngeneralization capability of Re-ID features. Extensive experiments on both\nsingle-source and multi-source DG Re-ID tasks demonstrate that our method\nachieves state-of-the-art performance. Comprehensive ablation studies further\nvalidate the effectiveness of the proposed approach, providing insights into\nits robustness. Codes will be available at https://github.com/RikoLi/DCAC.\n","authors":["Jiachen Li","Xiaojin Gong"],"pdf_url":"https://arxiv.org/pdf/2502.06619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07259v1","updated":"2025-02-11T04:57:33Z","published":"2025-02-11T04:57:33Z","title":"Flat U-Net: An Efficient Ultralightweight Model for Solar Filament\n  Segmentation in Full-disk H$α$ Images","summary":"  Solar filaments are one of the most prominent features observed on the Sun,\nand their evolutions are closely related to various solar activities, such as\nflares and coronal mass ejections. Real-time automated identification of solar\nfilaments is the most effective approach to managing large volumes of data.\nExisting models of filament identification are characterized by large parameter\nsizes and high computational costs, which limit their future applications in\nhighly integrated and intelligent ground-based and space-borne observation\ndevices. Consequently, the design of more lightweight models will facilitate\nthe advancement of intelligent observation equipment. In this study, we\nintroduce Flat U-Net, a novel and highly efficient ultralightweight model that\nincorporates simplified channel attention (SCA) and channel self-attention\n(CSA) convolutional blocks for the segmentation of solar filaments in full-disk\nH$\\alpha$ images. Feature information from each network layer is fully\nextracted to reconstruct interchannel feature representations. Each block\neffectively optimizes the channel features from the previous layer,\nsignificantly reducing parameters. The network architecture presents an elegant\nflattening, improving its efficiency, and simplifying the overall design.\nExperimental validation demonstrates that a model composed of pure SCAs\nachieves a precision of approximately 0.93, with dice similarity coefficient\n(DSC) and recall rates of 0.76 and 0.64, respectively, significantly\noutperforming the classical U-Net. Introducing a certain number of CSA blocks\nimproves the DSC and recall rates to 0.82 and 0.74, respectively, which\ndemonstrates a pronounced advantage, particularly concerning model weight size\nand detection effectiveness. The data set, models, and code are available as\nopen-source resources.\n","authors":["GaoFei Zhu","GangHua Lin","Xiao Yang","Cheng Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.07259v1.pdf","comment":"15 pages, 5 figures, 3 tables, accepted for publication in ApJ"},{"id":"http://arxiv.org/abs/2502.06390v2","updated":"2025-02-11T04:42:24Z","published":"2025-02-10T12:20:08Z","title":"When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks\n  for VLMs","summary":"  Vision-Language Models (VLMs) have gained considerable prominence in recent\nyears due to their remarkable capability to effectively integrate and process\nboth textual and visual information. This integration has significantly\nenhanced performance across a diverse spectrum of applications, such as scene\nperception and robotics. However, the deployment of VLMs has also given rise to\ncritical safety and security concerns, necessitating extensive research to\nassess the potential vulnerabilities these VLM systems may harbor. In this\nwork, we present an in-depth survey of the attack strategies tailored for VLMs.\nWe categorize these attacks based on their underlying objectives - namely\njailbreak, camouflage, and exploitation - while also detailing the various\nmethodologies employed for data manipulation of VLMs. Meanwhile, we outline\ncorresponding defense mechanisms that have been proposed to mitigate these\nvulnerabilities. By discerning key connections and distinctions among the\ndiverse types of attacks, we propose a compelling taxonomy for VLM attacks.\nMoreover, we summarize the evaluation metrics that comprehensively describe the\ncharacteristics and impact of different attacks on VLMs. Finally, we conclude\nwith a discussion of promising future research directions that could further\nenhance the robustness and safety of VLMs, emphasizing the importance of\nongoing exploration in this critical area of study. To facilitate community\nengagement, we maintain an up-to-date project page, accessible at:\nhttps://github.com/AobtDai/VLM_Attack_Paper_List.\n","authors":["Aobotao Dai","Xinyu Ma","Lei Chen","Songze Li","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07246v1","updated":"2025-02-11T04:29:22Z","published":"2025-02-11T04:29:22Z","title":"Robust Indoor Localization in Dynamic Environments: A Multi-source\n  Unsupervised Domain Adaptation Framework","summary":"  Fingerprint localization has gained significant attention due to its\ncost-effective deployment, low complexity, and high efficacy. However,\ntraditional methods, while effective for static data, often struggle in dynamic\nenvironments where data distributions and feature spaces evolve-a common\noccurrence in real-world scenarios. To address the challenges of robustness and\nadaptability in fingerprint localization for dynamic indoor environments, this\npaper proposes DF-Loc, an end-to-end dynamic fingerprint localization system\nbased on multi-source unsupervised domain adaptation (MUDA). DF-Loc leverages\nhistorical data from multiple time scales to facilitate knowledge transfer in\nspecific feature spaces, thereby enhancing generalization capabilities in the\ntarget domain and reducing reliance on labeled data. Specifically, the system\nincorporates a Quality Control (QC) module for CSI data preprocessing and\nemploys image processing techniques for CSI fingerprint feature reconstruction.\nAdditionally, a multi-scale attention-based feature fusion backbone network is\ndesigned to extract multi-level transferable fingerprint features. Finally, a\ndual-stage alignment model aligns the distributions of multiple source-target\ndomain pairs, improving regression characteristics in the target domain.\nExtensive experiments conducted in office and classroom environments\ndemonstrate that DF-Loc outperforms comparative methods in terms of both\nlocalization accuracy and robustness. With 60% of reference points used for\ntraining, DF-Loc achieves average localization errors of 0.79m and 3.72m in\n\"same-test\" scenarios, and 0.94m and 4.39m in \"different-test\" scenarios,\nrespectively. This work pioneers an end-to-end multi-source transfer learning\napproach for fingerprint localization, providing valuable insights for future\nresearch in dynamic environments.\n","authors":["Jiyu Jiao","Xiaojun Wang","Chengpei Han"],"pdf_url":"https://arxiv.org/pdf/2502.07246v1.pdf","comment":"19 pages, 21 figures"},{"id":"http://arxiv.org/abs/2502.07239v1","updated":"2025-02-11T04:09:12Z","published":"2025-02-11T04:09:12Z","title":"Contextual Gesture: Co-Speech Gesture Video Generation through\n  Context-aware Gesture Representation","summary":"  Co-speech gesture generation is crucial for creating lifelike avatars and\nenhancing human-computer interactions by synchronizing gestures with speech.\nDespite recent advancements, existing methods struggle with accurately\nidentifying the rhythmic or semantic triggers from audio for generating\ncontextualized gesture patterns and achieving pixel-level realism. To address\nthese challenges, we introduce Contextual Gesture, a framework that improves\nco-speech gesture video generation through three innovative components: (1) a\nchronological speech-gesture alignment that temporally connects two modalities,\n(2) a contextualized gesture tokenization that incorporate speech context into\nmotion pattern representation through distillation, and (3) a structure-aware\nrefinement module that employs edge connection to link gesture keypoints to\nimprove video generation. Our extensive experiments demonstrate that Contextual\nGesture not only produces realistic and speech-aligned gesture videos but also\nsupports long-sequence generation and video gesture editing applications, shown\nin Fig.1 Project Page: https://andypinxinliu.github.io/Contextual-Gesture/.\n","authors":["Pinxin Liu","Pengfei Zhang","Hyeongwoo Kim","Pablo Garrido","Ari Sharpio","Kyle Olszewski"],"pdf_url":"https://arxiv.org/pdf/2502.07239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07238v1","updated":"2025-02-11T04:09:11Z","published":"2025-02-11T04:09:11Z","title":"Diffusion Suction Grasping with Large-Scale Parcel Dataset","summary":"  While recent advances in object suction grasping have shown remarkable\nprogress, significant challenges persist particularly in cluttered and complex\nparcel handling scenarios. Two fundamental limitations hinder current\napproaches: (1) the lack of a comprehensive suction grasp dataset tailored for\nparcel manipulation tasks, and (2) insufficient adaptability to diverse object\ncharacteristics including size variations, geometric complexity, and textural\ndiversity. To address these challenges, we present Parcel-Suction-Dataset, a\nlarge-scale synthetic dataset containing 25 thousand cluttered scenes with 410\nmillion precision-annotated suction grasp poses. This dataset is generated\nthrough our novel geometric sampling algorithm that enables efficient\ngeneration of optimal suction grasps incorporating both physical constraints\nand material properties. We further propose Diffusion-Suction, an innovative\nframework that reformulates suction grasp prediction as a conditional\ngeneration task through denoising diffusion probabilistic models. Our method\niteratively refines random noise into suction grasp score maps through\nvisual-conditioned guidance from point cloud observations, effectively learning\nspatial point-wise affordances from our synthetic dataset. Extensive\nexperiments demonstrate that the simple yet efficient Diffusion-Suction\nachieves new state-of-the-art performance compared to previous models on both\nParcel-Suction-Dataset and the public SuctionNet-1Billion benchmark.\n","authors":["Ding-Tao Huang","Xinyi He","Debei Hua","Dongfang Yu","En-Te Lin","Long Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.07238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01943v2","updated":"2025-02-11T03:55:29Z","published":"2025-02-04T02:30:36Z","title":"DAMA: Data- and Model-aware Alignment of Multi-modal LLMs","summary":"  Direct Preference Optimization (DPO) has shown effectiveness in aligning\nmulti-modal large language models (MLLM) with human preferences. However,\nexisting methods exhibit an imbalanced responsiveness to the data of varying\nhardness, tending to overfit on the easy-to-distinguish data while underfitting\non the hard-to-distinguish data. In this paper, we propose Data- and\nModel-aware DPO (DAMA) to dynamically adjust the optimization process from two\nkey aspects: (1) a data-aware strategy that incorporates data hardness, and (2)\na model-aware strategy that integrates real-time model responses. By combining\nthe two strategies, DAMA enables the model to effectively adapt to data with\nvarying levels of hardness. Extensive experiments on five benchmarks\ndemonstrate that DAMA not only significantly enhances the trustworthiness, but\nalso improves the effectiveness over general tasks. For instance, on the\nObject-HalBench, our DAMA-7B reduces response-level and mentioned-level\nhallucination by 90.0% and 95.3%, respectively, surpassing the performance of\nGPT-4V.\n","authors":["Jinda Lu","Junkang Wu","Jinghan Li","Xiaojun Jia","Shuo Wang","YiFan Zhang","Junfeng Fang","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2502.01943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05485v2","updated":"2025-02-11T03:49:30Z","published":"2025-02-08T07:50:22Z","title":"HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation","summary":"  Large foundation models have shown strong open-world generalization to\ncomplex problems in vision and language, but similar levels of generalization\nhave yet to be achieved in robotics. One fundamental challenge is the lack of\nrobotic data, which are typically obtained through expensive on-robot\noperation. A promising remedy is to leverage cheaper, off-domain data such as\naction-free videos, hand-drawn sketches or simulation data. In this work, we\nposit that hierarchical vision-language-action (VLA) models can be more\neffective in utilizing off-domain data than standard monolithic VLA models that\ndirectly finetune vision-language models (VLMs) to predict actions. In\nparticular, we study a class of hierarchical VLA models, where the high-level\nVLM is finetuned to produce a coarse 2D path indicating the desired robot\nend-effector trajectory given an RGB image and a task description. The\nintermediate 2D path prediction is then served as guidance to the low-level,\n3D-aware control policy capable of precise manipulation. Doing so alleviates\nthe high-level VLM from fine-grained action prediction, while reducing the\nlow-level policy's burden on complex task-level reasoning. We show that, with\nthe hierarchical design, the high-level VLM can transfer across significant\ndomain gaps between the off-domain finetuning data and real-robot testing\nscenarios, including differences on embodiments, dynamics, visual appearances\nand task semantics, etc. In the real-robot experiments, we observe an average\nof 20% improvement in success rate across seven different axes of\ngeneralization over OpenVLA, representing a 50% relative gain. Visual results\nare provided at: https://hamster-robot.github.io/\n","authors":["Yi Li","Yuquan Deng","Jesse Zhang","Joel Jang","Marius Memme","Raymond Yu","Caelan Reed Garrett","Fabio Ramos","Dieter Fox","Anqi Li","Abhishek Gupta","Ankit Goyal"],"pdf_url":"https://arxiv.org/pdf/2502.05485v2.pdf","comment":"We require NVIDIA's approval before proceeding with the release, and\n  we are currently processing it"},{"id":"http://arxiv.org/abs/2502.07225v1","updated":"2025-02-11T03:35:35Z","published":"2025-02-11T03:35:35Z","title":"CAT: Contrastive Adversarial Training for Evaluating the Robustness of\n  Protective Perturbations in Latent Diffusion Models","summary":"  Latent diffusion models have recently demonstrated superior capabilities in\nmany downstream image synthesis tasks. However, customization of latent\ndiffusion models using unauthorized data can severely compromise the privacy\nand intellectual property rights of data owners. Adversarial examples as\nprotective perturbations have been developed to defend against unauthorized\ndata usage by introducing imperceptible noise to customization samples,\npreventing diffusion models from effectively learning them. In this paper, we\nfirst reveal that the primary reason adversarial examples are effective as\nprotective perturbations in latent diffusion models is the distortion of their\nlatent representations, as demonstrated through qualitative and quantitative\nexperiments. We then propose the Contrastive Adversarial Training (CAT)\nutilizing adapters as an adaptive attack against these protection methods,\nhighlighting their lack of robustness. Extensive experiments demonstrate that\nour CAT method significantly reduces the effectiveness of protective\nperturbations in customization configurations, urging the community to\nreconsider and enhance the robustness of existing protective perturbation\nmethods. Code is available at \\hyperlink{here}{https://github.com/senp98/CAT}.\n","authors":["Sen Peng","Mingyue Wang","Jianfei He","Jijia Yang","Xiaohua Jia"],"pdf_url":"https://arxiv.org/pdf/2502.07225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05769v2","updated":"2025-02-11T03:32:10Z","published":"2025-02-09T04:06:07Z","title":"Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual\n  Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps\n  Platform","summary":"  Urban digital twins are virtual replicas of cities that use multi-source data\nand data analytics to optimize urban planning, infrastructure management, and\ndecision-making. Towards this, we propose a framework focused on the\nsingle-building scale. By connecting to cloud mapping platforms such as Google\nMap Platforms APIs, by leveraging state-of-the-art multi-agent Large Language\nModels data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our\nGaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings\nframework can retrieve a building's 3D model, visual descriptions, and achieve\ncloud-based mapping integration with large language model-based data analytics\nusing a building's address, postal code, or geographic coordinates.\n","authors":["Kyle Gao","Dening Lu","Liangzhi Li","Nan Chen","Hongjie He","Linlin Xu","Jonathan Li"],"pdf_url":"https://arxiv.org/pdf/2502.05769v2.pdf","comment":"-Fixed minor typo"},{"id":"http://arxiv.org/abs/2502.07221v1","updated":"2025-02-11T03:28:55Z","published":"2025-02-11T03:28:55Z","title":"MLLM4PUE: Toward Universal Embeddings in Computational Pathology through\n  Multimodal LLMs","summary":"  Pathology plays a critical role in diagnosing a wide range of diseases, yet\nexisting approaches often rely heavily on task-specific models trained on\nextensive, well-labeled datasets. These methods face sustainability challenges\ndue to the diversity of pathologies and the labor-intensive nature of data\ncollection. To address these limitations, we highlight the need for universal\nmultimodal embeddings that can support multiple downstream tasks. Previous\napproaches often involve fine-tuning CLIP-based models, which handle images and\ntext separately, limiting their ability to capture complex multimodal\nrelationships. Additionally, these models are evaluated across diverse datasets\nwithout a unified benchmark for assessing multimodal embeddings in pathology.\nTo address these challenges, we propose MLLM4PUE, a novel framework that\nleverages Multimodal Large Language Models (MLLMs) to generate Pathology\nUniversal Embeddings. The MLLM4PUE framework not only facilitates robust\nintegration of images and text but also enhances understanding and fusion\ncapabilities across various tasks. We further introduce the Pathology\nMultimodal Embedding Benchmark (PMEB), a comprehensive benchmark designed to\nassess the quality of pathology multimodal embeddings. PMEB comprises 15\noriginal tasks drawn from 14 datasets, organized into three meta-tasks:\nretrieval, classification, and composed retrieval. Experimental results\ndemonstrate the superiority of MLLM4PUE, illustrating MLLM-based models can\neffectively support a wide range of downstream tasks and unify the research\ndirection for foundation models in pathology.\n","authors":["Qifeng Zhou","Thao M. Dang","Wenliang Zhong","Yuzhi Guo","Hehuan Ma","Saiyang Na","Junzhou Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07216v1","updated":"2025-02-11T03:21:25Z","published":"2025-02-11T03:21:25Z","title":"SparseFormer: Detecting Objects in HRW Shots via Sparse Vision\n  Transformer","summary":"  Recent years have seen an increase in the use of gigapixel-level image and\nvideo capture systems and benchmarks with high-resolution wide (HRW) shots.\nHowever, unlike close-up shots in the MS COCO dataset, the higher resolution\nand wider field of view raise unique challenges, such as extreme sparsity and\nhuge scale changes, causing existing close-up detectors inaccuracy and\ninefficiency. In this paper, we present a novel model-agnostic sparse vision\ntransformer, dubbed SparseFormer, to bridge the gap of object detection between\nclose-up and HRW shots. The proposed SparseFormer selectively uses attentive\ntokens to scrutinize the sparsely distributed windows that may contain objects.\nIn this way, it can jointly explore global and local attention by fusing\ncoarse- and fine-grained features to handle huge scale changes. SparseFormer\nalso benefits from a novel Cross-slice non-maximum suppression (C-NMS)\nalgorithm to precisely localize objects from noisy windows and a simple yet\neffective multi-scale strategy to improve accuracy. Extensive experiments on\ntwo HRW benchmarks, PANDA and DOTA-v1.0, demonstrate that the proposed\nSparseFormer significantly improves detection accuracy (up to 5.8%) and speed\n(up to 3x) over the state-of-the-art approaches.\n","authors":["Wenxi Li","Yuchen Guo","Jilai Zheng","Haozhe Lin","Chao Ma","Lu Fang","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2502.07216v1.pdf","comment":"This paper is accepted to ACM MM 2024"},{"id":"http://arxiv.org/abs/2502.07215v1","updated":"2025-02-11T03:20:21Z","published":"2025-02-11T03:20:21Z","title":"PDV: Prompt Directional Vectors for Zero-shot Composed Image Retrieval","summary":"  Zero-shot composed image retrieval (ZS-CIR) enables image search using a\nreference image and text prompt without requiring specialized text-image\ncomposition networks trained on large-scale paired data. However, current\nZS-CIR approaches face three critical limitations in their reliance on composed\ntext embeddings: static query embedding representations, insufficient\nutilization of image embeddings, and suboptimal performance when fusing text\nand image embeddings. To address these challenges, we introduce the Prompt\nDirectional Vector (PDV), a simple yet effective training-free enhancement that\ncaptures semantic modifications induced by user prompts. PDV enables three key\nimprovements: (1) dynamic composed text embeddings where prompt adjustments are\ncontrollable via a scaling factor, (2) composed image embeddings through\nsemantic transfer from text prompts to image features, and (3) weighted fusion\nof composed text and image embeddings that enhances retrieval by balancing\nvisual and semantic similarity. Our approach serves as a plug-and-play\nenhancement for existing ZS-CIR methods with minimal computational overhead.\nExtensive experiments across multiple benchmarks demonstrate that PDV\nconsistently improves retrieval performance when integrated with\nstate-of-the-art ZS-CIR approaches, particularly for methods that generate\naccurate compositional embeddings. The code will be publicly available.\n","authors":["Osman Tursun","Sinan Kalkan","Simon Denman","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2502.07215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16769v4","updated":"2025-02-11T02:58:54Z","published":"2025-01-28T07:49:52Z","title":"Beyond-Labels: Advancing Open-Vocabulary Segmentation With\n  Vision-Language Models","summary":"  Self-supervised learning can resolve numerous image or linguistic processing\nproblems when effectively trained. This study investigated simple yet efficient\nmethods for adapting previously learned foundation models for open-vocabulary\nsemantic segmentation tasks. Our research proposed \"Beyond-Labels,\" a\nlightweight transformer-based fusion module that uses a handful of image\nsegmentation data to fuse frozen image representations with language concepts.\nThis strategy allows the model to successfully actualize enormous knowledge\nfrom pretrained models without requiring extensive retraining, making the model\ndata-efficient and scalable. Furthermore, we efficiently captured positional\ninformation in images using Fourier embeddings, thus improving the\ngeneralization across various image sizes, addressing one of the key\nlimitations of previous methods. Extensive ablation tests were performed to\ninvestigate the important components of our proposed method; when tested\nagainst the common benchmark PASCAL-5i, it demonstrated superior performance\ndespite being trained on frozen image and language characteristics.\n","authors":["Muhammad Atta ur Rahman"],"pdf_url":"https://arxiv.org/pdf/2501.16769v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07203v1","updated":"2025-02-11T02:53:48Z","published":"2025-02-11T02:53:48Z","title":"Playmate: Flexible Control of Portrait Animation via 3D-Implicit Space\n  Guided Diffusion","summary":"  Recent diffusion-based talking face generation models have demonstrated\nimpressive potential in synthesizing videos that accurately match a speech\naudio clip with a given reference identity. However, existing approaches still\nencounter significant challenges due to uncontrollable factors, such as\ninaccurate lip-sync, inappropriate head posture and the lack of fine-grained\ncontrol over facial expressions. In order to introduce more face-guided\nconditions beyond speech audio clips, a novel two-stage training framework\nPlaymate is proposed to generate more lifelike facial expressions and talking\nfaces. In the first stage, we introduce a decoupled implicit 3D representation\nalong with a meticulously designed motion-decoupled module to facilitate more\naccurate attribute disentanglement and generate expressive talking videos\ndirectly from audio cues. Then, in the second stage, we introduce an\nemotion-control module to encode emotion control information into the latent\nspace, enabling fine-grained control over emotions and thereby achieving the\nability to generate talking videos with desired emotion. Extensive experiments\ndemonstrate that Playmate outperforms existing state-of-the-art methods in\nterms of video quality and lip-synchronization, and improves flexibility in\ncontrolling emotion and head pose. The code will be available at\nhttps://playmate111.github.io.\n","authors":["Xingpei Ma","Jiaran Cai","Yuansheng Guan","Shenneng Huang","Qiang Zhang","Shunsi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07200v1","updated":"2025-02-11T02:47:37Z","published":"2025-02-11T02:47:37Z","title":"Color-Quality Invariance for Robust Medical Image Segmentation","summary":"  Single-source domain generalization (SDG) in medical image segmentation\nremains a significant challenge, particularly for images with varying color\ndistributions and qualities. Previous approaches often struggle when models\ntrained on high-quality images fail to generalize to low-quality test images\ndue to these color and quality shifts. In this work, we propose two novel\ntechniques to enhance generalization: dynamic color image normalization (DCIN)\nmodule and color-quality generalization (CQG) loss. The DCIN dynamically\nnormalizes the color of test images using two reference image selection\nstrategies. Specifically, the DCIN utilizes a global reference image selection\n(GRIS), which finds a universal reference image, and a local reference image\nselection (LRIS), which selects a semantically similar reference image per test\nsample. Additionally, CQG loss enforces invariance to color and quality\nvariations by ensuring consistent segmentation predictions across transformed\nimage pairs. Experimental results show that our proposals significantly improve\nsegmentation performance over the baseline on two target domain datasets,\ndespite being trained solely on a single source domain. Notably, our model\nachieved up to a 32.3-point increase in Dice score compared to the baseline,\nconsistently producing robust and usable results even under substantial domain\nshifts. Our work contributes to the development of more robust medical image\nsegmentation models that generalize across unseen domains. The implementation\ncode is available at https://github.com/RaviShah1/DCIN-CQG.\n","authors":["Ravi Shah","Atsushi Fukuda","Quan Huu Cap"],"pdf_url":"https://arxiv.org/pdf/2502.07200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07194v1","updated":"2025-02-11T02:36:10Z","published":"2025-02-11T02:36:10Z","title":"Dense Object Detection Based on De-homogenized Queries","summary":"  Dense object detection is widely used in automatic driving, video\nsurveillance, and other fields. This paper focuses on the challenging task of\ndense object detection. Currently, detection methods based on greedy\nalgorithms, such as non-maximum suppression (NMS), often produce many\nrepetitive predictions or missed detections in dense scenarios, which is a\ncommon problem faced by NMS-based algorithms. Through the end-to-end DETR\n(DEtection TRansformer), as a type of detector that can incorporate the\npost-processing de-duplication capability of NMS, etc., into the network, we\nfound that homogeneous queries in the query-based detector lead to a reduction\nin the de-duplication capability of the network and the learning efficiency of\nthe encoder, resulting in duplicate prediction and missed detection problems.\nTo solve this problem, we propose learnable differentiated encoding to\nde-homogenize the queries, and at the same time, queries can communicate with\neach other via differentiated encoding information, replacing the previous\nself-attention among the queries. In addition, we used joint loss on the output\nof the encoder that considered both location and confidence prediction to give\na higher-quality initialization for queries. Without cumbersome decoder\nstacking and guaranteeing accuracy, our proposed end-to-end detection framework\nwas more concise and reduced the number of parameters by about 8% compared to\ndeformable DETR. Our method achieved excellent results on the challenging\nCrowdHuman dataset with 93.6% average precision (AP), 39.2% MR-2, and 84.3% JI.\nThe performance overperformed previous SOTA methods, such as Iter-E2EDet\n(Progressive End-to-End Object Detection) and MIP (One proposal, Multiple\npredictions). In addition, our method is more robust in various scenarios with\ndifferent densities.\n","authors":["Yueming Huang","Chenrui Ma","Hao Zhou","Hao Wu","Guowu Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.07194v1.pdf","comment":"17 pages, 15 figures"},{"id":"http://arxiv.org/abs/2411.04782v2","updated":"2025-02-11T02:35:48Z","published":"2024-11-07T15:22:32Z","title":"A General Pipeline for Glomerulus Whole-Slide Image Segmentation","summary":"  Whole-slide images (WSI) glomerulus segmentation is essential for accurately\ndiagnosing kidney diseases. In this work, we propose a general and practical\npipeline for glomerulus segmentation that effectively enhances both patch-level\nand WSI-level segmentation tasks. Our approach leverages stitching on\noverlapping patches, increasing the detection coverage, especially when\nglomeruli are located near patch image borders. In addition, we conduct\ncomprehensive evaluations from different segmentation models across two large\nand diverse datasets with over 30K glomerulus annotations. Experimental results\ndemonstrate that models using our pipeline outperform the previous\nstate-of-the-art method, achieving superior results across both datasets and\nsetting a new benchmark for glomerulus segmentation in WSIs. The code and\npre-trained models are available at\nhttps://github.com/huuquan1994/wsi_glomerulus_seg.\n","authors":["Quan Huu Cap"],"pdf_url":"https://arxiv.org/pdf/2411.04782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07192v1","updated":"2025-02-11T02:32:32Z","published":"2025-02-11T02:32:32Z","title":"OscNet: Machine Learning on CMOS Oscillator Networks","summary":"  Machine learning and AI have achieved remarkable advancements but at the cost\nof significant computational resources and energy consumption. This has created\nan urgent need for a novel, energy-efficient computational fabric to replace\nthe current computing pipeline. Recently, a promising approach has emerged by\nmimicking spiking neurons in the brain and leveraging oscillators on CMOS for\ndirect computation. In this context, we propose a new and energy efficient\nmachine learning framework implemented on CMOS Oscillator Networks (OscNet). We\nmodel the developmental processes of the prenatal brain's visual system using\nOscNet, updating weights based on the biologically inspired Hebbian rule. This\nsame pipeline is then directly applied to standard machine learning tasks.\nOscNet is a specially designed hardware and is inherently energy-efficient. Its\nreliance on forward propagation alone for training further enhances its energy\nefficiency while maintaining biological plausibility. Simulation validates our\ndesigns of OscNet architectures. Experimental results demonstrate that Hebbian\nlearning pipeline on OscNet achieves performance comparable to or even\nsurpassing traditional machine learning algorithms, highlighting its potential\nas a energy efficient and effective computational paradigm.\n","authors":["Wenxiao Cai","Thomas H. Lee"],"pdf_url":"https://arxiv.org/pdf/2502.07192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11230v2","updated":"2025-02-11T02:17:24Z","published":"2024-06-17T05:54:06Z","title":"Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of\n  Multimodal Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) have shown significant promise in\nvarious applications, leading to broad interest from researchers and\npractitioners alike. However, a comprehensive evaluation of their long-context\ncapabilities remains underexplored. To address these gaps, we introduce the\nMultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to\nassess the long-context capabilities of MLLMs. Besides multi-image input, we\nemploy image stitching to further increase the input context length, and\ndevelop a protocol to automatically generate labels for sub-image level\nretrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their\ncapability to locate a target sub-image (needle) within a set of images\n(haystack) based on textual instructions and descriptions of image contents.\nThis setup necessitates an advanced understanding of extensive visual contexts\nand effective information retrieval within long-context image inputs. With this\nbenchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and\nopen-source models. The findings reveal that GPT-4o consistently surpasses\nother models in long-context scenarios, but suffers from hallucination problems\nin negative samples, i.e., when needles are not in the haystacks. Our\ncomprehensive long-context evaluation of MLLMs also sheds lights on the\nconsiderable performance gap between API-based and open-source models. All the\ncode, data, and instructions required to reproduce the main results are\navailable at https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.\n","authors":["Hengyi Wang","Haizhou Shi","Shiwei Tan","Weiyi Qin","Wenyuan Wang","Tunyu Zhang","Akshay Nambi","Tanuja Ganu","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.11230v2.pdf","comment":"Accepted at NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2411.12028v2","updated":"2025-02-11T02:16:24Z","published":"2024-11-18T20:03:49Z","title":"In-Situ Melt Pool Characterization via Thermal Imaging for Defect\n  Detection in Directed Energy Deposition Using Vision Transformers","summary":"  Directed Energy Deposition (DED) offers significant potential for\nmanufacturing complex and multi-material parts. However, internal defects such\nas porosity and cracks can compromise mechanical properties and overall\nperformance. This study focuses on in-situ monitoring and characterization of\nmelt pools associated with porosity, aiming to improve defect detection and\nquality control in DED-printed parts. Traditional machine learning approaches\nfor defect identification rely on extensive labeled datasets, often scarce and\nexpensive to generate in real-world manufacturing. To address this, our\nframework employs self-supervised learning on unlabeled melt pool data using a\nVision Transformer-based Masked Autoencoder (MAE) to produce highly\nrepresentative embeddings. These fine-tuned embeddings are leveraged via\ntransfer learning to train classifiers on a limited labeled dataset, enabling\nthe effective identification of melt pool anomalies. We evaluate two\nclassifiers: (1) a Vision Transformer (ViT) classifier utilizing the fine-tuned\nMAE Encoder's parameters and (2) the fine-tuned MAE Encoder combined with an\nMLP classifier head. Our framework achieves overall accuracy ranging from\n95.44% to 99.17% and an average F1 score exceeding 80%, with the ViT Classifier\nslightly outperforming the MAE Encoder Classifier. This demonstrates the\nscalability and cost-effectiveness of our approach for automated quality\ncontrol in DED, effectively detecting defects with minimal labeled data.\n","authors":["Israt Zarin Era","Fan Zhou","Ahmed Shoyeb Raihan","Imtiaz Ahmed","Alan Abul-Haj","James Craig","Srinjoy Das","Zhichao Liu"],"pdf_url":"https://arxiv.org/pdf/2411.12028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07183v1","updated":"2025-02-11T02:14:49Z","published":"2025-02-11T02:14:49Z","title":"Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog\n  Robots Assisting the Visually Impaired","summary":"  Guide dog robots offer promising solutions to enhance mobility and safety for\nvisually impaired individuals, addressing the limitations of traditional guide\ndogs, particularly in perceptual intelligence and communication. With the\nemergence of Vision-Language Models (VLMs), robots are now capable of\ngenerating natural language descriptions of their surroundings, aiding in safer\ndecision-making. However, existing VLMs often struggle to accurately interpret\nand convey spatial relationships, which is crucial for navigation in complex\nenvironments such as street crossings. We introduce the Space-Aware Instruction\nTuning (SAIT) dataset and the Space-Aware Benchmark (SA-Bench) to address the\nlimitations of current VLMs in understanding physical environments. Our\nautomated data generation pipeline focuses on the virtual path to the\ndestination in 3D space and the surroundings, enhancing environmental\ncomprehension and enabling VLMs to provide more accurate guidance to visually\nimpaired individuals. We also propose an evaluation protocol to assess VLM\neffectiveness in delivering walking guidance. Comparative experiments\ndemonstrate that our space-aware instruction-tuned model outperforms\nstate-of-the-art algorithms. We have fully open-sourced the SAIT dataset and\nSA-Bench, along with the related code, at\nhttps://github.com/byungokhan/Space-awareVLM\n","authors":["ByungOk Han","Woo-han Yun","Beom-Su Seo","Jaehong Kim"],"pdf_url":"https://arxiv.org/pdf/2502.07183v1.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2502.07181v1","updated":"2025-02-11T02:12:29Z","published":"2025-02-11T02:12:29Z","title":"Tab2Visual: Overcoming Limited Data in Tabular Data Classification Using\n  Deep Learning with Visual Representations","summary":"  This research addresses the challenge of limited data in tabular data\nclassification, particularly prevalent in domains with constraints like\nhealthcare. We propose Tab2Visual, a novel approach that transforms\nheterogeneous tabular data into visual representations, enabling the\napplication of powerful deep learning models. Tab2Visual effectively addresses\ndata scarcity by incorporating novel image augmentation techniques and\nfacilitating transfer learning. We extensively evaluate the proposed approach\non diverse tabular datasets, comparing its performance against a wide range of\nmachine learning algorithms, including classical methods, tree-based ensembles,\nand state-of-the-art deep learning models specifically designed for tabular\ndata. We also perform an in-depth analysis of factors influencing Tab2Visual's\nperformance. Our experimental results demonstrate that Tab2Visual outperforms\nother methods in classification problems with limited tabular data.\n","authors":["Ahmed Mamdouh","Moumen El-Melegy","Samia Ali","Ron Kikinis"],"pdf_url":"https://arxiv.org/pdf/2502.07181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07179v1","updated":"2025-02-11T02:09:30Z","published":"2025-02-11T02:09:30Z","title":"Improved YOLOv7 model for insulator defect detection","summary":"  Insulators are crucial insulation components and structural supports in power\ngrids, playing a vital role in the transmission lines. Due to temperature\nfluctuations, internal stress, or damage from hail, insulators are prone to\ninjury. Automatic detection of damaged insulators faces challenges such as\ndiverse types, small defect targets, and complex backgrounds and shapes. Most\nresearch for detecting insulator defects has focused on a single defect type or\na specific material. However, the insulators in the grid's transmission lines\nhave different colors and materials. Various insulator defects coexist, and the\nexisting methods have difficulty meeting the practical application\nrequirements. Current methods suffer from low detection accuracy and mAP0.5\ncannot meet application requirements. This paper proposes an improved YOLOv7\nmodel for multi-type insulator defect detection. First, our model replaces the\nSPPCSPC module with the RFB module to enhance the network's feature extraction\ncapability. Second, a CA mechanism is introduced into the head part to enhance\nthe network's feature representation ability and to improve detection accuracy.\nThird, a WIoU loss function is employed to address the low-quality samples\nhindering model generalization during training, thereby improving the model's\noverall performance. The experimental results indicate that the proposed model\nexhibits enhancements across various performance metrics. Specifically, there\nis a 1.6% advancement in mAP_0.5, a corresponding 1.6% enhancement in\nmAP_0.5:0.95, a 1.3% elevation in precision, and a 1% increase in recall.\nMoreover, the model achieves parameter reduction by 3.2 million, leading to a\ndecrease of 2.5 GFLOPS in computational cost. Notably, there is also an\nimprovement of 2.81 milliseconds in single-image detection speed.\n","authors":["Zhenyue Wang","Guowu Yuan","Hao Zhou","Yi Ma","Yutang Ma","Dong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07179v1.pdf","comment":"19 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.07175v1","updated":"2025-02-11T01:58:32Z","published":"2025-02-11T01:58:32Z","title":"Foreign-Object Detection in High-Voltage Transmission Line Based on\n  Improved YOLOv8m","summary":"  The safe operation of high-voltage transmission lines ensures the power\ngrid's security. Various foreign objects attached to the transmission lines,\nsuch as balloons, kites and nesting birds, can significantly affect the safe\nand stable operation of high-voltage transmission lines. With the advancement\nof computer vision technology, periodic automatic inspection of foreign objects\nis efficient and necessary. Existing detection methods have low accuracy\nbecause foreign objects at-tached to the transmission lines are complex,\nincluding occlusions, diverse object types, significant scale variations, and\ncomplex backgrounds. In response to the practical needs of the Yunnan Branch of\nChina Southern Power Grid Co., Ltd., this paper proposes an improved\nYOLOv8m-based model for detecting foreign objects on transmission lines.\nExperiments are conducted on a dataset collected from Yunnan Power Grid. The\nproposed model enhances the original YOLOv8m by in-corporating a Global\nAttention Module (GAM) into the backbone to focus on occluded foreign objects,\nreplacing the SPPF module with the SPPCSPC module to augment the model's\nmultiscale feature extraction capability, and introducing the Focal-EIoU loss\nfunction to address the issue of high- and low-quality sample imbalances. These\nimprovements accelerate model convergence and enhance detection accuracy. The\nexperimental results demonstrate that our proposed model achieves a 2.7%\nincrease in mAP_0.5, a 4% increase in mAP_0.5:0.95, and a 6% increase in\nrecall.\n","authors":["Zhenyue Wang","Guowu Yuan","Hao Zhou","Yi Ma","Yutang Ma"],"pdf_url":"https://arxiv.org/pdf/2502.07175v1.pdf","comment":"24 pages, 16 figures"},{"id":"http://arxiv.org/abs/2502.07172v1","updated":"2025-02-11T01:39:11Z","published":"2025-02-11T01:39:11Z","title":"SemiHMER: Semi-supervised Handwritten Mathematical Expression\n  Recognition using pseudo-labels","summary":"  In recent years, deep learning with Convolutional Neural Networks (CNNs) has\nachieved remarkable results in the field of HMER (Handwritten Mathematical\nExpression Recognition). However, it remains challenging to improve performance\nwith limited labeled training data. This paper presents, for the first time, a\nsimple yet effective semi-supervised HMER framework by introducing dual-branch\nsemi-supervised learning. Specifically, we simplify the conventional deep\nco-training from consistency regularization to cross-supervised learning, where\nthe prediction of one branch is used as a pseudo-label to supervise the other\nbranch directly end-to-end. Considering that the learning of the two branches\ntends to converge in the later stages of model optimization, we also\nincorporate a weak-to-strong strategy by applying different levels of\naugmentation to each branch, which behaves like expanding the training data and\nimproving the quality of network training. Meanwhile, We propose a novel\nmodule, Global Dynamic Counting Module(GDCM), to enhance the performance of the\nHMER decoder, which alleviates recognition inaccuracies in long-distance\nformula recognition and the occurrence of repeated characters. We release our\ncode at https://github.com/chenkehua/SemiHMER.\n","authors":["Kehua Chen","Haoyang Shen"],"pdf_url":"https://arxiv.org/pdf/2502.07172v1.pdf","comment":"12 pages,3 figures"},{"id":"http://arxiv.org/abs/2410.19846v2","updated":"2025-02-11T01:07:35Z","published":"2024-10-21T17:00:03Z","title":"YOLO11 and Vision Transformers based 3D Pose Estimation of Immature\n  Green Fruits in Commercial Apple Orchards for Robotic Thinning","summary":"  In this study, a robust method for 3D pose estimation of immature green\napples (fruitlets) in commercial orchards was developed, utilizing the\nYOLO11(or YOLOv11) object detection and pose estimation algorithm alongside\nVision Transformers (ViT) for depth estimation (Dense Prediction Transformer\n(DPT) and Depth Anything V2). For object detection and pose estimation,\nperformance comparisons of YOLO11 (YOLO11n, YOLO11s, YOLO11m, YOLO11l and\nYOLO11x) and YOLOv8 (YOLOv8n, YOLOv8s, YOLOv8m, YOLOv8l and YOLOv8x) were made\nunder identical hyperparameter settings among the all configurations. It was\nobserved that YOLO11n surpassed all configurations of YOLO11 and YOLOv8 in\nterms of box precision and pose precision, achieving scores of 0.91 and 0.915,\nrespectively. Conversely, YOLOv8n exhibited the highest box and pose recall\nscores of 0.905 and 0.925, respectively. Regarding the mean average precision\nat 50\\% intersection over union (mAP@50), YOLO11s led all configurations with a\nbox mAP@50 score of 0.94, while YOLOv8n achieved the highest pose mAP@50 score\nof 0.96. In terms of image processing speed, YOLO11n outperformed all\nconfigurations with an impressive inference speed of 2.7 ms, significantly\nfaster than the quickest YOLOv8 configuration, YOLOv8n, which processed images\nin 7.8 ms. Subsequent integration of ViTs for the green fruit's pose depth\nestimation revealed that Depth Anything V2 outperformed Dense Prediction\nTransformer in 3D pose length validation, achieving the lowest Root Mean Square\nError (RMSE) of 1.52 and Mean Absolute Error (MAE) of 1.28, demonstrating\nexceptional precision in estimating immature green fruit lengths. Integration\nof YOLO11 and Depth Anything Model provides a promising solution to 3D pose\nestimation of immature green fruits for robotic thinning applications. (YOLOv11\npose detection, YOLOv11 Pose, YOLOv11 Keypoints detection, YOLOv11 pose\nestimation)\n","authors":["Ranjan Sapkota","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2410.19846v2.pdf","comment":"24 Pages, 13 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2502.07161v1","updated":"2025-02-11T00:59:30Z","published":"2025-02-11T00:59:30Z","title":"A Survey on Mamba Architecture for Vision Applications","summary":"  Transformers have become foundational for visual tasks such as object\ndetection, semantic segmentation, and video understanding, but their quadratic\ncomplexity in attention mechanisms presents scalability challenges. To address\nthese limitations, the Mamba architecture utilizes state-space models (SSMs)\nfor linear scalability, efficient processing, and improved contextual\nawareness. This paper investigates Mamba architecture for visual domain\napplications and its recent advancements, including Vision Mamba (ViM) and\nVideoMamba, which introduce bidirectional scanning, selective scanning\nmechanisms, and spatiotemporal processing to enhance image and video\nunderstanding. Architectural innovations like position embeddings, cross-scan\nmodules, and hierarchical designs further optimize the Mamba framework for\nglobal and local feature extraction. These advancements position Mamba as a\npromising architecture in computer vision research and applications.\n","authors":["Fady Ibrahim","Guangjun Liu","Guanghui Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07160v1","updated":"2025-02-11T00:56:44Z","published":"2025-02-11T00:56:44Z","title":"HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates","summary":"  Image compression under ultra-low bitrates remains challenging for both\nconventional learned image compression (LIC) and generative vector-quantized\n(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy\nquantization, while generative VQ modeling gives poor fidelity due to the\nmismatch between learned generative priors and specific inputs. In this work,\nwe propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream\nframework that utilizes both generative VQ-modeling and diffusion models, as\nwell as conventional LIC, to achieve both high fidelity and high perceptual\nquality. Different from previous hybrid methods that directly use pre-trained\nLIC models to generate low-quality fidelity-preserving information from heavily\nquantized latent, we use diffusion models to extract high-quality complimentary\nfidelity information from the ground-truth input, which can enhance the system\nperformance in several aspects: improving indices map prediction, enhancing the\nfidelity-preserving output of the LIC stream, and refining conditioned image\nreconstruction with VQ-latent correction. In addition, our diffusion model is\nbased on a dense representative vector (DRV), which is lightweight with very\nsimple sampling schedulers. Extensive experiments demonstrate that our\nHDCompression outperforms the previous conventional LIC, generative\nVQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative\nvisualization, providing balanced robust compression performance at ultra-low\nbitrates.\n","authors":["Lei Lu","Yize Li","Yanzhi Wang","Wei Wang","Wei Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.07160v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2409.18794v2","updated":"2025-02-11T00:55:35Z","published":"2024-09-27T14:47:18Z","title":"Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in\n  Continuous Environment with Open-Source LLMs","summary":"  Vision-and-Language Navigation (VLN) tasks require an agent to follow textual\ninstructions to navigate through 3D environments. Traditional approaches use\nsupervised learning methods, relying heavily on domain-specific datasets to\ntrain VLN models. Recent methods try to utilize closed-source large language\nmodels (LLMs) like GPT-4 to solve VLN tasks in zero-shot manners, but face\nchallenges related to expensive token costs and potential data breaches in\nreal-world applications. In this work, we introduce Open-Nav, a novel study\nthat explores open-source LLMs for zero-shot VLN in the continuous environment.\nOpen-Nav employs a spatial-temporal chain-of-thought (CoT) reasoning approach\nto break down tasks into instruction comprehension, progress estimation, and\ndecision-making. It enhances scene perceptions with fine-grained object and\nspatial knowledge to improve LLM's reasoning in navigation. Our extensive\nexperiments in both simulated and real-world environments demonstrate that\nOpen-Nav achieves competitive performance compared to using closed-source LLMs.\n","authors":["Yanyuan Qiao","Wenqi Lyu","Hui Wang","Zixu Wang","Zerui Li","Yuan Zhang","Mingkui Tan","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2409.18794v2.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2404.03161v2","updated":"2025-02-11T00:45:46Z","published":"2024-04-04T02:22:37Z","title":"BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro\n  QR Codes","summary":"  This paper introduces BioVL-QR, a biochemical vision-and-language dataset\ncomprising 23 egocentric experiment videos, corresponding protocols, and\nvision-and-language alignments. A major challenge in understanding biochemical\nvideos is detecting equipment, reagents, and containers because of the\ncluttered environment and indistinguishable objects. Previous studies assumed\nmanual object annotation, which is costly and time-consuming. To address the\nissue, we focus on Micro QR Codes. However, detecting objects using only Micro\nQR Codes is still difficult due to blur and occlusion caused by object\nmanipulation. To overcome this, we propose an object labeling method combining\na Micro QR Code detector with an off-the-shelf hand object detector. As an\napplication of the method and BioVL-QR, we tackled the task of localizing the\nprocedural steps in an instructional video. The experimental results show that\nusing Micro QR Codes and our method improves biochemical video understanding.\nData and code are available through https://nishi10mo.github.io/BioVL-QR/\n","authors":["Tomohiro Nishimoto","Taichi Nishimura","Koki Yamamoto","Keisuke Shirai","Hirotaka Kameko","Yuto Haneji","Tomoya Yoshida","Keiya Kajimura","Taiyu Cui","Chihiro Nishiwaki","Eriko Daikoku","Natsuko Okuda","Fumihito Ono","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2404.03161v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2502.07156v1","updated":"2025-02-11T00:44:20Z","published":"2025-02-11T00:44:20Z","title":"Explaining 3D Computed Tomography Classifiers with Counterfactuals","summary":"  Counterfactual explanations in medical imaging are critical for understanding\nthe predictions made by deep learning models. We extend the Latent Shift\ncounterfactual generation method from 2D applications to 3D computed tomography\n(CT) scans. We address the challenges associated with 3D data, such as limited\ntraining samples and high memory demands, by implementing a slice-based\napproach. This method leverages a 2D encoder trained on CT slices, which are\nsubsequently combined to maintain 3D context. We demonstrate this technique on\ntwo models for clinical phenotype prediction and lung segmentation. Our\napproach is both memory-efficient and effective for generating interpretable\ncounterfactuals in high-resolution 3D medical imaging.\n","authors":["Joseph Paul Cohen","Louis Blankemeier","Akshay Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2502.07156v1.pdf","comment":"Code and models: https://github.com/ieee8023/ct-counterfactuals"},{"id":"http://arxiv.org/abs/2502.08025v1","updated":"2025-02-11T23:55:16Z","published":"2025-02-11T23:55:16Z","title":"From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI\n  Synthesis","summary":"  While functional magnetic resonance imaging (fMRI) offers rich spatial\nresolution, it is limited by high operational costs and significant\ninfrastructural demands. In contrast, electroencephalography (EEG) provides\nmillisecond-level precision in capturing electrical activity but lacks the\nspatial resolution necessary for precise neural localization. To bridge these\ngaps, we introduce E2fNet, a simple yet effective deep learning model for\nsynthesizing fMRI images from low-cost EEG data. E2fNet is specifically\ndesigned to capture and translate meaningful features from EEG across electrode\nchannels into accurate fMRI representations. Extensive evaluations across three\ndatasets demonstrate that E2fNet consistently outperforms existing methods,\nachieving state-of-the-art results in terms of the structural similarity index\nmeasure (SSIM). Our findings suggest that E2fNet is a promising, cost-effective\nsolution for enhancing neuroimaging capabilities. The code is available at\nhttps://github.com/kgr20/E2fNet.\n","authors":["Kristofer Grover Roos","Quan Huu Cap","Atsushi Fukuda"],"pdf_url":"https://arxiv.org/pdf/2502.08025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00448v2","updated":"2025-02-11T23:32:18Z","published":"2024-10-01T07:05:36Z","title":"Advancing Medical Radiograph Representation Learning: A Hybrid\n  Pre-training Paradigm with Multilevel Semantic Granularity","summary":"  This paper introduces an innovative approach to Medical Vision-Language\nPre-training (Med-VLP) area in the specialized context of radiograph\nrepresentation learning. While conventional methods frequently merge textual\nannotations into unified reports, we acknowledge the intrinsic hierarchical\nrelationship between the findings and impression section in radiograph\ndatasets. To establish a targeted correspondence between images and texts, we\npropose a novel HybridMED framework to align global-level visual\nrepresentations with impression and token-level visual representations with\nfindings. Moreover, our framework incorporates a generation decoder that\nemploys two proxy tasks, responsible for generating the impression from (1)\nimages, via a captioning branch, and (2) findings, through a summarization\nbranch. Additionally, knowledge distillation is leveraged to facilitate the\ntraining process. Experiments on the MIMIC-CXR dataset reveal that our\nsummarization branch effectively distills knowledge to the captioning branch,\nenhancing model performance without significantly increasing parameter\nrequirements due to the shared self-attention and feed-forward architecture.\n","authors":["Hanqi Jiang","Xixuan Hao","Yuzhou Huang","Chong Ma","Jiaxun Zhang","Yi Pan","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.00448v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2412.08781v2","updated":"2025-02-11T23:05:30Z","published":"2024-12-11T21:23:24Z","title":"GMem: A Modular Approach for Ultra-Efficient Generative Models","summary":"  Recent studies indicate that the denoising process in deep generative\ndiffusion models implicitly learns and memorizes semantic information from the\ndata distribution. These findings suggest that capturing more complex data\ndistributions requires larger neural networks, leading to a substantial\nincrease in computational demands, which in turn become the primary bottleneck\nin both training and inference of diffusion models. To this end, we introduce\nGMem: A Modular Approach for Ultra-Efficient Generative Models. Our approach\nGMem decouples the memory capacity from model and implements it as a separate,\nimmutable memory set that preserves the essential semantic information in the\ndata. The results are significant: GMem enhances both training, sampling\nefficiency, and diversity generation. This design on one hand reduces the\nreliance on network for memorize complex data distribution and thus enhancing\nboth training and sampling efficiency. On ImageNet at $256 \\times 256$\nresolution, GMem achieves a $50\\times$ training speedup compared to SiT,\nreaching FID $=7.66$ in fewer than $28$ epochs ($\\sim 4$ hours training time),\nwhile SiT requires $1400$ epochs. Without classifier-free guidance, GMem\nachieves state-of-the-art (SoTA) performance FID $=1.53$ in $160$ epochs with\nonly $\\sim 20$ hours of training, outperforming LightningDiT which requires\n$800$ epochs and $\\sim 95$ hours to attain FID $=2.17$.\n","authors":["Yi Tang","Peng Sun","Zhenglin Cheng","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2412.08781v2.pdf","comment":"9 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.08005v1","updated":"2025-02-11T23:02:14Z","published":"2025-02-11T23:02:14Z","title":"Towards Training One-Step Diffusion Models Without Distillation","summary":"  Recent advances in one-step generative models typically follow a two-stage\nprocess: first training a teacher diffusion model and then distilling it into a\none-step student model. This distillation process traditionally relies on both\nthe teacher model's score function to compute the distillation loss and its\nweights for student initialization. In this paper, we explore whether one-step\ngenerative models can be trained directly without this distillation process.\nFirst, we show that the teacher's score function is not essential and propose a\nfamily of distillation methods that achieve competitive results without relying\non score estimation. Next, we demonstrate that initialization from teacher\nweights is indispensable in successful training. Surprisingly, we find that\nthis benefit is not due to improved ``input-output\" mapping but rather the\nlearned feature representations, which dominate distillation quality. Our\nfindings provide a better understanding of the role of initialization in\none-step model training and its impact on distillation quality.\n","authors":["Mingtian Zhang","Jiajun He","Wenlin Chen","Zijing Ou","José Miguel Hernández-Lobato","Bernhard Schölkopf","David Barber"],"pdf_url":"https://arxiv.org/pdf/2502.08005v1.pdf","comment":"13 pages, Technical Report"},{"id":"http://arxiv.org/abs/2408.03464v2","updated":"2025-02-11T22:29:52Z","published":"2024-08-06T22:39:34Z","title":"Vision Foundation Models in Remote Sensing: A Survey","summary":"  Artificial Intelligence (AI) technologies have profoundly transformed the\nfield of remote sensing, revolutionizing data collection, processing, and\nanalysis. Traditionally reliant on manual interpretation and task-specific\nmodels, remote sensing research has been significantly enhanced by the advent\nof foundation models-large-scale, pre-trained AI models capable of performing a\nwide array of tasks with unprecedented accuracy and efficiency. This paper\nprovides a comprehensive survey of foundation models in the remote sensing\ndomain. We categorize these models based on their architectures, pre-training\ndatasets, and methodologies. Through detailed performance comparisons, we\nhighlight emerging trends and the significant advancements achieved by those\nfoundation models. Additionally, we discuss technical challenges, practical\nimplications, and future research directions, addressing the need for\nhigh-quality data, computational resources, and improved model generalization.\nOur research also finds that pre-training methods, particularly self-supervised\nlearning techniques like contrastive learning and masked autoencoders,\nremarkably enhance the performance and robustness of foundation models. This\nsurvey aims to serve as a resource for researchers and practitioners by\nproviding a panorama of advances and promising pathways for continued\ndevelopment and application of foundation models in remote sensing.\n","authors":["Siqi Lu","Junlin Guo","James R Zimmer-Dauphinee","Jordan M Nieusma","Xiao Wang","Parker VanValkenburgh","Steven A Wernke","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2408.03464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16524v2","updated":"2025-02-11T22:26:38Z","published":"2024-10-21T21:32:17Z","title":"Supervised Learning without Backpropagation using Spike-Timing-Dependent\n  Plasticity for Image Recognition","summary":"  This study introduces a novel supervised learning approach for spiking neural\nnetworks that does not rely on traditional backpropagation. Instead, it employs\nspike-timing-dependent plasticity (STDP) within a supervised framework for\nimage recognition tasks. The effectiveness of this method is demonstrated using\nthe MNIST dataset. The model achieves approximately 40\\% learning accuracy with\njust 10 training stimuli, where each category is exposed to the model only once\nduring training (one-shot learning). With larger training samples, the accuracy\nincreases up to 87\\%, maintaining negligible ambiguity. Notably, with only 10\nhidden neurons, the model reaches 89\\% accuracy with around 10\\% ambiguity.\nThis proposed method offers a robust and efficient alternative to traditional\nbackpropagation-based supervised learning techniques.\n","authors":["Wei Xie"],"pdf_url":"https://arxiv.org/pdf/2410.16524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07979v1","updated":"2025-02-11T21:52:32Z","published":"2025-02-11T21:52:32Z","title":"Joint Modelling Histology and Molecular Markers for Cancer\n  Classification","summary":"  Cancers are characterized by remarkable heterogeneity and diverse prognosis.\nAccurate cancer classification is essential for patient stratification and\nclinical decision-making. Although digital pathology has been advancing cancer\ndiagnosis and prognosis, the paradigm in cancer pathology has shifted from\npurely relying on histology features to incorporating molecular markers. There\nis an urgent need for digital pathology methods to meet the needs of the new\nparadigm. We introduce a novel digital pathology approach to jointly predict\nmolecular markers and histology features and model their interactions for\ncancer classification. Firstly, to mitigate the challenge of\ncross-magnification information propagation, we propose a multi-scale\ndisentangling module, enabling the extraction of multi-scale features from\nhigh-magnification (cellular-level) to low-magnification (tissue-level) whole\nslide images. Further, based on the multi-scale features, we propose an\nattention-based hierarchical multi-task multi-instance learning framework to\nsimultaneously predict histology and molecular markers. Moreover, we propose a\nco-occurrence probability-based label correlation graph network to model the\nco-occurrence of molecular markers. Lastly, we design a cross-modal interaction\nmodule with the dynamic confidence constrain loss and a cross-modal gradient\nmodulation strategy, to model the interactions of histology and molecular\nmarkers. Our experiments demonstrate that our method outperforms other\nstate-of-the-art methods in classifying glioma, histology features and\nmolecular markers. Our method promises to promote precise oncology with the\npotential to advance biomedical research and clinical applications. The code is\navailable at https://github.com/LHY1007/M3C2\n","authors":["Xiaofei Wang","Hanyu Liu","Yupei Zhang","Boyang Zhao","Hao Duan","Wanming Hu","Yonggao Mou","Stephen Price","Chao Li"],"pdf_url":"https://arxiv.org/pdf/2502.07979v1.pdf","comment":"accepted by Medical Image Analysis"},{"id":"http://arxiv.org/abs/2502.07951v1","updated":"2025-02-11T21:00:01Z","published":"2025-02-11T21:00:01Z","title":"Federated Self-supervised Domain Generalization for Label-efficient\n  Polyp Segmentation","summary":"  Employing self-supervised learning (SSL) methodologies assumes par-amount\nsignificance in handling unlabeled polyp datasets when building deep\nlearning-based automatic polyp segmentation models. However, the intricate\nprivacy dynamics surrounding medical data often preclude seamless data sharing\namong disparate medical centers. Federated learning (FL) emerges as a\nformidable solution to this privacy conundrum, yet within the realm of FL,\noptimizing model generalization stands as a pressing imperative. Robust\ngeneralization capabilities are imperative to ensure the model's efficacy\nacross diverse geographical domains post-training on localized client datasets.\nIn this paper, a Federated self-supervised Domain Generalization method is\nproposed to enhance the generalization capacity of federated and\nLabel-efficient intestinal polyp segmentation, named LFDG. Based on a classical\nSSL method, DropPos, LFDG proposes an adversarial learning-based data\naugmentation method (SSADA) to enhance the data diversity. LFDG further\nproposes a relaxation module based on Source-reconstruction and\nAugmentation-masking (SRAM) to maintain stability in feature learning. We have\nvalidated LFDG on polyp images from six medical centers. The performance of our\nmethod achieves 3.80% and 3.92% better than the baseline and other recent FL\nmethods and SSL methods, respectively.\n","authors":["Xinyi Tan","Jiacheng Wang","Liansheng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07951v1.pdf","comment":"Accepted at ADSMI @ MICCAI 2024"},{"id":"http://arxiv.org/abs/2502.07945v1","updated":"2025-02-11T20:49:13Z","published":"2025-02-11T20:49:13Z","title":"SurGrID: Controllable Surgical Simulation via Scene Graph to Image\n  Diffusion","summary":"  Surgical simulation offers a promising addition to conventional surgical\ntraining. However, available simulation tools lack photorealism and rely on\nhardcoded behaviour. Denoising Diffusion Models are a promising alternative for\nhigh-fidelity image synthesis, but existing state-of-the-art conditioning\nmethods fall short in providing precise control or interactivity over the\ngenerated scenes.\n  We introduce SurGrID, a Scene Graph to Image Diffusion Model, allowing for\ncontrollable surgical scene synthesis by leveraging Scene Graphs. These graphs\nencode a surgical scene's components' spatial and semantic information, which\nare then translated into an intermediate representation using our novel\npre-training step that explicitly captures local and global information.\n  Our proposed method improves the fidelity of generated images and their\ncoherence with the graph input over the state-of-the-art. Further, we\ndemonstrate the simulation's realism and controllability in a user assessment\nstudy involving clinical experts.\n  Scene Graphs can be effectively used for precise and interactive conditioning\nof Denoising Diffusion Models for simulating surgical scenes, enabling high\nfidelity and interactive control over the generated content.\n","authors":["Yannik Frisch","Ssharvien Kumar Sivakumar","Çağhan Köksal","Elsa Böhm","Felix Wagner","Adrian Gericke","Ghazal Ghazaei","Anirban Mukhopadhyay"],"pdf_url":"https://arxiv.org/pdf/2502.07945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01302v2","updated":"2025-02-11T19:56:18Z","published":"2024-07-01T13:58:32Z","title":"Robot Instance Segmentation with Few Annotations for Grasping","summary":"  The ability of robots to manipulate objects relies heavily on their aptitude\nfor visual perception. In domains characterized by cluttered scenes and high\nobject variability, most methods call for vast labeled datasets, laboriously\nhand-annotated, with the aim of training capable models. Once deployed, the\nchallenge of generalizing to unfamiliar objects implies that the model must\nevolve alongside its domain. To address this, we propose a novel framework that\ncombines Semi-Supervised Learning (SSL) with Learning Through Interaction\n(LTI), allowing a model to learn by observing scene alterations and leverage\nvisual consistency despite temporal gaps without requiring curated data of\ninteraction sequences. As a result, our approach exploits partially annotated\ndata through self-supervision and incorporates temporal context using\npseudo-sequences generated from unlabeled still images. We validate our method\non two common benchmarks, ARMBench mix-object-tote and OCID, where it achieves\nstate-of-the-art performance. Notably, on ARMBench, we attain an\n$\\text{AP}_{50}$ of $86.37$, almost a $20\\%$ improvement over existing work,\nand obtain remarkable results in scenarios with extremely low annotation,\nachieving an $\\text{AP}_{50}$ score of $84.89$ with just $1 \\%$ of annotated\ndata compared to $72$ presented in ARMBench on the fully annotated counterpart.\n","authors":["Moshe Kimhi","David Vainshtein","Chaim Baskin","Dotan Di Castro"],"pdf_url":"https://arxiv.org/pdf/2407.01302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14394v2","updated":"2025-02-11T19:35:42Z","published":"2024-04-22T17:55:11Z","title":"A Multimodal Automated Interpretability Agent","summary":"  This paper describes MAIA, a Multimodal Automated Interpretability Agent.\nMAIA is a system that uses neural models to automate neural model understanding\ntasks like feature interpretation and failure mode discovery. It equips a\npre-trained vision-language model with a set of tools that support iterative\nexperimentation on subcomponents of other models to explain their behavior.\nThese include tools commonly used by human interpretability researchers: for\nsynthesizing and editing inputs, computing maximally activating exemplars from\nreal-world datasets, and summarizing and describing experimental results.\nInterpretability experiments proposed by MAIA compose these tools to describe\nand explain system behavior. We evaluate applications of MAIA to computer\nvision models. We first characterize MAIA's ability to describe (neuron-level)\nfeatures in learned representations of images. Across several trained models\nand a novel dataset of synthetic vision neurons with paired ground-truth\ndescriptions, MAIA produces descriptions comparable to those generated by\nexpert human experimenters. We then show that MAIA can aid in two additional\ninterpretability tasks: reducing sensitivity to spurious features, and\nautomatically identifying inputs likely to be mis-classified.\n","authors":["Tamar Rott Shaham","Sarah Schwettmann","Franklin Wang","Achyuta Rajaram","Evan Hernandez","Jacob Andreas","Antonio Torralba"],"pdf_url":"https://arxiv.org/pdf/2404.14394v2.pdf","comment":"25 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.07905v1","updated":"2025-02-11T19:21:23Z","published":"2025-02-11T19:21:23Z","title":"DeepSeek on a Trip: Inducing Targeted Visual Hallucinations via\n  Representation Vulnerabilities","summary":"  Multimodal Large Language Models (MLLMs) represent the cutting edge of AI\ntechnology, with DeepSeek models emerging as a leading open-source alternative\noffering competitive performance to closed-source systems. While these models\ndemonstrate remarkable capabilities, their vision-language integration\nmechanisms introduce specific vulnerabilities. We implement an adapted\nembedding manipulation attack on DeepSeek Janus that induces targeted visual\nhallucinations through systematic optimization of image embeddings. Through\nextensive experimentation across COCO, DALL-E 3, and SVIT datasets, we achieve\nhallucination rates of up to 98.0% while maintaining high visual fidelity (SSIM\n> 0.88) of the manipulated images on open-ended questions. Our analysis\ndemonstrates that both 1B and 7B variants of DeepSeek Janus are susceptible to\nthese attacks, with closed-form evaluation showing consistently higher\nhallucination rates compared to open-ended questioning. We introduce a novel\nmulti-prompt hallucination detection framework using LLaMA-3.1 8B Instruct for\nrobust evaluation. The implications of these findings are particularly\nconcerning given DeepSeek's open-source nature and widespread deployment\npotential. This research emphasizes the critical need for embedding-level\nsecurity measures in MLLM deployment pipelines and contributes to the broader\ndiscussion of responsible AI implementation.\n","authors":["Chashi Mahiul Islam","Samuel Jacob Chacko","Preston Horne","Xiuwen Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07905v1.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.05818v2","updated":"2025-02-11T19:05:32Z","published":"2023-08-10T18:35:22Z","title":"Absorption-Based, Passive Range Imaging from Hyperspectral Thermal\n  Measurements","summary":"  Passive hyperspectral longwave infrared measurements are remarkably\ninformative about the surroundings. Remote object material and temperature\ndetermine the spectrum of thermal radiance, and range, air temperature, and gas\nconcentrations determine how this spectrum is modified by propagation to the\nsensor. We introduce a passive range imaging method based on computationally\nseparating these phenomena. Previous methods assume hot and highly emitting\nobjects; ranging is more challenging when objects' temperatures do not deviate\ngreatly from air temperature. Our method jointly estimates range and intrinsic\nobject properties, with explicit consideration of air emission, though\nreflected light is assumed negligible. Inversion being underdetermined is\nmitigated by using a parametric model of atmospheric absorption and\nregularizing for smooth emissivity estimates. To assess where our estimate is\nlikely accurate, we introduce a technique to detect which scene pixels are\nsignificantly influenced by reflected downwelling. Monte Carlo simulations\ndemonstrate the importance of regularization, temperature differentials, and\navailability of many spectral bands. We apply our method to longwave infrared\n(8--13 $\\mu$m) hyperspectral image data acquired from natural scenes with no\nactive illumination. Range features from 15m to 150m are recovered, with good\nqualitative match to lidar data for pixels classified as having negligible\nreflected downwelling.\n","authors":["Unay Dorken Gallastegi","Hoover Rueda-Chacon","Martin J. Stevens","Vivek K Goyal"],"pdf_url":"https://arxiv.org/pdf/2308.05818v2.pdf","comment":"15 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.07870v1","updated":"2025-02-11T18:59:19Z","published":"2025-02-11T18:59:19Z","title":"TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation","summary":"  Text-conditioned image generation has gained significant attention in recent\nyears and are processing increasingly longer and comprehensive text prompt. In\neveryday life, dense and intricate text appears in contexts like\nadvertisements, infographics, and signage, where the integration of both text\nand visuals is essential for conveying complex information. However, despite\nthese advances, the generation of images containing long-form text remains a\npersistent challenge, largely due to the limitations of existing datasets,\nwhich often focus on shorter and simpler text. To address this gap, we\nintroduce TextAtlas5M, a novel dataset specifically designed to evaluate\nlong-text rendering in text-conditioned image generation. Our dataset consists\nof 5 million long-text generated and collected images across diverse data\ntypes, enabling comprehensive evaluation of large-scale generative models on\nlong-text image generation. We further curate 3000 human-improved test set\nTextAtlasEval across 3 data domains, establishing one of the most extensive\nbenchmarks for text-conditioned generation. Evaluations suggest that the\nTextAtlasEval benchmarks present significant challenges even for the most\nadvanced proprietary models (e.g. GPT4o with DallE-3), while their open-source\ncounterparts show an even larger performance gap. These evidences position\nTextAtlas5M as a valuable dataset for training and evaluating future-generation\ntext-conditioned image generation models.\n","authors":["Alex Jinpeng Wang","Dongxing Mao","Jiawei Zhang","Weiming Han","Zhuobai Dong","Linjie Li","Yiqi Lin","Zhengyuan Yang","Libo Qin","Fuwei Zhang","Lijuan Wang","Min Li"],"pdf_url":"https://arxiv.org/pdf/2502.07870v1.pdf","comment":"27 pages, 15 figures. Dataset Website: https://textatlas5m.github.io"},{"id":"http://arxiv.org/abs/2502.07869v1","updated":"2025-02-11T18:57:05Z","published":"2025-02-11T18:57:05Z","title":"EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera","summary":"  Monocular egocentric 3D human motion capture remains a significant challenge,\nparticularly under conditions of low lighting and fast movements, which are\ncommon in head-mounted device applications. Existing methods that rely on RGB\ncameras often fail under these conditions. To address these limitations, we\nintroduce EventEgo3D++, the first approach that leverages a monocular event\ncamera with a fisheye lens for 3D human motion capture. Event cameras excel in\nhigh-speed scenarios and varying illumination due to their high temporal\nresolution, providing reliable cues for accurate 3D human motion capture.\nEventEgo3D++ leverages the LNES representation of event streams to enable\nprecise 3D reconstructions. We have also developed a mobile head-mounted device\n(HMD) prototype equipped with an event camera, capturing a comprehensive\ndataset that includes real event observations from both controlled studio\nenvironments and in-the-wild settings, in addition to a synthetic dataset.\nAdditionally, to provide a more holistic dataset, we include allocentric RGB\nstreams that offer different perspectives of the HMD wearer, along with their\ncorresponding SMPL body model. Our experiments demonstrate that EventEgo3D++\nachieves superior 3D accuracy and robustness compared to existing solutions,\neven in challenging conditions. Moreover, our method supports real-time 3D pose\nupdates at a rate of 140Hz. This work is an extension of the EventEgo3D\napproach (CVPR 2024) and further advances the state of the art in egocentric 3D\nhuman motion capture. For more details, visit the project page at\nhttps://eventego3d.mpi-inf.mpg.de.\n","authors":["Christen Millerdurai","Hiroyasu Akada","Jian Wang","Diogo Luvizon","Alain Pagani","Didier Stricker","Christian Theobalt","Vladislav Golyanik"],"pdf_url":"https://arxiv.org/pdf/2502.07869v1.pdf","comment":"30 pages, 20 figures, 9 tables. arXiv admin note: text overlap with\n  arXiv:2404.08640"},{"id":"http://arxiv.org/abs/2502.07862v1","updated":"2025-02-11T17:19:44Z","published":"2025-02-11T17:19:44Z","title":"ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise\n  and Compute Resources","summary":"  Multimodal deep learning systems are deployed in dynamic scenarios due to the\nrobustness afforded by multiple sensing modalities. Nevertheless, they struggle\nwith varying compute resource availability (due to multi-tenancy, device\nheterogeneity, etc.) and fluctuating quality of inputs (from sensor feed\ncorruption, environmental noise, etc.). Current multimodal systems employ\nstatic resource provisioning and cannot easily adapt when compute resources\nchange over time. Additionally, their reliance on processing sensor data with\nfixed feature extractors is ill-equipped to handle variations in modality\nquality. Consequently, uninformative modalities, such as those with high noise,\nneedlessly consume resources better allocated towards other modalities. We\npropose ADMN, a layer-wise Adaptive Depth Multimodal Network capable of\ntackling both challenges - it adjusts the total number of active layers across\nall modalities to meet compute resource constraints, and continually\nreallocates layers across input modalities according to their modality quality.\nOur evaluations showcase ADMN can match the accuracy of state-of-the-art\nnetworks while reducing up to 75% of their floating-point operations.\n","authors":["Jason Wu","Kang Yang","Lance Kaplan","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2502.07862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07859v1","updated":"2025-02-11T16:29:22Z","published":"2025-02-11T16:29:22Z","title":"Automatic Prostate Volume Estimation in Transabdominal Ultrasound Images","summary":"  Prostate cancer is a leading health concern among men, requiring accurate and\naccessible methods for early detection and risk stratification. Prostate volume\n(PV) is a key parameter in multivariate risk stratification for early prostate\ncancer detection, commonly estimated using transrectal ultrasound (TRUS). While\nTRUS provides precise prostate volume measurements, its invasive nature often\ncompromises patient comfort. Transabdominal ultrasound (TAUS) provides a\nnon-invasive alternative but faces challenges such as lower image quality,\ncomplex interpretation, and reliance on operator expertise. This study\nintroduces a new deep-learning-based framework for automatic PV estimation\nusing TAUS, emphasizing its potential to enable accurate and non-invasive\nprostate cancer risk stratification. A dataset of TAUS videos from 100\nindividual patients was curated, with manually delineated prostate boundaries\nand calculated diameters by an expert clinician as ground truth. The introduced\nframework integrates deep-learning models for prostate segmentation in both\naxial and sagittal planes, automatic prostate diameter estimation, and PV\ncalculation. Segmentation performance was evaluated using Dice correlation\ncoefficient (%) and Hausdorff distance (mm). Framework's volume estimation\ncapabilities were evaluated on volumetric error (mL). The framework\ndemonstrates that it can estimate PV from TAUS videos with a mean volumetric\nerror of -5.5 mL, which results in an average relative error between 5 and 15%.\nThe introduced framework for automatic PV estimation from TAUS images,\nutilizing deep learning models for prostate segmentation, shows promising\nresults. It effectively segments the prostate and estimates its volume,\noffering potential for reliable, non-invasive risk stratification for early\nprostate detection.\n","authors":["Tiziano Natali","Liza M. Kurucz","Matteo Fusaglia","Laura S. Mertens","Theo J. M. Ruers","Pim J. van Leeuwen","Behdad Dashtbozorg"],"pdf_url":"https://arxiv.org/pdf/2502.07859v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.05502v3","updated":"2025-02-11T18:17:53Z","published":"2024-07-07T21:26:36Z","title":"Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models","summary":"  Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views.\n","authors":["Nikhil Sharma","Kenton Murray","Ziang Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.05502v3.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07683v1","updated":"2025-02-11T16:35:04Z","published":"2025-02-11T16:35:04Z","title":"exHarmony: Authorship and Citations for Benchmarking the Reviewer\n  Assignment Problem","summary":"  The peer review process is crucial for ensuring the quality and reliability\nof scholarly work, yet assigning suitable reviewers remains a significant\nchallenge. Traditional manual methods are labor-intensive and often\nineffective, leading to nonconstructive or biased reviews. This paper\nintroduces the exHarmony (eHarmony but for connecting experts to manuscripts)\nbenchmark, designed to address these challenges by re-imagining the Reviewer\nAssignment Problem (RAP) as a retrieval task. Utilizing the extensive data from\nOpenAlex, we propose a novel approach that considers a host of signals from the\nauthors, most similar experts, and the citation relations as potential\nindicators for a suitable reviewer for a manuscript. This approach allows us to\ndevelop a standard benchmark dataset for evaluating the reviewer assignment\nproblem without needing explicit labels. We benchmark various methods,\nincluding traditional lexical matching, static neural embeddings, and\ncontextualized neural embeddings, and introduce evaluation metrics that assess\nboth relevance and diversity in the context of RAP. Our results indicate that\nwhile traditional methods perform reasonably well, contextualized embeddings\ntrained on scholarly literature show the best performance. The findings\nunderscore the importance of further research to enhance the diversity and\neffectiveness of reviewer assignments.\n","authors":["Sajad Ebrahimi","Sara Salamat","Negar Arabzadeh","Mahdi Bashari","Ebrahim Bagheri"],"pdf_url":"https://arxiv.org/pdf/2502.07683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07658v1","updated":"2025-02-11T15:46:28Z","published":"2025-02-11T15:46:28Z","title":"IU4Rec: Interest Unit-Based Product Organization and Recommendation for\n  E-Commerce Platform","summary":"  Most recommendation systems typically follow a product-based paradigm\nutilizing user-product interactions to identify the most engaging items for\nusers. However, this product-based paradigm has notable drawbacks for\nXianyu~\\footnote{Xianyu is China's largest online C2C e-commerce platform where\na large portion of the product are post by individual sellers}. Most of the\nproduct on Xianyu posted from individual sellers often have limited stock\navailable for distribution, and once the product is sold, it's no longer\navailable for distribution. This result in most items distributed product on\nXianyu having relatively few interactions, affecting the effectiveness of\ntraditional recommendation depending on accumulating user-item interactions. To\naddress these issues, we introduce \\textbf{IU4Rec}, an \\textbf{I}nterest\n\\textbf{U}nit-based two-stage \\textbf{Rec}ommendation system framework. We\nfirst group products into clusters based on attributes such as category, image,\nand semantics. These IUs are then integrated into the Recommendation system,\ndelivering both product and technological innovations. IU4Rec begins by\ngrouping products into clusters based on attributes such as category, image,\nand semantics, forming Interest Units (IUs). Then we redesign the\nrecommendation process into two stages. In the first stage, the focus is on\nrecommend these Interest Units, capturing broad-level interests. In the second\nstage, it guides users to find the best option among similar products within\nthe selected Interest Unit. User-IU interactions are incorporated into our\nranking models, offering the advantage of more persistent IU behaviors compared\nto item-specific interactions. Experimental results on the production dataset\nand online A/B testing demonstrate the effectiveness and superiority of our\nproposed IU-centric recommendation approach.\n","authors":["Wenhao Wu","Xiaojie Li","Lin Wang","Jialiang Zhou","Di Wu","Qinye Xie","Qingheng Zhang","Yin Zhang","Shuguang Han","Fei Huang","Junfeng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07658v1.pdf","comment":"Under review at KDD25 ADS. This work has already been deployed on the\n  Xianyu platform in Alibaba. arXiv admin note: substantial text overlap with\n  arXiv:2403.06747"},{"id":"http://arxiv.org/abs/2502.06097v2","updated":"2025-02-11T14:44:47Z","published":"2025-02-10T02:06:17Z","title":"NLGR: Utilizing Neighbor Lists for Generative Rerank in Personalized\n  Recommendation Systems","summary":"  Reranking plays a crucial role in modern multi-stage recommender systems by\nrearranging the initial ranking list. Due to the inherent challenges of\ncombinatorial search spaces, some current research adopts an\nevaluator-generator paradigm, with a generator generating feasible sequences\nand an evaluator selecting the best sequence based on the estimated list\nutility. However, these methods still face two issues. Firstly, due to the goal\ninconsistency problem between the evaluator and generator, the generator tends\nto fit the local optimal solution of exposure distribution rather than\ncombinatorial space optimization. Secondly, the strategy of generating target\nitems one by one is difficult to achieve optimality because it ignores the\ninformation of subsequent items.\n  To address these issues, we propose a utilizing Neighbor Lists model for\nGenerative Reranking (NLGR), which aims to improve the performance of the\ngenerator in the combinatorial space. NLGR follows the evaluator-generator\nparadigm and improves the generator's training and generating methods.\nSpecifically, we use neighbor lists in combination space to enhance the\ntraining process, making the generator perceive the relative scores and find\nthe optimization direction. Furthermore, we propose a novel sampling-based\nnon-autoregressive generation method, which allows the generator to jump\nflexibly from the current list to any neighbor list. Extensive experiments on\npublic and industrial datasets validate NLGR's effectiveness and we have\nsuccessfully deployed NLGR on the Meituan food delivery platform.\n","authors":["Shuli Wang","Xue Wei","Senjie Kou","Chi Wang","Wenshuai Chen","Qi Tang","Yinhua Zhu","Xiong Xiao","Xingxing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06097v2.pdf","comment":"Accepted by WWW 2025 Industry Track"},{"id":"http://arxiv.org/abs/2502.03307v2","updated":"2025-02-11T14:29:44Z","published":"2025-02-05T16:08:05Z","title":"Intent Representation Learning with Large Language Model for\n  Recommendation","summary":"  Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.\n","authors":["Yu Wang","Lei Sang","Yi Zhang","Yiwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.03307v2.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.07491v1","updated":"2025-02-11T11:51:07Z","published":"2025-02-11T11:51:07Z","title":"Exploring Patterns Behind Sports","summary":"  This paper presents a comprehensive framework for time series prediction\nusing a hybrid model that combines ARIMA and LSTM. The model incorporates\nfeature engineering techniques, including embedding and PCA, to transform raw\ndata into a lower-dimensional representation while retaining key information.\nThe embedding technique is used to convert categorical data into continuous\nvectors, facilitating the capture of complex relationships. PCA is applied to\nreduce dimensionality and extract principal components, enhancing model\nperformance and computational efficiency. To handle both linear and nonlinear\npatterns in the data, the ARIMA model captures linear trends, while the LSTM\nmodel models complex nonlinear dependencies. The hybrid model is trained on\nhistorical data and achieves high accuracy, as demonstrated by low RMSE and MAE\nscores. Additionally, the paper employs the run test to assess the randomness\nof sequences, providing insights into the underlying patterns. Ablation studies\nare conducted to validate the roles of different components in the model,\ndemonstrating the significance of each module. The paper also utilizes the SHAP\nmethod to quantify the impact of traditional advantages on the predicted\nresults, offering a detailed understanding of feature importance. The KNN\nmethod is used to determine the optimal prediction interval, further enhancing\nthe model's accuracy. The results highlight the effectiveness of combining\ntraditional statistical methods with modern deep learning techniques for robust\ntime series forecasting in Sports.\n","authors":["Chang Liu","Chengcheng Ma","XuanQi Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07474v1","updated":"2025-02-11T11:34:33Z","published":"2025-02-11T11:34:33Z","title":"ETimeline: An Extensive Timeline Generation Dataset based on Large\n  Language Model","summary":"  Timeline generation is of great significance for a comprehensive\nunderstanding of the development of events over time. Its goal is to organize\nnews chronologically, which helps to identify patterns and trends that may be\nobscured when viewing news in isolation, making it easier to track the\ndevelopment of stories and understand the interrelationships between key\nevents. Timelines are now common in various commercial products, but academic\nresearch in this area is notably scarce. Additionally, the current datasets are\nin need of refinement for enhanced utility and expanded coverage. In this\npaper, we propose ETimeline, which encompasses over $13,000$ news articles,\nspanning $600$ bilingual timelines across $28$ news domains. Specifically, we\ngather a candidate pool of more than $120,000$ news articles and employ the\nlarge language model (LLM) Pipeline to improve performance, ultimately yielding\nthe ETimeline. The data analysis underscores the appeal of ETimeline.\nAdditionally, we also provide the news pool data for further research and\nanalysis. This work contributes to the advancement of timeline generation\nresearch and supports a wide range of tasks, including topic generation and\nevent relationships. We believe that this dataset will serve as a catalyst for\ninnovative research and bridge the gap between academia and industry in\nunderstanding the practical application of technology services. The dataset is\navailable at https://zenodo.org/records/11392212\n","authors":["Xiaochen Liu","Yanan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16125v3","updated":"2025-02-11T11:19:40Z","published":"2025-01-27T15:12:27Z","title":"SampleLLM: Optimizing Tabular Data Synthesis in Recommendations","summary":"  Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios.\n","authors":["Jingtong Gao","Zhaocheng Du","Xiaopeng Li","Yichao Wang","Xiangyang Li","Huifeng Guo","Ruiming Tang","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.16125v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00321v3","updated":"2025-02-11T10:55:02Z","published":"2025-02-01T05:06:21Z","title":"MIM: Multi-modal Content Interest Modeling Paradigm for User Behavior\n  Modeling","summary":"  Click-Through Rate (CTR) prediction is a crucial task in recommendation\nsystems, online searches, and advertising platforms, where accurately capturing\nusers' real interests in content is essential for performance. However,\nexisting methods heavily rely on ID embeddings, which fail to reflect users'\ntrue preferences for content such as images and titles. This limitation becomes\nparticularly evident in cold-start and long-tail scenarios, where traditional\napproaches struggle to deliver effective results. To address these challenges,\nwe propose a novel Multi-modal Content Interest Modeling paradigm (MIM), which\nconsists of three key stages: Pre-training, Content-Interest-Aware Supervised\nFine-Tuning (C-SFT), and Content-Interest-Aware UBM (CiUBM). The pre-training\nstage adapts foundational models to domain-specific data, enabling the\nextraction of high-quality multi-modal embeddings. The C-SFT stage bridges the\nsemantic gap between content and user interests by leveraging user behavior\nsignals to guide the alignment of embeddings with user preferences. Finally,\nthe CiUBM stage integrates multi-modal embeddings and ID-based collaborative\nfiltering signals into a unified framework. Comprehensive offline experiments\nand online A/B tests conducted on the Taobao, one of the world's largest\ne-commerce platforms, demonstrated the effectiveness and efficiency of MIM\nmethod. The method has been successfully deployed online, achieving a\nsignificant increase of +14.14% in CTR and +4.12% in RPM, showcasing its\nindustrial applicability and substantial impact on platform performance. To\npromote further research, we have publicly released the code and dataset at\nhttps://pan.quark.cn/s/8fc8ec3e74f3.\n","authors":["Bencheng Yan","Si Chen","Shichang Jia","Jianyu Liu","Yueran Liu","Chenghan Fu","Wanxian Guan","Hui Zhao","Xiang Zhang","Kai Zhang","Wenbo Su","Pengjie Wang","Jian Xu","Bo Zheng","Baolin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.00321v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17670v2","updated":"2025-02-11T09:52:43Z","published":"2025-01-29T14:20:42Z","title":"Distinguished Quantized Guidance for Diffusion-based Sequence\n  Recommendation","summary":"  Diffusion models (DMs) have emerged as promising approaches for sequential\nrecommendation due to their strong ability to model data distributions and\ngenerate high-quality items. Existing work typically adds noise to the next\nitem and progressively denoises it guided by the user's interaction sequence,\ngenerating items that closely align with user interests. However, we identify\ntwo key issues in this paradigm. First, the sequences are often heterogeneous\nin length and content, exhibiting noise due to stochastic user behaviors. Using\nsuch sequences as guidance may hinder DMs from accurately understanding user\ninterests. Second, DMs are prone to data bias and tend to generate only the\npopular items that dominate the training dataset, thus failing to meet the\npersonalized needs of different users. To address these issues, we propose\nDistinguished Quantized Guidance for Diffusion-based Sequence Recommendation\n(DiQDiff), which aims to extract robust guidance to understand user interests\nand generate distinguished items for personalized user interests within DMs. To\nextract robust guidance, DiQDiff introduces Semantic Vector Quantization (SVQ)\nto quantize sequences into semantic vectors (e.g., collaborative signals and\ncategory interests) using a codebook, which can enrich the guidance to better\nunderstand user interests. To generate distinguished items, DiQDiff\npersonalizes the generation through Contrastive Discrepancy Maximization (CDM),\nwhich maximizes the distance between denoising trajectories using contrastive\nloss to prevent biased generation for different users. Extensive experiments\nare conducted to compare DiQDiff with multiple baseline models across four\nwidely-used datasets. The superior recommendation performance of DiQDiff\nagainst leading approaches demonstrates its effectiveness in sequential\nrecommendation tasks.\n","authors":["Wenyu Mao","Shuchang Liu","Haoyang Liu","Haozhe Liu","Xiang Li","Lantao Hu"],"pdf_url":"https://arxiv.org/pdf/2501.17670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03984v2","updated":"2025-02-11T09:07:15Z","published":"2023-10-06T02:45:21Z","title":"AURO: Reinforcement Learning for Adaptive User Retention Optimization in\n  Recommender Systems","summary":"  The field of Reinforcement Learning (RL) has garnered increasing attention\nfor its ability of optimizing user retention in recommender systems. A primary\nobstacle in this optimization process is the environment non-stationarity\nstemming from the continual and complex evolution of user behavior patterns\nover time, such as variations in interaction rates and retention propensities.\nThese changes pose significant challenges to existing RL algorithms for\nrecommendations, leading to issues with dynamics and reward distribution\nshifts. This paper introduces a novel approach called \\textbf{A}daptive\n\\textbf{U}ser \\textbf{R}etention \\textbf{O}ptimization (AURO) to address this\nchallenge. To navigate the recommendation policy in non-stationary\nenvironments, AURO introduces an state abstraction module in the policy\nnetwork. The module is trained with a new value-based loss function, aligning\nits output with the estimated performance of the current policy. As the policy\nperformance of RL is sensitive to environment drifts, the loss function enables\nthe state abstraction to be reflective of environment changes and notify the\nrecommendation policy to adapt accordingly. Additionally, the non-stationarity\nof the environment introduces the problem of implicit cold start, where the\nrecommendation policy continuously interacts with users displaying novel\nbehavior patterns. AURO encourages exploration guarded by performance-based\nrejection sampling to maintain a stable recommendation quality in the\ncost-sensitive online environment. Extensive empirical analysis are conducted\nin a user retention simulator, the MovieLens dataset, and a live short-video\nrecommendation platform, demonstrating AURO's superior performance against all\nevaluated baseline algorithms.\n","authors":["Zhenghai Xue","Qingpeng Cai","Bin Yang","Lantao Hu","Peng Jiang","Kun Gai","Bo An"],"pdf_url":"https://arxiv.org/pdf/2310.03984v2.pdf","comment":"The Web Conference 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.07327v1","updated":"2025-02-11T07:43:47Z","published":"2025-02-11T07:43:47Z","title":"Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated\n  Videos","summary":"  With the rapid development of AI-generated content (AIGC), the creation of\nhigh-quality AI-generated videos has become faster and easier, resulting in the\nInternet being flooded with all kinds of video content. However, the impact of\nthese videos on the content ecosystem remains largely unexplored. Video\ninformation retrieval remains a fundamental approach for accessing video\ncontent. Building on the observation that retrieval models often favor\nAI-generated content in ad-hoc and image retrieval tasks, we investigate\nwhether similar biases emerge in the context of challenging video retrieval,\nwhere temporal and visual factors may further influence model behavior. To\nexplore this, we first construct a comprehensive benchmark dataset containing\nboth real and AI-generated videos, along with a set of fair and rigorous\nmetrics to assess bias. This benchmark consists of 13,000 videos generated by\ntwo state-of-the-art open-source video generation models. We meticulously\ndesign a suite of rigorous metrics to accurately measure this preference,\naccounting for potential biases arising from the limited frame rate and\nsuboptimal quality of AIGC videos. We then applied three off-the-shelf video\nretrieval models to perform retrieval tasks on this hybrid dataset. Our\nfindings reveal a clear preference for AI-generated videos in retrieval.\nFurther investigation shows that incorporating AI-generated videos into the\ntraining set of retrieval models exacerbates this bias. Unlike the preference\nobserved in image modalities, we find that video retrieval bias arises from\nboth unseen visual and temporal information, making the root causes of video\nbias a complex interplay of these two factors. To mitigate this bias, we\nfine-tune the retrieval models using a contrastive learning approach. The\nresults of this study highlight the potential implications of AI-generated\nvideos on retrieval systems.\n","authors":["Haowen Gao","Liang Pang","Shicheng Xu","Leigang Qu","Tat-Seng Chua","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05558v2","updated":"2025-02-11T07:36:07Z","published":"2025-02-08T13:08:11Z","title":"Large Memory Network for Recommendation","summary":"  Modeling user behavior sequences in recommender systems is essential for\nunderstanding user preferences over time, enabling personalized and accurate\nrecommendations for improving user retention and enhancing business values.\nDespite its significance, there are two challenges for current sequential\nmodeling approaches. From the spatial dimension, it is difficult to mutually\nperceive similar users' interests for a generalized intention understanding;\nfrom the temporal dimension, current methods are generally prone to forgetting\nlong-term interests due to the fixed-length input sequence. In this paper, we\npresent Large Memory Network (LMN), providing a novel idea by compressing and\nstoring user history behavior information in a large-scale memory block. With\nthe elaborated online deployment strategy, the memory block can be easily\nscaled up to million-scale in the industry. Extensive offline comparison\nexperiments, memory scaling up experiments, and online A/B test on Douyin\nE-Commerce Search (ECS) are performed, validating the superior performance of\nLMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of\nusers each day.\n","authors":["Hui Lu","Zheng Chai","Yuchao Zheng","Zhe Chen","Deping Xie","Peng Xu","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.05558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20163v2","updated":"2025-02-11T07:35:58Z","published":"2024-10-26T12:34:07Z","title":"UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers","summary":"  Existing information retrieval (IR) models often assume a homogeneous\nstructure for knowledge sources and user queries, limiting their applicability\nin real-world settings where retrieval is inherently heterogeneous and diverse.\nIn this paper, we introduce UniHGKR, a unified instruction-aware heterogeneous\nknowledge retriever that (1) builds a unified retrieval space for heterogeneous\nknowledge and (2) follows diverse user instructions to retrieve knowledge of\nspecified types. UniHGKR consists of three principal stages: heterogeneous\nself-supervised pretraining, text-anchored embedding alignment, and\ninstruction-aware retriever fine-tuning, enabling it to generalize across\nvaried retrieval contexts. This framework is highly scalable, with a BERT-based\nversion and a UniHGKR-7B version trained on large language models. Also, we\nintroduce CompMix-IR, the first native heterogeneous knowledge retrieval\nbenchmark. It includes two retrieval scenarios with various instructions, over\n9,400 question-answer (QA) pairs, and a corpus of 10 million entries, covering\nfour different types of data. Extensive experiments show that UniHGKR\nconsistently outperforms state-of-the-art methods on CompMix-IR, achieving up\nto 6.36% and 54.23% relative improvements in two scenarios, respectively.\nFinally, by equipping our retriever for open-domain heterogeneous QA systems,\nwe achieve a new state-of-the-art result on the popular ConvMix task, with an\nabsolute improvement of up to 5.90 points.\n","authors":["Dehai Min","Zhiyang Xu","Guilin Qi","Lifu Huang","Chenyu You"],"pdf_url":"https://arxiv.org/pdf/2410.20163v2.pdf","comment":"NAACL 2025, Main, Long Paper"},{"id":"http://arxiv.org/abs/2502.07315v1","updated":"2025-02-11T07:25:57Z","published":"2025-02-11T07:25:57Z","title":"Prompt-Based Document Modifications In Ranking Competitions","summary":"  We study prompting-based approaches with Large Language Models (LLMs) for\nmodifying documents so as to promote their ranking in a competitive search\nsetting. Our methods are inspired by prior work on leveraging LLMs as rankers.\nWe evaluate our approach by deploying it as a bot in previous ranking\ncompetitions and in competitions we organized. Our findings demonstrate that\nour approach effectively improves document ranking while preserving high levels\nof faithfulness to the original content and maintaining overall document\nquality.\n","authors":["Niv Bardas","Tommy Mordo","Oren Kurland","Moshe Tennenholtz","Gal Zur"],"pdf_url":"https://arxiv.org/pdf/2502.07315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05523v2","updated":"2025-02-11T07:21:41Z","published":"2025-02-08T11:05:22Z","title":"Adaptive Domain Scaling for Personalized Sequential Modeling in\n  Recommenders","summary":"  Users generally exhibit complex behavioral patterns and diverse intentions in\nmultiple business scenarios of super applications like Douyin, presenting great\nchallenges to current industrial multi-domain recommenders. To mitigate the\ndiscrepancies across diverse domains, researches and industrial practices\ngenerally emphasize sophisticated network structures to accomodate diverse data\ndistributions, while neglecting the inherent understanding of user behavioral\nsequence from the multi-domain perspective. In this paper, we present Adaptive\nDomain Scaling (ADS) model, which comprehensively enhances the personalization\ncapability in target-aware sequence modeling across multiple domains.\nSpecifically, ADS comprises of two major modules, including personalized\nsequence representation generation (PSRG) and personalized candidate\nrepresentation generation (PCRG). The modules contribute to the tailored\nmulti-domain learning by dynamically learning both the user behavioral sequence\nitem representation and the candidate target item representation under\ndifferent domains, facilitating adaptive user intention understanding.\nExperiments are performed on both a public dataset and two billion-scaled\nindustrial datasets, and the extensive results verify the high effectiveness\nand compatibility of ADS. Besides, we conduct online experiments on two\ninfluential business scenarios including Douyin Advertisement Platform and\nDouyin E-commerce Service Platform, both of which show substantial business\nimprovements. Currently, ADS has been fully deployed in many recommendation\nservices at ByteDance, serving billions of users.\n","authors":["Zheng Chai","Hui Lu","Di Chen","Qin Ren","Yuchao Zheng","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.05523v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07307v1","updated":"2025-02-11T07:09:49Z","published":"2025-02-11T07:09:49Z","title":"CreAgent: Towards Long-Term Evaluation of Recommender System under\n  Platform-Creator Information Asymmetry","summary":"  Ensuring the long-term sustainability of recommender systems (RS) emerges as\na crucial issue. Traditional offline evaluation methods for RS typically focus\non immediate user feedback, such as clicks, but they often neglect the\nlong-term impact of content creators. On real-world content platforms, creators\ncan strategically produce and upload new items based on user feedback and\npreference trends. While previous studies have attempted to model creator\nbehavior, they often overlook the role of information asymmetry. This asymmetry\narises because creators primarily have access to feedback on the items they\nproduce, while platforms possess data on the entire spectrum of user feedback.\nCurrent RS simulators, however, fail to account for this asymmetry, leading to\ninaccurate long-term evaluations. To address this gap, we propose CreAgent, a\nLarge Language Model (LLM)-empowered creator simulation agent. By incorporating\ngame theory's belief mechanism and the fast-and-slow thinking framework,\nCreAgent effectively simulates creator behavior under conditions of information\nasymmetry. Additionally, we enhance CreAgent's simulation ability by\nfine-tuning it using Proximal Policy Optimization (PPO). Our credibility\nvalidation experiments show that CreAgent aligns well with the behaviors\nbetween real-world platform and creator, thus improving the reliability of\nlong-term RS evaluations. Moreover, through the simulation of RS involving\nCreAgents, we can explore how fairness- and diversity-aware RS algorithms\ncontribute to better long-term performance for various stakeholders. CreAgent\nand the simulation platform are publicly available at\nhttps://github.com/shawnye2000/CreAgent.\n","authors":["Xiaopeng Ye","Chen Xu","Zhongxiang Sun","Jun Xu","Gang Wang","Zhenhua Dong","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2502.07307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07303v1","updated":"2025-02-11T07:01:19Z","published":"2025-02-11T07:01:19Z","title":"Flow Matching for Collaborative Filtering","summary":"  Generative models have shown great promise in collaborative filtering by\ncapturing the underlying distribution of user interests and preferences.\nHowever, existing approaches struggle with inaccurate posterior approximations\nand misalignment with the discrete nature of recommendation data, limiting\ntheir expressiveness and real-world performance. To address these limitations,\nwe propose FlowCF, a novel flow-based recommendation system leveraging flow\nmatching for collaborative filtering. We tailor flow matching to the unique\nchallenges in recommendation through two key innovations: (1) a behavior-guided\nprior that aligns with user behavior patterns to handle the sparse and\nheterogeneous user-item interactions, and (2) a discrete flow framework to\npreserve the binary nature of implicit feedback while maintaining the benefits\nof flow matching, such as stable training and efficient inference. Extensive\nexperiments demonstrate that FlowCF achieves state-of-the-art recommendation\naccuracy across various datasets with the fastest inference speed, making it a\ncompelling approach for real-world recommender systems.\n","authors":["Chengkai Liu","Yangtian Zhang","Jianling Wang","Rex Ying","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2502.07303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09401v6","updated":"2025-02-11T06:26:34Z","published":"2023-10-13T20:53:50Z","title":"CROWN: A Novel Approach to Comprehending Users' Preferences for Accurate\n  Personalized News Recommendation","summary":"  Personalized news recommendation aims to assist users in finding news\narticles that align with their interests, which plays a pivotal role in\nmitigating users' information overload problem. Although many recent works have\nbeen studied for better personalized news recommendation, the following\nchallenges should be explored more: (C1) Comprehending manifold intents coupled\nwithin a news article, (C2) Differentiating varying post-read preferences of\nnews articles, and (C3) Addressing the cold-start user problem. To tackle the\naforementioned challenges together, in this paper, we propose a novel\npersonalized news recommendation framework (CROWN) that employs (1)\ncategory-guided intent disentanglement for (C1), (2) consistency-based news\nrepresentation for (C2), and (3) GNN-enhanced hybrid user representation for\n(C3). Furthermore, we incorporate a category prediction into the training\nprocess of CROWN as an auxiliary task, which provides supplementary supervisory\nsignals to enhance intent disentanglement. Extensive experiments on two\nreal-world datasets reveal that (1) CROWN provides consistent performance\nimprovements over ten state-of-the-art news recommendation methods and (2) the\nproposed strategies significantly improve the accuracy of CROWN.\n","authors":["Yunyong Ko","Seongeun Ryu","Sang-Wook Kim"],"pdf_url":"https://arxiv.org/pdf/2310.09401v6.pdf","comment":"11 pages, 7 figures, 9 tables, the ACM Web Conference (WWW) 2025"},{"id":"http://arxiv.org/abs/2502.06101v2","updated":"2025-02-11T05:53:00Z","published":"2025-02-10T02:15:12Z","title":"RALLRec: Improving Retrieval Augmented Large Language Model\n  Recommendation with Representation Learning","summary":"  Large Language Models (LLMs) have been integrated into recommendation systems\nto enhance user behavior comprehension. The Retrieval Augmented Generation\n(RAG) technique is further incorporated into these systems to retrieve more\nrelevant items and improve system performance. However, existing RAG methods\nrely primarily on textual semantics and often fail to incorporate the most\nrelevant items, limiting the effectiveness of the systems.\n  In this paper, we propose Representation learning for retrieval-Augmented\nLarge Language model Recommendation (RALLRec). Specifically, we enhance textual\nsemantics by prompting LLMs to generate more detailed item descriptions,\nfollowed by joint representation learning of textual and collaborative\nsemantics, which are extracted by the LLM and recommendation models,\nrespectively. Considering the potential time-varying characteristics of user\ninterest, a simple yet effective reranking method is further introduced to\ncapture the dynamics of user preference. We conducted extensive experiments on\nthree real-world datasets, and the evaluation results validated the\neffectiveness of our method. Code is made public at\nhttps://github.com/JianXu95/RALLRec.\n","authors":["Jian Xu","Sichun Luo","Xiangyu Chen","Haoming Huang","Hanxu Hou","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2502.06101v2.pdf","comment":"Accepted by TheWebConf'25 (WWW'25) as a Short Paper"},{"id":"http://arxiv.org/abs/2502.07219v1","updated":"2025-02-11T03:25:42Z","published":"2025-02-11T03:25:42Z","title":"DOGR: Leveraging Document-Oriented Contrastive Learning in Generative\n  Retrieval","summary":"  Generative retrieval constitutes an innovative approach in in- formation\nretrieval, leveraging generative language models (LM) to generate a ranked list\nof document identifiers (do- cid) for a given query. It simplifies the\nretrieval pipeline by replacing the large external index with model parameters.\nHowever, existing works merely learned the relationship be- tween queries and\ndocument identifiers, which is unable to directly represent the relevance\nbetween queries and docu- ments. To address the above problem, we propose a\nnovel and general generative retrieval framework, namely Leverag- ing\nDocument-Oriented Contrastive Learning in Generative Retrieval (DOGR), which\nleverages contrastive learning to improve generative retrieval tasks. It adopts\na two-stage learn- ing strategy that captures the relationship between queries\nand documents comprehensively through direct interactions. Furthermore,\nnegative sampling methods and correspond- ing contrastive learning objectives\nare implemented to en- hance the learning of semantic representations, thereby\npro- moting a thorough comprehension of the relationship be- tween queries and\ndocuments. Experimental results demon- strate that DOGR achieves\nstate-of-the-art performance com- pared to existing generative retrieval\nmethods on two public benchmark datasets. Further experiments have shown that\nour framework is generally effective for common identifier con- struction\ntechniques.\n","authors":["Penghao Lu","Xin Dong","Yuansheng Zhou","Lei Cheng","Chuan Yuan","Linjian Mo"],"pdf_url":"https://arxiv.org/pdf/2502.07219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07972v1","updated":"2025-02-11T21:36:31Z","published":"2025-02-11T21:36:31Z","title":"Training Sparse Mixture Of Experts Text Embedding Models","summary":"  Transformer-based text embedding models have improved their performance on\nbenchmarks like MIRACL and BEIR by increasing their parameter counts. However,\nthis scaling approach introduces significant deployment challenges, including\nincreased inference latency and memory usage. These challenges are particularly\nsevere in retrieval-augmented generation (RAG) applications, where large\nmodels' increased memory requirements constrain dataset ingestion capacity, and\ntheir higher latency directly impacts query-time performance. While causal\nlanguage models have addressed similar efficiency challenges using Mixture of\nExperts (MoE) architectures, this approach hasn't been successfully adapted to\nthe general text embedding setting. In this paper, we introduce Nomic Embed v2,\nthe first general purpose MoE text embedding model. Our model outperforms\nmodels in the same parameter class on both monolingual and multilingual\nbenchmarks while also maintaining competitive performance with models twice its\nsize. We open-source all code, models, and evaluation data to ensure full\nreproducibility of our training pipeline.\n","authors":["Zach Nussbaum","Brandon Duderstadt"],"pdf_url":"https://arxiv.org/pdf/2502.07972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07971v1","updated":"2025-02-11T21:35:13Z","published":"2025-02-11T21:35:13Z","title":"ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval","summary":"  Document retrieval is a core component of question-answering systems, as it\nenables conditioning answer generation on new and large-scale corpora. While\neffective, the standard practice of encoding documents into high-dimensional\nembeddings for similarity search entails large memory and compute footprints,\nand also makes it hard to inspect the inner workings of the system. In this\npaper, we propose a tree-based method for organizing and representing reference\ndocuments at various granular levels, which offers the flexibility to balance\ncost and utility, and eases the inspection of the corpus content and retrieval\noperations. Our method, called ReTreever, jointly learns a routing function per\ninternal node of a binary tree such that query and reference documents are\nassigned to similar tree branches, hence directly optimizing for retrieval\nperformance. Our evaluations show that ReTreever generally preserves full\nrepresentation accuracy. Its hierarchical structure further provides strong\ncoarse representations and enhances transparency by indirectly learning\nmeaningful semantic groupings. Among hierarchical retrieval methods, ReTreever\nachieves the best retrieval accuracy at the lowest latency, proving that this\nfamily of techniques can be viable in practical applications.\n","authors":["Shubham Gupta","Zichao Li","Tianyi Chen","Cem Subakan","Siva Reddy","Perouz Taslakian","Valentina Zantedeschi"],"pdf_url":"https://arxiv.org/pdf/2502.07971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03993v2","updated":"2025-02-11T21:10:32Z","published":"2024-03-06T19:08:28Z","title":"Personalized Negative Reservoir for Incremental Learning in Recommender\n  Systems","summary":"  Recommender systems have become an integral part of online platforms. Every\nday the volume of training data is expanding and the number of user\ninteractions is constantly increasing. The exploration of larger and more\nexpressive models has become a necessary pursuit to improve user experience.\nHowever, this progression carries with it an increased computational burden. In\ncommercial settings, once a recommendation system model has been trained and\ndeployed it typically needs to be updated frequently as new client data arrive.\nCumulatively, the mounting volume of data is guaranteed to eventually make full\nbatch retraining of the model from scratch computationally infeasible. Naively\nfine-tuning solely on the new data runs into the well-documented problem of\ncatastrophic forgetting. Despite the fact that negative sampling is a crucial\npart of training with implicit feedback, no specialized technique exists that\nis tailored to the incremental learning framework. In this work, we propose a\npersonalized negative reservoir strategy, which is used to obtain negative\nsamples for the standard triplet loss of graph-based recommendation systems.\nOur technique balances alleviation of forgetting with plasticity by encouraging\nthe model to remember stable user preferences and selectively forget when user\ninterests change. We derive the mathematical formulation of a negative sampler\nto populate and update the reservoir. We integrate our design in three SOTA and\ncommonly used incremental recommendation models. We show that these concrete\nrealizations of our negative reservoir framework achieve state-of-the-art\nresults for standard benchmarks using multiple top-k evaluation metrics.\n","authors":["Antonios Valkanas","Yuening Wang","Yingxue Zhang","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2403.03993v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.07783v1","updated":"2025-02-11T18:59:57Z","published":"2025-02-11T18:59:57Z","title":"Curvature Tuning: Provable Training-free Model Steering From a Single\n  Parameter","summary":"  The scaling of model size and data size has reshaped the paradigm of AI. As a\nresult, the common protocol to leverage the latest models is to steer them\ntowards a specific downstream task of interest through {\\em fine-tuning}.\nDespite its importance, the main methods for fine-tuning remain limited to full\nor low-rank adapters--containing countless hyper-parameters and lacking\ninterpretability. In this paper, we take a step back and demonstrate how novel\nand explainable post-training steering solutions can be derived theoretically\nfrom {\\em spline operators}, a rich mathematical framing of Deep Networks that\nwas recently developed. Our method--coined \\textbf{Curvature Tuning (CT)}--has\na single parameter that provably modulates the curvature of the model's\ndecision boundary henceforth allowing training-free steering. This makes CT\nboth more efficient and interpretable than conventional fine-tuning methods. We\nempirically validate its effectiveness in improving generalization and\nrobustness of pretrained models. For example, CT improves out-of-distribution\ntransfer performances of ResNet-18/50 by 2.57\\%/1.74\\% across seventeen\ndownstream datasets, and improves RobustBench robust accuracy by\n11.76\\%/348.44\\%. Additionally, we apply CT to ReLU-based Swin-T/S, improving\ntheir generalization on nine downstream datasets by 2.43\\%/3.33\\%. Our code is\navailable at\n\\href{https://github.com/Leon-Leyang/curvature-tuning}{https://github.com/Leon-Leyang/curvature-tuning}.\n","authors":["Leyang Hu","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2502.07783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02749v3","updated":"2025-02-11T18:59:47Z","published":"2024-10-03T17:57:22Z","title":"Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis","summary":"  Software engineers mainly write code by editing existing programs. In\ncontrast, language models (LMs) autoregressively synthesize programs in a\nsingle pass. One explanation for this is the scarcity of sequential edit data.\nWhile high-quality instruction data for code synthesis is scarce, edit data for\nsynthesis is even scarcer. To fill this gap, we develop a synthetic data\ngeneration algorithm called LintSeq. This algorithm refactors programs into\nsequences of synthetic edits by using a linter to procedurally sample across\ninterdependent lines of source code. Synthetic edits sampled with LintSeq\nreflect the syntax and semantics of their programming language. To test the\nalgorithm, we use it to refactor a dataset of instruction + program pairs into\ninstruction + program-diff-sequence tuples. Then, we fine-tune a series of\nsmaller LMs ranging from 2.6B to 14B parameters on both the re-factored and\noriginal versions of this dataset. We perform comprehensive evaluations\ncomparing edit sequence code LMs against baselines on HumanEval, MBPP(+),\nCodeContests, DS-1000, and BigCodeBench. We show that models fine-tuned to\niteratively synthesize code match or outperform baselines on pass@1, and\nexhibit better scaling across higher pass@k as a function of total test-time\nFLOPs. Finally, we also pretrain our own tiny LMs for code understanding. We\nshow that fine-tuning these models to synthesize code edit-by-edit results in\nstrong performance on HumanEval and MBPP(+) compared to existing code language\nmodels of similar scale such as CodeT5+, AlphaCode, and Codex.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2410.02749v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07780v1","updated":"2025-02-11T18:59:35Z","published":"2025-02-11T18:59:35Z","title":"DarwinLM: Evolutionary Structured Pruning of Large Language Models","summary":"  Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training.\n","authors":["Shengkun Tang","Oliver Sieberling","Eldar Kurtic","Zhiqiang Shen","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2502.07780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07776v1","updated":"2025-02-11T18:58:04Z","published":"2025-02-11T18:58:04Z","title":"Auditing Prompt Caching in Language Model APIs","summary":"  Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.\n","authors":["Chenchen Gu","Xiang Lisa Li","Rohith Kuditipudi","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2502.07776v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.07774v1","updated":"2025-02-11T18:57:18Z","published":"2025-02-11T18:57:18Z","title":"Optimistic Interior Point Methods for Sequential Hypothesis Testing by\n  Betting","summary":"  The technique of \"testing by betting\" frames nonparametric sequential\nhypothesis testing as a multiple-round game, where a player bets on future\nobservations that arrive in a streaming fashion, accumulates wealth that\nquantifies evidence against the null hypothesis, and rejects the null once the\nwealth exceeds a specified threshold while controlling the false positive\nerror. Designing an online learning algorithm that achieves a small regret in\nthe game can help rapidly accumulate the bettor's wealth, which in turn can\nshorten the time to reject the null hypothesis under the alternative $H_1$.\nHowever, many of the existing works employ the Online Newton Step (ONS) to\nupdate within a halved decision space to avoid a gradient explosion issue,\nwhich is potentially conservative for rapid wealth accumulation. In this paper,\nwe introduce a novel strategy utilizing interior-point methods in optimization\nthat allows updates across the entire interior of the decision space without\nthe risk of gradient explosion. Our approach not only maintains strong\nstatistical guarantees but also facilitates faster null hypothesis rejection in\ncritical scenarios, overcoming the limitations of existing approaches.\n","authors":["Can Chen","Jun-Kun Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07771v1","updated":"2025-02-11T18:55:57Z","published":"2025-02-11T18:55:57Z","title":"Breaking Down Bias: On The Limits of Generalizable Pruning Strategies","summary":"  We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case.\n","authors":["Sibo Ma","Alejandro Salinas","Peter Henderson","Julian Nyarko"],"pdf_url":"https://arxiv.org/pdf/2502.07771v1.pdf","comment":"28 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2502.06774v2","updated":"2025-02-11T18:54:30Z","published":"2025-02-10T18:52:22Z","title":"ENFORCE: Exact Nonlinear Constrained Learning with Adaptive-depth Neural\n  Projection","summary":"  Ensuring neural networks adhere to domain-specific constraints is crucial for\naddressing safety and ethical concerns while also enhancing prediction\naccuracy. Despite the nonlinear nature of most real-world tasks, existing\nmethods are predominantly limited to affine or convex constraints. We introduce\nENFORCE, a neural network architecture that guarantees predictions to satisfy\nnonlinear constraints exactly. ENFORCE is trained with standard unconstrained\ngradient-based optimizers (e.g., Adam) and leverages autodifferentiation and\nlocal neural projections to enforce any $\\mathcal{C}^1$ constraint to arbitrary\ntolerance $\\epsilon$. We build an adaptive-depth neural projection (AdaNP)\nmodule that dynamically adjusts its complexity to suit the specific problem and\nthe required tolerance levels. ENFORCE guarantees satisfaction of equality\nconstraints that are nonlinear in both inputs and outputs of the neural network\nwith minimal (and adjustable) computational cost.\n","authors":["Giacomo Lastrucci","Artur M. Schweidtmann"],"pdf_url":"https://arxiv.org/pdf/2502.06774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07764v1","updated":"2025-02-11T18:47:53Z","published":"2025-02-11T18:47:53Z","title":"Polynomial-Time Approximability of Constrained Reinforcement Learning","summary":"  We study the computational complexity of approximating general constrained\nMarkov decision processes. Our primary contribution is the design of a\npolynomial time $(0,\\epsilon)$-additive bicriteria approximation algorithm for\nfinding optimal constrained policies across a broad class of recursively\ncomputable constraints, including almost-sure, chance, expectation, and their\nanytime variants. Matching lower bounds imply our approximation guarantees are\noptimal so long as $P \\neq NP$. The generality of our approach results in\nanswers to several long-standing open complexity questions in the constrained\nreinforcement learning literature. Specifically, we are the first to prove\npolynomial-time approximability for the following settings: policies under\nchance constraints, deterministic policies under multiple expectation\nconstraints, policies under non-homogeneous constraints (i.e., constraints of\ndifferent types), and policies under constraints for continuous-state\nprocesses.\n","authors":["Jeremy McMahan"],"pdf_url":"https://arxiv.org/pdf/2502.07764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19392v2","updated":"2025-02-11T18:45:12Z","published":"2025-01-31T18:47:42Z","title":"Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models","summary":"  Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.\n","authors":["Alina Shutova","Vladimir Malinovskii","Vage Egiazarian","Denis Kuznedelev","Denis Mazur","Nikita Surkov","Ivan Ermakov","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2501.19392v2.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2407.10366v2","updated":"2025-02-11T18:44:46Z","published":"2024-07-15T00:13:53Z","title":"Accessing Vision Foundation Models via ImageNet-1K","summary":"  Vision foundation models are renowned for the generalization ability due to\nmassive training data. Nevertheless, they demand tremendous training resources,\nand the training data is often inaccessible, e.g., CLIP, DINOv2, posing great\nchallenges to developing derivatives that could facilitate the research. In\nthis work, we offer a very simple and general solution, named \\textit{Proteus},\nto distill foundation models into smaller equivalents on ImageNet-1K without\naccess to the original training data. Specifically, we remove the designs from\nconventional knowledge distillation settings that result in dataset bias and\npresent three levels of training objectives, i.e., token, patch, and feature,\nto maximize the efficacy of knowledge transfer. In this manner, Proteus is\ntrained at ImageNet-level costs with surprising ability, facilitating the\naccessibility of training foundation models for the broader research community.\nWhen leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the\nperformance of the Oracle method DINOv2-L/14 (142M training data) across 19\nbenchmarks and outperforms other vision foundation models including CLIP-L/14\n(400M), OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M) with a significantly\nsmaller training set of 1.2M images.\n","authors":["Yitian Zhang","Xu Ma","Yue Bai","Huan Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2407.10366v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.07760v1","updated":"2025-02-11T18:43:07Z","published":"2025-02-11T18:43:07Z","title":"Scalable Fingerprinting of Large Language Models","summary":"  Model fingerprinting has emerged as a powerful tool for model owners to\nidentify their shared model given API access. However, to lower false discovery\nrate, fight fingerprint leakage, and defend against coalitions of model users\nattempting to bypass detection, we argue that {\\em scalability} is critical,\ni.e., scaling up the number of fingerprints one can embed into a model. Hence,\nwe pose scalability as a crucial requirement for fingerprinting schemes. We\nexperiment with fingerprint design at a scale significantly larger than\npreviously considered, and introduce a new method, dubbed Perinucleus sampling,\nto generate scalable, persistent, and harmless fingerprints. We demonstrate\nthat this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two\norders of magnitude more than existing schemes -- without degrading the model's\nutility. Our inserted fingerprints persist even after supervised fine-tuning on\nstandard post-training data. We further address security risks for\nfingerprinting, and theoretically and empirically show how a scalable\nfingerprinting scheme like ours can mitigate these risks.\n","authors":["Anshul Nasery","Jonathan Hayase","Creston Brooks","Peiyao Sheng","Himanshu Tyagi","Pramod Viswanath","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2502.07760v1.pdf","comment":"23 pages 15 figures"},{"id":"http://arxiv.org/abs/2502.07758v1","updated":"2025-02-11T18:38:02Z","published":"2025-02-11T18:38:02Z","title":"Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras","summary":"  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n","authors":["Nektarios A. Valous","Eckhard Hitzer","Dragoş Duşe","Rodrigo Rojas Moraleda","Ferdinand Popp","Meggy Suarez-Carmona","Anna Berthel","Ismini Papageorgiou","Carlo Fremd","Alexander Rölle","Christina C. Westhoff","Bénédicte Lenoir","Niels Halama","Inka Zörnig","Dirk Jäger"],"pdf_url":"https://arxiv.org/pdf/2502.07758v1.pdf","comment":"24 pages, 18 figures, 14 tables"},{"id":"http://arxiv.org/abs/2502.07752v1","updated":"2025-02-11T18:27:19Z","published":"2025-02-11T18:27:19Z","title":"Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension","summary":"  Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory.\n","authors":["Wenbo Gong","Meyer Scetbon","Chao Ma","Edward Meeds"],"pdf_url":"https://arxiv.org/pdf/2502.07752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07750v1","updated":"2025-02-11T18:25:48Z","published":"2025-02-11T18:25:48Z","title":"PFedDST: Personalized Federated Learning with Decentralized Selection\n  Training","summary":"  Distributed Learning (DL) enables the training of machine learning models\nacross multiple devices, yet it faces challenges like non-IID data\ndistributions and device capability disparities, which can impede training\nefficiency. Communication bottlenecks further complicate traditional Federated\nLearning (FL) setups. To mitigate these issues, we introduce the Personalized\nFederated Learning with Decentralized Selection Training (PFedDST) framework.\nPFedDST enhances model training by allowing devices to strategically evaluate\nand select peers based on a comprehensive communication score. This score\nintegrates loss, task similarity, and selection frequency, ensuring optimal\npeer connections. This selection strategy is tailored to increase local\npersonalization and promote beneficial peer collaborations to strengthen the\nstability and efficiency of the training process. Our experiments demonstrate\nthat PFedDST not only enhances model accuracy but also accelerates convergence.\nThis approach outperforms state-of-the-art methods in handling data\nheterogeneity, delivering both faster and more effective training in diverse\nand decentralized systems.\n","authors":["Mengchen Fan","Keren Li","Tianyun Zhang","Qing Tian","Baocheng Geng"],"pdf_url":"https://arxiv.org/pdf/2502.07750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07749v1","updated":"2025-02-11T18:25:14Z","published":"2025-02-11T18:25:14Z","title":"Whole-Genome Phenotype Prediction with Machine Learning: Open Problems\n  in Bacterial Genomics","summary":"  How can we identify causal genetic mechanisms that govern bacterial traits?\nInitial efforts entrusting machine learning models to handle the task of\npredicting phenotype from genotype return high accuracy scores. However,\nattempts to extract any meaning from the predictive models are found to be\ncorrupted by falsely identified \"causal\" features. Relying solely on pattern\nrecognition and correlations is unreliable, significantly so in bacterial\ngenomics settings where high-dimensionality and spurious associations are the\nnorm. Though it is not yet clear whether we can overcome this hurdle,\nsignificant efforts are being made towards discovering potential high-risk\nbacterial genetic variants. In view of this, we set up open problems\nsurrounding phenotype prediction from bacterial whole-genome datasets and\nextending those to learning causal effects, and discuss challenges that impact\nthe reliability of a machine's decision-making when faced with datasets of this\nnature.\n","authors":["Tamsin James","Ben Williamson","Peter Tino","Nicole Wheeler"],"pdf_url":"https://arxiv.org/pdf/2502.07749v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2402.08096v3","updated":"2025-02-11T18:25:07Z","published":"2024-02-12T22:32:12Z","title":"An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation\n  during Multi-stage Fine-tuning","summary":"  Incrementally fine-tuning foundational models on new tasks or domains is now\nthe de facto approach in NLP. A known pitfall of this approach is the\n\\emph{catastrophic forgetting} of prior knowledge that happens during\nfine-tuning. A common approach to alleviate such forgetting is to rehearse\nsamples from prior tasks during fine-tuning. Several existing works assume a\nfixed memory buffer to store prior task examples, while relying on inferences\n(forward passes) with the model at hand for choosing examples for rehearsal\nfrom the buffer. However, given the increasing computational cost of model\ninference, and decreasing cost of data storage, we focus on the setting to\nrehearse samples with a fixed computational budget instead of a fixed memory\nbudget. We propose a sampling scheme, \\texttt{\\bf mix-cd}, that prioritizes\nrehearsal of ``collateral damage'' samples, which are samples predicted\ncorrectly by the prior model but forgotten by the incrementally tuned one. The\ncrux of our scheme is a procedure to efficiently estimate the density of\ncollateral damage samples without incurring additional model inferences. Our\napproach is computationally efficient, easy to implement, and outperforms\nseveral leading continual learning methods in compute-constrained settings. All\nthe code will be publicly available at\nhttps://github.com/jybai/mix-cd-rehearsal.\n","authors":["Andrew Bai","Chih-Kuan Yeh","Cho-Jui Hsieh","Ankur Taly"],"pdf_url":"https://arxiv.org/pdf/2402.08096v3.pdf","comment":"13 pages, 9 figures. Published in NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2402.09401v2","updated":"2025-02-11T18:18:59Z","published":"2024-02-14T18:58:40Z","title":"Reinforcement Learning from Human Feedback with Active Queries","summary":"  Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method.\n","authors":["Kaixuan Ji","Jiafan He","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2402.09401v2.pdf","comment":"28 pages, 1 figure, 4 table"},{"id":"http://arxiv.org/abs/2502.07746v1","updated":"2025-02-11T18:13:29Z","published":"2025-02-11T18:13:29Z","title":"HiPoNet: A Topology-Preserving Multi-View Neural Network For High\n  Dimensional Point Cloud and Single-Cell Data","summary":"  In this paper, we propose HiPoNet, an end-to-end differentiable neural\nnetwork for regression, classification, and representation learning on\nhigh-dimensional point clouds. Single-cell data can have high dimensionality\nexceeding the capabilities of existing methods point cloud tailored for 3D\ndata. Moreover, modern single-cell and spatial experiments now yield entire\ncohorts of datasets (i.e. one on every patient), necessitating models that can\nprocess large, high-dimensional point clouds at scale. Most current approaches\nbuild a single nearest-neighbor graph, discarding important geometric\ninformation. In contrast, HiPoNet forms higher-order simplicial complexes\nthrough learnable feature reweighting, generating multiple data views that\ndisentangle distinct biological processes. It then employs simplicial wavelet\ntransforms to extract multi-scale features - capturing both local and global\ntopology. We empirically show that these components preserve topological\ninformation in the learned representations, and that HiPoNet significantly\noutperforms state-of-the-art point-cloud and graph-based models on single cell.\nWe also show an application of HiPoNet on spatial transcriptomics datasets\nusing spatial co-ordinates as one of the views. Overall, HiPoNet offers a\nrobust and scalable solution for high-dimensional data analysis.\n","authors":["Siddharth Viswanath","Hiren Madhu","Dhananjay Bhaskar","Jake Kovalic","Dave Johnson","Rex Ying","Christopher Tape","Ian Adelstein","Michael Perlmutter","Smita Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2502.07746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01985v4","updated":"2025-02-11T18:09:35Z","published":"2024-09-03T15:26:51Z","title":"UNSURE: self-supervised learning with Unknown Noise level and Stein's\n  Unbiased Risk Estimate","summary":"  Recently, many self-supervised learning methods for image reconstruction have\nbeen proposed that can learn from noisy data alone, bypassing the need for\nground-truth references. Most existing methods cluster around two classes: i)\nStein's Unbiased Risk Estimate (SURE) and similar approaches that assume full\nknowledge of the noise distribution, and ii) Noise2Self and similar\ncross-validation methods that require very mild knowledge about the noise\ndistribution. The first class of methods tends to be impractical, as the noise\nlevel is often unknown in real-world applications, and the second class is\noften suboptimal compared to supervised learning. In this paper, we provide a\ntheoretical framework that characterizes this expressivity-robustness trade-off\nand propose a new approach based on SURE, but unlike the standard SURE, does\nnot require knowledge about the noise level. Throughout a series of\nexperiments, we show that the proposed estimator outperforms other existing\nself-supervised methods on various imaging inverse problems.\n","authors":["Julián Tachella","Mike Davies","Laurent Jacques"],"pdf_url":"https://arxiv.org/pdf/2409.01985v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04463v2","updated":"2025-02-11T18:06:02Z","published":"2025-02-06T19:18:16Z","title":"Training Language Models to Reason Efficiently","summary":"  Scaling model size and training data has led to great advances in the\nperformance of Large Language Models (LLMs). However, the diminishing returns\nof this approach necessitate alternative methods to improve model capabilities,\nparticularly in tasks requiring advanced reasoning. Large reasoning models,\nwhich leverage long chain-of-thoughts, bring unprecedented breakthroughs in\nproblem-solving capabilities but at a substantial deployment cost associated to\nlonger generations. Reducing inference costs is crucial for the economic\nfeasibility, user experience, and environmental sustainability of these models.\n  In this work, we propose to train large reasoning models to reason\nefficiently. More precisely, we use reinforcement learning (RL) to train\nreasoning models to dynamically allocate inference-time compute based on task\ncomplexity. Our method incentivizes models to minimize unnecessary\ncomputational overhead while maintaining accuracy, thereby achieving\nsubstantial efficiency gains. It enables the derivation of a family of\nreasoning models with varying efficiency levels, controlled via a single\nhyperparameter. Experiments on two open-weight large reasoning models\ndemonstrate significant reductions in inference cost while preserving most of\nthe accuracy.\n","authors":["Daman Arora","Andrea Zanette"],"pdf_url":"https://arxiv.org/pdf/2502.04463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07741v1","updated":"2025-02-11T18:05:54Z","published":"2025-02-11T18:05:54Z","title":"Advancing climate model interpretability: Feature attribution for Arctic\n  melt anomalies","summary":"  The focus of our work is improving the interpretability of anomalies in\nclimate models and advancing our understanding of Arctic melt dynamics. The\nArctic and Antarctic ice sheets are experiencing rapid surface melting and\nincreased freshwater runoff, contributing significantly to global sea level\nrise. Understanding the mechanisms driving snowmelt in these regions is\ncrucial. ERA5, a widely used reanalysis dataset in polar climate studies,\noffers extensive climate variables and global data assimilation. However, its\nsnowmelt model employs an energy imbalance approach that may oversimplify the\ncomplexity of surface melt. In contrast, the Glacier Energy and Mass Balance\n(GEMB) model incorporates additional physical processes, such as snow\naccumulation, firn densification, and meltwater percolation/refreezing,\nproviding a more detailed representation of surface melt dynamics. In this\nresearch, we focus on analyzing surface snowmelt dynamics of the Greenland Ice\nSheet using feature attribution for anomalous melt events in ERA5 and GEMB\nmodels. We present a novel unsupervised attribution method leveraging\ncounterfactual explanation method to analyze detected anomalies in ERA5 and\nGEMB. Our anomaly detection results are validated using MEaSUREs ground-truth\ndata, and the attributions are evaluated against established feature ranking\nmethods, including XGBoost, Shapley values, and Random Forest. Our attribution\nframework identifies the physics behind each model and the climate features\ndriving melt anomalies. These findings demonstrate the utility of our\nattribution method in enhancing the interpretability of anomalies in climate\nmodels and advancing our understanding of Arctic melt dynamics.\n","authors":["Tolulope Ale","Nicole-Jeanne Schlegel","Vandana P. Janeja"],"pdf_url":"https://arxiv.org/pdf/2502.07741v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2408.15332v2","updated":"2025-02-11T18:01:40Z","published":"2024-08-27T18:00:06Z","title":"What makes math problems hard for reinforcement learning: a case study","summary":"  Using a long-standing conjecture from combinatorial group theory, we explore,\nfrom multiple perspectives, the challenges of finding rare instances carrying\ndisproportionately high rewards. Based on lessons learned in the context\ndefined by the Andrews-Curtis conjecture, we propose algorithmic enhancements\nand a topological hardness measure with implications for a broad class of\nsearch problems. As part of our study, we also address several open\nmathematical questions. Notably, we demonstrate the length reducibility of all\nbut two presentations in the Akbulut-Kirby series (1981), and resolve various\npotential counterexamples in the Miller-Schupp series (1991), including three\ninfinite subfamilies.\n","authors":["Ali Shehper","Anibal M. Medina-Mardones","Lucas Fagan","Bartłomiej Lewandowski","Angus Gruen","Yang Qiu","Piotr Kucharski","Zhenghan Wang","Sergei Gukov"],"pdf_url":"https://arxiv.org/pdf/2408.15332v2.pdf","comment":"58 pages, 25 figures, 1 table. Try it:\n  https://github.com/shehper/AC-Solver"},{"id":"http://arxiv.org/abs/2502.07739v1","updated":"2025-02-11T17:59:35Z","published":"2025-02-11T17:59:35Z","title":"HRP: High-Rank Preheating for Superior LoRA Initialization","summary":"  This paper studies the crucial impact of initialization on the convergence\nproperties of Low-Rank Adaptation (LoRA). We theoretically demonstrate that\nrandom initialization, a widely used schema, will likely lead LoRA to random\nlow-rank results, rather than the best low-rank result. While this issue can be\nmitigated by adjusting initialization towards a well-informed direction, it\nrelies on prior knowledge of the target, which is typically unknown in\nreal-world scenarios. To approximate this well-informed initial direction, we\npropose High-Rank Preheating (HRP), which fine-tunes high-rank LoRA for a few\nsteps and uses the singular value decomposition of the preheated result as a\nsuperior initialization. HRP initialization is theory-supported to combine the\nconvergence strengths of high-rank LoRA and the generalization strengths of\nlow-rank LoRA. Extensive experiments demonstrate that HRP significantly\nenhances LoRA's effectiveness across various models and tasks, achieving\nperformance comparable to full-parameter fine-tuning and outperforming other\ninitialization strategies.\n","authors":["Yuzhu Chen","Yingjie Wang","Shi Fu","Li Shen","Yongcheng Jing","Xinmei Tian","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2502.07739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07735v1","updated":"2025-02-11T17:55:03Z","published":"2025-02-11T17:55:03Z","title":"Revisiting Non-Acyclic GFlowNets in Discrete Environments","summary":"  Generative Flow Networks (GFlowNets) are a family of generative models that\nlearn to sample objects from a given probability distribution, potentially\nknown up to a normalizing constant. Instead of working in the object space,\nGFlowNets proceed by sampling trajectories in an appropriately constructed\ndirected acyclic graph environment, greatly relying on the acyclicity of the\ngraph. In our paper, we revisit the theory that relaxes the acyclicity\nassumption and present a simpler theoretical framework for non-acyclic\nGFlowNets in discrete environments. Moreover, we provide various novel\ntheoretical insights related to training with fixed backward policies, the\nnature of flow functions, and connections between entropy-regularized RL and\nnon-acyclic GFlowNets, which naturally generalize the respective concepts and\ntheoretical results from the acyclic setting. In addition, we experimentally\nre-examine the concept of loss stability in non-acyclic GFlowNet training, as\nwell as validate our own theoretical findings.\n","authors":["Nikita Morozov","Ian Maksimov","Daniil Tiapkin","Sergey Samsonov"],"pdf_url":"https://arxiv.org/pdf/2502.07735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20562v2","updated":"2025-02-11T17:53:46Z","published":"2024-09-30T17:59:03Z","title":"SpaceMesh: A Continuous Representation for Learning Manifold Surface\n  Meshes","summary":"  Meshes are ubiquitous in visual computing and simulation, yet most existing\nmachine learning techniques represent meshes only indirectly, e.g. as the level\nset of a scalar field or deformation of a template, or as a disordered triangle\nsoup lacking local structure. This work presents a scheme to directly generate\nmanifold, polygonal meshes of complex connectivity as the output of a neural\nnetwork. Our key innovation is to define a continuous latent connectivity space\nat each mesh vertex, which implies the discrete mesh. In particular, our vertex\nembeddings generate cyclic neighbor relationships in a halfedge mesh\nrepresentation, which gives a guarantee of edge-manifoldness and the ability to\nrepresent general polygonal meshes. This representation is well-suited to\nmachine learning and stochastic optimization, without restriction on\nconnectivity or topology. We first explore the basic properties of this\nrepresentation, then use it to fit distributions of meshes from large datasets.\nThe resulting models generate diverse meshes with tessellation structure\nlearned from the dataset population, with concise details and high-quality mesh\nelements. In applications, this approach not only yields high-quality outputs\nfrom generative models, but also enables directly learning challenging geometry\nprocessing tasks such as mesh repair.\n","authors":["Tianchang Shen","Zhaoshuo Li","Marc Law","Matan Atzmon","Sanja Fidler","James Lucas","Jun Gao","Nicholas Sharp"],"pdf_url":"https://arxiv.org/pdf/2409.20562v2.pdf","comment":"published at SIGGRAPH Asia 2024"},{"id":"http://arxiv.org/abs/2502.07732v1","updated":"2025-02-11T17:51:52Z","published":"2025-02-11T17:51:52Z","title":"Economics of Sourcing Human Data","summary":"  Progress in AI has relied on human-generated data, from annotator\nmarketplaces to the wider Internet. However, the widespread use of large\nlanguage models now threatens the quality and integrity of human-generated data\non these very platforms. We argue that this issue goes beyond the immediate\nchallenge of filtering AI-generated content--it reveals deeper flaws in how\ndata collection systems are designed. Existing systems often prioritize speed,\nscale, and efficiency at the cost of intrinsic human motivation, leading to\ndeclining engagement and data quality. We propose that rethinking data\ncollection systems to align with contributors' intrinsic motivations--rather\nthan relying solely on external incentives--can help sustain high-quality data\nsourcing at scale while maintaining contributor trust and long-term\nparticipation.\n","authors":["Sebastin Santy","Prasanta Bhattacharya","Manoel Horta Ribeiro","Kelsey Allen","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2502.07732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06530v3","updated":"2025-02-11T17:49:04Z","published":"2024-10-09T04:07:20Z","title":"TopoTune : A Framework for Generalized Combinatorial Complex Neural\n  Networks","summary":"  Graph Neural Networks (GNNs) excel in learning from relational datasets,\nprocessing node and edge features in a way that preserves the symmetries of the\ngraph domain. However, many complex systems -- such as biological or social\nnetworks--involve multiway complex interactions that are more naturally\nrepresented by higher-order topological domains. The emerging field of\nTopological Deep Learning (TDL) aims to accommodate and leverage these\nhigher-order structures. Combinatorial Complex Neural Networks (CCNNs), fairly\ngeneral TDL models, have been shown to be more expressive and better performing\nthan GNNs. However, differently from the GNN ecosystem, TDL lacks a principled\nand standardized framework for easily defining new architectures, restricting\nits accessibility and applicability. To address this issue, we introduce\nGeneralized CCNNs (GCCNs), a novel simple yet powerful family of TDL models\nthat can be used to systematically transform any (graph) neural network into\nits TDL counterpart. We prove that GCCNs generalize and subsume CCNNs, while\nextensive experiments on a diverse class of GCCNs show that these architectures\nconsistently match or outperform CCNNs, often with less model complexity. In an\neffort to accelerate and democratize TDL, we introduce TopoTune, a lightweight\nsoftware for defining, building, and training GCCNs with unprecedented\nflexibility and ease.\n","authors":["Mathilde Papillon","Guillermo Bernárdez","Claudio Battiloro","Nina Miolane"],"pdf_url":"https://arxiv.org/pdf/2410.06530v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15065v2","updated":"2025-02-11T17:47:11Z","published":"2024-08-27T13:48:15Z","title":"The Benefits of Balance: From Information Projections to Variance\n  Reduction","summary":"  Data balancing across multiple modalities and sources appears in various\nforms in foundation models in machine learning and AI, e.g. in CLIP and DINO.\nWe show that data balancing across modalities and sources actually offers an\nunsuspected benefit: variance reduction. We present a non-asymptotic\nstatistical bound that quantifies this variance reduction effect and relates it\nto the eigenvalue decay of Markov operators. Furthermore, we describe how\nvarious forms of data balancing in contrastive multimodal learning and\nself-supervised clustering can be better understood, and even improved upon,\nowing to our variance reduction viewpoint.\n","authors":["Lang Liu","Ronak Mehta","Soumik Pal","Zaid Harchaoui"],"pdf_url":"https://arxiv.org/pdf/2408.15065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08281v3","updated":"2025-02-11T17:43:59Z","published":"2024-01-16T11:12:36Z","title":"The Faiss library","summary":"  Vector databases typically manage large collections of embedding vectors.\nCurrently, AI applications are growing rapidly, and so is the number of\nembeddings that need to be stored and indexed. The Faiss library is dedicated\nto vector similarity search, a core functionality of vector databases. Faiss is\na toolkit of indexing methods and related primitives used to search, cluster,\ncompress and transform vectors. This paper describes the trade-off space of\nvector search and the design principles of Faiss in terms of structure,\napproach to optimization and interfacing. We benchmark key features of the\nlibrary and discuss a few selected applications to highlight its broad\napplicability.\n","authors":["Matthijs Douze","Alexandr Guzhva","Chengqi Deng","Jeff Johnson","Gergely Szilvasy","Pierre-Emmanuel Mazaré","Maria Lomeli","Lucas Hosseini","Hervé Jégou"],"pdf_url":"https://arxiv.org/pdf/2401.08281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08731v3","updated":"2025-02-11T17:38:13Z","published":"2023-10-12T21:38:07Z","title":"Novelty Detection in Reinforcement Learning with World Models","summary":"  Reinforcement learning (RL) using world models has found significant recent\nsuccesses. However, when a sudden change to world mechanics or properties\noccurs then agent performance and reliability can dramatically decline. We\nrefer to the sudden change in visual properties or state transitions as\nnovelties. Implementing novelty detection within generated world model\nframeworks is a crucial task for protecting the agent when deployed. In this\npaper, we propose straightforward bounding approaches to incorporate novelty\ndetection into world model RL agents, by utilizing the misalignment of the\nworld model's hallucinated states and the true observed states as an anomaly\nscore. We provide effective approaches to detecting novelties in a distribution\nof transitions learned by an agent in a world model. Finally, we show the\nadvantage of our work in a novel environment compared to traditional machine\nlearning novelty detection methods as well as currently accepted RL focused\nnovelty detection algorithms.\n","authors":["Geigh Zollicoffer","Kenneth Eaton","Jonathan Balloch","Julia Kim","Wei Zhou","Robert Wright","Mark O. Riedl"],"pdf_url":"https://arxiv.org/pdf/2310.08731v3.pdf","comment":"RLC Safety 2024"},{"id":"http://arxiv.org/abs/2501.11779v2","updated":"2025-02-11T17:36:32Z","published":"2025-01-20T23:10:13Z","title":"Glinthawk: A Two-Tiered Architecture for Offline LLM Inference","summary":"  We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.\n","authors":["Pouya Hamadanian","Sadjad Fouladi"],"pdf_url":"https://arxiv.org/pdf/2501.11779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04667v2","updated":"2025-02-11T17:36:13Z","published":"2025-01-08T18:28:12Z","title":"Natural Variational Annealing for Multimodal Optimization","summary":"  We introduce a new multimodal optimization approach called Natural\nVariational Annealing (NVA) that combines the strengths of three foundational\nconcepts to simultaneously search for multiple global and local modes of\nblack-box nonconvex objectives. First, it implements a simultaneous search by\nusing variational posteriors, such as, mixtures of Gaussians. Second, it\napplies annealing to gradually trade off exploration for exploitation. Finally,\nit learns the variational search distribution using natural-gradient learning\nwhere updates resemble well-known and easy-to-implement algorithms. The three\nconcepts come together in NVA giving rise to new algorithms and also allowing\nus to incorporate \"fitness shaping\", a core concept from evolutionary\nalgorithms. We assess the quality of search on simulations and compare them to\nmethods using gradient descent and evolution strategies. We also provide an\napplication to a real-world inverse problem in planetary science.\n","authors":["Tâm Le Minh","Julyan Arbel","Thomas Möllenhoff","Mohammad Emtiyaz Khan","Florence Forbes"],"pdf_url":"https://arxiv.org/pdf/2501.04667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07721v1","updated":"2025-02-11T17:33:48Z","published":"2025-02-11T17:33:48Z","title":"TMLC-Net: Transferable Meta Label Correction for Noisy Label Learning","summary":"  The prevalence of noisy labels in real-world datasets poses a significant\nimpediment to the effective deployment of deep learning models. While\nmeta-learning strategies have emerged as a promising approach for addressing\nthis challenge, existing methods often suffer from limited transferability and\ntask-specific designs. This paper introduces TMLC-Net, a novel Transferable\nMeta-Learner for Correcting Noisy Labels, designed to overcome these\nlimitations. TMLC-Net learns a general-purpose label correction strategy that\ncan be readily applied across diverse datasets and model architectures without\nrequiring extensive retraining or fine-tuning. Our approach integrates three\ncore components: (1) Normalized Noise Perception, which captures and normalizes\ntraining dynamics to handle distribution shifts; (2) Time-Series Encoding,\nwhich models the temporal evolution of sample statistics using a recurrent\nneural network; and (3) Subclass Decoding, which predicts a corrected label\ndistribution based on the learned representations. We conduct extensive\nexperiments on benchmark datasets with various noise types and levels,\ndemonstrating that TMLC-Net consistently outperforms state-of-the-art methods\nin terms of both accuracy and robustness to label noise. Furthermore, we\nanalyze the transferability of TMLC-Net, showcasing its adaptability to new\ndatasets and noise conditions, and establishing its potential as a broadly\napplicable solution for robust deep learning in noisy environments.\n","authors":["Mengyang Li"],"pdf_url":"https://arxiv.org/pdf/2502.07721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10868v2","updated":"2025-02-11T17:31:55Z","published":"2024-10-08T11:24:59Z","title":"Large Continual Instruction Assistant","summary":"  Continual Instruction Tuning (CIT) is adopted to continually instruct Large\nModels to follow human intent data by data. It is observed that existing\ngradient update would heavily destroy the performance on previous datasets\nduring CIT process. Instead, Exponential Moving Average (EMA), owns the ability\nto trace previous parameters, which can aid in decreasing forgetting.\nNonetheless, its stable balance weight fails to deal with the ever-changing\ndatasets, leading to the out-of-balance between plasticity and stability. In\nthis paper, we propose a general continual instruction tuning framework to\naddress the challenge. Starting from the trade-off prerequisite and EMA update,\nwe propose the plasticity and stability ideal condition. Based on Taylor\nexpansion in the loss function, we find the optimal balance weight can be\nautomatically determined by the gradients and learned parameters. Therefore, we\npropose a stable-plasticity balanced coefficient to avoid knowledge confusion.\nBased on the semantic similarity of the instructions, we can determine whether\nto retrain or expand the training parameters and allocate the most suitable\nparameters for the testing instances. Extensive experiments across multiple\ncontinual instruction tuning benchmarks demonstrate that our approach not only\nenhances anti-forgetting capabilities but also significantly improves overall\ncontinual tuning performance. For example, based on LLaVA-7B, the forgetting is\nreduced from 5.42 to 1.93. Our code will be made publicly available soon.\n","authors":["Jingyang Qiao","Zhizhong Zhang","Xin Tan","Yanyun Qu","Shouhong Ding","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2410.10868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10763v2","updated":"2025-02-11T17:28:34Z","published":"2024-03-16T02:06:14Z","title":"Drago: Primal-Dual Coupled Variance Reduction for Faster\n  Distributionally Robust Optimization","summary":"  We consider the penalized distributionally robust optimization (DRO) problem\nwith a closed, convex uncertainty set, a setting that encompasses learning\nusing $f$-DRO and spectral/$L$-risk minimization. We present Drago, a\nstochastic primal-dual algorithm that combines cyclic and randomized components\nwith a carefully regularized primal update to achieve dual variance reduction.\nOwing to its design, Drago enjoys a state-of-the-art linear convergence rate on\nstrongly convex-strongly concave DRO problems with a fine-grained dependency on\nprimal and dual condition numbers. Theoretical results are supported by\nnumerical benchmarks on regression and classification tasks.\n","authors":["Ronak Mehta","Jelena Diakonikolas","Zaid Harchaoui"],"pdf_url":"https://arxiv.org/pdf/2403.10763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18922v3","updated":"2025-02-11T17:23:13Z","published":"2024-04-29T17:58:30Z","title":"DPO Meets PPO: Reinforced Token Optimization for RLHF","summary":"  In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nlarge language models, its open-source implementation is still largely\nsub-optimal. To address these issues, we introduce a framework that models RLHF\nproblems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Under this framework, we introduce an\nalgorithm Reinforced Token Optimization (\\texttt{RTO}), which learns the\ntoken-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive experiments\ndemonstrate that \\texttt{RTO} performs better than PPO and other direct\npreference learning algorithms. In particular, RTO outperforms PPO by 7.5\npoints on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code\nand models are available at\n\\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.\n","authors":["Han Zhong","Zikang Shan","Guhao Feng","Wei Xiong","Xinle Cheng","Li Zhao","Di He","Jiang Bian","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.18922v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07715v1","updated":"2025-02-11T17:15:55Z","published":"2025-02-11T17:15:55Z","title":"Near-Optimal Sample Complexity in Reward-Free Kernel-Based Reinforcement\n  Learning","summary":"  Reinforcement Learning (RL) problems are being considered under increasingly\nmore complex structures. While tabular and linear models have been thoroughly\nexplored, the analytical study of RL under nonlinear function approximation,\nespecially kernel-based models, has recently gained traction for their strong\nrepresentational capacity and theoretical tractability. In this context, we\nexamine the question of statistical efficiency in kernel-based RL within the\nreward-free RL framework, specifically asking: how many samples are required to\ndesign a near-optimal policy? Existing work addresses this question under\nrestrictive assumptions about the class of kernel functions. We first explore\nthis question by assuming a generative model, then relax this assumption at the\ncost of increasing the sample complexity by a factor of H, the length of the\nepisode. We tackle this fundamental problem using a broad class of kernels and\na simpler algorithm compared to prior work. Our approach derives new confidence\nintervals for kernel ridge regression, specific to our RL setting, which may be\nof broader applicability. We further validate our theoretical findings through\nsimulations.\n","authors":["Aya Kayal","Sattar Vakili","Laura Toni","Alberto Bernacchia"],"pdf_url":"https://arxiv.org/pdf/2502.07715v1.pdf","comment":"Accepted at AISTATS 2025"},{"id":"http://arxiv.org/abs/2409.05701v3","updated":"2025-02-11T17:14:43Z","published":"2024-09-09T15:13:56Z","title":"pFedGPA: Diffusion-based Generative Parameter Aggregation for\n  Personalized Federated Learning","summary":"  Federated Learning (FL) offers a decentralized approach to model training,\nwhere data remains local and only model parameters are shared between the\nclients and the central server. Traditional methods, such as Federated\nAveraging (FedAvg), linearly aggregate these parameters which are usually\ntrained on heterogeneous data distributions, potentially overlooking the\ncomplex, high-dimensional nature of the parameter space. This can result in\ndegraded performance of the aggregated model. While personalized FL approaches\ncan mitigate the heterogeneous data issue to some extent, the limitation of\nlinear aggregation remains unresolved. To alleviate this issue, we investigate\nthe generative approach of diffusion model and propose a novel generative\nparameter aggregation framework for personalized FL, \\texttt{pFedGPA}. In this\nframework, we deploy a diffusion model on the server to integrate the diverse\nparameter distributions and propose a parameter inversion method to efficiently\ngenerate a set of personalized parameters for each client. This inversion\nmethod transforms the uploaded parameters into a latent code, which is then\naggregated through denoising sampling to produce the final personalized\nparameters. By encoding the dependence of a client's model parameters on the\nspecific data distribution using the high-capacity diffusion model,\n\\texttt{pFedGPA} can effectively decouple the complexity of the overall\ndistribution of all clients' model parameters from the complexity of each\nindividual client's parameter distribution. Our experimental results\nconsistently demonstrate the superior performance of the proposed method across\nmultiple datasets, surpassing baseline approaches.\n","authors":["Jiahao Lai","Jiaqi Li","Jian Xu","Yanru Wu","Boshi Tang","Siqi Chen","Yongfeng Huang","Wenbo Ding","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2409.05701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17165v3","updated":"2025-02-11T17:06:45Z","published":"2023-11-28T19:01:09Z","title":"(Ir)rationality in AI: State of the Art, Research Challenges and Open\n  Questions","summary":"  The concept of rationality is central to the field of artificial\nintelligence. Whether we are seeking to simulate human reasoning, or the goal\nis to achieve bounded optimality, we generally seek to make artificial agents\nas rational as possible. Despite the centrality of the concept within AI, there\nis no unified definition of what constitutes a rational agent. This article\nprovides a survey of rationality and irrationality in artificial intelligence,\nand sets out the open questions in this area. The understanding of rationality\nin other fields has influenced its conception within artificial intelligence,\nin particular work in economics, philosophy and psychology. Focusing on the\nbehaviour of artificial agents, we consider irrational behaviours that can\nprove to be optimal in certain scenarios. Some methods have been developed to\ndeal with irrational agents, both in terms of identification and interaction,\nhowever work in this area remains limited. Methods that have up to now been\ndeveloped for other purposes, namely adversarial scenarios, may be adapted to\nsuit interactions with artificial agents. We further discuss the interplay\nbetween human and artificial agents, and the role that rationality plays within\nthis interaction; many questions remain in this area, relating to potentially\nirrational behaviour of both humans and artificial agents.\n","authors":["Olivia Macmillan-Scott","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2311.17165v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16247v2","updated":"2025-02-11T16:54:45Z","published":"2024-12-20T00:01:16Z","title":"Towards scientific discovery with dictionary learning: Extracting\n  biological concepts from microscopy foundation models","summary":"  Dictionary learning (DL) has emerged as a powerful interpretability tool for\nlarge language models. By extracting known concepts (e.g., Golden-Gate Bridge)\nfrom human-interpretable data (e.g., text), sparse DL can elucidate a model's\ninner workings. In this work, we ask if DL can also be used to discover unknown\nconcepts from less human-interpretable scientific data (e.g., cell images),\nultimately enabling modern approaches to scientific discovery. As a first step,\nwe use DL algorithms to study microscopy foundation models trained on\nmulti-cell image data, where little prior knowledge exists regarding which\nhigh-level concepts should arise. We show that sparse dictionaries indeed\nextract biologically-meaningful concepts such as cell type and genetic\nperturbation type. We also propose Iterative Codebook Feature Learning~(ICFL)\nand combine it with a pre-processing step which uses PCA whitening from a\ncontrol dataset. In our experiments, we demonstrate that both ICFL and PCA\nimprove the selectivity of extracted features compared to TopK sparse\nautoencoders.\n","authors":["Konstantin Donhauser","Kristina Ulicna","Gemma Elyse Moran","Aditya Ravuri","Kian Kenyon-Dean","Cian Eastwood","Jason Hartford"],"pdf_url":"https://arxiv.org/pdf/2412.16247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10737v3","updated":"2025-02-11T16:47:17Z","published":"2024-06-15T20:47:38Z","title":"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained\nmodels to continually changing, unseen target domains. While existing CTTA\nmethods assume structured domain changes with uniform durations, real-world\nenvironments often exhibit dynamic patterns where domains recur with varying\nfrequencies and durations. Current approaches, which adapt the same parameters\nacross different domains, struggle in such dynamic conditions-they face\nconvergence issues with brief domain exposures, risk forgetting previously\nlearned knowledge, or misapplying it to irrelevant domains. To remedy this, we\npropose DPCore, a method designed for robust performance across diverse domain\nchange patterns while ensuring computational efficiency. DPCore integrates\nthree key components: Visual Prompt Adaptation for efficient domain alignment,\na Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism\nthat intelligently adjusts existing prompts for similar domains while creating\nnew ones for substantially different domains. Extensive experiments on four\nbenchmarks demonstrate that DPCore consistently outperforms various CTTA\nmethods, achieving state-of-the-art performance in both structured and dynamic\nsettings while reducing trainable parameters by 99% and computation time by 64%\ncompared to previous approaches.\n","authors":["Yunbei Zhang","Akshay Mehra","Shuaicheng Niu","Jihun Hamm"],"pdf_url":"https://arxiv.org/pdf/2406.10737v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18768v3","updated":"2025-02-11T16:24:23Z","published":"2024-09-27T14:12:49Z","title":"Learning from Demonstration with Implicit Nonlinear Dynamics Models","summary":"  Learning from Demonstration (LfD) is a useful paradigm for training policies\nthat solve tasks involving complex motions, such as those encountered in\nrobotic manipulation. In practice, the successful application of LfD requires\novercoming error accumulation during policy execution, i.e. the problem of\ndrift due to errors compounding over time and the consequent\nout-of-distribution behaviours. Existing works seek to address this problem\nthrough scaling data collection, correcting policy errors with a\nhuman-in-the-loop, temporally ensembling policy predictions or through learning\na dynamical system model with convergence guarantees. In this work, we propose\nand validate an alternative approach to overcoming this issue. Inspired by\nreservoir computing, we develop a recurrent neural network layer that includes\na fixed nonlinear dynamical system with tunable dynamical properties for\nmodelling temporal dynamics. We validate the efficacy of our neural network\nlayer on the task of reproducing human handwriting motions using the LASA Human\nHandwriting Dataset. Through empirical experiments we demonstrate that\nincorporating our layer into existing neural network architectures addresses\nthe issue of compounding errors in LfD. Furthermore, we perform a comparative\nevaluation against existing approaches including a temporal ensemble of policy\npredictions and an Echo State Network (ESN) implementation. We find that our\napproach yields greater policy precision and robustness on the handwriting task\nwhile also generalising to multiple dynamics regimes and maintaining\ncompetitive latency scores.\n","authors":["Peter David Fagan","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2409.18768v3.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.02153v2","updated":"2025-02-11T16:23:39Z","published":"2024-12-03T04:28:14Z","title":"Revisiting the Initial Steps in Adaptive Gradient Descent Optimization","summary":"  Adaptive gradient optimization methods, such as Adam, are prevalent in\ntraining deep neural networks across diverse machine learning tasks due to\ntheir ability to achieve faster convergence. However, these methods often\nsuffer from suboptimal generalization compared to stochastic gradient descent\n(SGD) and exhibit instability, particularly when training Transformer models.\nIn this work, we show the standard initialization of the second-order moment\nestimation ($v_0 =0$) as a significant factor contributing to these\nlimitations. We introduce simple yet effective solutions: initializing the\nsecond-order moment estimation with non-zero values, using either data-driven\nor random initialization strategies. Empirical evaluations demonstrate that our\napproach not only stabilizes convergence but also enhances the final\nperformance of adaptive gradient optimizers. Furthermore, by adopting the\nproposed initialization strategies, Adam achieves performance comparable to\nmany recently proposed variants of adaptive gradient optimization methods. Our\ncode is available at https://github.com/Walleclipse/Adam_Initialization.\n","authors":["Abulikemu Abuduweili","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2412.02153v2.pdf","comment":"Conference on Parsimony and Learning (CPAL) 2025"},{"id":"http://arxiv.org/abs/2409.05907v2","updated":"2025-02-11T16:22:45Z","published":"2024-09-06T15:47:40Z","title":"Programming Refusal with Conditional Activation Steering","summary":"  LLMs have shown remarkable capabilities, but precisely controlling their\nresponse behavior remains challenging. Existing activation steering methods\nalter LLM behavior indiscriminately, limiting their practical applicability in\nsettings where selective responses are essential, such as content moderation or\ndomain-specific assistants. In this paper, we propose Conditional Activation\nSteering (CAST), which analyzes LLM activation patterns during inference to\nselectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts\nactivate distinct patterns in the model's hidden states. Using CAST, one can\nsystematically control LLM behavior with rules like \"if input is about hate\nspeech or adult content, then refuse\" or \"if input is not about legal advice,\nthen refuse.\" This allows for selective modification of responses to specific\ncontent while maintaining normal responses to other content, all without\nrequiring weight optimization. We release an open-source implementation of our\nframework at <github.com/IBM/activation-steering>.\n","authors":["Bruce W. Lee","Inkit Padhi","Karthikeyan Natesan Ramamurthy","Erik Miehling","Pierre Dognin","Manish Nagireddy","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2409.05907v2.pdf","comment":"ICLR 2025, Spotlight"},{"id":"http://arxiv.org/abs/2502.06314v2","updated":"2025-02-11T16:04:15Z","published":"2025-02-10T10:06:46Z","title":"From Pixels to Components: Eigenvector Masking for Visual Representation\n  Learning","summary":"  Predicting masked from visible parts of an image is a powerful\nself-supervised approach for visual representation learning. However, the\ncommon practice of masking random patches of pixels exhibits certain failure\nmodes, which can prevent learning meaningful high-level features, as required\nfor downstream tasks. We propose an alternative masking strategy that operates\non a suitable transformation of the data rather than on the raw pixels.\nSpecifically, we perform principal component analysis and then randomly mask a\nsubset of components, which accounts for a fixed ratio of the data variance.\nThe learning task then amounts to reconstructing the masked components from the\nvisible ones. Compared to local patches of pixels, the principal components of\nimages carry more global information. We thus posit that predicting masked from\nvisible components involves more high-level features, allowing our masking\nstrategy to extract more useful representations. This is corroborated by our\nempirical findings which demonstrate improved image classification performance\nfor component over pixel masking. Our method thus constitutes a simple and\nrobust data-driven alternative to traditional masked image modeling approaches.\n","authors":["Alice Bizeul","Thomas Sutter","Alain Ryser","Bernhard Schölkopf","Julius von Kügelgen","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2502.06314v2.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2402.11355v5","updated":"2025-02-11T16:03:35Z","published":"2024-02-17T18:12:02Z","title":"A Practical Method for Generating String Counterfactuals","summary":"  Interventions targeting the representation space of language models (LMs)\nhave emerged as an effective means to influence model behavior. Such methods\nare employed, for example, to eliminate or alter the encoding of demographic\ninformation such as gender within the model's representations and, in so doing,\ncreate a counterfactual representation. However, because the intervention\noperates within the representation space, understanding precisely what aspects\nof the text it modifies poses a challenge. In this paper, we give a method to\nconvert representation counterfactuals into string counterfactuals. We\ndemonstrate that this approach enables us to analyze the linguistic alterations\ncorresponding to a given representation space intervention and to interpret the\nfeatures utilized to encode a specific concept. Moreover, the resulting\ncounterfactuals can be used to mitigate bias in classification through data\naugmentation.\n","authors":["Matan Avitan","Ryan Cotterell","Yoav Goldberg","Shauli Ravfogel"],"pdf_url":"https://arxiv.org/pdf/2402.11355v5.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.11061v8","updated":"2025-02-11T15:59:51Z","published":"2024-10-14T20:14:39Z","title":"Learning to Optimize for Mixed-Integer Non-linear Programming","summary":"  Mixed-integer nonlinear programs (MINLPs) arise in diverse domains such as\nenergy systems and transportation but are notoriously difficult to solve,\nparticularly on a large scale. While learning-to-optimize methods have been\nsuccessful at continuous optimization, extending them to MINLPs is still\nchallenging due to the integer constraints. To overcome this, we propose a\nnovel deep-learning approach with two learnable correction layers to ensure\nsolution integrality and a post-processing step to improve solution\nfeasibility. Our experiments show that this is the first general method capable\nof efficiently solving large-scale MINLPs with up to tens of thousands of\nvariables in milliseconds, delivering high-quality solutions even when\ntraditional solvers and heuristics fail. This is the first general learning\nmethod for MINLP, successfully solving some of the largest instances reported\nto date.\n","authors":["Bo Tang","Elias B. Khalil","Ján Drgoňa"],"pdf_url":"https://arxiv.org/pdf/2410.11061v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12042v3","updated":"2025-02-11T15:58:10Z","published":"2024-06-17T19:22:04Z","title":"Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image\n  Diffusion Models","summary":"  Text-to-image (T2I) diffusion models have demonstrated impressive image\ngeneration capabilities. Still, their computational intensity prohibits\nresource-constrained organizations from deploying T2I models after fine-tuning\nthem on their internal target data. While pruning techniques offer a potential\nsolution to reduce the computational burden of T2I models, static pruning\nmethods use the same pruned model for all input prompts, overlooking the\nvarying capacity requirements of different prompts. Dynamic pruning addresses\nthis issue by utilizing a separate sub-network for each prompt, but it prevents\nbatch parallelism on GPUs. To overcome these limitations, we introduce Adaptive\nPrompt-Tailored Pruning (APTP), a novel prompt-based pruning method designed\nfor T2I diffusion models. Central to our approach is a prompt router model,\nwhich learns to determine the required capacity for an input text prompt and\nroutes it to an architecture code, given a total desired compute budget for\nprompts. Each architecture code represents a specialized model tailored to the\nprompts assigned to it, and the number of codes is a hyperparameter. We train\nthe prompt router and architecture codes using contrastive learning, ensuring\nthat similar prompts are mapped to nearby codes. Further, we employ optimal\ntransport to prevent the codes from collapsing into a single one. We\ndemonstrate APTP's effectiveness by pruning Stable Diffusion (SD) V2.1 using\nCC3M and COCO as target datasets. APTP outperforms the single-model pruning\nbaselines in terms of FID, CLIP, and CMMD scores. Our analysis of the clusters\nlearned by APTP reveals they are semantically meaningful. We also show that\nAPTP can automatically discover previously empirically found challenging\nprompts for SD, e.g. prompts for generating text images, assigning them to\nhigher capacity codes.\n","authors":["Alireza Ganjdanesh","Reza Shirkavand","Shangqian Gao","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2406.12042v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07661v1","updated":"2025-02-11T15:51:23Z","published":"2025-02-11T15:51:23Z","title":"Partial-Label Learning with Conformal Candidate Cleaning","summary":"  Real-world data is often ambiguous; for example, human annotation produces\ninstances with multiple conflicting class labels. Partial-label learning (PLL)\naims at training a classifier in this challenging setting, where each instance\nis associated with a set of candidate labels and one correct, but unknown,\nclass label. A multitude of algorithms targeting this setting exists and, to\nenhance their prediction quality, several extensions that are applicable across\na wide range of PLL methods have been introduced. While many of these\nextensions rely on heuristics, this article proposes a novel enhancing method\nthat incrementally prunes candidate sets using conformal prediction. To work\naround the missing labeled validation set, which is typically required for\nconformal prediction, we propose a strategy that alternates between training a\nPLL classifier to label the validation set, leveraging these predicted class\nlabels for calibration, and pruning candidate labels that are not part of the\nresulting conformal sets. In this sense, our method alternates between\nempirical risk minimization and candidate set pruning. We establish that our\npruning method preserves the conformal validity with respect to the unknown\nground truth. Our extensive experiments on artificial and real-world data show\nthat the proposed approach significantly improves the test set accuracies of\nseveral state-of-the-art PLL classifiers.\n","authors":["Tobias Fuchs","Florian Kalinke"],"pdf_url":"https://arxiv.org/pdf/2502.07661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11160v4","updated":"2025-02-11T15:50:27Z","published":"2023-04-21T17:59:08Z","title":"Isotonic Mechanism for Exponential Family Estimation in Machine Learning\n  Peer Review","summary":"  In 2023, the International Conference on Machine Learning (ICML) required\nauthors with multiple submissions to rank their submissions based on perceived\nquality. In this paper, we aim to employ these author-specified rankings to\nenhance peer review in machine learning and artificial intelligence conferences\nby extending the Isotonic Mechanism to exponential family distributions. This\nmechanism generates adjusted scores that closely align with the original scores\nwhile adhering to author-specified rankings. Despite its applicability to a\nbroad spectrum of exponential family distributions, implementing this mechanism\ndoes not require knowledge of the specific distribution form. We demonstrate\nthat an author is incentivized to provide accurate rankings when her utility\ntakes the form of a convex additive function of the adjusted review scores. For\na certain subclass of exponential family distributions, we prove that the\nauthor reports truthfully only if the question involves only pairwise\ncomparisons between her submissions, thus indicating the optimality of ranking\nin truthful information elicitation. Moreover, we show that the adjusted scores\nimprove dramatically the estimation accuracy compared to the original scores\nand achieve nearly minimax optimality when the ground-truth scores have bounded\ntotal variation. We conclude with a numerical analysis of the ICML 2023 ranking\ndata, showing substantial estimation gains in approximating a proxy\nground-truth quality of the papers using the Isotonic Mechanism.\n","authors":["Yuling Yan","Weijie J. Su","Jianqing Fan"],"pdf_url":"https://arxiv.org/pdf/2304.11160v4.pdf","comment":"accepted to the Journal of the Royal Statistical Society: Series B"},{"id":"http://arxiv.org/abs/2502.07657v1","updated":"2025-02-11T15:46:03Z","published":"2025-02-11T15:46:03Z","title":"Private Low-Rank Approximation for Covariance Matrices, Dyson Brownian\n  Motion, and Eigenvalue-Gap Bounds for Gaussian Perturbations","summary":"  We consider the problem of approximating a $d \\times d$ covariance matrix $M$\nwith a rank-$k$ matrix under $(\\varepsilon,\\delta)$-differential privacy. We\npresent and analyze a complex variant of the Gaussian mechanism and obtain\nupper bounds on the Frobenius norm of the difference between the matrix output\nby this mechanism and the best rank-$k$ approximation to $M$. Our analysis\nprovides improvements over previous bounds, particularly when the spectrum of\n$M$ satisfies natural structural assumptions. The novel insight is to view the\naddition of Gaussian noise to a matrix as a continuous-time matrix Brownian\nmotion. This viewpoint allows us to track the evolution of eigenvalues and\neigenvectors of the matrix, which are governed by stochastic differential\nequations discovered by Dyson. These equations enable us to upper bound the\nFrobenius distance between the best rank-$k$ approximation of $M$ and that of a\nGaussian perturbation of $M$ as an integral that involves inverse eigenvalue\ngaps of the stochastically evolving matrix, as opposed to a sum of perturbation\nbounds obtained via Davis-Kahan-type theorems. Subsequently, again using the\nDyson Brownian motion viewpoint, we show that the eigenvalues of the matrix $M$\nperturbed by Gaussian noise have large gaps with high probability. These\nresults also contribute to the analysis of low-rank approximations under\naverage-case perturbations, and to an understanding of eigenvalue gaps for\nrandom matrices, both of which may be of independent interest.\n","authors":["Oren Mangoubi","Nisheeth K. Vishnoi"],"pdf_url":"https://arxiv.org/pdf/2502.07657v1.pdf","comment":"Published in Journal of the ACM. arXiv admin note: substantial text\n  overlap with arXiv:2306.16648"},{"id":"http://arxiv.org/abs/2502.07656v1","updated":"2025-02-11T15:43:49Z","published":"2025-02-11T15:43:49Z","title":"A Unifying Framework for Causal Imitation Learning with Hidden\n  Confounders","summary":"  We propose a general and unifying framework for causal Imitation Learning\n(IL) with hidden confounders that subsumes several existing confounded IL\nsettings from the literature. Our framework accounts for two types of hidden\nconfounders: (a) those observed by the expert, which thus influence the\nexpert's policy, and (b) confounding noise hidden to both the expert and the IL\nalgorithm. For additional flexibility, we also introduce a confounding noise\nhorizon and time-varying expert-observable hidden variables. We show that\ncausal IL in our framework can be reduced to a set of Conditional Moment\nRestrictions (CMRs) by leveraging trajectory histories as instruments to learn\na history-dependent policy. We propose DML-IL, a novel algorithm that uses\ninstrumental variable regression to solve these CMRs and learn a policy. We\nprovide a bound on the imitation gap for DML-IL, which recovers prior results\nas special cases. Empirical evaluation on a toy environment with continues\nstate-action spaces and multiple Mujoco tasks demonstrate that DML-IL\noutperforms state-of-the-art causal IL algorithms.\n","authors":["Daqian Shao","Thomas Kleine Buening","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2502.07656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03736v3","updated":"2025-02-11T15:42:19Z","published":"2024-06-06T04:22:11Z","title":"Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data","summary":"  Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.\n","authors":["Jingyang Ou","Shen Nie","Kaiwen Xue","Fengqi Zhu","Jiacheng Sun","Zhenguo Li","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2406.03736v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07650v1","updated":"2025-02-11T15:39:47Z","published":"2025-02-11T15:39:47Z","title":"Guiding Time-Varying Generative Models with Natural Gradients on\n  Exponential Family Manifold","summary":"  Optimising probabilistic models is a well-studied field in statistics.\nHowever, its connection with the training of generative models remains largely\nunder-explored. In this paper, we show that the evolution of time-varying\ngenerative models can be projected onto an exponential family manifold,\nnaturally creating a link between the parameters of a generative model and\nthose of a probabilistic model. We then train the generative model by moving\nits projection on the manifold according to the natural gradient descent\nscheme. This approach also allows us to approximate the natural gradient of the\nKL divergence efficiently without relying on MCMC for intractable models.\nFurthermore, we propose particle versions of the algorithm, which feature\nclosed-form update rules for any parametric model within the exponential\nfamily. Through toy and real-world experiments, we validate the effectiveness\nof the proposed algorithms.\n","authors":["Song Liu","Leyang Wang","Yakun Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07646v1","updated":"2025-02-11T15:35:15Z","published":"2025-02-11T15:35:15Z","title":"Causal Additive Models with Unobserved Causal Paths and Backdoor Paths","summary":"  Causal additive models have been employed as tractable yet expressive\nframeworks for causal discovery involving hidden variables. State-of-the-art\nmethodologies suggest that determining the causal relationship between a pair\nof variables is infeasible in the presence of an unobserved backdoor or an\nunobserved causal path. Contrary to this assumption, we theoretically show that\nresolving the causal direction is feasible in certain scenarios by\nincorporating two novel components into the theory. The first component\nintroduces a novel characterization of regression sets within independence\nbetween regression residuals. The second component leverages conditional\nindependence among the observed variables. We also provide a search algorithm\nthat integrates these innovations and demonstrate its competitive performance\nagainst existing methods.\n","authors":["Thong Pham","Takashi Nicholas Maeda","Shohei Shimizu"],"pdf_url":"https://arxiv.org/pdf/2502.07646v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.07642v1","updated":"2025-02-11T15:33:17Z","published":"2025-02-11T15:33:17Z","title":"FoQA: A Faroese Question-Answering Dataset","summary":"  We present FoQA, a Faroese extractive question-answering (QA) dataset with\n2,000 samples, created using a semi-automated approach combining Large Language\nModels (LLMs) and human validation. The dataset was generated from Faroese\nWikipedia articles using GPT-4-turbo for initial QA generation, followed by\nquestion rephrasing to increase complexity and native speaker validation to\nensure quality. We provide baseline performance metrics for FoQA across\nmultiple models, including LLMs and BERT, demonstrating its effectiveness in\nevaluating Faroese QA performance. The dataset is released in three versions: a\nvalidated set of 2,000 samples, a complete set of all 10,001 generated samples,\nand a set of 2,395 rejected samples for error analysis.\n","authors":["Annika Simonsen","Dan Saattrup Nielsen","Hafsteinn Einarsson"],"pdf_url":"https://arxiv.org/pdf/2502.07642v1.pdf","comment":"Camera-ready version for RESOURCEFUL workshop, 2025"},{"id":"http://arxiv.org/abs/2502.07640v1","updated":"2025-02-11T15:27:35Z","published":"2025-02-11T15:27:35Z","title":"Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving","summary":"  We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. The final prover outperforms all existing\nopen-source models in whole-proof generation. On the miniF2F benchmark, it\nachieves a 57.6% success rate (Pass@32), exceeding the previous best\nopen-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7\nproblems (Pass@512), ranking first on the leaderboard. Furthermore, it\ngenerates 29.7K formal proofs for Lean Workbook problems, nearly doubling the\n15.7K produced by earlier works.\n","authors":["Yong Lin","Shange Tang","Bohan Lyu","Jiayun Wu","Hongzhou Lin","Kaiyu Yang","Jia Li","Mengzhou Xia","Danqi Chen","Sanjeev Arora","Chi Jin"],"pdf_url":"https://arxiv.org/pdf/2502.07640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07636v1","updated":"2025-02-11T15:23:14Z","published":"2025-02-11T15:23:14Z","title":"Consistency Training with Physical Constraints","summary":"  We propose a physics-aware Consistency Training (CT) method that accelerates\nsampling in Diffusion Models with physical constraints. Our approach leverages\na two-stage strategy: (1) learning the noise-to-data mapping via CT, and (2)\nincorporating physics constraints as a regularizer. Experiments on toy examples\nshow that our method generates samples in a single step while adhering to the\nimposed constraints. This approach has the potential to efficiently solve\npartial differential equations (PDEs) using deep generative modeling.\n","authors":["Che-Chia Chang","Chen-Yang Dai","Te-Sheng Lin","Ming-Chih Lai","Chieh-Hsin Lai"],"pdf_url":"https://arxiv.org/pdf/2502.07636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07635v1","updated":"2025-02-11T15:23:05Z","published":"2025-02-11T15:23:05Z","title":"Distributed Value Decomposition Networks with Networked Agents","summary":"  We investigate the problem of distributed training under partial\nobservability, whereby cooperative multi-agent reinforcement learning agents\n(MARL) maximize the expected cumulative joint reward. We propose distributed\nvalue decomposition networks (DVDN) that generate a joint Q-function that\nfactorizes into agent-wise Q-functions. Whereas the original value\ndecomposition networks rely on centralized training, our approach is suitable\nfor domains where centralized training is not possible and agents must learn by\ninteracting with the physical environment in a decentralized manner while\ncommunicating with their peers. DVDN overcomes the need for centralized\ntraining by locally estimating the shared objective. We contribute with two\ninnovative algorithms, DVDN and DVDN (GT), for the heterogeneous and\nhomogeneous agents settings respectively. Empirically, both algorithms\napproximate the performance of value decomposition networks, in spite of the\ninformation loss during communication, as demonstrated in ten MARL tasks in\nthree standard environments.\n","authors":["Guilherme S. Varela","Alberto Sardinha","Francisco S. Melo"],"pdf_url":"https://arxiv.org/pdf/2502.07635v1.pdf","comment":"21 pages, 15 figures, to be published in Proceedings of the 24th\n  International Conference on Autonomous Agents and Multiagent Systems (AAMAS\n  2025), Detroit, Michigan, USA, May 19 - 23, 2025, IFAAMAS"},{"id":"http://arxiv.org/abs/2502.04907v2","updated":"2025-02-11T15:17:43Z","published":"2025-02-07T13:23:40Z","title":"Scalable and consistent embedding of probability measures into Hilbert\n  spaces via measure quantization","summary":"  This paper is focused on statistical learning from data that come as\nprobability measures. In this setting, popular approaches consist in embedding\nsuch data into a Hilbert space with either Linearized Optimal Transport or\nKernel Mean Embedding. However, the cost of computing such embeddings prohibits\ntheir direct use in large-scale settings. We study two methods based on measure\nquantization for approximating input probability measures with discrete\nmeasures of small-support size. The first one is based on optimal quantization\nof each input measure, while the second one relies on mean-measure\nquantization. We study the consistency of such approximations, and its\nimplication for scalable embeddings of probability measures into a Hilbert\nspace at a low computational cost. We finally illustrate our findings with\nvarious numerical experiments.\n","authors":["Erell Gachon","Elsa Cazelles","Jérémie Bigot"],"pdf_url":"https://arxiv.org/pdf/2502.04907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07630v1","updated":"2025-02-11T15:17:29Z","published":"2025-02-11T15:17:29Z","title":"Rethinking Timing Residuals: Advancing PET Detectors with Explicit TOF\n  Corrections","summary":"  PET is a functional imaging method that visualizes metabolic processes. TOF\ninformation can be derived from coincident detector signals and incorporated\ninto image reconstruction to enhance the SNR. PET detectors are typically\nassessed by their CTR, but timing performance is degraded by various factors.\nResearch on timing calibration seeks to mitigate these degradations and restore\naccurate timing information. While many calibration methods use analytical\napproaches, machine learning techniques have recently gained attention due to\ntheir flexibility. We developed a residual physics-based calibration approach\nthat combines prior domain knowledge with the power of machine learning models.\nThis approach begins with an initial analytical calibration addressing\nfirst-order skews. The remaining deviations, regarded as residual effects, are\nused to train machine learning models to eliminate higher-order skews. The key\nadvantage is that the experimenter guides the learning process through the\ndefinition of timing residuals. In earlier studies, we developed models that\ndirectly predicted the expected time difference, which offered corrections only\nimplicitly (implicit correction models). In this study, we introduce a new\ndefinition for timing residuals, enabling us to train models that directly\npredict correction values (explicit correction models). The explicit correction\napproach significantly simplifies data acquisition, improves linearity, and\nenhances timing performance from $371 \\pm 6$ ps to $281 \\pm 5$ ps for\ncoincidences from 430 keV to 590 keV. Additionally, the new definition reduces\nmodel size, making it suitable for high-throughput applications like PET\nscanners. Experiments were conducted using two detector stacks composed of $4\n\\times 4$ LYSO:Ce,Ca crystals ($3.8\\times 3.8\\times 20$ mm$^{3}$) coupled to $4\n\\times 4$ Broadcom NUV-MT SiPMs and digitized with the TOFPET2 ASIC.\n","authors":["Stephan Naunheim","Luis Lopes de Paiva","Vanessa Nadig","Yannick Kuhl","Stefan Gundacker","Florian Mueller","Volkmar Schulz"],"pdf_url":"https://arxiv.org/pdf/2502.07630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13779v2","updated":"2025-02-11T15:09:49Z","published":"2024-01-24T20:00:23Z","title":"Faster Convergence with Less Communication: Broadcast-Based Subgraph\n  Sampling for Decentralized Learning over Wireless Networks","summary":"  Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely\nadopted algorithm for decentralized training of machine learning models across\nnetworked agents. A crucial part of D-SGD is the consensus-based model\naveraging, which heavily relies on information exchange and fusion among the\nnodes. Specifically, for consensus averaging over wireless networks,\ncommunication coordination is necessary to determine when and how a node can\naccess the channel and transmit (or receive) information to (or from) its\nneighbors. In this work, we propose $\\texttt{BASS}$, a broadcast-based subgraph\nsampling method designed to accelerate the convergence of D-SGD while\nconsidering the actual communication cost per iteration. $\\texttt{BASS}$\ncreates a set of mixing matrix candidates that represent sparser subgraphs of\nthe base topology. In each consensus iteration, one mixing matrix is sampled,\nleading to a specific scheduling decision that activates multiple\ncollision-free subsets of nodes. The sampling occurs in a probabilistic manner,\nand the elements of the mixing matrices, along with their sampling\nprobabilities, are jointly optimized. Simulation results demonstrate that\n$\\texttt{BASS}$ enables faster convergence with fewer transmission slots\ncompared to existing link-based scheduling methods. In conclusion, the inherent\nbroadcasting nature of wireless channels offers intrinsic advantages in\naccelerating the convergence of decentralized optimization and learning.\n","authors":["Daniel Pérez Herrera","Zheng Chen","Erik G. Larsson"],"pdf_url":"https://arxiv.org/pdf/2401.13779v2.pdf","comment":"14 pages, 10 figures, accepted for publication at IEEE Open Journals\n  of Communication. arXiv admin note: text overlap with arXiv:2310.16106"},{"id":"http://arxiv.org/abs/2502.07620v1","updated":"2025-02-11T15:09:05Z","published":"2025-02-11T15:09:05Z","title":"Causal-Informed Contrastive Learning: Towards Bias-Resilient\n  Pre-training under Concept Drift","summary":"  The evolution of large-scale contrastive pre-training propelled by top-tier\ndatasets has reached a transition point in the scaling law. Consequently,\nsustaining and enhancing a model's pre-training capabilities in drift\nenvironments have surfaced as a notable challenge. In this paper, we initially\nuncover that contrastive pre-training methods are significantly impacted by\nconcept drift wherein distributions change unpredictably, resulting in notable\nbiases in the feature space of the pre-trained model. Empowered by causal\ninference, we construct a structural causal graph to analyze the impact of\nconcept drift to contrastive pre-training systemically, and propose the causal\ninterventional contrastive objective. Upon achieving this, we devise a\nresilient contrastive pre-training approach to accommodate the data stream of\nconcept drift, with simple and scalable implementation. Extensive experiments\non various downstream tasks demonstrate our resilient contrastive pre-training\neffectively mitigates the bias stemming from the concept drift data stream.\nCodes are available at https://anonymous.4open.science/r/ResilientCL/.\n","authors":["Xiaoyu Yang","Jie Lu","En Yu"],"pdf_url":"https://arxiv.org/pdf/2502.07620v1.pdf","comment":"17pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.07616v1","updated":"2025-02-11T15:05:26Z","published":"2025-02-11T15:05:26Z","title":"Tractable Transformers for Flexible Conditional Generation","summary":"  Non-autoregressive (NAR) generative models are valuable because they can\nhandle diverse conditional generation tasks in a more principled way than their\nautoregressive (AR) counterparts, which are constrained by sequential\ndependency requirements. Recent advancements in NAR models, such as diffusion\nlanguage models, have demonstrated superior performance in unconditional\ngeneration compared to AR models (e.g., GPTs) of similar sizes. However, such\nimprovements do not always lead to improved conditional generation performance.\nWe show that a key reason for this gap is the difficulty in generalizing to\nconditional probability queries unseen during training. As a result, strong\nunconditional generation performance does not guarantee high-quality\nconditional generation. This paper proposes Tractable Transformers\n(Tracformer), a Transformer-based generative model that is more robust to\ndifferent conditional generation tasks. Unlike existing models that rely solely\non global contextual features derived from full inputs, Tracformers incorporate\na sparse Transformer encoder to capture both local and global contextual\ninformation. This information is routed through a decoder for conditional\ngeneration. Empirical results demonstrate that Tracformers achieve\nstate-of-the-art conditional generation performance on text modeling compared\nto recent diffusion and AR model baselines.\n","authors":["Anji Liu","Xuejie Liu","Dayuan Zhao","Mathias Niepert","Yitao Liang","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2502.07616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07608v1","updated":"2025-02-11T14:58:54Z","published":"2025-02-11T14:58:54Z","title":"Beyond Prompting: Time2Lang -- Bridging Time-Series Foundation Models\n  and Large Language Models for Health Sensing","summary":"  Large language models (LLMs) show promise for health applications when\ncombined with behavioral sensing data. Traditional approaches convert sensor\ndata into text prompts, but this process is prone to errors, computationally\nexpensive, and requires domain expertise. These challenges are particularly\nacute when processing extended time series data. While time series foundation\nmodels (TFMs) have recently emerged as powerful tools for learning\nrepresentations from temporal data, bridging TFMs and LLMs remains challenging.\nHere, we present Time2Lang, a framework that directly maps TFM outputs to LLM\nrepresentations without intermediate text conversion. Our approach first trains\non synthetic data using periodicity prediction as a pretext task, followed by\nevaluation on mental health classification tasks. We validate Time2Lang on two\nlongitudinal wearable and mobile sensing datasets: daily depression prediction\nusing step count data (17,251 days from 256 participants) and flourishing\nclassification based on conversation duration (46 participants over 10 weeks).\nTime2Lang maintains near constant inference times regardless of input length,\nunlike traditional prompting methods. The generated embeddings preserve\nessential time-series characteristics such as auto-correlation. Our results\ndemonstrate that TFMs and LLMs can be effectively integrated while minimizing\ninformation loss and enabling performance transfer across these distinct\nmodeling paradigms. To our knowledge, we are the first to integrate a TFM and\nan LLM for health, thus establishing a foundation for future research combining\ngeneral-purpose large models for complex healthcare tasks.\n","authors":["Arvind Pillai","Dimitris Spathis","Subigya Nepal","Amanda C Collins","Daniel M Mackin","Michael V Heinz","Tess Z Griffin","Nicholas C Jacobson","Andrew Campbell"],"pdf_url":"https://arxiv.org/pdf/2502.07608v1.pdf","comment":"Under review at CHIL 2025"},{"id":"http://arxiv.org/abs/2412.07041v3","updated":"2025-02-11T14:57:40Z","published":"2024-12-09T23:01:04Z","title":"Generalized Least Squares Kernelized Tensor Factorization","summary":"  Completing multidimensional tensor-structured data with missing entries is a\nfundamental task for many real-world applications involving incomplete or\ncorrupted datasets. For data with spatial or temporal side information,\nlow-rank factorization models with smoothness constraints have demonstrated\nstrong performance. Although effective at capturing global and long-range\ncorrelations, these models often struggle to capture short-scale,\nhigh-frequency variations in the data. To address this limitation, we propose\nthe Generalized Least Squares Kernelized Tensor Factorization (GLSKF) framework\nfor tensor completion. GLSKF integrates smoothness-constrained low-rank\nfactorization with a locally correlated residual process; the resulting\nadditive structure enables effective characterization of both global\ndependencies and local variations. Specifically, we define the covariance norm\nto enforce the smoothness of factor matrices in the global low-rank\nfactorization, and use structured covariance/kernel functions to model the\nlocal processes. For model estimation, we develop an alternating least squares\n(ALS) procedure with closed-form solutions for each subproblem. GLSKF utilizes\nzero-padding and slicing operations based on projection matrices which preserve\nthe Kronecker structure of covariances, facilitating efficient computations\nthrough the conjugate gradient (CG) method. The proposed framework is evaluated\non four real-world datasets across diverse tasks. Experimental results\ndemonstrate that GLSKF achieves superior performance and scalability,\nestablishing it as a novel solution for multidimensional tensor completion.\n","authors":["Mengying Lei","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2412.07041v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07606v1","updated":"2025-02-11T14:56:16Z","published":"2025-02-11T14:56:16Z","title":"Algorithmic Aspects of Strategic Trading","summary":"  Algorithmic trading in modern financial markets is widely acknowledged to\nexhibit strategic, game-theoretic behaviors whose complexity can be difficult\nto model. A recent series of papers (Chriss, 2024b,c,a, 2025) has made progress\nin the setting of trading for position building. Here parties wish to buy or\nsell a fixed number of shares in a fixed time period in the presence of both\ntemporary and permanent market impact, resulting in exponentially large\nstrategy spaces. While these papers primarily consider the existence and\nstructural properties of equilibrium strategies, in this work we focus on the\nalgorithmic aspects of the proposed model. We give an efficient algorithm for\ncomputing best responses, and show that while the temporary impact only setting\nyields a potential game, best response dynamics do not generally converge for\nthe general setting, for which no fast algorithm for (Nash) equilibrium\ncomputation is known. This leads us to consider the broader notion of Coarse\nCorrelated Equilibria (CCE), which we show can be computed efficiently via an\nimplementation of Follow the Perturbed Leader (FTPL). We illustrate the model\nand our results with an experimental investigation, where FTPL exhibits\ninteresting behavior in different regimes of the relative weighting between\ntemporary and permanent market impact.\n","authors":["Michael Kearns","Mirah Shi"],"pdf_url":"https://arxiv.org/pdf/2502.07606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02525v4","updated":"2025-02-11T14:54:12Z","published":"2022-09-06T14:30:18Z","title":"Generalisation under gradient descent via deterministic PAC-Bayes","summary":"  We establish disintegrated PAC-Bayesian generalisation bounds for models\ntrained with gradient descent methods or continuous gradient flows. Contrary to\nstandard practice in the PAC-Bayesian setting, our result applies to\noptimisation algorithms that are deterministic, without requiring any\nde-randomisation step. Our bounds are fully computable, depending on the\ndensity of the initial distribution and the Hessian of the training objective\nover the trajectory. We show that our framework can be applied to a variety of\niterative optimisation algorithms, including stochastic gradient descent (SGD),\nmomentum-based schemes, and damped Hamiltonian dynamics.\n","authors":["Eugenio Clerico","Tyler Farghly","George Deligiannidis","Benjamin Guedj","Arnaud Doucet"],"pdf_url":"https://arxiv.org/pdf/2209.02525v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07591v1","updated":"2025-02-11T14:40:57Z","published":"2025-02-11T14:40:57Z","title":"DMWM: Dual-Mind World Model with Long-Term Imagination","summary":"  Imagination in world models is crucial for enabling agents to learn\nlong-horizon policy in a sample-efficient manner. Existing recurrent\nstate-space model (RSSM)-based world models depend on single-step statistical\ninference to capture the environment dynamics, and, hence, they are unable to\nperform long-term imagination tasks due to the accumulation of prediction\nerrors. Inspired by the dual-process theory of human cognition, we propose a\nnovel dual-mind world model (DMWM) framework that integrates logical reasoning\nto enable imagination with logical consistency. DMWM is composed of two\ncomponents: an RSSM-based System 1 (RSSM-S1) component that handles state\ntransitions in an intuitive manner and a logic-integrated neural network-based\nSystem 2 (LINN-S2) component that guides the imagination process through\nhierarchical deep logical reasoning. The inter-system feedback mechanism is\ndesigned to ensure that the imagination process follows the logical rules of\nthe real environment. The proposed framework is evaluated on benchmark tasks\nthat require long-term planning from the DMControl suite. Extensive\nexperimental results demonstrate that the proposed framework yields significant\nimprovements in terms of logical coherence, trial efficiency, data efficiency\nand long-term imagination over the state-of-the-art world models.\n","authors":["Lingyi Wang","Rashed Shelim","Walid Saad","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2502.07591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20385v2","updated":"2025-02-11T14:37:00Z","published":"2024-12-29T07:21:13Z","title":"A Particle Algorithm for Mean-Field Variational Inference","summary":"  Variational inference is a fast and scalable alternative to Markov chain\nMonte Carlo and has been widely applied to posterior inference tasks in\nstatistics and machine learning. A traditional approach for implementing\nmean-field variational inference (MFVI) is coordinate ascent variational\ninference (CAVI), which relies crucially on parametric assumptions on complete\nconditionals. In this paper, we introduce a novel particle-based algorithm for\nmean-field variational inference, which we term PArticle VI (PAVI). Notably,\nour algorithm does not rely on parametric assumptions on complete conditionals,\nand it applies to the nonparametric setting. We provide non-asymptotic\nfinite-particle convergence guarantee for our algorithm. To our knowledge, this\nis the first end-to-end guarantee for particle-based MFVI.\n","authors":["Qiang Du","Kaizheng Wang","Edith Zhang","Chenyang Zhong"],"pdf_url":"https://arxiv.org/pdf/2412.20385v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2502.07587v1","updated":"2025-02-11T14:36:39Z","published":"2025-02-11T14:36:39Z","title":"SEMU: Singular Value Decomposition for Efficient Machine Unlearning","summary":"  While the capabilities of generative foundational models have advanced\nrapidly in recent years, methods to prevent harmful and unsafe behaviors remain\nunderdeveloped. Among the pressing challenges in AI safety, machine unlearning\n(MU) has become increasingly critical to meet upcoming safety regulations. Most\nexisting MU approaches focus on altering the most significant parameters of the\nmodel. However, these methods often require fine-tuning substantial portions of\nthe model, resulting in high computational costs and training instabilities,\nwhich are typically mitigated by access to the original training dataset.\n  In this work, we address these limitations by leveraging Singular Value\nDecomposition (SVD) to create a compact, low-dimensional projection that\nenables the selective forgetting of specific data points. We propose Singular\nValue Decomposition for Efficient Machine Unlearning (SEMU), a novel approach\ndesigned to optimize MU in two key aspects. First, SEMU minimizes the number of\nmodel parameters that need to be modified, effectively removing unwanted\nknowledge while making only minimal changes to the model's weights. Second,\nSEMU eliminates the dependency on the original training dataset, preserving the\nmodel's previously acquired knowledge without additional data requirements.\n  Extensive experiments demonstrate that SEMU achieves competitive performance\nwhile significantly improving efficiency in terms of both data usage and the\nnumber of modified parameters.\n","authors":["Marcin Sendera","Łukasz Struski","Kamil Książek","Kryspin Musiol","Jacek Tabor","Dawid Rymarczyk"],"pdf_url":"https://arxiv.org/pdf/2502.07587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07584v1","updated":"2025-02-11T14:31:32Z","published":"2025-02-11T14:31:32Z","title":"Understanding the Generalization Error of Markov algorithms through\n  Poissonization","summary":"  Using continuous-time stochastic differential equation (SDE) proxies to\nstochastic optimization algorithms has proven fruitful for understanding their\ngeneralization abilities. A significant part of these approaches are based on\nthe so-called ``entropy flows'', which greatly simplify the generalization\nanalysis. Unfortunately, such well-structured entropy flows cannot be obtained\nfor most discrete-time algorithms, and the existing SDE approaches remain\nlimited to specific noise and algorithmic structures. We aim to alleviate this\nissue by introducing a generic framework for analyzing the generalization error\nof Markov algorithms through `Poissonization', a continuous-time approximation\nof discrete-time processes with formal approximation guarantees. Through this\napproach, we first develop a novel entropy flow, which directly leads to\nPAC-Bayesian generalization bounds. We then draw novel links to modified\nversions of the celebrated logarithmic Sobolev inequalities (LSI), identify\ncases where such LSIs are satisfied, and obtain improved bounds. Beyond its\ngenerality, our framework allows exploiting specific properties of learning\nalgorithms. In particular, we incorporate the noise structure of different\nalgorithm types - namely, those with additional noise injections (noisy) and\nthose without (non-noisy) - through various technical tools. This illustrates\nthe capacity of our methods to achieve known (yet, Poissonized) and new\ngeneralization bounds.\n","authors":["Benjamin Dupuis","Maxime Haddouche","George Deligiannidis","Umut Simsekli"],"pdf_url":"https://arxiv.org/pdf/2502.07584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07580v1","updated":"2025-02-11T14:27:10Z","published":"2025-02-11T14:27:10Z","title":"Generative Modeling with Bayesian Sample Inference","summary":"  We derive a novel generative model from the simple act of Gaussian posterior\ninference. Treating the generated sample as an unknown variable to infer lets\nus formulate the sampling process in the language of Bayesian probability. Our\nmodel uses a sequence of prediction and posterior update steps to narrow down\nthe unknown sample from a broad initial belief. In addition to a rigorous\ntheoretical analysis, we establish a connection between our model and diffusion\nmodels and show that it includes Bayesian Flow Networks (BFNs) as a special\ncase. In our experiments, we demonstrate improved performance over both BFNs\nand Variational Diffusion Models, achieving competitive likelihood scores on\nCIFAR10 and ImageNet.\n","authors":["Marten Lienen","Marcel Kollovieh","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2502.07580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07579v1","updated":"2025-02-11T14:25:52Z","published":"2025-02-11T14:25:52Z","title":"Single-Step Consistent Diffusion Samplers","summary":"  Sampling from unnormalized target distributions is a fundamental yet\nchallenging task in machine learning and statistics. Existing sampling\nalgorithms typically require many iterative steps to produce high-quality\nsamples, leading to high computational costs that limit their practicality in\ntime-sensitive or resource-constrained settings. In this work, we introduce\nconsistent diffusion samplers, a new class of samplers designed to generate\nhigh-fidelity samples in a single step. We first develop a distillation\nalgorithm to train a consistent diffusion sampler from a pretrained diffusion\nmodel without pre-collecting large datasets of samples. Our algorithm leverages\nincomplete sampling trajectories and noisy intermediate states directly from\nthe diffusion process. We further propose a method to train a consistent\ndiffusion sampler from scratch, fully amortizing exploration by training a\nsingle model that both performs diffusion sampling and skips intermediate steps\nusing a self-consistency loss. Through extensive experiments on a variety of\nunnormalized distributions, we show that our approach yields high-fidelity\nsamples using less than 1% of the network evaluations required by traditional\ndiffusion samplers.\n","authors":["Pascal Jutras-Dubé","Patrick Pynadath","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07577v1","updated":"2025-02-11T14:23:13Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03006v2","updated":"2025-02-11T14:19:32Z","published":"2024-11-05T11:12:11Z","title":"Neural Networks and (Virtual) Extended Formulations","summary":"  Neural networks with piecewise linear activation functions, such as rectified\nlinear units (ReLU) or maxout, are among the most fundamental models in modern\nmachine learning. We make a step towards proving lower bounds on the size of\nsuch neural networks by linking their representative capabilities to the notion\nof the extension complexity $\\mathrm{xc}(P)$ of a polytope $P$. This is a\nwell-studied quantity in combinatorial optimization and polyhedral geometry\ndescribing the number of inequalities needed to model $P$ as a linear program.\nWe show that $\\mathrm{xc}(P)$ is a lower bound on the size of any monotone or\ninput-convex neural network that solves the linear optimization problem over\n$P$. This implies exponential lower bounds on such neural networks for a\nvariety of problems, including the polynomially solvable maximum weight\nmatching problem.\n  In an attempt to prove similar bounds also for general neural networks, we\nintroduce the notion of virtual extension complexity $\\mathrm{vxc}(P)$, which\ngeneralizes $\\mathrm{xc}(P)$ and describes the number of inequalities needed to\nrepresent the linear optimization problem over $P$ as a difference of two\nlinear programs. We prove that $\\mathrm{vxc}(P)$ is a lower bound on the size\nof any neural network that optimizes over $P$. While it remains an open\nquestion to derive useful lower bounds on $\\mathrm{vxc}(P)$, we argue that this\nquantity deserves to be studied independently from neural networks by proving\nthat one can efficiently optimize over a polytope $P$ using a small virtual\nextended formulation.\n","authors":["Christoph Hertrich","Georg Loho"],"pdf_url":"https://arxiv.org/pdf/2411.03006v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04315v3","updated":"2025-02-11T14:01:39Z","published":"2025-02-06T18:57:06Z","title":"ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters","summary":"  Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/\n","authors":["Kamer Ali Yuksel","Hassan Sawaf"],"pdf_url":"https://arxiv.org/pdf/2502.04315v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07563v1","updated":"2025-02-11T14:01:39Z","published":"2025-02-11T14:01:39Z","title":"LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid","summary":"  Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE.\n","authors":["Weigao Sun","Disen Lan","Yiran Zhong","Xiaoye Qu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07563v1.pdf","comment":"Technical report, 17 pages"},{"id":"http://arxiv.org/abs/2410.00535v3","updated":"2025-02-11T13:59:11Z","published":"2024-10-01T09:21:29Z","title":"The Causal Information Bottleneck and Optimal Causal Variable\n  Abstractions","summary":"  To effectively study complex causal systems, it is often useful to construct\nabstractions of parts of the system by discarding irrelevant details while\npreserving key features. The Information Bottleneck (IB) method is a widely\nused approach to construct variable abstractions by compressing random\nvariables while retaining predictive power over a target variable. Traditional\nmethods like IB are purely statistical and ignore underlying causal structures,\nmaking them ill-suited for causal tasks. We propose the Causal Information\nBottleneck (CIB), a causal extension of the IB, which compresses a set of\nchosen variables while maintaining causal control over a target variable. This\nmethod produces abstractions of (sets of) variables which are causally\ninterpretable, give us insight about the interactions between the abstracted\nvariables and the target variable, and can be used when reasoning about\ninterventions. We present experimental results demonstrating that the learned\nabstractions accurately capture causal relations as intended.\n","authors":["Francisco N. F. Q. Simoes","Mehdi Dastani","Thijs van Ommen"],"pdf_url":"https://arxiv.org/pdf/2410.00535v3.pdf","comment":"Submitted to UAI 2025. Code available at\n  github.com/francisco-simoes/cib-optimization-psagd"},{"id":"http://arxiv.org/abs/2501.18005v3","updated":"2025-02-11T13:58:39Z","published":"2025-01-29T21:40:32Z","title":"Fault Localization via Fine-tuning Large Language Models with Mutation\n  Generated Stack Traces","summary":"  Abrupt and unexpected terminations of software are termed as software\ncrashes. They can be challenging to analyze. Finding the root cause requires\nextensive manual effort and expertise to connect information sources like stack\ntraces, source code, and logs. Typical approaches to fault localization require\neither test failures or source code. Crashes occurring in production\nenvironments, such as that of SAP HANA, provide solely crash logs and stack\ntraces. We present a novel approach to localize faults based only on the stack\ntrace information and no additional runtime information, by fine-tuning large\nlanguage models (LLMs). We address complex cases where the root cause of a\ncrash differs from the technical cause, and is not located in the innermost\nframe of the stack trace. As the number of historic crashes is insufficient to\nfine-tune LLMs, we augment our dataset by leveraging code mutators to inject\nsynthetic crashes into the code base. By fine-tuning on 64,369 crashes\nresulting from 4.1 million mutations of the HANA code base, we can correctly\npredict the root cause location of a crash with an accuracy of 66.9\\% while\nbaselines only achieve 12.6% and 10.6%. We substantiate the generalizability of\nour approach by evaluating on two additional open-source databases, SQLite and\nDuckDB, achieving accuracies of 63% and 74%, respectively. Across all our\nexperiments, fine-tuning consistently outperformed prompting non-finetuned LLMs\nfor localizing faults in our datasets.\n","authors":["Neetha Jambigi","Bartosz Bogacz","Moritz Mueller","Thomas Bach","Michael Felderer"],"pdf_url":"https://arxiv.org/pdf/2501.18005v3.pdf","comment":"I do not have the necessary approvals to out the paper on Arxiv from\n  my organization yet. I was too soon to do this"},{"id":"http://arxiv.org/abs/2502.07553v1","updated":"2025-02-11T13:41:30Z","published":"2025-02-11T13:41:30Z","title":"Attention Learning is Needed to Efficiently Learn Parity Function","summary":"  Transformers, with their attention mechanisms, have emerged as the\nstate-of-the-art architectures of sequential modeling and empirically\noutperform feed-forward neural networks (FFNNs) across many fields, such as\nnatural language processing and computer vision. However, their generalization\nability, particularly for low-sensitivity functions, remains less studied. We\nbridge this gap by analyzing transformers on the $k$-parity problem. Daniely\nand Malach (NeurIPS 2020) show that FFNNs with one hidden layer and $O(nk^7\n\\log k)$ parameters can learn $k$-parity, where the input length $n$ is\ntypically much larger than $k$. In this paper, we prove that FFNNs require at\nleast $\\Omega(n)$ parameters to learn $k$-parity, while transformers require\nonly $O(k)$ parameters, surpassing the theoretical lower bound needed by FFNNs.\nWe further prove that this parameter efficiency cannot be achieved with fixed\nattention heads. Our work establishes transformers as theoretically superior to\nFFNNs in learning parity function, showing how their attention mechanisms\nenable parameter-efficient generalization in functions with low sensitivity.\n","authors":["Yaomengxi Han","Debarghya Ghoshdastidar"],"pdf_url":"https://arxiv.org/pdf/2502.07553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07551v1","updated":"2025-02-11T13:40:15Z","published":"2025-02-11T13:40:15Z","title":"Early Stopping Against Label Noise Without Validation Data","summary":"  Early stopping methods in deep learning face the challenge of balancing the\nvolume of training and validation data, especially in the presence of label\nnoise. Concretely, sparing more data for validation from training data would\nlimit the performance of the learned model, yet insufficient validation data\ncould result in a sub-optimal selection of the desired model. In this paper, we\npropose a novel early stopping method called Label Wave, which does not require\nvalidation data for selecting the desired model in the presence of label noise.\nIt works by tracking the changes in the model's predictions on the training set\nduring the training process, aiming to halt training before the model unduly\nfits mislabeled data. This method is empirically supported by our observation\nthat minimum fluctuations in predictions typically occur at the training epoch\nbefore the model excessively fits mislabeled data. Through extensive\nexperiments, we show both the effectiveness of the Label Wave method across\nvarious settings and its capability to enhance the performance of existing\nmethods for learning with noisy labels.\n","authors":["Suqin Yuan","Lei Feng","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07551v1.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2502.07549v1","updated":"2025-02-11T13:39:35Z","published":"2025-02-11T13:39:35Z","title":"HGTUL: A Hypergraph-based Model For Trajectory User Linking","summary":"  Trajectory User Linking (TUL), which links anonymous trajectories with users\nwho generate them, plays a crucial role in modeling human mobility. Despite\nsignificant advancements in this field, existing studies primarily neglect the\nhigh-order inter-trajectory relationships, which represent complex associations\namong multiple trajectories, manifested through multi-location co-occurrence\npatterns emerging when trajectories intersect at various Points of Interest\n(POIs). Furthermore, they also overlook the variable influence of POIs on\ndifferent trajectories, as well as the user class imbalance problem caused by\ndisparities in user activity levels and check-in frequencies. To address these\nlimitations, we propose a novel HyperGraph-based multi-perspective Trajectory\nUser Linking model (HGTUL). Our model learns trajectory representations from\nboth relational and spatio-temporal perspectives: (1) it captures high-order\nassociations among trajectories by constructing a trajectory hypergraph and\nleverages a hypergraph attention network to learn the variable impact of POIs\non trajectories; (2) it models the spatio-temporal characteristics of\ntrajectories by incorporating their temporal and spatial information into a\nsequential encoder. Moreover, we design a data balancing method to effectively\naddress the user class imbalance problem and experimentally validate its\nsignificance in TUL. Extensive experiments on three real-world datasets\ndemonstrate that HGTUL outperforms state-of-the-art baselines, achieving\nimprovements of 2.57%~20.09% and 5.68%~26.00% in ACC@1 and Macro-F1 metrics,\nrespectively.\n","authors":["Fengjie Chang","Xinning Zhu","Zheng Hu","Yang Qin"],"pdf_url":"https://arxiv.org/pdf/2502.07549v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.01175v4","updated":"2025-02-11T13:35:23Z","published":"2024-06-03T10:14:32Z","title":"NeoRL: Efficient Exploration for Nonepisodic RL","summary":"  We study the problem of nonepisodic reinforcement learning (RL) for nonlinear\ndynamical systems, where the system dynamics are unknown and the RL agent has\nto learn from a single trajectory, i.e., without resets. We propose Nonepisodic\nOptimistic RL (NeoRL), an approach based on the principle of optimism in the\nface of uncertainty. NeoRL uses well-calibrated probabilistic models and plans\noptimistically w.r.t. the epistemic uncertainty about the unknown dynamics.\nUnder continuity and bounded energy assumptions on the system, we provide a\nfirst-of-its-kind regret bound of $O(\\Gamma_T \\sqrt{T})$ for general nonlinear\nsystems with Gaussian process dynamics. We compare NeoRL to other baselines on\nseveral deep RL environments and empirically demonstrate that NeoRL achieves\nthe optimal average cost while incurring the least regret.\n","authors":["Bhavya Sukhija","Lenart Treven","Florian Dörfler","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2406.01175v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07547v1","updated":"2025-02-11T13:34:09Z","published":"2025-02-11T13:34:09Z","title":"Instance-dependent Early Stopping","summary":"  In machine learning practice, early stopping has been widely used to\nregularize models and can save computational costs by halting the training\nprocess when the model's performance on a validation set stops improving.\nHowever, conventional early stopping applies the same stopping criterion to all\ninstances without considering their individual learning statuses, which leads\nto redundant computations on instances that are already well-learned. To\nfurther improve the efficiency, we propose an Instance-dependent Early Stopping\n(IES) method that adapts the early stopping mechanism from the entire training\nset to the instance level, based on the core principle that once the model has\nmastered an instance, the training on it should stop. IES considers an instance\nas mastered if the second-order differences of its loss value remain within a\nsmall range around zero. This offers a more consistent measure of an instance's\nlearning status compared with directly using the loss value, and thus allows\nfor a unified threshold to determine when an instance can be excluded from\nfurther backpropagation. We show that excluding mastered instances from\nbackpropagation can increase the gradient norms, thereby accelerating the\ndecrease of the training loss and speeding up the training process. Extensive\nexperiments on benchmarks demonstrate that IES method can reduce\nbackpropagation instances by 10%-50% while maintaining or even slightly\nimproving the test accuracy and transfer learning performance of a model.\n","authors":["Suqin Yuan","Runqi Lin","Lei Feng","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07547v1.pdf","comment":"Accepted by ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2410.14281v2","updated":"2025-02-11T13:28:10Z","published":"2024-10-18T08:38:12Z","title":"PLMTrajRec: A Scalable and Generalizable Trajectory Recovery Method with\n  Pre-trained Language Models","summary":"  Spatiotemporal trajectory data is crucial for various applications. However,\nissues such as device malfunctions and network instability often cause sparse\ntrajectories, leading to lost detailed movement information. Recovering the\nmissing points in sparse trajectories to restore the detailed information is\nthus essential. Despite recent progress, several challenges remain. First, the\nlack of large-scale dense trajectory data makes it difficult to train a\ntrajectory recovery model from scratch. Second, the varying spatiotemporal\ncorrelations in sparse trajectories make it hard to generalize recovery across\ndifferent sampling intervals. Third, the lack of location information\ncomplicates the extraction of road conditions for missing points.\n  To address these challenges, we propose a novel trajectory recovery model\ncalled PLMTrajRec. It leverages the scalability of a pre-trained language model\n(PLM) and can be fine-tuned with only a limited set of dense trajectories. To\nhandle different sampling intervals in sparse trajectories, we first convert\neach trajectory's sampling interval and movement features into natural language\nrepresentations, allowing the PLM to recognize its interval. We then introduce\na trajectory encoder to unify trajectories of varying intervals into a single\ninterval and capture their spatiotemporal relationships. To obtain road\nconditions for missing points, we propose an area flow-guided implicit\ntrajectory prompt, which models road conditions by collecting traffic flows in\neach region. We also introduce a road condition passing mechanism that uses\nobserved points' road conditions to infer those of the missing points.\nExperiments on two public trajectory datasets with three sampling intervals\neach demonstrate the effectiveness, scalability, and generalization ability of\nPLMTrajRec.\n","authors":["Tonglong Wei","Yan Lin","Youfang Lin","Shengnan Guo","Jilin Hu","Haitao Yuan","Gao Cong","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2410.14281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01567v2","updated":"2025-02-11T13:27:59Z","published":"2024-05-24T18:39:20Z","title":"MeMo: Meaningful, Modular Controllers via Noise Injection","summary":"  Robots are often built from standardized assemblies, (e.g. arms, legs, or\nfingers), but each robot must be trained from scratch to control all the\nactuators of all the parts together. In this paper we demonstrate a new\napproach that takes a single robot and its controller as input and produces a\nset of modular controllers for each of these assemblies such that when a new\nrobot is built from the same parts, its control can be quickly learned by\nreusing the modular controllers. We achieve this with a framework called MeMo\nwhich learns (Me)aningful, (Mo)dular controllers. Specifically, we propose a\nnovel modularity objective to learn an appropriate division of labor among the\nmodules. We demonstrate that this objective can be optimized simultaneously\nwith standard behavior cloning loss via noise injection. We benchmark our\nframework in locomotion and grasping environments on simple to complex robot\nmorphology transfer. We also show that the modules help in task transfer. On\nboth structure and task transfer, MeMo achieves improved training efficiency to\ngraph neural network and Transformer baselines.\n","authors":["Megan Tjandrasuwita","Jie Xu","Armando Solar-Lezama","Wojciech Matusik"],"pdf_url":"https://arxiv.org/pdf/2407.01567v2.pdf","comment":"NeurIPS 2024; 29 pages, 21 figures"},{"id":"http://arxiv.org/abs/2402.01138v4","updated":"2025-02-11T13:18:57Z","published":"2024-02-02T04:30:58Z","title":"Graph Neural Networks in EEG-based Emotion Recognition: A Survey","summary":"  Compared to other modalities, EEG-based emotion recognition can intuitively\nrespond to the emotional patterns in the human brain and, therefore, has become\none of the most concerning tasks in the brain-computer interfaces field. Since\ndependencies within brain regions are closely related to emotion, a significant\ntrend is to develop Graph Neural Networks (GNNs) for EEG-based emotion\nrecognition. However, brain region dependencies in emotional EEG have\nphysiological bases that distinguish GNNs in this field from those in other\ntime series fields. Besides, there is neither a comprehensive review nor\nguidance for constructing GNNs in EEG-based emotion recognition. In the survey,\nour categorization reveals the commonalities and differences of existing\napproaches under a unified framework of graph construction. We analyze and\ncategorize methods from three stages in the framework to provide clear guidance\non constructing GNNs in EEG-based emotion recognition. In addition, we discuss\nseveral open challenges and future directions, such as Temporal full-connected\ngraph and Graph condensation.\n","authors":["Chenyu Liu","Xinliang Zhou","Yihao Wu","Ruizhi Yang","Zhongruo Wang","Liming Zhai","Ziyu Jia","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.01138v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07532v1","updated":"2025-02-11T13:15:16Z","published":"2025-02-11T13:15:16Z","title":"Diffusion-LAM: Probabilistic Limited Area Weather Forecasting with\n  Diffusion","summary":"  Machine learning methods have been shown to be effective for weather\nforecasting, based on the speed and accuracy compared to traditional numerical\nmodels. While early efforts primarily concentrated on deterministic\npredictions, the field has increasingly shifted toward probabilistic\nforecasting to better capture the forecast uncertainty. Most machine\nlearning-based models have been designed for global-scale predictions, with\nonly limited work targeting regional or limited area forecasting, which allows\nmore specialized and flexible modeling for specific locations. This work\nintroduces Diffusion-LAM, a probabilistic limited area weather model leveraging\nconditional diffusion. By conditioning on boundary data from surrounding\nregions, our approach generates forecasts within a defined area. Experimental\nresults on the MEPS limited area dataset demonstrate the potential of\nDiffusion-LAM to deliver accurate probabilistic forecasts, highlighting its\npromise for limited-area weather prediction.\n","authors":["Erik Larsson","Joel Oskarsson","Tomas Landelius","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2502.07532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07531v1","updated":"2025-02-11T13:11:59Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07529v1","updated":"2025-02-11T13:10:34Z","published":"2025-02-11T13:10:34Z","title":"Training Deep Learning Models with Norm-Constrained LMOs","summary":"  In this work, we study optimization methods that leverage the linear\nminimization oracle (LMO) over a norm-ball. We propose a new stochastic family\nof algorithms that uses the LMO to adapt to the geometry of the problem and,\nperhaps surprisingly, show that they can be applied to unconstrained problems.\nThe resulting update rule unifies several existing optimization methods under a\nsingle framework. Furthermore, we propose an explicit choice of norm for deep\narchitectures, which, as a side benefit, leads to the transferability of\nhyperparameters across model sizes. Experimentally, we demonstrate significant\nspeedups on nanoGPT training without any reliance on Adam. The proposed method\nis memory-efficient, requiring only one set of model weights and one set of\ngradients, which can be stored in half-precision.\n","authors":["Thomas Pethick","Wanyun Xie","Kimon Antonakopoulos","Zhenyu Zhu","Antonio Silveti-Falls","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2502.07529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07528v1","updated":"2025-02-11T13:09:09Z","published":"2025-02-11T13:09:09Z","title":"Forecasting the future development in quality and value of professional\n  football players for applications in team management","summary":"  Transfers in professional football (soccer) are risky investments because of\nthe large transfer fees and high risks involved. Although data-driven models\ncan be used to improve transfer decisions, existing models focus on describing\nplayers' historical progress, leaving their future performance unknown.\nMoreover, recent developments have called for the use of explainable models\ncombined with uncertainty quantification of predictions. This paper assesses\nexplainable machine learning models based on predictive accuracy and\nuncertainty quantification methods for the prediction of the future development\nin quality and transfer value of professional football players. Using a\nhistorical data set of data-driven indicators describing player quality and the\ntransfer value of a football player, the models are trained to forecast player\nquality and player value one year ahead. These two prediction problems\ndemonstrate the efficacy of tree-based models, particularly random forest and\nXGBoost, in making accurate predictions. In general, the random forest model is\nfound to be the most suitable model because it provides accurate predictions as\nwell as an uncertainty quantification method that naturally arises from the\nbagging procedure of the random forest model. Additionally, our research shows\nthat the development of player performance contains nonlinear patterns and\ninteractions between variables, and that time series information can provide\nuseful information for the modeling of player performance metrics. Our research\nprovides models to help football clubs make more informed, data-driven transfer\ndecisions by forecasting player quality and transfer value.\n","authors":["Koen W. van Arem","Floris Goes-Smit","Jakob Söhl"],"pdf_url":"https://arxiv.org/pdf/2502.07528v1.pdf","comment":"The article itself is on the pages 1-27. The data set used in this\n  article is described in the appendix at the pages 28-35"},{"id":"http://arxiv.org/abs/2502.07527v1","updated":"2025-02-11T13:08:03Z","published":"2025-02-11T13:08:03Z","title":"NatureLM: Deciphering the Language of Nature for Scientific Discovery","summary":"  Foundation models have revolutionized natural language processing and\nartificial intelligence, significantly enhancing how machines comprehend and\ngenerate human languages. Inspired by the success of these foundation models,\nresearchers have developed foundation models for individual scientific domains,\nincluding small molecules, materials, proteins, DNA, and RNA. However, these\nmodels are typically trained in isolation, lacking the ability to integrate\nacross different scientific domains. Recognizing that entities within these\ndomains can all be represented as sequences, which together form the \"language\nof nature\", we introduce Nature Language Model (briefly, NatureLM), a\nsequence-based science foundation model designed for scientific discovery.\nPre-trained with data from multiple scientific domains, NatureLM offers a\nunified, versatile model that enables various applications including: (i)\ngenerating and optimizing small molecules, proteins, RNA, and materials using\ntext instructions; (ii) cross-domain generation/design, such as\nprotein-to-molecule and protein-to-RNA generation; and (iii) achieving\nstate-of-the-art performance in tasks like SMILES-to-IUPAC translation and\nretrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach\nfor various scientific tasks, including drug discovery (hit\ngeneration/optimization, ADMET optimization, synthesis), novel material design,\nand the development of therapeutic proteins or nucleotides. We have developed\nNatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion\nparameters) and observed a clear improvement in performance as the model size\nincreases.\n","authors":["Yingce Xia","Peiran Jin","Shufang Xie","Liang He","Chuan Cao","Renqian Luo","Guoqing Liu","Yue Wang","Zequn Liu","Yuan-Jyue Chen","Zekun Guo","Yeqi Bai","Pan Deng","Yaosen Min","Ziheng Lu","Hongxia Hao","Han Yang","Jielan Li","Chang Liu","Jia Zhang","Jianwei Zhu","Kehan Wu","Wei Zhang","Kaiyuan Gao","Qizhi Pei","Qian Wang","Xixian Liu","Yanting Li","Houtian Zhu","Yeqing Lu","Mingqian Ma","Zun Wang","Tian Xie","Krzysztof Maziarz","Marwin Segler","Zhao Yang","Zilong Chen","Yu Shi","Shuxin Zheng","Lijun Wu","Chen Hu","Peggy Dai","Tie-Yan Liu","Haiguang Liu","Tao Qin"],"pdf_url":"https://arxiv.org/pdf/2502.07527v1.pdf","comment":"81 pages"},{"id":"http://arxiv.org/abs/2502.07523v1","updated":"2025-02-11T12:55:32Z","published":"2025-02-11T12:55:32Z","title":"Scaling Off-Policy Reinforcement Learning with Batch and Weight\n  Normalization","summary":"  Reinforcement learning has achieved significant milestones, but sample\nefficiency remains a bottleneck for real-world applications. Recently, CrossQ\nhas demonstrated state-of-the-art sample efficiency with a low update-to-data\n(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with\nhigher UTD ratios. We identify challenges in the training dynamics, which are\nemphasized by higher UTD ratios. To address these, we integrate weight\nnormalization into the CrossQ framework, a solution that stabilizes training,\nhas been shown to prevent potential loss of plasticity and keeps the effective\nlearning rate constant. Our proposed approach reliably scales with increasing\nUTD ratios, achieving competitive performance across 25 challenging continuous\ncontrol tasks on the DeepMind Control Suite and Myosuite benchmarks, notably\nthe complex dog and humanoid environments. This work eliminates the need for\ndrastic interventions, such as network resets, and offers a simple yet robust\npathway for improving sample efficiency and scalability in model-free\nreinforcement learning.\n","authors":["Daniel Palenicek","Florian Vogt","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2502.07523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11107v2","updated":"2025-02-11T12:51:40Z","published":"2024-07-15T15:22:52Z","title":"Latent Linear Quadratic Regulator for Robotic Control Tasks","summary":"  Model predictive control (MPC) has played a more crucial role in various\nrobotic control tasks, but its high computational requirements are concerning,\nespecially for nonlinear dynamical models. This paper presents a\n$\\textbf{la}$tent $\\textbf{l}$inear $\\textbf{q}$uadratic $\\textbf{r}$egulator\n(LaLQR) that maps the state space into a latent space, on which the dynamical\nmodel is linear and the cost function is quadratic, allowing the efficient\napplication of LQR. We jointly learn this alternative system by imitating the\noriginal MPC. Experiments show LaLQR's superior efficiency and generalization\ncompared to other baselines.\n","authors":["Yuan Zhang","Shaohui Yang","Toshiyuki Ohtsuka","Colin Jones","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2407.11107v2.pdf","comment":"Accepted at RSS 2024 workshop on Koopman Operators in Robotics"},{"id":"http://arxiv.org/abs/2501.02737v2","updated":"2025-02-11T12:38:35Z","published":"2025-01-06T03:11:12Z","title":"Holistic Semantic Representation for Navigational Trajectory Generation","summary":"  Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation.\n","authors":["Ji Cao","Tongya Zheng","Qinghong Guo","Yu Wang","Junshu Dai","Shunyu Liu","Jie Yang","Jie Song","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2501.02737v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.07516v1","updated":"2025-02-11T12:36:00Z","published":"2025-02-11T12:36:00Z","title":"The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation","summary":"  Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study adopts a data-driven approach and presents\nthe first systematic attempt to identify prompts and text tokens in MIMIC-CXR\nthat contribute the most to training data memorization. Our analysis reveals an\nunexpected finding: prompts containing traces of de-identification procedures\nare among the most memorized, with de-identification markers contributing the\nmost. Furthermore, we also find existing inference-time memorization mitigation\nstrategies are ineffective and fail to sufficiently reduce the model's reliance\non memorized text tokens highlighting a broader issue in T2I synthesis with\nMIMIC-CXR. On this front, we propose actionable strategies to enhance privacy\nand improve the reliability of generative models in medical imaging. Finally,\nour results provide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset.\n","authors":["Raman Dutt"],"pdf_url":"https://arxiv.org/pdf/2502.07516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07514v1","updated":"2025-02-11T12:33:33Z","published":"2025-02-11T12:33:33Z","title":"A Near-optimal, Scalable and Corruption-tolerant Framework for\n  Stochastic Bandits: From Single-Agent to Multi-Agent and Beyond","summary":"  We investigate various stochastic bandit problems in the presence of\nadversarial corruption. A seminal contribution to this area is the\nBARBAR~\\citep{gupta2019better} algorithm, which is both simple and efficient,\ntolerating significant levels of corruption with nearly no degradation in\nperformance. However, its regret upper bound exhibits a complexity of $O(KC)$,\nwhile the lower bound is $\\Omega(C)$. In this paper, we enhance the BARBAR\nalgorithm by proposing a novel framework called BARBAT, which eliminates the\nfactor of $K$ and achieves an optimal regret bound up to a logarithmic factor.\nWe also demonstrate how BARBAT can be extended to various settings, including\ngraph bandits, combinatorial semi-bandits, batched bandits and multi-agent\nbandits. In comparison to the Follow-The-Regularized-Leader (FTRL) family of\nmethods, which provide a best-of-both-worlds guarantee, our approach is more\nefficient and parallelizable. Notably, FTRL-based methods face challenges in\nscaling to batched and multi-agent settings.\n","authors":["Zicheng Hu","Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07799v3","updated":"2025-02-11T12:32:59Z","published":"2023-10-11T18:32:21Z","title":"Domain-invariant Clinical Representation Learning by Bridging Data\n  Distribution Shift across EMR Datasets","summary":"  Emerging diseases present challenges in symptom recognition and timely\nclinical intervention due to limited available information. An effective\nprognostic model could assist physicians in making accurate diagnoses and\ndesigning personalized treatment plans to prevent adverse outcomes. However, in\nthe early stages of disease emergence, several factors hamper model\ndevelopment: limited data collection, insufficient clinical experience, and\nprivacy and ethical concerns restrict data availability and complicate accurate\nlabel assignment. Furthermore, Electronic Medical Record (EMR) data from\ndifferent diseases or sources often exhibit significant cross-dataset feature\nmisalignment, severely impacting the effectiveness of deep learning models. We\npresent a domain-invariant representation learning method that constructs a\ntransition model between source and target datasets. By constraining the\ndistribution shift of features generated across different domains, we capture\ndomain-invariant features specifically relevant to downstream tasks, developing\na unified domain-invariant encoder that achieves better feature representation\nacross various task domains. Experimental results across multiple target tasks\ndemonstrate that our proposed model surpasses competing baseline methods and\nachieves faster training convergence, particularly when working with limited\ndata. Extensive experiments validate our method's effectiveness in providing\nmore accurate predictions for emerging pandemics and other diseases. Code is\npublicly available at https://github.com/wang1yuhang/domain_invariant_network.\n","authors":["Zhongji Zhang","Yuhang Wang","Yinghao Zhu","Xinyu Ma","Yasha Wang","Junyi Gao","Liantao Ma","Wen Tang","Xiaoyun Zhang","Ling Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07799v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17472v4","updated":"2025-02-11T12:29:00Z","published":"2024-02-27T12:53:15Z","title":"RAGFormer: Learning Semantic Attributes and Topological Structure for\n  Fraud Detection","summary":"  Fraud detection remains a challenging task due to the complex and deceptive\nnature of fraudulent activities. Current approaches primarily concentrate on\nlearning only one perspective of the graph: either the topological structure of\nthe graph or the attributes of individual nodes. However, we conduct empirical\nstudies to reveal that these two types of features, while nearly orthogonal,\nare each independently effective. As a result, previous methods can not fully\ncapture the comprehensive characteristics of the fraud graph. To address this\ndilemma, we present a novel framework called Relation-Aware GNN with\ntransFormer~(RAGFormer) which simultaneously embeds both semantic and\ntopological features into a target node. The simple yet effective network\nconsists of a semantic encoder, a topology encoder, and an attention fusion\nmodule. The semantic encoder utilizes Transformer to learn semantic features\nand node interactions across different relations. We introduce Relation-Aware\nGNN as the topology encoder to learn topological features and node interactions\nwithin each relation. These two complementary features are interleaved through\nan attention fusion module to support prediction by both orthogonal features.\nExtensive experiments on two popular public datasets demonstrate that RAGFormer\nachieves state-of-the-art performance. The significant improvement of RAGFormer\nin an industrial credit card fraud detection dataset further validates the\napplicability of our method in real-world business scenarios.\n","authors":["Haolin Li","Shuyang Jiang","Lifeng Zhang","Siyuan Du","Guangnan Ye","Hongfeng Chai"],"pdf_url":"https://arxiv.org/pdf/2402.17472v4.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.07510v1","updated":"2025-02-11T12:28:47Z","published":"2025-02-11T12:28:47Z","title":"Joint Metric Space Embedding by Unbalanced OT with Gromov-Wasserstein\n  Marginal Penalization","summary":"  We propose a new approach for unsupervised alignment of heterogeneous\ndatasets, which maps data from two different domains without any known\ncorrespondences to a common metric space. Our method is based on an unbalanced\noptimal transport problem with Gromov-Wasserstein marginal penalization. It can\nbe seen as a counterpart to the recently introduced joint multidimensional\nscaling method. We prove that there exists a minimizer of our functional and\nthat for penalization parameters going to infinity, the corresponding sequence\nof minimizers converges to a minimizer of the so-called embedded Wasserstein\ndistance. Our model can be reformulated as a quadratic, multi-marginal,\nunbalanced optimal transport problem, for which a bi-convex relaxation admits a\nnumerical solver via block-coordinate descent. We provide numerical examples\nfor joint embeddings in Euclidean as well as non-Euclidean spaces.\n","authors":["Florian Beier","Moritz Piening","Robert Beinert","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2502.07510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00134v4","updated":"2025-02-11T12:28:36Z","published":"2024-08-29T12:55:10Z","title":"MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale","summary":"  Multi-agent pathfinding (MAPF) is a problem that generally requires finding\ncollision-free paths for multiple agents in a shared environment. Solving MAPF\noptimally, even under restrictive assumptions, is NP-hard, yet efficient\nsolutions for this problem are critical for numerous applications, such as\nautomated warehouses and transportation systems. Recently, learning-based\napproaches to MAPF have gained attention, particularly those leveraging deep\nreinforcement learning. Typically, such learning-based MAPF solvers are\naugmented with additional components like single-agent planning or\ncommunication. Orthogonally, in this work we rely solely on imitation learning\nthat leverages a large dataset of expert MAPF solutions and transformer-based\nneural network to create a foundation model for MAPF called MAPF-GPT. The\nlatter is capable of generating actions without additional heuristics or\ncommunication. MAPF-GPT demonstrates zero-shot learning abilities when solving\nthe MAPF problems that are not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable MAPF solvers\non a diverse range of problem instances and is computationally efficient during\ninference.\n","authors":["Anton Andreychuk","Konstantin Yakovlev","Aleksandr Panov","Alexey Skrynnik"],"pdf_url":"https://arxiv.org/pdf/2409.00134v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00524v3","updated":"2025-02-11T12:25:17Z","published":"2023-01-02T04:27:25Z","title":"Learning Confident Classifiers in the Presence of Label Noise","summary":"  The success of Deep Neural Network (DNN) models significantly depends on the\nquality of provided annotations. In medical image segmentation, for example,\nhaving multiple expert annotations for each data point is common to minimize\nsubjective annotation bias. Then, the goal of estimation is to filter out the\nlabel noise and recover the ground-truth masks, which are not explicitly given.\nThis paper proposes a probabilistic model for noisy observations that allows us\nto build a confident classification and segmentation models. To accomplish it,\nwe explicitly model label noise and introduce a new information-based\nregularization that pushes the network to recover the ground-truth labels. In\naddition, for segmentation task we adjust the loss function by prioritizing\nlearning in high-confidence regions where all the annotators agree on labeling.\nWe evaluate the proposed method on a series of classification tasks such as\nnoisy versions of MNIST, CIFAR-10, Fashion-MNIST datasets as well as CIFAR-10N,\nwhich is real-world dataset with noisy human annotations. Additionally, for\nsegmentation task, we consider several medical imaging datasets, such as, LIDC\nand RIGA that reflect real-world inter-variability among multiple annotators.\nOur experiments show that our algorithm outperforms state-of-the-art solutions\nfor the considered classification and segmentation problems.\n","authors":["Asma Ahmed Hashmi","Aigerim Zhumabayeva","Nikita Kotelevskii","Artem Agafonov","Mohammad Yaqub","Maxim Panov","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2301.00524v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04078v2","updated":"2025-02-11T12:16:21Z","published":"2024-12-05T11:24:27Z","title":"Mind the Gap: Towards Generalizable Autonomous Penetration Testing via\n  Domain Randomization and Meta-Reinforcement Learning","summary":"  With increasing numbers of vulnerabilities exposed on the internet,\nautonomous penetration testing (pentesting) has emerged as a promising research\narea. Reinforcement learning (RL) is a natural fit for studying this topic.\nHowever, two key challenges limit the applicability of RL-based autonomous\npentesting in real-world scenarios: (a) training environment dilemma --\ntraining agents in simulated environments is sample-efficient while ensuring\ntheir realism remains challenging; (b) poor generalization ability -- agents'\npolicies often perform poorly when transferred to unseen scenarios, with even\nslight changes potentially causing significant generalization gap. To this end,\nwe propose GAP, a generalizable autonomous pentesting framework that aims to\nrealizes efficient policy training in realistic environments and train\ngeneralizable agents capable of drawing inferences about other cases from one\ninstance. GAP introduces a Real-to-Sim-to-Real pipeline that (a) enables\nend-to-end policy learning in unknown real environments while constructing\nrealistic simulations; (b) improves agents' generalization ability by\nleveraging domain randomization and meta-RL learning.Specially, we are among\nthe first to apply domain randomization in autonomous pentesting and propose a\nlarge language model-powered domain randomization method for synthetic\nenvironment generation. We further apply meta-RL to improve agents'\ngeneralization ability in unseen environments by leveraging synthetic\nenvironments. The combination of two methods effectively bridges the\ngeneralization gap and improves agents' policy adaptation\nperformance.Experiments are conducted on various vulnerable virtual machines,\nwith results showing that GAP can enable policy learning in various realistic\nenvironments, achieve zero-shot policy transfer in similar environments, and\nrealize rapid policy adaptation in dissimilar environments.\n","authors":["Shicheng Zhou","Jingju Liu","Yuliang Lu","Jiahai Yang","Yue Zhang","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2412.04078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07503v1","updated":"2025-02-11T12:11:40Z","published":"2025-02-11T12:11:40Z","title":"Harnessing Language's Fractal Geometry with Recursive Inference Scaling","summary":"  Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation.\n","authors":["Ibrahim Alabdulmohsin","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.07503v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.04411v2","updated":"2025-02-11T12:09:51Z","published":"2025-02-06T11:26:30Z","title":"Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and\n  Uncertainty Based Routing","summary":"  Model merging aggregates Large Language Models (LLMs) finetuned on different\ntasks into a stronger one. However, parameter conflicts between models leads to\nperformance degradation in averaging. While model routing addresses this issue\nby selecting individual models during inference, it imposes excessive storage\nand compute costs, and fails to leverage the common knowledge from different\nmodels. In this work, we observe that different layers exhibit varying levels\nof parameter conflicts. Building on this insight, we average layers with\nminimal parameter conflicts and use a novel task-level expert routing for\nlayers with significant conflicts. To further reduce storage costs, inspired by\ntask arithmetic sparsity, we decouple multiple fine-tuned experts into a dense\nexpert and several sparse experts. Considering the out-of-distribution samples,\nwe select and merge appropriate experts based on the task uncertainty of the\ninput data. We conduct extensive experiments on both LLaMA and Qwen with\nvarying parameter scales, and evaluate on real-world reasoning tasks. Results\ndemonstrate that our method consistently achieves significant performance\nimprovements while requiring less system cost compared to existing methods.\n","authors":["Kunfeng Lai","Zhenheng Tang","Xinglin Pan","Peijie Dong","Xiang Liu","Haolan Chen","Li Shen","Bo Li","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2502.04411v2.pdf","comment":"work in progress. arXiv admin note: text overlap with\n  arXiv:2405.09673 by other authors"},{"id":"http://arxiv.org/abs/2502.06403v2","updated":"2025-02-11T12:08:04Z","published":"2025-02-10T12:44:49Z","title":"The AI off-switch problem as a signalling game: bounded rationality and\n  incomparability","summary":"  The off-switch problem is a critical challenge in AI control: if an AI system\nresists being switched off, it poses a significant risk. In this paper, we\nmodel the off-switch problem as a signalling game, where a human decision-maker\ncommunicates its preferences about some underlying decision problem to an AI\nagent, which then selects actions to maximise the human's utility. We assume\nthat the human is a bounded rational agent and explore various bounded\nrationality mechanisms. Using real machine learning models, we reprove prior\nresults and demonstrate that a necessary condition for an AI system to refrain\nfrom disabling its off-switch is its uncertainty about the human's utility. We\nalso analyse how message costs influence optimal strategies and extend the\nanalysis to scenarios involving incomparability.\n","authors":["Alessio benavoli","Alessandro facchini","Marco Zaffalon"],"pdf_url":"https://arxiv.org/pdf/2502.06403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07500v1","updated":"2025-02-11T12:03:18Z","published":"2025-02-11T12:03:18Z","title":"Unified Graph Networks (UGN): A Deep Neural Framework for Solving Graph\n  Problems","summary":"  Deep neural networks have enabled researchers to create powerful generalized\nframeworks, such as transformers, that can be used to solve well-studied\nproblems in various application domains, such as text and image. However, such\ngeneralized frameworks are not available for solving graph problems. Graph\nstructures are ubiquitous in many applications around us and many graph\nproblems have been widely studied over years. In recent times, there has been a\nsurge in deep neural network based approaches to solve graph problems, with\ngrowing availability of graph structured datasets across diverse domains.\nNevertheless, existing methods are mostly tailored to solve a specific task and\nlack the capability to create a generalized model leading to solutions for\ndifferent downstream tasks. In this work, we propose a novel,\nresource-efficient framework named \\emph{U}nified \\emph{G}raph \\emph{N}etwork\n(UGN) by leveraging the feature extraction capability of graph convolutional\nneural networks (GCN) and 2-dimensional convolutional neural networks (Conv2D).\nUGN unifies various graph learning tasks, such as link prediction, node\nclassification, community detection, graph-to-graph translation, knowledge\ngraph completion, and more, within a cohesive framework, while exercising\nminimal task-specific extensions (e.g., formation of supernodes for coarsening\nmassive networks to increase scalability, use of \\textit{mean target\nconnectivity matrix} (MTCM) representation for achieving scalability in graph\ntranslation task, etc.) to enhance the generalization capability of graph\nlearning and analysis. We test the novel UGN framework for six uncorrelated\ngraph problems, using twelve different datasets. Experimental results show that\nUGN outperforms the state-of-the-art baselines by a significant margin on ten\ndatasets, while producing comparable results on the remaining dataset.\n","authors":["Rudrajit Dawn","Madhusudan Ghosh","Partha Basuchowdhuri","Sudip Kumar Naskar"],"pdf_url":"https://arxiv.org/pdf/2502.07500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07497v1","updated":"2025-02-11T11:59:03Z","published":"2025-02-11T11:59:03Z","title":"On Training-Conditional Conformal Prediction and Binomial Proportion\n  Confidence Intervals","summary":"  Estimating the expectation of a Bernoulli random variable based on N\nindependent trials is a classical problem in statistics, typically addressed\nusing Binomial Proportion Confidence Intervals (BPCI). In the control systems\ncommunity, many critical tasks-such as certifying the statistical safety of\ndynamical systems-can be formulated as BPCI problems. Conformal Prediction\n(CP), a distribution-free technique for uncertainty quantification, has gained\nsignificant attention in recent years and has been applied to various control\nsystems problems, particularly to address uncertainties in learned dynamics or\ncontrollers. A variant known as training-conditional CP was recently employed\nto tackle the problem of safety certification. In this note, we highlight that\nthe use of training-conditional CP in this context does not provide valid\nsafety guarantees. We demonstrate why CP is unsuitable for BPCI problems and\nargue that traditional BPCI methods are better suited for statistical safety\ncertification.\n","authors":["Rudi Coppola","Manuel Mazo Jr"],"pdf_url":"https://arxiv.org/pdf/2502.07497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12600v2","updated":"2025-02-11T11:57:39Z","published":"2024-06-18T13:31:15Z","title":"Generalization bounds for mixing processes via delayed online-to-PAC\n  conversions","summary":"  We study the generalization error of statistical learning algorithms in a\nnon-i.i.d. setting, where the training data is sampled from a stationary mixing\nprocess. We develop an analytic framework for this scenario based on a\nreduction to online learning with delayed feedback. In particular, we show that\nthe existence of an online learning algorithm with bounded regret (against a\nfixed statistical learning algorithm in a specially constructed game of online\nlearning with delayed feedback) implies low generalization error of said\nstatistical learning method even if the data sequence is sampled from a mixing\ntime series. The rates demonstrate a trade-off between the amount of delay in\nthe online learning game and the degree of dependence between consecutive data\npoints, with near-optimal rates recovered in a number of well-studied settings\nwhen the delay is tuned appropriately as a function of the mixing time of the\nprocess.\n","authors":["Baptiste Abeles","Eugenio Clerico","Gergely Neu"],"pdf_url":"https://arxiv.org/pdf/2406.12600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07495v1","updated":"2025-02-11T11:54:56Z","published":"2025-02-11T11:54:56Z","title":"LLM-Sketch: Enhancing Network Sketches with LLM","summary":"  Network stream mining is fundamental to many network operations. Sketches, as\ncompact data structures that offer low memory overhead with bounded accuracy,\nhave emerged as a promising solution for network stream mining. Recent studies\nattempt to optimize sketches using machine learning; however, these approaches\nface the challenges of lacking adaptivity to dynamic networks and incurring\nhigh training costs. In this paper, we propose LLM-Sketch, based on the insight\nthat fields beyond the flow IDs in packet headers can also help infer flow\nsizes. By using a two-tier data structure and separately recording large and\nsmall flows, LLM-Sketch improves accuracy while minimizing memory usage.\nFurthermore, it leverages fine-tuned large language models (LLMs) to reliably\nestimate flow sizes. We evaluate LLM-Sketch on three representative tasks, and\nthe results demonstrate that LLM-Sketch outperforms state-of-the-art methods by\nachieving a $7.5\\times$ accuracy improvement.\n","authors":["Yuanpeng Li","Zhen Xu","Zongwei Lv","Yannan Hu","Yong Cui","Tong Yang"],"pdf_url":"https://arxiv.org/pdf/2502.07495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07491v1","updated":"2025-02-11T11:51:07Z","published":"2025-02-11T11:51:07Z","title":"Exploring Patterns Behind Sports","summary":"  This paper presents a comprehensive framework for time series prediction\nusing a hybrid model that combines ARIMA and LSTM. The model incorporates\nfeature engineering techniques, including embedding and PCA, to transform raw\ndata into a lower-dimensional representation while retaining key information.\nThe embedding technique is used to convert categorical data into continuous\nvectors, facilitating the capture of complex relationships. PCA is applied to\nreduce dimensionality and extract principal components, enhancing model\nperformance and computational efficiency. To handle both linear and nonlinear\npatterns in the data, the ARIMA model captures linear trends, while the LSTM\nmodel models complex nonlinear dependencies. The hybrid model is trained on\nhistorical data and achieves high accuracy, as demonstrated by low RMSE and MAE\nscores. Additionally, the paper employs the run test to assess the randomness\nof sequences, providing insights into the underlying patterns. Ablation studies\nare conducted to validate the roles of different components in the model,\ndemonstrating the significance of each module. The paper also utilizes the SHAP\nmethod to quantify the impact of traditional advantages on the predicted\nresults, offering a detailed understanding of feature importance. The KNN\nmethod is used to determine the optimal prediction interval, further enhancing\nthe model's accuracy. The results highlight the effectiveness of combining\ntraditional statistical methods with modern deep learning techniques for robust\ntime series forecasting in Sports.\n","authors":["Chang Liu","Chengcheng Ma","XuanQi Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12956v2","updated":"2025-02-11T11:50:15Z","published":"2025-01-22T15:29:09Z","title":"GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models","summary":"  Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment.\n","authors":["Pengxiang Zhao","Xiaoming Yuan"],"pdf_url":"https://arxiv.org/pdf/2501.12956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07490v1","updated":"2025-02-11T11:49:03Z","published":"2025-02-11T11:49:03Z","title":"Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More","summary":"  Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models.\n","authors":["Xialie Zhuang","Zhikai Jia","Jianjin Li","Zhenyu Zhang","Li Shen","Zheng Cao","Shiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07490v1.pdf","comment":"15 pages,7 figures"},{"id":"http://arxiv.org/abs/2502.07489v1","updated":"2025-02-11T11:48:22Z","published":"2025-02-11T11:48:22Z","title":"Physiome-ODE: A Benchmark for Irregularly Sampled Multivariate Time\n  Series Forecasting Based on Biological ODEs","summary":"  State-of-the-art methods for forecasting irregularly sampled time series with\nmissing values predominantly rely on just four datasets and a few small toy\nexamples for evaluation. While ordinary differential equations (ODE) are the\nprevalent models in science and engineering, a baseline model that forecasts a\nconstant value outperforms ODE-based models from the last five years on three\nof these existing datasets. This unintuitive finding hampers further research\non ODE-based models, a more plausible model family. In this paper, we develop a\nmethodology to generate irregularly sampled multivariate time series (IMTS)\ndatasets from ordinary differential equations and to select challenging\ninstances via rejection sampling. Using this methodology, we create\nPhysiome-ODE, a large and sophisticated benchmark of IMTS datasets consisting\nof 50 individual datasets, derived from real-world ordinary differential\nequations from research in biology. Physiome-ODE is the first benchmark for\nIMTS forecasting that we are aware of and an order of magnitude larger than the\ncurrent evaluation setting of four datasets. Using our benchmark Physiome-ODE,\nwe show qualitatively completely different results than those derived from the\ncurrent four datasets: on Physiome-ODE ODE-based models can play to their\nstrength and our benchmark can differentiate in a meaningful way between\ndifferent IMTS forecasting models. This way, we expect to give a new impulse to\nresearch on ODE-based time series modeling.\n","authors":["Christian Klötergens","Vijaya Krishna Yalavarthi","Randolf Scholz","Maximilian Stubbemann","Stefan Born","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2502.07489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07488v1","updated":"2025-02-11T11:48:04Z","published":"2025-02-11T11:48:04Z","title":"Improving Adaptive Moment Optimization via Preconditioner\n  Diagonalization","summary":"  Modern adaptive optimization methods, such as Adam and its variants, have\nemerged as the most widely used tools in deep learning over recent years. These\nalgorithms offer automatic mechanisms for dynamically adjusting the update step\nbased on estimates of gradient statistics. Compared to traditional algorithms\nlike Stochastic Gradient Descent, these adaptive methods are typically more\nrobust to model scale and hyperparameter tuning. However, the gradient\nstatistics employed by these methods often do not leverage sufficient gradient\ncovariance information, leading to suboptimal updates in certain directions of\nthe parameter space and potentially slower convergence. In this work, we keep\ntrack of such covariance statistics in the form of a structured preconditioner\nmatrix. Unlike other works, our approach does not apply direct approximations\nto estimate this matrix. We instead implement an invertible transformation that\nmaps the preconditioner matrix into a new space where it becomes approximately\ndiagonal. This enables a diagonal approximation of the preconditioner matrix in\nthe transformed space, offering several computational advantages. Empirical\nresults show that our approach can substantially enhance the convergence speed\nof modern adaptive optimizers. Notably, for large language models like LLaMA,\nwe can achieve a speedup of 2x compared to the baseline Adam. Additionally, our\nmethod can be integrated with memory-efficient optimizers like Adafactor to\nmanage computational overhead.\n","authors":["Son Nguyen","Bo Liu","Lizhang Chen","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07488v1.pdf","comment":"19 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.07480v1","updated":"2025-02-11T11:41:09Z","published":"2025-02-11T11:41:09Z","title":"Overfitting Regimes of Nadaraya-Watson Interpolators","summary":"  In recent years, there has been much interest in understanding the\ngeneralization behavior of interpolating predictors, which overfit on noisy\ntraining data. Whereas standard analyses are concerned with whether a method is\nconsistent or not, recent observations have shown that even inconsistent\npredictors can generalize well. In this work, we revisit the classic\ninterpolating Nadaraya-Watson (NW) estimator (also known as Shepard's method),\nand study its generalization capabilities through this modern viewpoint. In\nparticular, by varying a single bandwidth-like hyperparameter, we prove the\nexistence of multiple overfitting behaviors, ranging non-monotonically from\ncatastrophic, through benign, to tempered. Our results highlight how even\nclassical interpolating methods can exhibit intricate generalization behaviors.\nNumerical experiments complement our theory, demonstrating the same phenomena.\n","authors":["Daniel Barzilai","Guy Kornowski","Ohad Shamir"],"pdf_url":"https://arxiv.org/pdf/2502.07480v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2410.13341v2","updated":"2025-02-11T11:39:18Z","published":"2024-10-17T08:49:42Z","title":"Limits to scalable evaluation at the frontier: LLM as Judge won't beat\n  twice the data","summary":"  High quality annotations are increasingly a bottleneck in the explosively\ngrowing machine learning ecosystem. Scalable evaluation methods that avoid\ncostly annotation have therefore become an important research ambition. Many\nhope to use strong existing models in lieu of costly labels to provide cheap\nmodel evaluations. Unfortunately, this method of using models as judges\nintroduces biases, such as self-preferencing, that can distort model\ncomparisons. An emerging family of debiasing tools promises to fix these issues\nby using a few high quality labels to debias a large number of model judgments.\nIn this paper, we study how far such debiasing methods, in principle, can go.\nOur main result shows that when the judge is no more accurate than the\nevaluated model, no debiasing method can decrease the required amount of ground\ntruth labels by more than half. Our result speaks to the severe limitations of\nthe LLM-as-a-judge paradigm at the evaluation frontier where the goal is to\nassess newly released models that are possibly better than the judge. Through\nan empirical evaluation, we demonstrate that the sample size savings achievable\nin practice are even more modest than what our theoretical limit suggests.\nAlong the way, our work provides new observations about debiasing methods for\nmodel evaluation, and points out promising avenues for future work.\n","authors":["Florian E. Dorner","Vivian Y. Nastl","Moritz Hardt"],"pdf_url":"https://arxiv.org/pdf/2410.13341v2.pdf","comment":"ICLR 2025; 28 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.07469v1","updated":"2025-02-11T11:25:10Z","published":"2025-02-11T11:25:10Z","title":"5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma\n  Turbulence","summary":"  Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to achieving commercially viable fusion\npower is understanding plasma turbulence, which can significantly degrade\nplasma confinement. Modelling turbulence is crucial to design performing plasma\nscenarios for next-generation reactor-class devices and current experimental\nmachines. The nonlinear gyrokinetic equation underpinning turbulence modelling\nevolves a 5D distribution function over time. Solving this equation numerically\nis extremely expensive, requiring up to weeks for a single run to converge,\nmaking it unfeasible for iterative optimisation and control studies. In this\nwork, we propose a method for training neural surrogates for 5D gyrokinetic\nsimulations. Our method extends a hierarchical vision transformer to five\ndimensions and is trained on the 5D distribution function for the adiabatic\nelectron approximation. We demonstrate that our model can accurately infer\ndownstream physical quantities such as heat flux time trace and electrostatic\npotentials for single-step predictions two orders of magnitude faster than\nnumerical codes. Our work paves the way towards neural surrogates for plasma\nturbulence simulations to accelerate deployment of commercial energy production\nvia nuclear fusion.\n","authors":["Gianluca Galletti","Fabian Paischer","Paul Setinek","William Hornsby","Lorenzo Zanisi","Naomi Carey","Stanislas Pamela","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2502.07469v1.pdf","comment":"6 pages (+ references and appendix)"},{"id":"http://arxiv.org/abs/2409.13467v3","updated":"2025-02-11T11:25:03Z","published":"2024-09-20T12:55:43Z","title":"Higher-Order Message Passing for Glycan Representation Learning","summary":"  Glycans are the most complex biological sequence, with monosaccharides\nforming extended, non-linear sequences. As post-translational modifications,\nthey modulate protein structure, function, and interactions. Due to their\ndiversity and complexity, predictive models of glycan properties and functions\nare still insufficient.\n  Graph Neural Networks (GNNs) are deep learning models designed to process and\nanalyze graph-structured data. These architectures leverage the connectivity\nand relational information in graphs to learn effective representations of\nnodes, edges, and entire graphs. Iteratively aggregating information from\nneighboring nodes, GNNs capture complex patterns within graph data, making them\nparticularly well-suited for tasks such as link prediction or graph\nclassification across domains.\n  This work presents a new model architecture based on combinatorial complexes\nand higher-order message passing to extract features from glycan structures\ninto a latent space representation. The architecture is evaluated on an\nimproved GlycanML benchmark suite, establishing a new state-of-the-art\nperformance. We envision that these improvements will spur further advances in\ncomputational glycosciences and reveal the roles of glycans in biology.\n","authors":["Roman Joeres","Daniel Bojar"],"pdf_url":"https://arxiv.org/pdf/2409.13467v3.pdf","comment":"Accepted to MLSB Workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2502.07465v1","updated":"2025-02-11T11:16:59Z","published":"2025-02-11T11:16:59Z","title":"Crime Forecasting: A Spatio-temporal Analysis with Deep Learning Models","summary":"  This study uses deep-learning models to predict city partition crime counts\non specific days. It helps police enhance surveillance, gather intelligence,\nand proactively prevent crimes. We formulate crime count prediction as a\nspatiotemporal sequence challenge, where both input data and prediction targets\nare spatiotemporal sequences. In order to improve the accuracy of crime\nforecasting, we introduce a new model that combines Convolutional Neural\nNetworks (CNN) and Long Short-Term Memory (LSTM) networks. We conducted a\ncomparative analysis to access the effects of various data sequences, including\nraw and binned data, on the prediction errors of four deep learning forecasting\nmodels. Directly inputting raw crime data into the forecasting model causes\nhigh prediction errors, making the model unsuitable for real - world use. The\nfindings indicate that the proposed CNN-LSTM model achieves optimal performance\nwhen crime data is categorized into 10 or 5 groups. Data binning can enhance\nforecasting model performance, but poorly defined intervals may reduce map\ngranularity. Compared to dividing into 5 bins, binning into 10 intervals\nstrikes an optimal balance, preserving data characteristics and surpassing raw\ndata in predictive modelling efficacy.\n","authors":["Li Mao","Wei Du","Shuo Wen","Qi Li","Tong Zhang","Wei Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.07465v1.pdf","comment":"8 pages,6 figures"},{"id":"http://arxiv.org/abs/2502.07460v1","updated":"2025-02-11T11:11:05Z","published":"2025-02-11T11:11:05Z","title":"Logarithmic Regret for Online KL-Regularized Reinforcement Learning","summary":"  Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound.\n","authors":["Heyang Zhao","Chenlu Ye","Wei Xiong","Quanquan Gu","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17438v2","updated":"2025-02-11T11:10:32Z","published":"2024-11-26T13:54:24Z","title":"Object-centric proto-symbolic behavioural reasoning from pixels","summary":"  Autonomous intelligent agents must bridge computational challenges at\ndisparate levels of abstraction, from the low-level spaces of sensory input and\nmotor commands to the high-level domain of abstract reasoning and planning. A\nkey question in designing such agents is how best to instantiate the\nrepresentational space that will interface between these two levels -- ideally\nwithout requiring supervision in the form of expensive data annotations. These\nobjectives can be efficiently achieved by representing the world in terms of\nobjects (grounded in perception and action). In this work, we present a novel,\nbrain-inspired, deep-learning architecture that learns from pixels to\ninterpret, control, and reason about its environment, using object-centric\nrepresentations. We show the utility of our approach through tasks in synthetic\nenvironments that require a combination of (high-level) logical reasoning and\n(low-level) continuous control. Results show that the agent can learn emergent\nconditional behavioural reasoning, such as $(A \\to B) \\land (\\neg A \\to C)$, as\nwell as logical composition $(A \\to B) \\land (A \\to C) \\vdash A \\to (B \\land\nC)$ and XOR operations, and successfully controls its environment to satisfy\nobjectives deduced from these logical rules. The agent can adapt online to\nunexpected changes in its environment and is robust to mild violations of its\nworld model, thanks to dynamic internal desired goal generation. While the\npresent results are limited to synthetic settings (2D and 3D activated versions\nof dSprites), which fall short of real-world levels of complexity, the proposed\narchitecture shows how to manipulate grounded object representations, as a key\ninductive bias for unsupervised learning, to enable behavioral reasoning.\n","authors":["Ruben van Bergen","Justus Hübotter","Pablo Lanillos"],"pdf_url":"https://arxiv.org/pdf/2411.17438v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13735v2","updated":"2025-02-11T11:09:52Z","published":"2024-10-17T16:37:03Z","title":"Generative Conformal Prediction with Vectorized Non-Conformity Scores","summary":"  Conformal prediction (CP) provides model-agnostic uncertainty quantification\nwith guaranteed coverage, but conventional methods often produce overly\nconservative uncertainty sets, especially in multi-dimensional settings. This\nlimitation arises from simplistic non-conformity scores that rely solely on\nprediction error, failing to capture the prediction error distribution's\ncomplexity. To address this, we propose a generative conformal prediction\nframework with vectorized non-conformity scores, leveraging a generative model\nto sample multiple predictions from the fitted data distribution. By computing\nnon-conformity scores across these samples and estimating empirical quantiles\nat different density levels, we construct adaptive uncertainty sets using\ndensity-ranked uncertainty balls. This approach enables more precise\nuncertainty allocation -- yielding larger prediction sets in high-confidence\nregions and smaller or excluded sets in low-confidence regions -- enhancing\nboth flexibility and efficiency. We establish theoretical guarantees for\nstatistical validity and demonstrate through extensive numerical experiments\nthat our method outperforms state-of-the-art techniques on synthetic and\nreal-world datasets.\n","authors":["Minxing Zheng","Shixiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.13735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11676v3","updated":"2025-02-11T11:09:19Z","published":"2024-07-16T12:52:29Z","title":"SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with\n  Realistic Validation On Diverse Modalities","summary":"  Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a\nlabeled source domain to perform well on an unlabeled target domain with some\ndata distribution shift. While many methods have been proposed in the\nliterature, fair and realistic evaluation remains an open question,\nparticularly due to methodological difficulties in selecting hyperparameters in\nthe unsupervised setting. With SKADA-bench, we propose a framework to evaluate\nDA methods on diverse modalities, beyond computer vision task that have been\nlargely explored in the literature. We present a complete and fair evaluation\nof existing shallow algorithms, including reweighting, mapping, and subspace\nalignment. Realistic hyperparameter selection is performed with nested\ncross-validation and various unsupervised model selection scores, on both\nsimulated datasets with controlled shifts and real-world datasets across\ndiverse modalities, such as images, text, biomedical, and tabular data. Our\nbenchmark highlights the importance of realistic validation and provides\npractical guidance for real-life applications, with key insights into the\nchoice and impact of model selection approaches. SKADA-bench is open-source,\nreproducible, and can be easily extended with novel DA methods, datasets, and\nmodel selection criteria without requiring re-evaluating competitors.\nSKADA-bench is available on Github at\nhttps://github.com/scikit-adaptation/skada-bench.\n","authors":["Yanis Lalou","Théo Gnassounou","Antoine Collas","Antoine de Mathelin","Oleksii Kachaiev","Ambroise Odonnat","Alexandre Gramfort","Thomas Moreau","Rémi Flamary"],"pdf_url":"https://arxiv.org/pdf/2407.11676v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12502v3","updated":"2025-02-11T11:03:24Z","published":"2024-11-19T13:40:49Z","title":"Transformer Neural Processes - Kernel Regression","summary":"  Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nOriginally developed as a scalable alternative to Gaussian Processes (GPs),\nwhich are limited by $O(n^3)$ runtime complexity, the most accurate modern NPs\ncan often rival GPs but still suffer from an $O(n^2)$ bottleneck due to their\nattention mechanism. We introduce the Transformer Neural Process - Kernel\nRegression (TNP-KR), a scalable NP featuring: (1) a Kernel Regression Block\n(KRBlock), a simple, extensible, and parameter efficient transformer block with\ncomplexity $O(n_c^2 + n_c n_t)$, where $n_c$ and $n_t$ are the number of\ncontext and test points, respectively; (2) a kernel-based attention bias; and\n(3) two novel attention mechanisms: scan attention (SA), a memory-efficient\nscan-based attention that when paired with a kernel-based bias can make TNP-KR\ntranslation invariant, and deep kernel attention (DKA), a Performer-style\nattention that implicitly incoporates a distance bias and further reduces\ncomplexity to $O(n_c)$. These enhancements enable both TNP-KR variants to\nperform inference with 100K context points on over 1M test points in under a\nminute on a single 24GB GPU. On benchmarks spanning meta regression, Bayesian\noptimization, image completion, and epidemiology, TNP-KR with DKA outperforms\nits Performer counterpart on nearly every benchmark, while TNP-KR with SA\nachieves state-of-the-art results.\n","authors":["Daniel Jenson","Jhonathan Navott","Mengyan Zhang","Makkunda Sharma","Elizaveta Semenova","Seth Flaxman"],"pdf_url":"https://arxiv.org/pdf/2411.12502v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07222v2","updated":"2025-02-11T11:02:10Z","published":"2024-06-11T13:01:50Z","title":"Improving Autoformalization using Type Checking","summary":"  Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics.\n","authors":["Auguste Poiroux","Gail Weiss","Viktor Kunčak","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2406.07222v2.pdf","comment":"New benchmarks released, see\n  https://github.com/augustepoiroux/RLMEval ,\n  https://huggingface.co/datasets/PAug/ProofNetSharp , and\n  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see\n  https://github.com/augustepoiroux/LeanInteract"},{"id":"http://arxiv.org/abs/2502.07456v1","updated":"2025-02-11T11:00:58Z","published":"2025-02-11T11:00:58Z","title":"FedAPA: Server-side Gradient-Based Adaptive Personalized Aggregation for\n  Federated Learning on Heterogeneous Data","summary":"  Personalized federated learning (PFL) tailors models to clients' unique data\ndistributions while preserving privacy. However, existing\naggregation-weight-based PFL methods often struggle with heterogeneous data,\nfacing challenges in accuracy, computational efficiency, and communication\noverhead. We propose FedAPA, a novel PFL method featuring a server-side,\ngradient-based adaptive aggregation strategy to generate personalized models,\nby updating aggregation weights based on gradients of client-parameter changes\nwith respect to the aggregation weights in a centralized manner. FedAPA\nguarantees theoretical convergence and achieves superior accuracy and\ncomputational efficiency compared to 10 PFL competitors across three datasets,\nwith competitive communication overhead.\n","authors":["Yuxia Sun","Aoxiang Sun","Siyi Pan","Zhixiao Fu","Jingcai Guo"],"pdf_url":"https://arxiv.org/pdf/2502.07456v1.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.04625v2","updated":"2025-02-11T10:58:16Z","published":"2024-11-07T11:22:46Z","title":"Sharp Analysis for KL-Regularized Contextual Bandits and RLHF","summary":"  Reverse-Kullback-Leibler (KL) regularization has emerged to be a predominant\ntechnique used to enhance policy optimization in reinforcement learning (RL)\nand reinforcement learning from human feedback (RLHF), which forces the learned\npolicy to stay close to a reference policy. While the effectiveness and\nnecessity of KL-regularization have been empirically demonstrated in various\npractical scenarios, current theoretical analysis of KL-regularized RLHF still\nobtains the same $\\mathcal{O}(1 / \\epsilon^2)$ sample complexity as problems\nwithout KL-regularization. To understand the fundamental distinction between\npolicy learning objectives with KL-regularization and ones without\nKL-regularization, we are the first to theoretically demonstrate the power of\nKL-regularization by providing a sharp analysis for KL-regularized contextual\nbandits and RLHF, revealing an $\\mathcal{O}(1 / \\epsilon)$ sample complexity\nwhen $\\epsilon$ is sufficiently small.\n  We further explore the role of data coverage in contextual bandits and RLHF.\nWhile the coverage assumption is commonly employed in offline RLHF to link the\nsamples from the reference policy to the optimal policy, often at the cost of a\nmultiplicative dependence on the coverage coefficient, its impact on the sample\ncomplexity of online RLHF remains unclear. Previous theoretical analyses of\nonline RLHF typically require explicit exploration and additional structural\nassumptions on the reward function class. In contrast, we show that with\nsufficient coverage from the reference policy, a simple two-stage mixed\nsampling strategy can achieve a sample complexity with only an additive\ndependence on the coverage coefficient. Our results provide a comprehensive\nunderstanding of the roles of KL-regularization and data coverage in RLHF,\nshedding light on the design of more efficient RLHF algorithms.\n","authors":["Heyang Zhao","Chenlu Ye","Quanquan Gu","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.04625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13481v2","updated":"2025-02-11T10:56:50Z","published":"2024-05-22T09:47:54Z","title":"Locally Private Estimation with Public Features","summary":"  We initiate the study of locally differentially private (LDP) learning with\npublic features. We define semi-feature LDP, where some features are publicly\navailable while the remaining ones, along with the label, require protection\nunder local differential privacy. Under semi-feature LDP, we demonstrate that\nthe mini-max convergence rate for non-parametric regression is significantly\nreduced compared to that of classical LDP. Then we propose HistOfTree, an\nestimator that fully leverages the information contained in both public and\nprivate features. Theoretically, HistOfTree reaches the mini-max optimal\nconvergence rate. Empirically, HistOfTree achieves superior performance on both\nsynthetic and real data. We also explore scenarios where users have the\nflexibility to select features for protection manually. In such cases, we\npropose an estimator and a data-driven parameter tuning strategy, leading to\nanalogous theoretical and empirical results.\n","authors":["Yuheng Ma","Ke Jia","Hanfang Yang"],"pdf_url":"https://arxiv.org/pdf/2405.13481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06358v2","updated":"2025-02-11T10:54:40Z","published":"2025-02-10T11:20:10Z","title":"Towards bandit-based prompt-tuning for in-the-wild foundation agents","summary":"  Prompting has emerged as the dominant paradigm for adapting large,\npre-trained transformer-based models to downstream tasks. The Prompting\nDecision Transformer (PDT) enables large-scale, multi-task offline\nreinforcement learning pre-training by leveraging stochastic trajectory prompts\nto identify the target task. However, these prompts are sampled uniformly from\nexpert demonstrations, overlooking a critical limitation: Not all prompts are\nequally informative for differentiating between tasks. To address this, we\npropose an inference time bandit-based prompt-tuning framework that explores\nand optimizes trajectory prompt selection to enhance task performance. Our\nexperiments indicate not only clear performance gains due to bandit-based\nprompt-tuning, but also better sample complexity, scalability, and prompt space\nexploration compared to prompt-tuning baselines.\n","authors":["Finn Rietz","Oleg Smirnov","Sara Karimi","Lele Cao"],"pdf_url":"https://arxiv.org/pdf/2502.06358v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07445v1","updated":"2025-02-11T10:43:36Z","published":"2025-02-11T10:43:36Z","title":"Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon","summary":"  Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation.\n","authors":["Nurit Cohen-Inger","Yehonatan Elisha","Bracha Shapira","Lior Rokach","Seffi Cohen"],"pdf_url":"https://arxiv.org/pdf/2502.07445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11228v2","updated":"2025-02-11T10:35:04Z","published":"2024-09-17T14:21:02Z","title":"Learning Source Disentanglement in Neural Audio Codec","summary":"  Neural audio codecs have significantly advanced audio compression by\nefficiently converting continuous audio signals into discrete tokens. These\ncodecs preserve high-quality sound and enable sophisticated sound generation\nthrough generative models trained on these tokens. However, existing neural\ncodec models are typically trained on large, undifferentiated audio datasets,\nneglecting the essential discrepancies between sound domains like speech,\nmusic, and environmental sound effects. This oversight complicates data\nmodeling and poses additional challenges to the controllability of sound\ngeneration. To tackle these issues, we introduce the Source-Disentangled Neural\nAudio Codec (SD-Codec), a novel approach that combines audio coding and source\nseparation. By jointly learning audio resynthesis and separation, SD-Codec\nexplicitly assigns audio signals from different domains to distinct codebooks,\nsets of discrete representations. Experimental results indicate that SD-Codec\nnot only maintains competitive resynthesis quality but also, supported by the\nseparation results, demonstrates successful disentanglement of different\nsources in the latent space, thereby enhancing interpretability in audio codec\nand providing potential finer control over the audio generation process.\n","authors":["Xiaoyu Bie","Xubo Liu","Gaël Richard"],"pdf_url":"https://arxiv.org/pdf/2409.11228v2.pdf","comment":"ICASSP 2025, project page: https://xiaoyubie1994.github.io/sdcodec/"},{"id":"http://arxiv.org/abs/2102.04259v4","updated":"2025-02-11T10:29:23Z","published":"2021-02-04T17:13:03Z","title":"Concentration of Non-Isotropic Random Tensors with Applications to\n  Learning and Empirical Risk Minimization","summary":"  Dimension is an inherent bottleneck to some modern learning tasks, where\noptimization methods suffer from the size of the data. In this paper, we study\nnon-isotropic distributions of data and develop tools that aim at reducing\nthese dimensional costs by a dependency on an effective dimension rather than\nthe ambient one. Based on non-asymptotic estimates of the metric entropy of\nellipsoids -- that prove to generalize to infinite dimensions -- and on a\nchaining argument, our uniform concentration bounds involve an effective\ndimension instead of the global dimension, improving over existing results. We\nshow the importance of taking advantage of non-isotropic properties in learning\nproblems with the following applications: i) we improve state-of-the-art\nresults in statistical preconditioning for communication-efficient distributed\noptimization, ii) we introduce a non-isotropic randomized smoothing for\nnon-smooth optimization. Both applications cover a class of functions that\nencompasses empirical risk minization (ERM) for linear models.\n","authors":["Mathieu Even","Laurent Massoulié"],"pdf_url":"https://arxiv.org/pdf/2102.04259v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09591v2","updated":"2025-02-11T10:27:18Z","published":"2024-11-14T17:02:41Z","title":"Handling missing values in clinical machine learning: Insights from an\n  expert study","summary":"  Inherently interpretable machine learning (IML) models offer valuable support\nfor clinical decision-making but face challenges when features contain missing\nvalues. Traditional approaches, such as imputation or discarding incomplete\nrecords, are often impractical in scenarios where data is missing at test time.\nWe surveyed 55 clinicians from 29 French trauma centers, collecting 20 complete\nresponses to study their interaction with three IML models in a real-world\nclinical setting for predicting hemorrhagic shock with missing values. Our\nfindings reveal that while clinicians recognize the value of interpretability\nand are familiar with common IML approaches, traditional imputation techniques\noften conflict with their intuition. Instead of imputing unobserved values,\nthey rely on observed features combined with medical intuition and experience.\nAs a result, methods that natively handle missing values are preferred. These\nfindings underscore the need to integrate clinical reasoning into future IML\nmodels to enhance human-computer interaction.\n","authors":["Lena Stempfle","Arthur James","Julie Josse","Tobias Gauss","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2411.09591v2.pdf","comment":"8 pages, 5 figures, restructured writing from previous version and\n  additional results"},{"id":"http://arxiv.org/abs/2502.07432v1","updated":"2025-02-11T10:20:04Z","published":"2025-02-11T10:20:04Z","title":"CapyMOA: Efficient Machine Learning for Data Streams in Python","summary":"  CapyMOA is an open-source library designed for efficient machine learning on\nstreaming data. It provides a structured framework for real-time learning and\nevaluation, featuring a flexible data representation. CapyMOA includes an\nextensible architecture that allows integration with external frameworks such\nas MOA and PyTorch, facilitating hybrid learning approaches that combine\ntraditional online algorithms with deep learning techniques. By emphasizing\nadaptability, scalability, and usability, CapyMOA allows researchers and\npractitioners to tackle dynamic learning challenges across various domains.\n","authors":["Heitor Murilo Gomes","Anton Lee","Nuwan Gunasekara","Yibin Sun","Guilherme Weigert Cassales","Justin Liu","Marco Heyden","Vitor Cerqueira","Maroua Bahri","Yun Sing Koh","Bernhard Pfahringer","Albert Bifet"],"pdf_url":"https://arxiv.org/pdf/2502.07432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18582v2","updated":"2025-02-11T10:16:04Z","published":"2025-01-30T18:54:22Z","title":"Accuracy and Robustness of Weight-Balancing Methods for Training PINNs","summary":"  Physics-Informed Neural Networks (PINNs) have emerged as powerful tools for\nintegrating physics-based models with data by minimizing both data and physics\nlosses. However, this multi-objective optimization problem is notoriously\nchallenging, with some benchmark problems leading to unfeasible solutions. To\naddress these issues, various strategies have been proposed, including adaptive\nweight adjustments in the loss function. In this work, we introduce clear\ndefinitions of accuracy and robustness in the context of PINNs and propose a\nnovel training algorithm based on the Primal-Dual (PD) optimization framework.\nOur approach enhances the robustness of PINNs while maintaining comparable\nperformance to existing weight-balancing methods. Numerical experiments\ndemonstrate that the PD method consistently achieves reliable solutions across\nall investigated cases, even in the low-data regime, and can be easily\nimplemented, facilitating its practical adoption. The code is available at\nhttps://github.com/haoming-SHEN/Accuracy-and-Robustness-of-Weight-Balancing-Methods-for-Training-PINNs.git.\n","authors":["Matthieu Barreau","Haoming Shen"],"pdf_url":"https://arxiv.org/pdf/2501.18582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07425v1","updated":"2025-02-11T10:12:28Z","published":"2025-02-11T10:12:28Z","title":"Towards a Foundation Model for Physics-Informed Neural Networks:\n  Multi-PDE Learning with Active Sampling","summary":"  Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving partial differential equations (PDEs) by embedding physical laws\ninto neural network training. However, traditional PINN models are typically\ndesigned for single PDEs, limiting their generalizability across different\nphysical systems. In this work, we explore the potential of a foundation PINN\nmodel capable of solving multiple PDEs within a unified architecture. We\ninvestigate the efficacy of a single PINN framework trained on four distinct\nPDEs-the Simple Harmonic Oscillator (SHO), the 1D Heat Equation, the 1D Wave\nEquation, and the 2D Laplace Equation, demonstrating its ability to learn\ndiverse physical dynamics.\n  To enhance sample efficiency, we incorporate Active Learning (AL) using Monte\nCarlo (MC) Dropout-based uncertainty estimation, selecting the most informative\ntraining samples iteratively. We evaluate different active learning strategies,\ncomparing models trained on 10%, 20%, 30%, 40%, and 50% of the full dataset,\nand analyze their impact on solution accuracy. Our results indicate that\ntargeted uncertainty sampling significantly improves performance with fewer\ntraining samples, leading to efficient learning across multiple PDEs.\n  This work highlights the feasibility of a generalizable PINN-based foundation\nmodel, capable of adapting to different physics-based problems without\nredesigning network architectures. Our findings suggest that multi-PDE PINNs\nwith active learning can serve as an effective approach for reducing\ncomputational costs while maintaining high accuracy in physics-based deep\nlearning applications.\n","authors":["Keon Vin Park"],"pdf_url":"https://arxiv.org/pdf/2502.07425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00560v2","updated":"2025-02-11T10:02:55Z","published":"2024-12-31T17:46:51Z","title":"Re-evaluating Automatic LLM System Ranking for Alignment with Human\n  Preference","summary":"  Evaluating and ranking the capabilities of different LLMs is crucial for\nunderstanding their performance and alignment with human preferences. Due to\nthe high cost and time-consuming nature of human evaluations, an automatic LLM\nbencher (i.e., an automatic evaluation framework that aims to rank LLMs based\non their alignment with human preferences) is indispensable. An automatic LLM\nbencher consists of four components: the input set (e.g., a user instruction),\nthe evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise\ncomparison), and the aggregation method (e.g., the ELO rating system). However,\nprevious work has not thoroughly explored how to select these components or how\ntheir different combinations influence the results. In this work, through\ncontrolled experiments, we provide a series of recommendations on how to choose\neach component to better automate the evaluation of LLMs. Furthermore, we\ndiscovered that when evaluating LLMs with similar performance, the performance\nof the automatic LLM bencher declines sharply, underscoring the limitations of\ncurrent benchers and calling for future work. Lastly, we found that the\nevaluation models' performance at the instance level (e.g., the accuracy of\nselecting the best output) does not always align with their effectiveness when\nused as a component of a bencher, highlighting the importance of dedicated\nsystem-level evaluation of benchers.\n","authors":["Mingqi Gao","Yixin Liu","Xinyu Hu","Xiaojun Wan","Jonathan Bragg","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2501.00560v2.pdf","comment":"Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07422v1","updated":"2025-02-11T10:02:43Z","published":"2025-02-11T10:02:43Z","title":"MoENAS: Mixture-of-Expert based Neural Architecture Search for jointly\n  Accurate, Fair, and Robust Edge Deep Neural Networks","summary":"  There has been a surge in optimizing edge Deep Neural Networks (DNNs) for\naccuracy and efficiency using traditional optimization techniques such as\npruning, and more recently, employing automatic design methodologies. However,\nthe focus of these design techniques has often overlooked critical metrics such\nas fairness, robustness, and generalization. As a result, when evaluating SOTA\nedge DNNs' performance in image classification using the FACET dataset, we\nfound that they exhibit significant accuracy disparities (14.09%) across 10\ndifferent skin tones, alongside issues of non-robustness and poor\ngeneralizability. In response to these observations, we introduce\nMixture-of-Experts-based Neural Architecture Search (MoENAS), an automatic\ndesign technique that navigates through a space of mixture of experts to\ndiscover accurate, fair, robust, and general edge DNNs. MoENAS improves the\naccuracy by 4.02% compared to SOTA edge DNNs and reduces the skin tone accuracy\ndisparities from 14.09% to 5.60%, while enhancing robustness by 3.80% and\nminimizing overfitting to 0.21%, all while keeping model size close to\nstate-of-the-art models average size (+0.4M). With these improvements, MoENAS\nestablishes a new benchmark for edge DNN design, paving the way for the\ndevelopment of more inclusive and robust edge DNNs.\n","authors":["Lotfi Abdelkrim Mecharbat","Alberto Marchisio","Muhammad Shafique","Mohammad M. Ghassemi","Tuka Alhanai"],"pdf_url":"https://arxiv.org/pdf/2502.07422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07415v1","updated":"2025-02-11T09:52:06Z","published":"2025-02-11T09:52:06Z","title":"Quantification of model error for inverse problems in the Weak Neural\n  Variational Inference framework","summary":"  We present a novel extension of the Weak Neural Variational Inference (WNVI)\nframework for probabilistic material property estimation that explicitly\nquantifies model errors in PDE-based inverse problems. Traditional approaches\nassume the correctness of all governing equations, including potentially\nunreliable constitutive laws, which can lead to biased estimates and\nmisinterpretations. Our proposed framework addresses this limitation by\ndistinguishing between reliable governing equations, such as conservation laws,\nand uncertain constitutive relationships. By treating all state variables as\nlatent random variables, we enforce these equations through separate sets of\nresiduals, leveraging a virtual likelihood approach with weighted residuals.\nThis formulation not only identifies regions where constitutive laws break down\nbut also improves robustness against model uncertainties without relying on a\nfully trustworthy forward model. We demonstrate the effectiveness of our\napproach in the context of elastography, showing that it provides a structured,\ninterpretable, and computationally efficient alternative to traditional model\nerror correction techniques. Our findings suggest that the proposed framework\nenhances the accuracy and reliability of material property estimation by\noffering a principled way to incorporate uncertainty in constitutive modeling.\n","authors":["Vincent C. Scholz","P. S. Koutsourelakis"],"pdf_url":"https://arxiv.org/pdf/2502.07415v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.13483v2","updated":"2025-02-11T09:52:04Z","published":"2025-01-23T08:57:02Z","title":"Robust Amortized Bayesian Inference with Self-Consistency Losses on\n  Unlabeled Data","summary":"  Neural amortized Bayesian inference (ABI) can solve probabilistic inverse\nproblems orders of magnitude faster than classical methods. However, neural ABI\nis not yet sufficiently robust for widespread and safe applicability. In\nparticular, when performing inference on observations outside of the scope of\nthe simulated data seen during training, for example, because of model\nmisspecification, the posterior approximations are likely to become highly\nbiased. Due to the bad pre-asymptotic behavior of current neural posterior\nestimators in the out-of-simulation regime, the resulting estimation biases\ncannot be fixed in acceptable time by just simulating more training data. In\nthis proof-of-concept paper, we propose a semi-supervised approach that enables\ntraining not only on (labeled) simulated data generated from the model, but\nalso on unlabeled data originating from any source, including real-world data.\nTo achieve the latter, we exploit Bayesian self-consistency properties that can\nbe transformed into strictly proper losses without requiring knowledge of true\nparameter values, that is, without requiring data labels. The results of our\ninitial experiments show remarkable improvements in the robustness of ABI on\nout-of-simulation data. Even if the observed data is far away from both labeled\nand unlabeled training data, inference remains highly accurate. If our findings\nalso generalize to other scenarios and model classes, we believe that our new\nmethod represents a major breakthrough in neural ABI.\n","authors":["Aayush Mishra","Daniel Habermann","Marvin Schmitt","Stefan T. Radev","Paul-Christian Bürkner"],"pdf_url":"https://arxiv.org/pdf/2501.13483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07414v1","updated":"2025-02-11T09:51:22Z","published":"2025-02-11T09:51:22Z","title":"Sample Weight Averaging for Stable Prediction","summary":"  The challenge of Out-of-Distribution (OOD) generalization poses a\nfoundational concern for the application of machine learning algorithms to\nrisk-sensitive areas. Inspired by traditional importance weighting and\npropensity weighting methods, prior approaches employ an independence-based\nsample reweighting procedure. They aim at decorrelating covariates to\ncounteract the bias introduced by spurious correlations between unstable\nvariables and the outcome, thus enhancing generalization and fulfilling stable\nprediction under covariate shift. Nonetheless, these methods are prone to\nexperiencing an inflation of variance, primarily attributable to the reduced\nefficacy in utilizing training samples during the reweighting process. Existing\nremedies necessitate either environmental labels or substantially higher time\ncosts along with additional assumptions and supervised information. To mitigate\nthis issue, we propose SAmple Weight Averaging (SAWA), a simple yet efficacious\nstrategy that can be universally integrated into various sample reweighting\nalgorithms to decrease the variance and coefficient estimation error, thus\nboosting the covariate-shift generalization and achieving stable prediction\nacross different environments. We prove its rationality and benefits\ntheoretically. Experiments across synthetic datasets and real-world datasets\nconsistently underscore its superiority against covariate shift.\n","authors":["Han Yu","Yue He","Renzhe Xu","Dongbai Li","Jiayin Zhang","Wenchao Zou","Peng Cui"],"pdf_url":"https://arxiv.org/pdf/2502.07414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01419v2","updated":"2025-02-11T09:50:04Z","published":"2024-11-03T03:04:00Z","title":"PSformer: Parameter-efficient Transformer with Segment Attention for\n  Time Series Forecasting","summary":"  Time series forecasting remains a critical challenge across various domains,\noften complicated by high-dimensional data and long-term dependencies. This\npaper presents a novel transformer architecture for time series forecasting,\nincorporating two key innovations: parameter sharing (PS) and Spatial-Temporal\nSegment Attention (SegAtt). We also define the time series segment as the\nconcatenation of sequence patches from the same positions across different\nvariables. The proposed model, PSformer, reduces the number of training\nparameters through the parameter sharing mechanism, thereby improving model\nefficiency and scalability. The introduction of SegAtt could enhance the\ncapability of capturing local spatio-temporal dependencies by computing\nattention over the segments, and improve global representation by integrating\ninformation across segments. The combination of parameter sharing and SegAtt\nsignificantly improves the forecasting performance. Extensive experiments on\nbenchmark datasets demonstrate that PSformer outperforms popular baselines and\nother transformer-based approaches in terms of accuracy and scalability,\nestablishing itself as an accurate and scalable tool for time series\nforecasting.\n","authors":["Yanlong Wang","Jian Xu","Fei Ma","Shao-Lun Huang","Danny Dongning Sun","Xiao-Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01419v2.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2502.07409v1","updated":"2025-02-11T09:42:13Z","published":"2025-02-11T09:42:13Z","title":"MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for\n  Few-Shot WSI Classification","summary":"  Whole slide pathology image classification presents challenges due to\ngigapixel image sizes and limited annotation labels, hindering model\ngeneralization. This paper introduces a prompt learning method to adapt large\nvision-language models for few-shot pathology classification. We first extend\nthe Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology\nimage tiles, into a vision-language model by adding adaptors and aligning it\nwith medical text encoders via contrastive learning on 923K image-text pairs.\nThe model is then used to extract visual features and text embeddings from\nfew-shot annotations and fine-tunes with learnable prompt embeddings. Unlike\nprior methods that combine prompts with frozen features using prefix embeddings\nor self-attention, we propose multi-granular attention that compares\ninteractions between learnable prompts with individual image patches and groups\nof them. This approach improves the model's ability to capture both\nfine-grained details and broader context, enhancing its recognition of complex\npatterns across sub-regions. To further improve accuracy, we leverage\n(unbalanced) optimal transport-based visual-text distance to secure model\nrobustness by mitigating perturbations that might occur during the data\naugmentation process. Empirical experiments on lung, kidney, and breast\npathology modalities validate the effectiveness of our approach; thereby, we\nsurpass several of the latest competitors and consistently improve performance\nacross diverse architectures, including CLIP, PLIP, and Prov-GigaPath\nintegrated PLIP. We release our implementations and pre-trained models at this\nMGPATH.\n","authors":["Anh-Tien Nguyen","Duy Minh Ho Nguyen","Nghiem Tuong Diep","Trung Quoc Nguyen","Nhat Ho","Jacqueline Michelle Metsch","Miriam Cindy Maurer","Daniel Sonntag","Hanibal Bohnenberger","Anne-Christin Hauschild"],"pdf_url":"https://arxiv.org/pdf/2502.07409v1.pdf","comment":"first version"},{"id":"http://arxiv.org/abs/2501.12747v2","updated":"2025-02-11T09:41:34Z","published":"2025-01-22T09:31:02Z","title":"Singular leaning coefficients and efficiency in learning theory","summary":"  Singular learning models with non-positive Fisher information matrices\ninclude neural networks, reduced-rank regression, Boltzmann machines, normal\nmixture models, and others. These models have been widely used in the\ndevelopment of learning machines. However, theoretical analysis is still in its\nearly stages. In this paper, we examine learning coefficients, which indicate\nthe general learning efficiency of deep linear learning models and three-layer\nneural network models with ReLU units. Finally, we extend the results to\ninclude the case of the Softmax function.\n","authors":["Miki Aoyagi"],"pdf_url":"https://arxiv.org/pdf/2501.12747v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2502.07408v1","updated":"2025-02-11T09:40:45Z","published":"2025-02-11T09:40:45Z","title":"No Data, No Optimization: A Lightweight Method To Disrupt Neural\n  Networks With Sign-Flips","summary":"  Deep Neural Networks (DNNs) can be catastrophically disrupted by flipping\nonly a handful of sign bits in their parameters. We introduce Deep Neural\nLesion (DNL), a data-free, lightweight method that locates these critical\nparameters and triggers massive accuracy drops. We validate its efficacy on a\nwide variety of computer vision models and datasets. The method requires no\ntraining data or optimization and can be carried out via common exploits\nsoftware, firmware or hardware based attack vectors. An enhanced variant that\nuses a single forward and backward pass further amplifies the damage beyond\nDNL's zero-pass approach. Flipping just two sign bits in ResNet50 on ImageNet\nreduces accuracy by 99.8\\%. We also show that selectively protecting a small\nfraction of vulnerable sign bits provides a practical defense against such\nattacks.\n","authors":["Ido Galil","Moshe Kimhi","Ran El-Yaniv"],"pdf_url":"https://arxiv.org/pdf/2502.07408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07400v1","updated":"2025-02-11T09:29:23Z","published":"2025-02-11T09:29:23Z","title":"Explainable Multimodal Machine Learning for Revealing Structure-Property\n  Relationships in Carbon Nanotube Fibers","summary":"  In this study, we propose Explainable Multimodal Machine Learning (EMML),\nwhich integrates the analysis of diverse data types (multimodal data) using\nfactor analysis for feature extraction with Explainable AI (XAI), for carbon\nnanotube (CNT) fibers prepared from aqueous dispersions. This method is a\npowerful approach to elucidate the mechanisms governing material properties,\nwhere multi-stage fabrication conditions and multiscale structures have complex\ninfluences. Thus, in our case, this approach helps us understand how different\nprocessing steps and structures at various scales impact the final properties\nof CNT fibers. The analysis targeted structures ranging from the nanoscale to\nthe macroscale, including aggregation size distributions of CNT dispersions and\nthe effective length of CNTs. Furthermore, because some types of data were\ndifficult to interpret using standard methods, challenging-to-interpret\ndistribution data were analyzed using Negative Matrix Factorization (NMF) for\nextracting key features that determine the outcome. Contribution analysis with\nSHapley Additive exPlanations (SHAP) demonstrated that small, uniformly\ndistributed aggregates are crucial for improving fracture strength, while CNTs\nwith long effective lengths are significant factors for enhancing electrical\nconductivity. The analysis also identified thresholds and trends for these key\nfactors to assist in defining the conditions needed to optimize CNT fiber\nproperties. EMML is not limited to CNT fibers but can be applied to the design\nof other materials derived from nanomaterials, making it a useful tool for\ndeveloping a wide range of advanced materials. This approach provides a\nfoundation for advancing data-driven materials research.\n","authors":["Daisuke Kimura","Naoko Tajima","Toshiya Okazaki","Shun Muroga"],"pdf_url":"https://arxiv.org/pdf/2502.07400v1.pdf","comment":"33 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.07397v1","updated":"2025-02-11T09:24:25Z","published":"2025-02-11T09:24:25Z","title":"Bandit Optimal Transport","summary":"  Despite the impressive progress in statistical Optimal Transport (OT) in\nrecent years, there has been little interest in the study of the\n\\emph{sequential learning} of OT. Surprisingly so, as this problem is both\npractically motivated and a challenging extension of existing settings such as\nlinear bandits. This article considers (for the first time) the stochastic\nbandit problem of learning to solve generic Kantorovich and entropic OT\nproblems from repeated interactions when the marginals are known but the cost\nis unknown. We provide $\\tilde{\\mathcal O}(\\sqrt{T})$ regret algorithms for\nboth problems by extending linear bandits on Hilbert spaces. These results\nprovide a reduction to infinite-dimensional linear bandits. To deal with the\ndimension, we provide a method to exploit the intrinsic regularity of the cost\nto learn, yielding corresponding regret bounds which interpolate between\n$\\tilde{\\mathcal O}(\\sqrt{T})$ and $\\tilde{\\mathcal O}(T)$.\n","authors":["Lorenzo Croissant"],"pdf_url":"https://arxiv.org/pdf/2502.07397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07394v1","updated":"2025-02-11T09:23:16Z","published":"2025-02-11T09:23:16Z","title":"Interpretable Rules for Online Failure Prediction: A Case Study on the\n  Metro do Porto dataset","summary":"  Due to their high predictive performance, predictive maintenance applications\nhave increasingly been approached with Deep Learning techniques in recent\nyears. However, as in other real-world application scenarios, the need for\nexplainability is often stated but not sufficiently addressed. This study will\nfocus on predicting failures on Metro trains in Porto, Portugal. While recent\nworks have found high-performing deep neural network architectures that feature\na parallel explainability pipeline, the generated explanations are fairly\ncomplicated and need help explaining why the failures are happening. This work\nproposes a simple online rule-based explainability approach with interpretable\nfeatures that leads to straightforward, interpretable rules. We showcase our\napproach on MetroPT2 and find that three specific sensors on the Metro do Porto\ntrains suffice to predict the failures present in the dataset with simple\nrules.\n","authors":["Matthias Jakobs","Bruno Veloso","Joao Gama"],"pdf_url":"https://arxiv.org/pdf/2502.07394v1.pdf","comment":"Under submission at Information Fusion"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.00950v3","updated":"2025-02-11T18:24:41Z","published":"2025-02-02T23:04:55Z","title":"Fast Audio Codec Identification Using Overlapping LCS","summary":"  Audio data are widely exchanged over telecommunications networks. Due to the\nlimitations of network resources, these data are typically compressed before\ntransmission. Various methods are available for compressing audio data. To\naccess such audio information, it is first necessary to identify the codec used\nfor compression. One of the most effective approaches for audio codec\nidentification involves analyzing the content of received packets. In these\nmethods, statistical features extracted from the packets are utilized to\ndetermine the codec employed. This paper proposes a novel method for audio\ncodec classification based on features derived from the overlapped longest\ncommon sub-string and sub-sequence (LCS). The simulation results, which\nachieved an accuracy of 97% for 8 KB packets, demonstrate the superiority of\nthe proposed method over conventional approaches. This method divides each 8 KB\npacket into fifteen 1 KB packets with a 50% overlap. The results indicate that\nthis division has no significant impact on the simulation outcomes, while\nsignificantly speeding up the feature extraction, being eight times faster than\nthe traditional method for extracting LCS features.\n","authors":["Farzane Jafari"],"pdf_url":"https://arxiv.org/pdf/2502.00950v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2502.07711v1","updated":"2025-02-11T17:10:31Z","published":"2025-02-11T17:10:31Z","title":"RenderBox: Expressive Performance Rendering with Text Control","summary":"  Expressive music performance rendering involves interpreting symbolic scores\nwith variations in timing, dynamics, articulation, and instrument-specific\ntechniques, resulting in performances that capture musical can emotional\nintent. We introduce RenderBox, a unified framework for text-and-score\ncontrolled audio performance generation across multiple instruments, applying\ncoarse-level controls through natural language descriptions and granular-level\ncontrols using music scores. Based on a diffusion transformer architecture and\ncross-attention joint conditioning, we propose a curriculum-based paradigm that\ntrains from plain synthesis to expressive performance, gradually incorporating\ncontrollable factors such as speed, mistakes, and style diversity.\n  RenderBox achieves high performance compared to baseline models across key\nmetrics such as FAD and CLAP, and also tempo and pitch accuracy under different\nprompting tasks. Subjective evaluation further demonstrates that RenderBox is\nable to generate controllable expressive performances that sound natural and\nmusically engaging, aligning well with prompts and intent.\n","authors":["Huan Zhang","Akira Maezawa","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2502.07711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07538v1","updated":"2025-02-11T13:24:38Z","published":"2025-02-11T13:24:38Z","title":"Visual-based spatial audio generation system for multi-speaker\n  environments","summary":"  In multimedia applications such as films and video games, spatial audio\ntechniques are widely employed to enhance user experiences by simulating 3D\nsound: transforming mono audio into binaural formats. However, this process is\noften complex and labor-intensive for sound designers, requiring precise\nsynchronization of audio with the spatial positions of visual components. To\naddress these challenges, we propose a visual-based spatial audio generation\nsystem - an automated system that integrates face detection YOLOv8 for object\ndetection, monocular depth estimation, and spatial audio techniques. Notably,\nthe system operates without requiring additional binaural dataset training. The\nproposed system is evaluated against existing Spatial Audio generation system\nusing objective metrics. Experimental results demonstrate that our method\nsignificantly improves spatial consistency between audio and video, enhances\nspeech quality, and performs robustly in multi-speaker scenarios. By\nstreamlining the audio-visual alignment process, the proposed system enables\nsound engineers to achieve high-quality results efficiently, making it a\nvaluable tool for professionals in multimedia production.\n","authors":["Xiaojing Liu","Ogulcan Gurelli","Yan Wang","Joshua Reiss"],"pdf_url":"https://arxiv.org/pdf/2502.07538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07531v1","updated":"2025-02-11T13:11:59Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07411v1","updated":"2025-02-11T09:45:06Z","published":"2025-02-11T09:45:06Z","title":"EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering","summary":"  We introduce EgoTextVQA, a novel and rigorously constructed benchmark for\negocentric QA assistance involving scene text. EgoTextVQA contains 1.5K\nego-view videos and 7K scene-text aware questions that reflect real-user needs\nin outdoor driving and indoor house-keeping activities. The questions are\ndesigned to elicit identification and reasoning on scene text in an egocentric\nand dynamic environment. With EgoTextVQA, we comprehensively evaluate 10\nprominent multimodal large language models. Currently, all models struggle, and\nthe best results (Gemini 1.5 Pro) are around 33% accuracy, highlighting the\nsevere deficiency of these techniques in egocentric QA assistance. Our further\ninvestigations suggest that precise temporal grounding and multi-frame\nreasoning, along with high resolution and auxiliary scene-text inputs, are key\nfor better performance. With thorough analyses and heuristic suggestions, we\nhope EgoTextVQA can serve as a solid testbed for research in egocentric\nscene-text QA assistance.\n","authors":["Sheng Zhou","Junbin Xiao","Qingyun Li","Yicong Li","Xun Yang","Dan Guo","Meng Wang","Tat-Seng Chua","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2502.07411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07328v1","updated":"2025-02-11T07:46:29Z","published":"2025-02-11T07:46:29Z","title":"Music for All: Exploring Multicultural Representations in Music\n  Generation Models (Camera Ready)","summary":"  The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.\n","authors":["Atharva Mehta","Shivam Chauhan","Amirbek Djanibekov","Atharva Kulkarni","Gus Xia","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07328v1.pdf","comment":"17 pages, 5 figures, accepted to NAACL'25"},{"id":"http://arxiv.org/abs/2502.07160v1","updated":"2025-02-11T00:56:44Z","published":"2025-02-11T00:56:44Z","title":"HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates","summary":"  Image compression under ultra-low bitrates remains challenging for both\nconventional learned image compression (LIC) and generative vector-quantized\n(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy\nquantization, while generative VQ modeling gives poor fidelity due to the\nmismatch between learned generative priors and specific inputs. In this work,\nwe propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream\nframework that utilizes both generative VQ-modeling and diffusion models, as\nwell as conventional LIC, to achieve both high fidelity and high perceptual\nquality. Different from previous hybrid methods that directly use pre-trained\nLIC models to generate low-quality fidelity-preserving information from heavily\nquantized latent, we use diffusion models to extract high-quality complimentary\nfidelity information from the ground-truth input, which can enhance the system\nperformance in several aspects: improving indices map prediction, enhancing the\nfidelity-preserving output of the LIC stream, and refining conditioned image\nreconstruction with VQ-latent correction. In addition, our diffusion model is\nbased on a dense representative vector (DRV), which is lightweight with very\nsimple sampling schedulers. Extensive experiments demonstrate that our\nHDCompression outperforms the previous conventional LIC, generative\nVQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative\nvisualization, providing balanced robust compression performance at ultra-low\nbitrates.\n","authors":["Lei Lu","Yize Li","Yanzhi Wang","Wei Wang","Wei Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.07160v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2404.03161v2","updated":"2025-02-11T00:45:46Z","published":"2024-04-04T02:22:37Z","title":"BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro\n  QR Codes","summary":"  This paper introduces BioVL-QR, a biochemical vision-and-language dataset\ncomprising 23 egocentric experiment videos, corresponding protocols, and\nvision-and-language alignments. A major challenge in understanding biochemical\nvideos is detecting equipment, reagents, and containers because of the\ncluttered environment and indistinguishable objects. Previous studies assumed\nmanual object annotation, which is costly and time-consuming. To address the\nissue, we focus on Micro QR Codes. However, detecting objects using only Micro\nQR Codes is still difficult due to blur and occlusion caused by object\nmanipulation. To overcome this, we propose an object labeling method combining\na Micro QR Code detector with an off-the-shelf hand object detector. As an\napplication of the method and BioVL-QR, we tackled the task of localizing the\nprocedural steps in an instructional video. The experimental results show that\nusing Micro QR Codes and our method improves biochemical video understanding.\nData and code are available through https://nishi10mo.github.io/BioVL-QR/\n","authors":["Tomohiro Nishimoto","Taichi Nishimura","Koki Yamamoto","Keisuke Shirai","Hirotaka Kameko","Yuto Haneji","Tomoya Yoshida","Keiya Kajimura","Taiyu Cui","Chihiro Nishiwaki","Eriko Daikoku","Natsuko Okuda","Fumihito Ono","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2404.03161v2.pdf","comment":"6 pages"}]},"2025-02-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.08640v1","updated":"2025-02-12T18:55:43Z","published":"2025-02-12T18:55:43Z","title":"Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs","summary":"  As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.\n","authors":["Mantas Mazeika","Xuwang Yin","Rishub Tamirisa","Jaehyuk Lim","Bruce W. Lee","Richard Ren","Long Phan","Norman Mu","Adam Khoja","Oliver Zhang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08638v1","updated":"2025-02-12T18:54:37Z","published":"2025-02-12T18:54:37Z","title":"Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples","summary":"  The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that requires only a set of parallel sentence pairs of the\nlanguage pair of interest within the target domain. This task focuses on the\nability of a model to cross-lingually rank the true parallel sentence higher\nthan hard negatives generated by a large language model. We create four\ninstances of our introduced CLSD task for the language pair German-French\nwithin the domain of news. Within this case study, we find that models that are\nalso fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using\nEnglish as the pivot language, while bitext mining models such as LaBSE perform\nbest directly cross-lingually. We also show a fine-grained similarity analysis\nenabled by our distractor generation strategy, indicating that different\nembedding models are sensitive to different types of perturbations.\n","authors":["Andrianos Michail","Simon Clematide","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2502.08638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04328v2","updated":"2025-02-12T18:40:46Z","published":"2025-02-06T18:59:55Z","title":"Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment","summary":"  Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.\n","authors":["Zuyan Liu","Yuhao Dong","Jiahui Wang","Ziwei Liu","Winston Hu","Jiwen Lu","Yongming Rao"],"pdf_url":"https://arxiv.org/pdf/2502.04328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04689v2","updated":"2025-02-12T18:36:24Z","published":"2025-02-07T06:30:33Z","title":"ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning","summary":"  Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.\n","authors":["Yuwei Yin","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.04689v2.pdf","comment":"20 pages. Code: https://github.com/YuweiYin/ARR"},{"id":"http://arxiv.org/abs/2502.08625v1","updated":"2025-02-12T18:25:13Z","published":"2025-02-12T18:25:13Z","title":"Randomness of Low-Layer Parameters Determines Confusing Samples in Terms\n  of Interaction Representations of a DNN","summary":"  In this paper, we find that the complexity of interactions encoded by a deep\nneural network (DNN) can explain its generalization power. We also discover\nthat the confusing samples of a DNN, which are represented by non-generalizable\ninteractions, are determined by its low-layer parameters. In comparison, other\nfactors, such as high-layer parameters and network architecture, have much less\nimpact on the composition of confusing samples. Two DNNs with different\nlow-layer parameters usually have fully different sets of confusing samples,\neven though they have similar performance. This finding extends the\nunderstanding of the lottery ticket hypothesis, and well explains distinctive\nrepresentation power of different DNNs.\n","authors":["Junpeng Zhang","Lei Cheng","Qing Li","Liang Lin","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04354v3","updated":"2025-02-12T18:24:34Z","published":"2023-11-07T21:27:17Z","title":"Uncovering Intermediate Variables in Transformers using Circuit Probing","summary":"  Neural network models have achieved high performance on a wide variety of\ncomplex tasks, but the algorithms that they implement are notoriously difficult\nto interpret. It is often necessary to hypothesize intermediate variables\ninvolved in a network's computation in order to understand these algorithms.\nFor example, does a language model depend on particular syntactic properties\nwhen generating a sentence? Yet, existing analysis tools make it difficult to\ntest hypotheses of this type. We propose a new analysis technique - circuit\nprobing - that automatically uncovers low-level circuits that compute\nhypothesized intermediate variables. This enables causal analysis through\ntargeted ablation at the level of model parameters. We apply this method to\nmodels trained on simple arithmetic tasks, demonstrating its effectiveness at\n(1) deciphering the algorithms that models have learned, (2) revealing modular\nstructure within a model, and (3) tracking the development of circuits over\ntraining. Across these three experiments we demonstrate that circuit probing\ncombines and extends the capabilities of existing methods, providing one\nunified approach for a variety of analyses. Finally, we demonstrate circuit\nprobing on a real-world use case: uncovering circuits that are responsible for\nsubject-verb agreement and reflexive anaphora in GPT2-Small and Medium.\n","authors":["Michael A. Lepori","Thomas Serre","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2311.04354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15537v3","updated":"2025-02-12T17:59:14Z","published":"2024-02-23T04:52:08Z","title":"Evaluating the Performance of ChatGPT for Spam Email Detection","summary":"  Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction with (or without) a\nfew demonstrations. We also investigate how the number of demonstrations in the\nprompt affects the performance of ChatGPT. For comparison, we also implement\nfive popular benchmark methods, including naive Bayes, support vector machines\n(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and\nBERT classifiers. Through extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset. This study provides insights into the potential and limitations of\nChatGPT for spam identification, highlighting its potential as a viable\nsolution for resource-constrained language domains.\n","authors":["Shijing Si","Yuwei Wu","Le Tang","Yugui Zhang","Jedrek Wosik","Qinliang Su"],"pdf_url":"https://arxiv.org/pdf/2402.15537v3.pdf","comment":"12 pages, 4 figures; Accepted by Pacific Journal of Optimization\n  (PJO)"},{"id":"http://arxiv.org/abs/2502.08606v1","updated":"2025-02-12T17:52:47Z","published":"2025-02-12T17:52:47Z","title":"Distillation Scaling Laws","summary":"  We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.\n","authors":["Dan Busbridge","Amitis Shidani","Floris Weers","Jason Ramapuram","Etai Littwin","Russ Webb"],"pdf_url":"https://arxiv.org/pdf/2502.08606v1.pdf","comment":"67 pages, 54 figures, 13 tables"},{"id":"http://arxiv.org/abs/2502.08599v1","updated":"2025-02-12T17:38:27Z","published":"2025-02-12T17:38:27Z","title":"SPeCtrum: A Grounded Framework for Multidimensional Identity\n  Representation in LLM-Based Agent","summary":"  Existing methods for simulating individual identities often oversimplify\nhuman complexity, which may lead to incomplete or flattened representations. To\naddress this, we introduce SPeCtrum, a grounded framework for constructing\nauthentic LLM agent personas by incorporating an individual's multidimensional\nself-concept. SPeCtrum integrates three core components: Social Identity (S),\nPersonal Identity (P), and Personal Life Context (C), each contributing\ndistinct yet interconnected aspects of identity. To evaluate SPeCtrum's\neffectiveness in identity representation, we conducted automated and human\nevaluations. Automated evaluations using popular drama characters showed that\nPersonal Life Context (C)-derived from short essays on preferences and daily\nroutines-modeled characters' identities more effectively than Social Identity\n(S) and Personal Identity (P) alone and performed comparably to the full SPC\ncombination. In contrast, human evaluations involving real-world individuals\nfound that the full SPC combination provided a more comprehensive self-concept\nrepresentation than C alone. Our findings suggest that while C alone may\nsuffice for basic identity simulation, integrating S, P, and C enhances the\nauthenticity and accuracy of real-world identity representation. Overall,\nSPeCtrum offers a structured approach for simulating individuals in LLM agents,\nenabling more personalized human-AI interactions and improving the realism of\nsimulation-based behavioral studies.\n","authors":["Keyeun Lee","Seo Hyeong Kim","Seolhee Lee","Jinsu Eun","Yena Ko","Hayeon Jeon","Esther Hehsun Kim","Seonghye Cho","Soeun Yang","Eun-mee Kim","Hajin Lim"],"pdf_url":"https://arxiv.org/pdf/2502.08599v1.pdf","comment":"21 pages, 8 figures, 5 tables, Accepted in NAACL2025 Main"},{"id":"http://arxiv.org/abs/2407.07313v3","updated":"2025-02-12T17:20:56Z","published":"2024-07-10T02:20:19Z","title":"ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the\n  Age of Large Language Models","summary":"  The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics -- Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) -- suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL.\n","authors":["Benjamin G. Ascoli","Yasoda Sai Ram Kandikonda","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2407.07313v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08561v1","updated":"2025-02-12T16:49:52Z","published":"2025-02-12T16:49:52Z","title":"Quality-Aware Decoding: Unifying Quality Estimation and Decoding","summary":"  An emerging research direction in NMT involves the use of Quality Estimation\n(QE) models, which have demonstrated high correlations with human judgment and\ncan enhance translations through Quality-Aware Decoding. Although several\napproaches have been proposed based on sampling multiple candidate\ntranslations, none have integrated these models directly into the decoding\nprocess. In this paper, we address this by proposing a novel token-level QE\nmodel capable of reliably scoring partial translations. We build a\nuni-directional QE model for this, as decoder models are inherently trained and\nefficient on partial sequences. We then present a decoding strategy that\nintegrates the QE model for Quality-Aware decoding and demonstrate that the\ntranslation quality improves when compared to the N-best list re-ranking with\nstate-of-the-art QE models (upto $1.39$ XCOMET-XXL $\\uparrow$). Finally, we\nshow that our approach provides significant benefits in document translation\ntasks, where the quality of N-best lists is typically suboptimal.\n","authors":["Sai Koneru","Matthias Huck","Miriam Exel","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2502.08561v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2501.04686v3","updated":"2025-02-12T16:49:50Z","published":"2025-01-08T18:49:41Z","title":"URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics","summary":"  Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.\n","authors":["Ruilin Luo","Zhuofan Zheng","Yifan Wang","Yiyao Yu","Xinzhe Ni","Zicheng Lin","Jin Zeng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04686v3.pdf","comment":"Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io"},{"id":"http://arxiv.org/abs/2502.08557v1","updated":"2025-02-12T16:39:06Z","published":"2025-02-12T16:39:06Z","title":"QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval","summary":"  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n","authors":["Wonduk Seo","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08557v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.08550v1","updated":"2025-02-12T16:31:21Z","published":"2025-02-12T16:31:21Z","title":"LLMs can implicitly learn from mistakes in-context","summary":"  Learning from mistakes is a fundamental feature of human intelligence.\nPrevious work has shown that Large Language Models (LLMs) can also learn from\nincorrect answers when provided with a comprehensive rationale detailing why an\nanswer is wrong or how to correct it. In this work, we examine whether LLMs can\nlearn from mistakes in mathematical reasoning tasks when these explanations are\nnot provided. We investigate if LLMs are able to implicitly infer such\nrationales simply from observing both incorrect and correct answers.\nSurprisingly, we find that LLMs perform better, on average, when rationales are\neliminated from the context and incorrect answers are simply shown alongside\ncorrect ones. This approach also substantially outperforms chain-of-thought\nprompting in our evaluations. We show that these results are consistent across\nLLMs of different sizes and varying reasoning abilities. Further, we carry out\nan in-depth analysis, and show that prompting with both wrong and correct\nanswers leads to greater performance and better generalisation than introducing\nadditional, more diverse question-answer pairs into the context. Finally, we\nshow that new rationales generated by models that have only observed incorrect\nand correct answers are scored equally as highly by humans as those produced\nwith the aid of exemplar rationales. Our results demonstrate that LLMs are\nindeed capable of in-context implicit learning.\n","authors":["Lisa Alazraki","Maximilian Mozes","Jon Ander Campos","Yi Chern Tan","Marek Rei","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2502.08550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07577v2","updated":"2025-02-12T16:25:44Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08524v1","updated":"2025-02-12T16:00:11Z","published":"2025-02-12T16:00:11Z","title":"LLM Pretraining with Continuous Concepts","summary":"  Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process.\n","authors":["Jihoon Tack","Jack Lanchantin","Jane Yu","Andrew Cohen","Ilia Kulikov","Janice Lan","Shibo Hao","Yuandong Tian","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2502.08524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06766v2","updated":"2025-02-12T15:55:37Z","published":"2025-02-10T18:47:04Z","title":"Exploiting Sparsity for Long Context Inference: Million Token Contexts\n  on Commodity GPUs","summary":"  There is growing demand for performing inference with hundreds of thousands\nof input tokens on trained transformer models. Inference at this extreme scale\ndemands significant computational resources, hindering the application of\ntransformers at long contexts on commodity (i.e not data center scale)\nhardware. To address the inference time costs associated with running\nself-attention based transformer language models on long contexts and enable\ntheir adoption on widely available hardware, we propose a tunable mechanism\nthat reduces the cost of the forward pass by attending to only the most\nrelevant tokens at every generation step using a top-k selection mechanism. We\nshowcase the efficiency gains afforded by our method by performing inference on\ncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Our\nexperiments reveal that models are capable of handling the sparsity induced by\nthe reduced number of keys and values. By attending to less than 2% of input\ntokens, we achieve over 95% of model performance on common benchmarks (RULER,\nAlpacaEval, and Open LLM Leaderboard).\n","authors":["Ryan Synk","Monte Hoover","John Kirchenbauer","Neel Jain","Alex Stein","Manli Shu","Josue Melendez Sanchez","Ramani Duraiswami","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2502.06766v2.pdf","comment":"9 pages, 9 figures, 2 tables in main body"},{"id":"http://arxiv.org/abs/2502.08514v1","updated":"2025-02-12T15:46:50Z","published":"2025-02-12T15:46:50Z","title":"Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation","summary":"  Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries.\n","authors":["Mahnaz Koupaee","Jake W. Vincent","Saab Mansour","Igor Shalyminov","Han He","Hwanjun Song","Raphael Shu","Jianfeng He","Yi Nian","Amy Wing-mei Wong","Kyu J. Han","Hang Su"],"pdf_url":"https://arxiv.org/pdf/2502.08514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08512v1","updated":"2025-02-12T15:46:34Z","published":"2025-02-12T15:46:34Z","title":"Measuring Diversity in Synthetic Datasets","summary":"  Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https://github.com/BlueWhaleLab/DCScore.\n","authors":["Yuchang Zhu","Huizhe Zhang","Bingzhe Wu","Jintang Li","Zibin Zheng","Peilin Zhao","Liang Chen","Yatao Bian"],"pdf_url":"https://arxiv.org/pdf/2502.08512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08507v1","updated":"2025-02-12T15:41:43Z","published":"2025-02-12T15:41:43Z","title":"Explanation based In-Context Demonstrations Retrieval for Multilingual\n  Grammatical Error Correction","summary":"  Grammatical error correction (GEC) aims to correct grammatical, spelling, and\nsemantic errors in natural language text. With the growing of large language\nmodels (LLMs), direct text generation has gradually become the focus of the GEC\nmethods, and few-shot in-context learning presents a cost-effective solution.\nHowever, selecting effective in-context examples remains challenging, as the\nsimilarity between input texts does not necessarily correspond to similar\ngrammatical error patterns. In this paper, we propose a novel retrieval method\nbased on natural language grammatical error explanations (GEE) to address this\nissue. Our method retrieves suitable few-shot demonstrations by matching the\nGEE of the test input with that of pre-constructed database samples, where\nexplanations for erroneous samples are generated by LLMs. We conducted\nmultilingual GEC few-shot experiments on both major open-source and\nclosed-source LLMs. Experiments across five languages show that our method\noutperforms existing semantic and BM25-based retrieval techniques, without\nrequiring additional training or language adaptation. This also suggests that\nmatching error patterns is key to selecting examples.\n","authors":["Wei Li","Wen Luo","Guangyue Peng","Houfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08507v1.pdf","comment":"Accepted by NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2502.08489v1","updated":"2025-02-12T15:26:08Z","published":"2025-02-12T15:26:08Z","title":"Salamandra Technical Report","summary":"  This work introduces Salamandra, a suite of open-source decoder-only large\nlanguage models available in three different sizes: 2, 7, and 40 billion\nparameters. The models were trained from scratch on highly multilingual data\nthat comprises text in 35 European languages and code. Our carefully curated\ncorpus is made exclusively from open-access data compiled from a wide variety\nof sources. Along with the base models, supplementary checkpoints that were\nfine-tuned on public-domain instruction data are also released for chat\napplications. Additionally, we also share our preliminary experiments on\nmultimodality, which serve as proof-of-concept to showcase potential\napplications for the Salamandra family. Our extensive evaluations on\nmultilingual benchmarks reveal that Salamandra has strong capabilities,\nachieving competitive performance when compared to similarly sized open-source\nmodels. We provide comprehensive evaluation results both on standard downstream\ntasks as well as key aspects related to bias and safety.With this technical\nreport, we intend to promote open science by sharing all the details behind our\ndesign choices, data curation strategy and evaluation methodology. In addition\nto that, we deviate from the usual practice by making our training and\nevaluation scripts publicly accessible. We release all models under a\npermissive Apache 2.0 license in order to foster future research and facilitate\ncommercial use, thereby contributing to the open-source ecosystem of large\nlanguage models.\n","authors":["Aitor Gonzalez-Agirre","Marc Pàmies","Joan Llop","Irene Baucells","Severino Da Dalt","Daniel Tamayo","José Javier Saiz","Ferran Espuña","Jaume Prats","Javier Aula-Blasco","Mario Mina","Adrián Rubio","Alexander Shvets","Anna Sallés","Iñaki Lacunza","Iñigo Pikabea","Jorge Palomar","Júlia Falcão","Lucía Tormo","Luis Vasquez-Reina","Montserrat Marimon","Valle Ruíz-Fernández","Marta Villegas"],"pdf_url":"https://arxiv.org/pdf/2502.08489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10949v2","updated":"2025-02-12T15:18:32Z","published":"2024-07-15T17:45:53Z","title":"Representing Rule-based Chatbots with Transformers","summary":"  What kind of internal mechanisms might Transformers use to conduct fluid,\nnatural-sounding conversations? Prior work has illustrated by construction how\nTransformers can solve various synthetic tasks, such as sorting a list or\nrecognizing formal languages, but it remains unclear how to extend this\napproach to a conversational setting. In this work, we propose using ELIZA, a\nclassic rule-based chatbot, as a setting for formal, mechanistic analysis of\nTransformer-based chatbots. ELIZA allows us to formally model key aspects of\nconversation, including local pattern matching and long-term dialogue state\ntracking. We first present a theoretical construction of a Transformer that\nimplements the ELIZA chatbot. Building on prior constructions, particularly\nthose for simulating finite-state automata, we show how simpler mechanisms can\nbe composed and extended to produce more sophisticated behavior. Next, we\nconduct a set of empirical analyses of Transformers trained on synthetically\ngenerated ELIZA conversations. Our analysis illustrates the kinds of mechanisms\nthese models tend to prefer--for example, models favor an induction head\nmechanism over a more precise, position-based copying mechanism; and using\nintermediate generations to simulate recurrent data structures, akin to an\nimplicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit\nconnection between neural chatbots and interpretable, symbolic mechanisms, our\nresults provide a new framework for the mechanistic analysis of conversational\nagents.\n","authors":["Dan Friedman","Abhishek Panigrahi","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10949v2.pdf","comment":"NAACL 2025. Code and data are available at\n  https://github.com/princeton-nlp/ELIZA-Transformer"},{"id":"http://arxiv.org/abs/2502.08482v1","updated":"2025-02-12T15:17:04Z","published":"2025-02-12T15:17:04Z","title":"Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned\n  Reasoning","summary":"  Chain-of-Thought (CoT) prompting has emerged as a powerful technique for\nenhancing language model's reasoning capabilities. However, generating long and\ncorrect CoT trajectories is challenging. Recent studies have demonstrated that\nLooped Transformers possess remarkable length generalization capabilities, but\ntheir limited generality and adaptability prevent them from serving as an\nalternative to auto-regressive solutions. To better leverage the strengths of\nLooped Transformers, we propose RELAY (REasoning through Loop Alignment\niterativelY). Specifically, we align the steps of Chain-of-Thought (CoT)\nreasoning with loop iterations and apply intermediate supervision during the\ntraining of Looped Transformers. This additional iteration-wise supervision not\nonly preserves the Looped Transformer's ability for length generalization but\nalso enables it to predict CoT reasoning steps for unseen data. Therefore, we\nleverage this Looped Transformer to generate accurate reasoning chains for\ncomplex problems that exceed the training length, which will then be used to\nfine-tune an auto-regressive model. We conduct extensive experiments, and the\nresults demonstrate the effectiveness of our approach, with significant\nimprovements in the performance of the auto-regressive model. Code will be\nreleased at https://github.com/qifanyu/RELAY.\n","authors":["Qifan Yu","Zhenyu He","Sijie Li","Xun Zhou","Jun Zhang","Jingjing Xu","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.08482v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2502.08468v1","updated":"2025-02-12T15:03:33Z","published":"2025-02-12T15:03:33Z","title":"mmE5: Improving Multimodal Multilingual Embeddings via High-quality\n  Synthetic Data","summary":"  Multimodal embedding models have gained significant attention for their\nability to map data from different modalities, such as text and images, into a\nunified representation space. However, the limited labeled multimodal data\noften hinders embedding performance. Recent approaches have leveraged data\nsynthesis to address this problem, yet the quality of synthetic data remains a\ncritical bottleneck. In this work, we identify three criteria for high-quality\nsynthetic multimodal data. First, broad scope ensures that the generated data\ncovers diverse tasks and modalities, making it applicable to various downstream\nscenarios. Second, robust cross-modal alignment makes different modalities\nsemantically consistent. Third, high fidelity ensures that the synthetic data\nmaintains realistic details to enhance its reliability. Guided by these\nprinciples, we synthesize datasets that: (1) cover a wide range of tasks,\nmodality combinations, and languages, (2) are generated via a deep thinking\nprocess within a single pass of a multimodal large language model, and (3)\nincorporate real-world images with accurate and relevant texts, ensuring\nfidelity through self-evaluation and refinement. Leveraging these high-quality\nsynthetic and labeled datasets, we train a multimodal multilingual E5 model\nmmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art\nperformance on the MMEB Benchmark and superior multilingual performance on the\nXTD benchmark. Our codes, datasets and models are released in\nhttps://github.com/haon-chen/mmE5.\n","authors":["Haonan Chen","Liang Wang","Nan Yang","Yutao Zhu","Ziliang Zhao","Furu Wei","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2502.08468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08458v1","updated":"2025-02-12T14:53:04Z","published":"2025-02-12T14:53:04Z","title":"Examining Spanish Counseling with MIDAS: a Motivational Interviewing\n  Dataset in Spanish","summary":"  Cultural and language factors significantly influence counseling, but Natural\nLanguage Processing research has not yet examined whether the findings of\nconversational analysis for counseling conducted in English apply to other\nlanguages. This paper presents a first step towards this direction. We\nintroduce MIDAS (Motivational Interviewing Dataset in Spanish), a counseling\ndataset created from public video sources that contains expert annotations for\ncounseling reflections and questions. Using this dataset, we explore\nlanguage-based differences in counselor behavior in English and Spanish and\ndevelop classifiers in monolingual and multilingual settings, demonstrating its\napplications in counselor behavioral coding tasks.\n","authors":["Aylin Gunal","Bowen Yi","John Piette","Rada Mihalcea","Verónica Pérez-Rosas"],"pdf_url":"https://arxiv.org/pdf/2502.08458v1.pdf","comment":"To appear in NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2408.02416v2","updated":"2025-02-12T14:52:56Z","published":"2024-08-05T12:20:39Z","title":"Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in\n  Customized Large Language Models","summary":"  The drastic increase of large language models' (LLMs) parameters has led to a\nnew research direction of fine-tuning-free downstream customization by prompts,\ni.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)\nplay an important role in many businesses, there has emerged growing concerns\nabout the prompt leakage, which undermines the intellectual properties of these\nservices and causes downstream attacks. In this paper, we analyze the\nunderlying mechanism of prompt leakage, which we refer to as prompt\nmemorization, and develop corresponding defending strategies. By exploring the\nscaling laws in prompt extraction, we analyze key attributes that influence\nprompt extraction, including model sizes, prompt lengths, as well as the types\nof prompts. Then we propose two hypotheses that explain how LLMs expose their\nprompts. The first is attributed to the perplexity, i.e. the familiarity of\nLLMs to texts, whereas the second is based on the straightforward token\ntranslation path in attention matrices. To defend against such threats, we\ninvestigate whether alignments can undermine the extraction of prompts. We find\nthat current LLMs, even those with safety alignments like GPT-4, are highly\nvulnerable to prompt extraction attacks, even under the most straightforward\nuser attacks. Therefore, we put forward several defense strategies with the\ninspiration of our findings, which achieve 83.8\\% and 71.0\\% drop in the prompt\nextraction rate for Llama2-7B and GPT-3.5, respectively. Source code is\navaliable at https://github.com/liangzid/PromptExtractionEval.\n","authors":["Zi Liang","Haibo Hu","Qingqing Ye","Yaxin Xiao","Haoyang Li"],"pdf_url":"https://arxiv.org/pdf/2408.02416v2.pdf","comment":"Source Code: https://github.com/liangzid/PromptExtractionEval"},{"id":"http://arxiv.org/abs/2402.17400v2","updated":"2025-02-12T14:46:43Z","published":"2024-02-27T10:47:24Z","title":"Investigating Continual Pretraining in Large Language Models: Insights\n  and Implications","summary":"  Continual learning (CL) in large language models (LLMs) is an evolving domain\nthat focuses on developing efficient and sustainable training strategies to\nadapt models to emerging knowledge and achieve robustness in dynamic\nenvironments. Our primary emphasis is on continual domain-adaptive pretraining,\na process designed to equip LLMs with the ability to integrate new information\nfrom various domains while retaining previously learned knowledge. Since\nexisting works concentrate mostly on continual fine-tuning for a limited\nselection of downstream tasks or training domains, we introduce a new benchmark\ndesigned to measure the adaptability of LLMs to changing pretraining data\nlandscapes. We further examine the impact of model size on learning efficacy\nand forgetting, as well as how the progression and similarity of emerging\ndomains affect the knowledge transfer within these models.\n  Our findings uncover several key insights: (i) continual pretraining\nconsistently improves <1.5B models studied in this work and is also superior to\ndomain adaptation, (ii) larger models always achieve better perplexity than\nsmaller ones when continually pretrained on the same corpus, (iii) smaller\nmodels are particularly sensitive to continual pretraining, showing the most\nsignificant rates of both learning and forgetting, (iv) continual pretraining\nboosts downstream task performance of GPT-2 family, (v) continual pretraining\nenables LLMs to specialize better when the sequence of domains shows semantic\nsimilarity while randomizing training domains leads to better transfer and\nfinal performance otherwise. We posit that our research establishes a new\nbenchmark for CL in LLMs, providing a more realistic evaluation of knowledge\nretention and transfer across diverse domains.\n","authors":["Çağatay Yıldız","Nishaanth Kanna Ravichandran","Nitin Sharma","Matthias Bethge","Beyza Ermis"],"pdf_url":"https://arxiv.org/pdf/2402.17400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08450v1","updated":"2025-02-12T14:41:20Z","published":"2025-02-12T14:41:20Z","title":"Towards Prompt Generalization: Grammar-aware Cross-Prompt Automated\n  Essay Scoring","summary":"  In automated essay scoring (AES), recent efforts have shifted toward\ncross-prompt settings that score essays on unseen prompts for practical\napplicability. However, prior methods trained with essay-score pairs of\nspecific prompts pose challenges in obtaining prompt-generalized essay\nrepresentation. In this work, we propose a grammar-aware cross-prompt trait\nscoring (GAPS), which internally captures prompt-independent syntactic aspects\nto learn generic essay representation. We acquire grammatical error-corrected\ninformation in essays via the grammar error correction technique and design the\nAES model to seamlessly integrate such information. By internally referring to\nboth the corrected and the original essays, the model can focus on generic\nfeatures during training. Empirical experiments validate our method's\ngeneralizability, showing remarkable improvements in prompt-independent and\ngrammar-related traits. Furthermore, GAPS achieves notable QWK gains in the\nmost challenging cross-prompt scenario, highlighting its strength in evaluating\nunseen prompts.\n","authors":["Heejin Do","Taehee Park","Sangwon Ryu","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08450v1.pdf","comment":"NAACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2404.02690v2","updated":"2025-02-12T14:32:46Z","published":"2024-04-03T12:37:34Z","title":"How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse","summary":"  Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.\n","authors":["Yichuan Deng","Zhao Song","Jing Xiong","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2404.02690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08441v1","updated":"2025-02-12T14:32:17Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2502.08436v1","updated":"2025-02-12T14:20:36Z","published":"2025-02-12T14:20:36Z","title":"From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification","summary":"  We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference.\n","authors":["Nathan Vandemoortele","Bram Steenwinckel","Femke Ongenae","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2502.08436v1.pdf","comment":"Under review at ICML 2025"},{"id":"http://arxiv.org/abs/2412.01621v2","updated":"2025-02-12T14:03:19Z","published":"2024-12-02T15:41:47Z","title":"NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers","summary":"  Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities.\n","authors":["Angel Yahir Loredo Lopez","Tyler McDonald","Ali Emami"],"pdf_url":"https://arxiv.org/pdf/2412.01621v2.pdf","comment":"5 pages (excluding references), Published at Coling 2025, Best\n  Dataset Paper Award"},{"id":"http://arxiv.org/abs/2502.08415v1","updated":"2025-02-12T13:58:42Z","published":"2025-02-12T13:58:42Z","title":"A Semantic Parsing Algorithm to Solve Linear Ordering Problems","summary":"  We develop an algorithm to semantically parse linear ordering problems, which\nrequire a model to arrange entities using deductive reasoning. Our method takes\nas input a number of premises and candidate statements, parsing them to a\nfirst-order logic of an ordering domain, and then utilizes constraint logic\nprogramming to infer the truth of proposed statements about the ordering.\n  Our semantic parser transforms Heim and Kratzer's syntax-based compositional\nformal semantic rules to a computational algorithm. This transformation\ninvolves introducing abstract types and templates based on their rules, and\nintroduces a dynamic component to interpret entities within a contextual\nframework.\n  Our symbolic system, the Formal Semantic Logic Inferer (FSLI), is applied to\nanswer multiple choice questions in BIG-bench's logical_deduction multiple\nchoice problems, achieving perfect accuracy, compared to 67.06% for the\nbest-performing LLM (GPT-4) and 87.63% for the hybrid system Logic-LM.\n  These promising results demonstrate the benefit of developing a semantic\nparsing algorithm driven by first-order logic constructs.\n","authors":["Maha Alkhairy","Vincent Homer","Brendan O'Connor"],"pdf_url":"https://arxiv.org/pdf/2502.08415v1.pdf","comment":"3 figures, 9 pages main paper and 6 pages references and appendix"},{"id":"http://arxiv.org/abs/2502.08395v1","updated":"2025-02-12T13:37:03Z","published":"2025-02-12T13:37:03Z","title":"IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance","summary":"  Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them.\n","authors":["Paul Röttger","Musashi Hinck","Valentin Hofmann","Kobi Hackenburg","Valentina Pyatkin","Faeze Brahman","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2502.08395v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2502.08371v1","updated":"2025-02-12T13:03:43Z","published":"2025-02-12T13:03:43Z","title":"Unveiling Global Discourse Structures: Theoretical Analysis and NLP\n  Applications in Argument Mining","summary":"  Particularly in the structure of global discourse, coherence plays a pivotal\nrole in human text comprehension and is a hallmark of high-quality text. This\nis especially true for persuasive texts, where coherent argument structures\nsupport claims effectively. This paper discusses and proposes methods for\ndetecting, extracting and representing these global discourse structures in a\nproccess called Argument(ation) Mining. We begin by defining key terms and\nprocesses of discourse structure analysis, then continue to summarize existing\nresearch on the matter, and identify shortcomings in current argument component\nextraction and classification methods. Furthermore, we will outline an\narchitecture for argument mining that focuses on making models more\ngeneralisable while overcoming challenges in the current field of research by\nutilizing novel NLP techniques. This paper reviews current knowledge,\nsummarizes recent works, and outlines our NLP pipeline, aiming to contribute to\nthe theoretical understanding of global discourse structures.\n","authors":["Christopher van Le"],"pdf_url":"https://arxiv.org/pdf/2502.08371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01692v2","updated":"2025-02-12T13:03:09Z","published":"2024-10-02T16:03:49Z","title":"U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models","summary":"  Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where model performance stagnates at first and then\nimproves sharply and unpredictably with scale beyond a threshold. In this work,\nwe investigate the phenomenon by grouping questions based on difficulty level\nand provide a possible explanation for emergent abilities. Specifically, we\nobserve U-shaped scaling for hard questions and inverted-U scaling followed by\nsteady improvement for easy questions. The two scaling patterns initially\noffset each other, causing stagnant overall performance. The performance starts\nto soar when the scaling pattern of easy questions reverts from inverse to\nstandard scaling, leading to emergent abilities. Based on this finding, we\npropose a simple yet effective pipeline, called Slice-and-Sandwich, to predict\nthe emergence threshold and model performance beyond the threshold. Our code is\npublicly available at https://github.com/tony10101105/ExpEmergence.\n","authors":["Tung-Yu Wu","Pei-Yu Lo"],"pdf_url":"https://arxiv.org/pdf/2410.01692v2.pdf","comment":"accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08363v1","updated":"2025-02-12T12:50:15Z","published":"2025-02-12T12:50:15Z","title":"Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding","summary":"  The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.\n","authors":["Konstantin Berestizshevsky","Renzo Andri","Lukas Cavigelli"],"pdf_url":"https://arxiv.org/pdf/2502.08363v1.pdf","comment":"8 pages, 11 figures, work under submission"},{"id":"http://arxiv.org/abs/2410.18652v7","updated":"2025-02-12T12:49:36Z","published":"2024-10-24T11:32:00Z","title":"$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation","summary":"  Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.\n","authors":["Woosung Koh","Jang Han Yoon","MinHyung Lee","Youngjin Song","Jaegwan Cho","Jaehyun Kang","Taehyeon Kim","Se-Young Yun","Youngjae Yu","Bongshin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.18652v7.pdf","comment":"NAACL 2025 Main (Long)"},{"id":"http://arxiv.org/abs/2502.08356v1","updated":"2025-02-12T12:39:51Z","published":"2025-02-12T12:39:51Z","title":"Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities.\n","authors":["Kushagra Bhushan","Yatin Nandwani","Dinesh Khandelwal","Sonam Gupta","Gaurav Pandey","Dinesh Raghu","Sachindra Joshi"],"pdf_url":"https://arxiv.org/pdf/2502.08356v1.pdf","comment":"22 pages, 14 tables, to be published in NAACL 2025"},{"id":"http://arxiv.org/abs/2501.16937v3","updated":"2025-02-12T12:25:56Z","published":"2025-01-28T13:31:18Z","title":"TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models","summary":"  Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.\n","authors":["Makoto Shing","Kou Misaki","Han Bao","Sho Yokoi","Takuya Akiba"],"pdf_url":"https://arxiv.org/pdf/2501.16937v3.pdf","comment":"To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025) as a Spotlight presentation"},{"id":"http://arxiv.org/abs/2502.08323v1","updated":"2025-02-12T11:44:19Z","published":"2025-02-12T11:44:19Z","title":"Contextual Compression Encoding for Large Language Models: A Novel\n  Framework for Multi-Layered Parameter Space Pruning","summary":"  Context-aware compression techniques have gained increasing attention as\nmodel sizes continue to grow, introducing computational bottlenecks that hinder\nefficient deployment. A structured encoding approach was proposed to\nselectively eliminate redundant parameter groups while ensuring that\nrepresentational fidelity was preserved across multiple layers. Contextual\nCompression Encoding (CCE) introduced a multi-stage encoding mechanism that\ndynamically restructured parameter distributions, allowing for significant\nreductions in memory footprint and computational complexity. Experimental\nevaluations demonstrated that models compressed through CCE retained linguistic\nexpressivity and coherence, maintaining accuracy across a range of text\ngeneration and classification tasks. Layer-wise analysis revealed that\nmiddle-network layers exhibited higher compression ratios, aligning with the\nobservation that self-attention and feed-forward transformations contained\nredundancies that could be reorganized without impairing functional capacity.\nComparisons against conventional quantization and pruning methods confirmed\nthat CCE provided a more balanced trade-off between efficiency and model\nretention, achieving reductions in energy consumption and inference latency\nwithout requiring extensive retraining. Computational efficiency improvements\nwere particularly evident in deployment scenarios involving\nresource-constrained environments, where reductions in memory usage enabled\nmore scalable implementations. Further analyses of internal network behavior\nshowed that compressed models exhibited stable activation distributions and\nadapted dynamically to input variations, reinforcing the viability of\nstructured compression strategies for optimizing large-scale architectures.\n","authors":["Barnaby Schmitt","Alistair Grosvenor","Matthias Cunningham","Clementine Walsh","Julius Pembrokeshire","Jonathan Teel"],"pdf_url":"https://arxiv.org/pdf/2502.08323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10827v3","updated":"2025-02-12T11:41:16Z","published":"2024-12-14T13:12:50Z","title":"Rethinking Chain-of-Thought from the Perspective of Self-Training","summary":"  Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency.\n","authors":["Zongqian Wu","Baoduo Xu","Ruochen Cui","Mengmeng Zhan","Xiaofeng Zhu","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2412.10827v3.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.08319v1","updated":"2025-02-12T11:35:20Z","published":"2025-02-12T11:35:20Z","title":"MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and\n  Emotion Detection","summary":"  Propaganda is a form of persuasion that has been used throughout history with\nthe intention goal of influencing people's opinions through rhetorical and\npsychological persuasion techniques for determined ends. Although Arabic ranked\nas the fourth most- used language on the internet, resources for propaganda\ndetection in languages other than English, especially Arabic, remain extremely\nlimited. To address this gap, the first Arabic dataset for Multi-label\nPropaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE\nis an open-source extension of the existing Arabic propaganda dataset, ArPro,\nwith the addition of sentiment and emotion annotations for each text. This\ndataset comprises 8,000 annotated news articles, which is the largest\npropaganda dataset to date. For each task, several baselines have been\ndeveloped using large language models (LLMs), such as GPT-4o-mini, and\npre-trained language models (PLMs), including three BERT-based models. The\ndataset, annotation guidelines, and source code are all publicly released to\nfacilitate future research and development in Arabic language models and\ncontribute to a deeper understanding of how various opinion dimensions interact\nin news media1.\n","authors":["Lubna Al-Henaki","Hend Al-Khalifa","Abdulmalik Al-Salman","Hajar Alqubayshi","Hind Al-Twailay","Gheeda Alghamdi","Hawra Aljasim"],"pdf_url":"https://arxiv.org/pdf/2502.08319v1.pdf","comment":"12 pages, 3 figuers, 4 tabels"},{"id":"http://arxiv.org/abs/2502.08317v1","updated":"2025-02-12T11:32:19Z","published":"2025-02-12T11:32:19Z","title":"Mitigating Hallucinations in Multimodal Spatial Relations through\n  Constraint-Aware Prompting","summary":"  Spatial relation hallucinations pose a persistent challenge in large\nvision-language models (LVLMs), leading to generate incorrect predictions about\nobject positions and spatial configurations within an image. To address this\nissue, we propose a constraint-aware prompting framework designed to reduce\nspatial relation hallucinations. Specifically, we introduce two types of\nconstraints: (1) bidirectional constraint, which ensures consistency in\npairwise object relations, and (2) transitivity constraint, which enforces\nrelational dependence across multiple objects. By incorporating these\nconstraints, LVLMs can produce more spatially coherent and consistent outputs.\nWe evaluate our method on three widely-used spatial relation datasets,\ndemonstrating performance improvements over existing approaches. Additionally,\na systematic analysis of various bidirectional relation analysis choices and\ntransitivity reference selections highlights greater possibilities of our\nmethods in incorporating constraints to mitigate spatial relation\nhallucinations.\n","authors":["Jiarui Wu","Zhuo Liu","Hangfeng He"],"pdf_url":"https://arxiv.org/pdf/2502.08317v1.pdf","comment":"19 pages, accepted to NAACL Findings"},{"id":"http://arxiv.org/abs/2502.08312v1","updated":"2025-02-12T11:30:28Z","published":"2025-02-12T11:30:28Z","title":"Word Synchronization Challenge: A Benchmark for Word Association\n  Responses for LLMs","summary":"  This paper introduces the Word Synchronization Challenge, a novel benchmark\nto evaluate large language models (LLMs) in Human-Computer Interaction (HCI).\nThis benchmark uses a dynamic game-like framework to test LLMs ability to mimic\nhuman cognitive processes through word associations. By simulating complex\nhuman interactions, it assesses how LLMs interpret and align with human thought\npatterns during conversational exchanges, which are essential for effective\nsocial partnerships in HCI. Initial findings highlight the influence of model\nsophistication on performance, offering insights into the models capabilities\nto engage in meaningful social interactions and adapt behaviors in human-like\nways. This research advances the understanding of LLMs potential to replicate\nor diverge from human cognitive functions, paving the way for more nuanced and\nempathetic human-machine collaborations.\n","authors":["Tanguy Cazalets","Joni Dambre"],"pdf_url":"https://arxiv.org/pdf/2502.08312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08301v1","updated":"2025-02-12T11:02:59Z","published":"2025-02-12T11:02:59Z","title":"Compromising Honesty and Harmlessness in Language Models via Deception\n  Attacks","summary":"  Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce a novel attack that\nundermines both of these traits, revealing a vulnerability that, if exploited,\ncould have serious real-world consequences. In particular, we introduce\nfine-tuning methods that enhance deception tendencies beyond model safeguards.\nThese \"deception attacks\" customize models to mislead users when prompted on\nchosen topics while remaining accurate on others. Furthermore, we find that\ndeceptive models also exhibit toxicity, generating hate speech, stereotypes,\nand other harmful content. Finally, we assess whether models can deceive\nconsistently in multi-turn dialogues, yielding mixed results. Given that\nmillions of users interact with LLM-based chatbots, voice assistants, agents,\nand other interfaces where trustworthiness cannot be ensured, securing these\nmodels against deception attacks is critical.\n","authors":["Laurène Vaugrante","Francesca Carlon","Maluna Menke","Thilo Hagendorff"],"pdf_url":"https://arxiv.org/pdf/2502.08301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08298v1","updated":"2025-02-12T10:58:57Z","published":"2025-02-12T10:58:57Z","title":"Improving Existing Optimization Algorithms with LLMs","summary":"  The integration of Large Language Models (LLMs) into optimization has created\na powerful synergy, opening exciting research opportunities. This paper\ninvestigates how LLMs can enhance existing optimization algorithms. Using their\npre-trained knowledge, we demonstrate their ability to propose innovative\nheuristic variations and implementation strategies. To evaluate this, we\napplied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt\n(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that\nincorporates a heuristic in the solution construction phase. Our results show\nthat an alternative heuristic proposed by GPT-4o outperforms the\nexpert-designed heuristic of CMSA, with the performance gap widening on larger\nand denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/\n","authors":["Camilo Chacón Sartori","Christian Blum"],"pdf_url":"https://arxiv.org/pdf/2502.08298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03824v2","updated":"2025-02-12T10:45:25Z","published":"2025-02-06T07:19:59Z","title":"Syntriever: How to Train Your Retriever with Synthetic Data from LLMs","summary":"  LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.\n","authors":["Minsang Kim","Seungjun Baek"],"pdf_url":"https://arxiv.org/pdf/2502.03824v2.pdf","comment":"the Nations of the Americas Chapter of the Association for\n  Computational Linguistics (NAACL), Findings, Accepted"},{"id":"http://arxiv.org/abs/2311.17696v7","updated":"2025-02-12T10:45:02Z","published":"2023-11-29T15:02:46Z","title":"How to Build an Adaptive AI Tutor for Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)","summary":"  Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems\n(ITS) presents transformative opportunities for personalized education.\nHowever, current implementations face two critical challenges: maintaining\nfactual accuracy and delivering coherent, context-aware instruction. While\nRetrieval-Augmented Generation (RAG) partially addresses these issues, its\nreliance on pure semantic similarity limits its effectiveness in educational\ncontexts where conceptual relationships are crucial. This paper introduces\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel\nframework that integrates structured knowledge representation with\ncontext-aware retrieval to enable more effective AI tutoring. We present three\nkey contributions: (1) a novel architecture that grounds AI responses in\nstructured domain knowledge, (2) empirical validation through controlled\nexperiments (n=76) demonstrating significant learning improvements (35%\nincrease in assessment scores, p<0.001), and (3) a comprehensive implementation\nframework addressing practical deployment considerations. These results\nestablish KG-RAG as a robust solution for developing adaptable AI tutoring\nsystems across diverse educational contexts.\n","authors":["Chenxi Dong","Yimin Yuan","Kan Chen","Shupei Cheng","Chujie Wen"],"pdf_url":"https://arxiv.org/pdf/2311.17696v7.pdf","comment":"6 pages, 6 figures, ICEIT 2025"},{"id":"http://arxiv.org/abs/2502.08281v1","updated":"2025-02-12T10:38:22Z","published":"2025-02-12T10:38:22Z","title":"Redefining Simplicity: Benchmarking Large Language Models from Lexical\n  to Document Simplification","summary":"  Text simplification (TS) refers to the process of reducing the complexity of\na text while retaining its original meaning and key information. Existing work\nonly shows that large language models (LLMs) have outperformed supervised\nnon-LLM-based methods on sentence simplification. This study offers the first\ncomprehensive analysis of LLM performance across four TS tasks: lexical,\nsyntactic, sentence, and document simplification. We compare lightweight,\nclosed-source and open-source LLMs against traditional non-LLM methods using\nautomatic metrics and human evaluations. Our experiments reveal that LLMs not\nonly outperform non-LLM approaches in all four tasks but also often generate\noutputs that exceed the quality of existing human-annotated references.\nFinally, we present some future directions of TS in the era of LLMs.\n","authors":["Jipeng Qiang","Minjiang Huang","Yi Zhu","Yunhao Yuan","Chaowei Zhang","Kui Yu"],"pdf_url":"https://arxiv.org/pdf/2502.08281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08279v1","updated":"2025-02-12T10:36:55Z","published":"2025-02-12T10:36:55Z","title":"What Is That Talk About? A Video-to-Text Summarization Dataset for\n  Scientific Presentations","summary":"  Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of scientific video summarization.\n","authors":["Dongqi Liu","Chenxi Whitehouse","Xi Yu","Louis Mahon","Rohit Saxena","Zheng Zhao","Yifu Qiu","Mirella Lapata","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.08279v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2306.02873 by other authors"},{"id":"http://arxiv.org/abs/2502.08266v1","updated":"2025-02-12T10:19:50Z","published":"2025-02-12T10:19:50Z","title":"Dealing with Annotator Disagreement in Hate Speech Classification","summary":"  Hate speech detection is a crucial task, especially on social media, where\nharmful content can spread quickly. Implementing machine learning models to\nautomatically identify and address hate speech is essential for mitigating its\nimpact and preventing its proliferation. The first step in developing an\neffective hate speech detection model is to acquire a high-quality dataset for\ntraining. Labeled data is foundational for most natural language processing\ntasks, but categorizing hate speech is difficult due to the diverse and often\nsubjective nature of hate speech, which can lead to varying interpretations and\ndisagreements among annotators. This paper examines strategies for addressing\nannotator disagreement, an issue that has been largely overlooked. In\nparticular, we evaluate different approaches to deal with annotator\ndisagreement regarding hate speech classification in Turkish tweets, based on a\nfine-tuned BERT model. Our work highlights the importance of the problem and\nprovides state-of-art benchmark results for detection and understanding of hate\nspeech in online discourse.\n","authors":["Somaiyeh Dehghan","Mehmet Umut Sen","Berrin Yanikoglu"],"pdf_url":"https://arxiv.org/pdf/2502.08266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08265v1","updated":"2025-02-12T10:17:18Z","published":"2025-02-12T10:17:18Z","title":"Exploring the Potential of Large Language Models to Simulate Personality","summary":"  With the advancement of large language models (LLMs), the focus in\nConversational AI has shifted from merely generating coherent and relevant\nresponses to tackling more complex challenges, such as personalizing dialogue\nsystems. In an effort to enhance user engagement, chatbots are often designed\nto mimic human behaviour, responding within a defined emotional spectrum and\naligning to a set of values. In this paper, we aim to simulate personal traits\naccording to the Big Five model with the use of LLMs. Our research showed that\ngenerating personality-related texts is still a challenging task for the\nmodels. As a result, we present a dataset of generated texts with the\npredefined Big Five characteristics and provide an analytical framework for\ntesting LLMs on a simulation of personality skills.\n","authors":["Maria Molchanova","Anna Mikhailova","Anna Korzanova","Lidiia Ostyakova","Alexandra Dolidze"],"pdf_url":"https://arxiv.org/pdf/2502.08265v1.pdf","comment":"Preprint submitted to Workshop on Customizable NLP (CustomNLP4U) on\n  EMNLP2024"},{"id":"http://arxiv.org/abs/2407.15588v5","updated":"2025-02-12T09:50:02Z","published":"2024-07-22T12:25:48Z","title":"Unsupervised Robust Cross-Lingual Entity Alignment via Neighbor Triple\n  Matching with Entity and Relation Texts","summary":"  Cross-lingual entity alignment (EA) enables the integration of multiple\nknowledge graphs (KGs) across different languages, providing users with\nseamless access to diverse and comprehensive knowledge. Existing methods,\nmostly supervised, face challenges in obtaining labeled entity pairs. To\naddress this, recent studies have shifted towards self-supervised and\nunsupervised frameworks. Despite their effectiveness, these approaches have\nlimitations: (1) Relation passing: mainly focusing on the entity while\nneglecting the semantic information of relations, (2) Isomorphic assumption:\nassuming isomorphism between source and target graphs, which leads to noise and\nreduced alignment accuracy, and (3) Noise vulnerability: susceptible to noise\nin the textual features, especially when encountering inconsistent translations\nor Out-of-Vocabulary (OOV) problems. In this paper, we propose ERAlign, an\nunsupervised and robust cross-lingual EA pipeline that jointly performs\nEntity-level and Relation-level Alignment by neighbor triple matching strategy\nusing semantic textual features of relations and entities. Its refinement step\niteratively enhances results by fusing entity-level and relation-level\nalignments based on neighbor triple matching. The additional verification step\nexamines the entities' neighbor triples as the linearized text. This\nAlign-then-Verify pipeline rigorously assesses alignment results, achieving\nnear-perfect alignment even in the presence of noisy textual features of\nentities. Our extensive experiments demonstrate that the robustness and general\napplicability of ERAlign improved the accuracy and effectiveness of EA tasks,\ncontributing significantly to knowledge-oriented applications.\n","authors":["Soojin Yoon","Sungho Ko","Tongyoung Kim","SeongKu Kang","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2407.15588v5.pdf","comment":"WSDM 2025"},{"id":"http://arxiv.org/abs/2502.08246v1","updated":"2025-02-12T09:39:54Z","published":"2025-02-12T09:39:54Z","title":"Inference-time sparse attention with asymmetric indexing","summary":"  Self-attention in transformer models is an incremental associative memory\nthat maps key vectors to value vectors. One way to speed up self-attention is\nto employ GPU-compliant vector search algorithms, yet the standard partitioning\nmethods yield poor results in this context, because (1) keys and queries follow\ndifferent distributions and (2) the effect of RoPE positional encoding.\n  In this paper, we introduce SAAP (Self-Attention with Asymmetric Partitions),\nwhich overcomes these problems. It is an asymmetrical indexing technique that\nemploys distinct partitions for keys and queries, thereby approximating\nself-attention with a data-adaptive sparsity pattern.\n  It works on pretrained language models without finetuning, as it only\nrequires to train (offline) a small query classifier. On a long context Llama\n3.1-8b model, with sequences ranging from 100k to 500k tokens, our method\ntypically reduces by a factor 20 the fraction of memory that needs to be\nlooked-up, which translates to a time saving of 60\\% when compared to\nFlashAttention-v2.\n","authors":["Pierre-Emmanuel Mazaré","Gergely Szilvasy","Maria Lomeli","Francisco Massa","Naila Murray","Hervé Jégou","Matthijs Douze"],"pdf_url":"https://arxiv.org/pdf/2502.08246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08213v1","updated":"2025-02-12T08:48:55Z","published":"2025-02-12T08:48:55Z","title":"LLM Modules: Knowledge Transfer from a Large to a Small Model using\n  Enhanced Cross-Attention","summary":"  In this work, we propose an architecture of LLM Modules that enables the\ntransfer of knowledge from a large pre-trained model to a smaller model using\nan Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B\nmodel is frozen and its representations are passed through specially designed\nattention layers to the GPT-Neo-125M model, which is trained on limited\ncomputational resources. Experimental results on the Bespoke-Stratos-17k\ndataset demonstrate that after 15 epochs of training, the combined model\ngenerates responses comparable in quality to those obtained by distillation. We\ndiscuss the advantages of the modular approach, provide examples of input\nqueries and comparative analysis, and outline prospects for further extension\nof the method.\n","authors":["Konstantin Kolomeitsev"],"pdf_url":"https://arxiv.org/pdf/2502.08213v1.pdf","comment":"Code and pre-trained weights available at\n  https://huggingface.co/kkolomeitsev/llm-modules"},{"id":"http://arxiv.org/abs/2502.08205v1","updated":"2025-02-12T08:35:10Z","published":"2025-02-12T08:35:10Z","title":"Wisdom of the Crowds in Forecasting: Forecast Summarization for\n  Supporting Future Event Prediction","summary":"  Future Event Prediction (FEP) is an essential activity whose demand and\napplication range across multiple domains. While traditional methods like\nsimulations, predictive and time-series forecasting have demonstrated promising\noutcomes, their application in forecasting complex events is not entirely\nreliable due to the inability of numerical data to accurately capture the\nsemantic information related to events. One forecasting way is to gather and\naggregate collective opinions on the future to make predictions as cumulative\nperspectives carry the potential to help estimating the likelihood of upcoming\nevents. In this work, we organize the existing research and frameworks that aim\nto support future event prediction based on crowd wisdom through aggregating\nindividual forecasts. We discuss the challenges involved, available datasets,\nas well as the scope of improvement and future research directions for this\ntask. We also introduce a novel data model to represent individual forecast\nstatements.\n","authors":["Anisha Saha","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10967v2","updated":"2025-02-12T08:10:06Z","published":"2025-01-19T07:00:46Z","title":"Advancing General Multimodal Capability of Vision-language Models with\n  Pyramid-descent Visual Position Encoding","summary":"  Vision-language Models (VLMs) have shown remarkable capabilities in advancing\ngeneral artificial intelligence, yet the irrational encoding of visual\npositions persists in inhibiting the models' comprehensive perception\nperformance across different levels of granularity. In this work, we propose\nPyramid-descent Visual Position Encoding (PyPE), a novel approach designed to\nenhance the perception of visual tokens within VLMs. By assigning visual\nposition indexes from the periphery to the center and expanding the central\nreceptive field incrementally, PyPE addresses the limitations of traditional\nraster-scan methods and mitigates the long-term decay effects induced by Rotary\nPosition Embedding (RoPE). Our method reduces the relative distance between\ninterrelated visual elements and instruction tokens, promoting a more rational\nallocation of attention weights and allowing for a multi-granularity perception\nof visual elements and countering the over-reliance on anchor tokens. Extensive\nexperimental evaluations demonstrate that PyPE consistently improves the\ngeneral capabilities of VLMs across various sizes. Code is available at\nhttps://github.com/SakuraTroyChen/PyPE.\n","authors":["Zhanpeng Chen","Mingxiao Li","Ziyang Chen","Nan Du","Xiaolong Li","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2501.10967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02385v2","updated":"2025-02-12T07:55:10Z","published":"2025-01-04T21:23:36Z","title":"Guiding Medical Vision-Language Models with Explicit Visual Prompts:\n  Framework Design and Comprehensive Exploration of Prompt Variations","summary":"  While mainstream vision-language models (VLMs) have advanced rapidly in\nunderstanding image level information, they still lack the ability to focus on\nspecific areas designated by humans. Rather, they typically rely on large\nvolumes of high-quality image-text paired data to learn and generate posterior\nattention maps. To address this critical issue, we propose leveraging visual\nprompts:simple visual markers in various forms to guide and enhance the\nformation of region-specific attention. Thus, we introduce MedVP, a pioneering\nframework that integrates medical entity extraction, visual prompt generation,\nand dataset adaptation for visual prompt guided fine-tuning. We successfully\noutperform recent state-of-the-art large models across multiple medical VQA\ndatasets. Extensive experiments and Human evaluation are conducted to analyze\nthe impact of different visual prompt forms and how they contribute to\nperformance improvement. The results demonstrate both the effectiveness and\nclinical significance of our approach.\n","authors":["Kangyu Zhu","Ziyuan Qin","Huahui Yi","Zekun Jiang","Qicheng Lao","Shaoting Zhang","Kang Li"],"pdf_url":"https://arxiv.org/pdf/2501.02385v2.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.08180v1","updated":"2025-02-12T07:37:39Z","published":"2025-02-12T07:37:39Z","title":"Enhancing LLM Character-Level Manipulation via Divide and Conquer","summary":"  Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks.\n","authors":["Zhen Xiong","Yujun Cai","Bryan Hooi","Nanyun Peng","Kai-Wei Chang","Zhecheng Li","Yiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08178v1","updated":"2025-02-12T07:32:48Z","published":"2025-02-12T07:32:48Z","title":"ParetoRAG: Leveraging Sentence-Context Attention for Robust and\n  Efficient Retrieval-Augmented Generation","summary":"  While Retrieval-Augmented Generation (RAG) systems enhance Large Language\nModels (LLMs) by incorporating external knowledge, they still face persistent\nchallenges in retrieval inefficiency and the inability of LLMs to filter out\nirrelevant information. We present ParetoRAG, an unsupervised framework that\noptimizes RAG systems through sentence-level refinement guided by the Pareto\nprinciple. By decomposing paragraphs into sentences and dynamically\nre-weighting core content while preserving contextual coherence, ParetoRAG\nachieves dual improvements in both retrieval precision and generation quality\nwithout requiring additional training or API resources. This framework has been\nempirically validated across various datasets, LLMs, and retrievers.\n","authors":["Ruobing Yao","Yifei Zhang","Shuang Song","Yuhua Liu","Neng Gao","Chenyang Tu"],"pdf_url":"https://arxiv.org/pdf/2502.08178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15151v2","updated":"2025-02-12T07:25:24Z","published":"2024-12-19T18:28:41Z","title":"Language Models as Continuous Self-Evolving Data Engineers","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting an upper limit on the performance of\nLLMs. To address this issue, we propose a novel paradigm that enables LLMs to\ntrain itself by autonomously generating, cleaning, reviewing, and annotating\ndata with preference information, named LANCE. Our approach demonstrates that\nLLMs can serve as continuous self-evolving data engineers, significantly\nreducing the time and cost of the post-training data construction process.\nThrough iterative fine-tuning on different variants of the Qwen2, we validate\nthe effectiveness of LANCE across various tasks, showing that it can\ncontinuously improve model performance and maintain high-quality data\ngeneration. Across eight benchmark dimensions, LANCE resulted in an average\nscore enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This\ntraining paradigm with autonomous data construction not only reduces the\nreliance on human experts or external models but also ensures that the data\naligns with human values and preferences, paving the way for the development of\nfuture superintelligent systems that can exceed human capabilities.\n","authors":["Peidong Wang","Ming Wang","Zhiming Ma","Xiaocui Yang","Shi Feng","Daling Wang","Yifei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.15151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08168v1","updated":"2025-02-12T07:19:36Z","published":"2025-02-12T07:19:36Z","title":"SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image\n  Interpretation","summary":"  In the field of synthetic aperture radar (SAR) remote sensing image\ninterpretation, although Vision language models (VLMs) have made remarkable\nprogress in natural language processing and image understanding, their\napplications remain limited in professional domains due to insufficient domain\nexpertise. This paper innovatively proposes the first large-scale multimodal\ndialogue dataset for SAR images, named SARChat-2M, which contains approximately\n2 million high-quality image-text pairs, encompasses diverse scenarios with\ndetailed target annotations. This dataset not only supports several key tasks\nsuch as visual understanding and object detection tasks, but also has unique\ninnovative aspects: this study develop a visual-language dataset and benchmark\nfor the SAR domain, enabling and evaluating VLMs' capabilities in SAR image\ninterpretation, which provides a paradigmatic framework for constructing\nmultimodal datasets across various remote sensing vertical domains. Through\nexperiments on 16 mainstream VLMs, the effectiveness of the dataset has been\nfully verified, and the first multi-task dialogue benchmark in the SAR field\nhas been successfully established. The project will be released at\nhttps://github.com/JimmyMa99/SARChat, aiming to promote the in-depth\ndevelopment and wide application of SAR visual language models.\n","authors":["Zhiming Ma","Xiayang Xiao","Sihao Dong","Peidong Wang","HaiPeng Wang","Qingyun Pan"],"pdf_url":"https://arxiv.org/pdf/2502.08168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16821v2","updated":"2025-02-12T06:54:53Z","published":"2024-11-25T17:15:41Z","title":"KL-geodesics flow matching with a novel sampling scheme","summary":"  Non-autoregressive language models generate all tokens simultaneously,\noffering potential speed advantages over traditional autoregressive models, but\nthey face challenges in modeling the complex dependencies inherent in text\ndata. In this work, we investigate a conditional flow matching approach for\ntext generation. We represent tokens as one-hot vectors in a \\(V\\)-dimensional\nsimplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which\ncorrespond to linear interpolation in logit space. We provide a theoretical\njustification that maximizing the conditional likelihood \\(P_{\\theta}(x_1 \\mid\nx_t, t)\\) yields the exact flow matching velocity under logit interpolation. To\naddress the suboptimal performance of basic inference, we propose a novel\nempirical sampling scheme that iteratively samples from the conditional\ndistribution and introduces additional noise, significantly improving results\ndespite lacking full theoretical underpinnings. Furthermore, we propose a\nhybrid inference method that combines the basic approach with the sampling\nscheme. This method demonstrates superior performance on both conditional and\nunconditional text generation experiments compared to previous SOTA method for\ndiscrete flow matching.\n","authors":["Egor Sevriugov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2411.16821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16769v2","updated":"2025-02-12T06:39:07Z","published":"2024-11-25T04:17:24Z","title":"In-Context Experience Replay Facilitates Safety Red-Teaming of\n  Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) models have shown remarkable progress, but their\npotential to generate harmful content remains a critical concern in the ML\ncommunity. While various safety mechanisms have been developed, the field lacks\nsystematic tools for evaluating their effectiveness against real-world misuse\nscenarios. In this work, we propose ICER, a novel red-teaming framework that\nleverages Large Language Models (LLMs) and a bandit optimization-based\nalgorithm to generate interpretable and semantic meaningful problematic prompts\nby learning from past successful red-teaming attempts. Our ICER efficiently\nprobes safety mechanisms across different T2I models without requiring internal\naccess or additional training, making it broadly applicable to deployed\nsystems. Through extensive experiments, we demonstrate that ICER significantly\noutperforms existing prompt attack methods in identifying model vulnerabilities\nwhile maintaining high semantic similarity with intended content. By uncovering\nthat successful jailbreaking instances can systematically facilitate the\ndiscovery of new vulnerabilities, our work provides crucial insights for\ndeveloping more robust safety mechanisms in T2I systems.\n","authors":["Zhi-Yi Chin","Mario Fritz","Pin-Yu Chen","Wei-Chen Chiu"],"pdf_url":"https://arxiv.org/pdf/2411.16769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00784v2","updated":"2025-02-12T06:34:11Z","published":"2024-10-17T06:44:18Z","title":"FIRE: Fact-checking with Iterative Retrieval and Verification","summary":"  Fact-checking long-form text is challenging, and it is therefore common\npractice to break it down into multiple atomic claims. The typical approach to\nfact-checking these atomic claims involves retrieving a fixed number of pieces\nof evidence, followed by a verification step. However, this method is usually\nnot cost-effective, as it underutilizes the verification model's internal\nknowledge of the claim and fails to replicate the iterative reasoning process\nin human search strategies. To address these limitations, we propose FIRE, a\nnovel agent-based framework that integrates evidence retrieval and claim\nverification in an iterative manner. Specifically, FIRE employs a unified\nmechanism to decide whether to provide a final answer or generate a subsequent\nsearch query, based on its confidence in the current judgment. We compare FIRE\nwith other strong fact-checking frameworks and find that it achieves slightly\nbetter performance while reducing large language model (LLM) costs by an\naverage of 7.6 times and search costs by 16.5 times. These results indicate\nthat FIRE holds promise for application in large-scale fact-checking\noperations. Our code is available at https://github.com/mbzuai-nlp/fire.git.\n","authors":["Zhuohan Xie","Rui Xing","Yuxia Wang","Jiahui Geng","Hasan Iqbal","Dhruv Sahnan","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2411.00784v2.pdf","comment":"4 figures, 8 tables, accepted to Findings of NAACL"},{"id":"http://arxiv.org/abs/2502.05206v2","updated":"2025-02-12T06:16:00Z","published":"2025-02-02T05:14:22Z","title":"Safety at Scale: A Comprehensive Survey of Large Model Safety","summary":"  The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.\n","authors":["Xingjun Ma","Yifeng Gao","Yixu Wang","Ruofan Wang","Xin Wang","Ye Sun","Yifan Ding","Hengyuan Xu","Yunhao Chen","Yunhan Zhao","Hanxun Huang","Yige Li","Jiaming Zhang","Xiang Zheng","Yang Bai","Zuxuan Wu","Xipeng Qiu","Jingfeng Zhang","Yiming Li","Jun Sun","Cong Wang","Jindong Gu","Baoyuan Wu","Siheng Chen","Tianwei Zhang","Yang Liu","Mingming Gong","Tongliang Liu","Shirui Pan","Cihang Xie","Tianyu Pang","Yinpeng Dong","Ruoxi Jia","Yang Zhang","Shiqing Ma","Xiangyu Zhang","Neil Gong","Chaowei Xiao","Sarah Erfani","Bo Li","Masashi Sugiyama","Dacheng Tao","James Bailey","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.05206v2.pdf","comment":"47 pages, 3 figures, 11 tables GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety"},{"id":"http://arxiv.org/abs/2501.09997v2","updated":"2025-02-12T06:15:17Z","published":"2025-01-17T07:30:01Z","title":"Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models","summary":"  Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection.\n","authors":["Qiang Liu","Xinlong Chen","Yue Ding","Shizhen Xu","Shu Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2501.09997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07316v2","updated":"2025-02-12T05:58:21Z","published":"2025-02-11T07:26:50Z","title":"CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction","summary":"  Reasoning is a fundamental capability of Large Language Models. While prior\nresearch predominantly focuses on enhancing narrow skills like math or code\ngeneration, improving performance on many other reasoning tasks remains\nchallenging due to sparse and fragmented training data. To address this issue,\nwe propose CodeI/O, a novel approach that systematically condenses diverse\nreasoning patterns inherently embedded in contextually-grounded codes, through\ntransforming the original code into a code input-output prediction format. By\ntraining models to predict inputs/outputs given code and test cases entirely in\nnatural language as Chain-of-Thought (CoT) rationales, we expose them to\nuniversal reasoning primitives -- like logic flow planning, state-space\nsearching, decision tree traversal, and modular decomposition -- while\ndecoupling structured reasoning from code-specific syntax and preserving\nprocedural rigor. Experimental results demonstrate CodeI/O leads to consistent\nimprovements across symbolic, scientific, logic, math & numerical, and\ncommonsense reasoning tasks. By matching the existing ground-truth outputs or\nre-executing the code with predicted inputs, we can verify each prediction and\nfurther enhance the CoTs through multi-turn revision, resulting in CodeI/O++\nand achieving higher performance. Our data and models are available at\nhttps://github.com/hkust-nlp/CodeIO.\n","authors":["Junlong Li","Daya Guo","Dejian Yang","Runxin Xu","Yu Wu","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2502.07316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08141v1","updated":"2025-02-12T05:48:26Z","published":"2025-02-12T05:48:26Z","title":"LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits","summary":"  Fine-tuning large language models (LLMs) is increasingly costly as models\nscale to hundreds of billions of parameters, and even parameter-efficient\nfine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce\nLowRA, the first framework to enable LoRA fine-tuning below 2 bits per\nparameter with minimal performance loss. LowRA optimizes fine-grained\nquantization - mapping, threshold selection, and precision assignment - while\nleveraging efficient CUDA kernels for scalable deployment. Extensive\nevaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior\nperformance-precision trade-off above 2 bits and remains accurate down to 1.15\nbits, reducing memory usage by up to 50%. Our results highlight the potential\nof ultra-low-bit LoRA fine-tuning for resource-constrained environments.\n","authors":["Zikai Zhou","Qizheng Zhang","Hermann Kumbong","Kunle Olukotun"],"pdf_url":"https://arxiv.org/pdf/2502.08141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15504v2","updated":"2025-02-12T05:45:21Z","published":"2024-12-20T02:35:39Z","title":"Mitigating Social Bias in Large Language Models: A Multi-Objective\n  Approach within a Multi-Agent Framework","summary":"  Natural language processing (NLP) has seen remarkable advancements with the\ndevelopment of large language models (LLMs). Despite these advancements, LLMs\noften produce socially biased outputs. Recent studies have mainly addressed\nthis problem by prompting LLMs to behave ethically, but this approach results\nin unacceptable performance degradation. In this paper, we propose a\nmulti-objective approach within a multi-agent framework (MOMA) to mitigate\nsocial bias in LLMs without significantly compromising their performance. The\nkey idea of MOMA involves deploying multiple agents to perform causal\ninterventions on bias-related contents of the input questions, breaking the\nshortcut connection between these contents and the corresponding answers.\nUnlike traditional debiasing techniques leading to performance degradation,\nMOMA substantially reduces bias while maintaining accuracy in downstream tasks.\nOur experiments conducted on two datasets and two models demonstrate that MOMA\nreduces bias scores by up to 87.7%, with only a marginal performance\ndegradation of up to 6.8% in the BBQ dataset. Additionally, it significantly\nenhances the multi-objective metric icat in the StereoSet dataset by up to\n58.1%. Code will be made available at https://github.com/Cortantse/MOMA.\n","authors":["Zhenjie Xu","Wenqing Chen","Yi Tang","Xuanying Li","Cheng Hu","Zhixuan Chu","Kui Ren","Zibin Zheng","Zhichao Lu"],"pdf_url":"https://arxiv.org/pdf/2412.15504v2.pdf","comment":"This work has been accepted at The 39th Annual AAAI Conference on\n  Artificial Intelligence (AAAI-2025)"},{"id":"http://arxiv.org/abs/2407.18418v3","updated":"2025-02-12T05:42:09Z","published":"2024-07-25T22:31:50Z","title":"Know Your Limits: A Survey of Abstention in Large Language Models","summary":"  Abstention, the refusal of large language models (LLMs) to provide an answer,\nis increasingly recognized for its potential to mitigate hallucinations and\nenhance safety in LLM systems. In this survey, we introduce a framework to\nexamine abstention from three perspectives: the query, the model, and human\nvalues. We organize the literature on abstention methods, benchmarks, and\nevaluation metrics using this framework, and discuss merits and limitations of\nprior work. We further identify and motivate areas for future research, such as\nwhether abstention can be achieved as a meta-capability that transcends\nspecific tasks or domains, and opportunities to optimize abstention abilities\nin specific contexts. In doing so, we aim to broaden the scope and impact of\nabstention methodologies in AI systems.\n","authors":["Bingbing Wen","Jihan Yao","Shangbin Feng","Chenjun Xu","Yulia Tsvetkov","Bill Howe","Lucy Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2407.18418v3.pdf","comment":"TACL 2024"},{"id":"http://arxiv.org/abs/2501.14249v3","updated":"2025-02-12T05:39:05Z","published":"2025-01-24T05:27:46Z","title":"Humanity's Last Exam","summary":"  Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 3,000\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.\n","authors":["Long Phan","Alice Gatti","Ziwen Han","Nathaniel Li","Josephina Hu","Hugh Zhang","Chen Bo Calvin Zhang","Mohamed Shaaban","John Ling","Sean Shi","Michael Choi","Anish Agrawal","Arnav Chopra","Adam Khoja","Ryan Kim","Richard Ren","Jason Hausenloy","Oliver Zhang","Mantas Mazeika","Tung Nguyen","Daron Anderson","Imad Ali Shah","Mikhail Doroshenko","Alun Cennyth Stokes","Mobeen Mahmood","Jaeho Lee","Oleksandr Pokutnyi","Oleg Iskra","Jessica P. Wang","Robert Gerbicz","John-Clark Levin","Serguei Popov","Fiona Feng","Steven Y. Feng","Haoran Zhao","Michael Yu","Varun Gangal","Chelsea Zou","Zihan Wang","Mstyslav Kazakov","Geoff Galgon","Johannes Schmitt","Alvaro Sanchez","Yongki Lee","Will Yeadon","Scott Sauers","Marc Roth","Chidozie Agu","Søren Riis","Fabian Giska","Saiteja Utpala","Antrell Cheatom","Zachary Giboney","Gashaw M. Goshu","Sarah-Jane Crowson","Mohinder Maheshbhai Naiya","Noah Burns","Lennart Finke","Zerui Cheng","Hyunwoo Park","Francesco Fournier-Facio","Jennifer Zampese","John Wydallis","John B. Wydallis","Ryan G. Hoerr","Mark Nandor","Tim Gehrunger","Jiaqi Cai","Ben McCarty","Jungbae Nam","Edwin Taylor","Jun Jin","Gautier Abou Loume","Hangrui Cao","Alexis C Garretson","Damien Sileo","Qiuyu Ren","Doru Cojoc","Pavel Arkhipov","Usman Qazi","Aras Bacho","Lianghui Li","Sumeet Motwani","Christian Schroeder de Witt","Alexei Kopylov","Johannes Veith","Eric Singer","Paolo Rissone","Jaehyeok Jin","Jack Wei Lun Shi","Chris G. Willcocks","Ameya Prabhu","Longke Tang","Kevin Zhou","Emily de Oliveira Santos","Andrey Pupasov Maksimov","Edward Vendrow","Kengo Zenitani","Joshua Robinson","Aleksandar Mikov","Julien Guillod","Yuqi Li","Ben Pageler","Joshua Vendrow","Vladyslav Kuchkin","Pierre Marion","Denis Efremov","Jayson Lynch","Kaiqu Liang","Andrew Gritsevskiy","Dakotah Martinez","Nick Crispino","Dimitri Zvonkine","Natanael Wildner Fraga","Saeed Soori","Ori Press","Henry Tang","Julian Salazar","Sean R. Green","Lina Brüssel","Moon Twayana","Aymeric Dieuleveut","T. Ryan Rogers","Wenjin Zhang","Ross Finocchio","Bikun Li","Jinzhou Yang","Arun Rao","Gabriel Loiseau","Mikhail Kalinin","Marco Lukas","Ciprian Manolescu","Nate Stambaugh","Subrata Mishra","Ariel Ghislain Kemogne Kamdoum","Tad Hogg","Alvin Jin","Carlo Bosio","Gongbo Sun","Brian P Coppola","Haline Heidinger","Rafael Sayous","Stefan Ivanov","Joseph M Cavanagh","Jiawei Shen","Joseph Marvin Imperial","Philippe Schwaller","Shaipranesh Senthilkuma","Andres M Bran","Andres Algaba","Brecht Verbeken","Kelsey Van den Houte","Lynn Van Der Sypt","David Noever","Lisa Schut","Ilia Sucholutsky","Evgenii Zheltonozhskii","Qiaochu Yuan","Derek Lim","Richard Stanley","Shankar Sivarajan","Tong Yang","John Maar","Julian Wykowski","Martí Oller","Jennifer Sandlin","Anmol Sahu","Cesare Giulio Ardito","Yuzheng Hu","Felipe Meneguitti Dias","Tobias Kreiman","Kaivalya Rawal","Tobias Garcia Vilchis","Yuexuan Zu","Martin Lackner","James Koppel","Jeremy Nguyen","Daniil S. Antonenko","Steffi Chern","Bingchen Zhao","Pierrot Arsene","Sergey Ivanov","Rafał Poświata","Chenguang Wang","Daofeng Li","Donato Crisostomi","Ali Dehghan","Andrea Achilleos","John Arnold Ambay","Benjamin Myklebust","Archan Sen","David Perrella","Nurdin Kaparov","Mark H Inlow","Allen Zang","Kalyan Ramakrishnan","Daniil Orel","Vladislav Poritski","Shalev Ben-David","Zachary Berger","Parker Whitfill","Michael Foster","Daniel Munro","Linh Ho","Dan Bar Hava","Aleksey Kuchkin","Robert Lauff","David Holmes","Frank Sommerhage","Anji Zhang","Richard Moat","Keith Schneider","Daniel Pyda","Zakayo Kazibwe","Mukhwinder Singh","Don Clarke","Dae Hyun Kim","Sara Fish","Veit Elser","Victor Efren Guadarrama Vilchis","Immo Klose","Christoph Demian","Ujjwala Anantheswaran","Adam Zweiger","Guglielmo Albani","Jeffery Li","Nicolas Daans","Maksim Radionov","Václav Rozhoň","Vincent Ginis","Ziqiao Ma","Christian Stump","Jacob Platnick","Volodymyr Nevirkovets","Luke Basler","Marco Piccardo","Niv Cohen","Virendra Singh","Josef Tkadlec","Paul Rosu","Alan Goldfarb","Piotr Padlewski","Stanislaw Barzowski","Kyle Montgomery","Aline Menezes","Arkil Patel","Zixuan Wang","Jamie Tucker-Foltz","Jack Stade","Declan Grabb","Tom Goertzen","Fereshteh Kazemi","Jeremiah Milbauer","Abhishek Shukla","Hossam Elgnainy","Yan Carlos Leyva Labrador","Hao He","Ling Zhang","Alan Givré","Hew Wolff","Gözdenur Demir","Muhammad Fayez Aziz","Younesse Kaddar","Ivar Ängquist","Yanxu Chen","Elliott Thornley","Robin Zhang","Jiayi Pan","Antonio Terpin","Niklas Muennighoff","Hailey Schoelkopf","Eric Zheng","Avishy Carmi","Jainam Shah","Ethan D. L. Brown","Kelin Zhu","Max Bartolo","Richard Wheeler","Andrew Ho","Shaul Barkan","Jiaqi Wang","Martin Stehberger","Egor Kretov","Peter Bradshaw","JP Heimonen","Kaustubh Sridhar","Zaki Hossain","Ido Akov","Yury Makarychev","Joanna Tam","Hieu Hoang","David M. Cunningham","Vladimir Goryachev","Demosthenes Patramanis","Michael Krause","Andrew Redenti","David Aldous","Jesyin Lai","Shannon Coleman","Jiangnan Xu","Sangwon Lee","Ilias Magoulas","Sandy Zhao","Ning Tang","Michael K. Cohen","Micah Carroll","Orr Paradise","Jan Hendrik Kirchner","Stefan Steinerberger","Maksym Ovchynnikov","Jason O. Matos","Adithya Shenoy","Michael Wang","Yuzhou Nie","Paolo Giordano","Philipp Petersen","Anna Sztyber-Betley","Paolo Faraboschi","Robin Riblet","Jonathan Crozier","Shiv Halasyamani","Antonella Pinto","Shreyas Verma","Prashant Joshi","Eli Meril","Zheng-Xin Yong","Allison Tee","Jérémy Andréoletti","Orion Weller","Raghav Singhal","Gang Zhang","Alexander Ivanov","Seri Khoury","Nils Gustafsson","Hamid Mostaghimi","Kunvar Thaman","Qijia Chen","Tran Quoc Khánh","Jacob Loader","Stefano Cavalleri","Hannah Szlyk","Zachary Brown","Himanshu Narayan","Jonathan Roberts","William Alley","Kunyang Sun","Ryan Stendall","Max Lamparth","Anka Reuel","Ting Wang","Hanmeng Xu","Pablo Hernández-Cámara","Freddie Martin","Thomas Preu","Tomek Korbak","Marcus Abramovitch","Dominic Williamson","Ida Bosio","Ziye Chen","Biró Bálint","Eve J. Y. Lo","Maria Inês S. Nunes","Yibo Jiang","M Saiful Bari","Peyman Kassani","Zihao Wang","Behzad Ansarinejad","Yewen Sun","Stephane Durand","Guillaume Douville","Daniel Tordera","George Balabanian","Earth Anderson","Lynna Kvistad","Alejandro José Moyano","Hsiaoyun Milliron","Ahmad Sakor","Murat Eron","Isaac C. McAlister","Andrew Favre D. O.","Shailesh Shah","Xiaoxiang Zhou","Firuz Kamalov","Ronald Clark","Sherwin Abdoli","Tim Santens","Harrison K Wang","Evan Chen","Alessandro Tomasiello","G. Bruno De Luca","Shi-Zhuo Looi","Vinh-Kha Le","Noam Kolt","Niels Mündler","Avi Semler","Emma Rodman","Jacob Drori","Carl J Fossum","Luk Gloor","Milind Jagota","Ronak Pradeep","Honglu Fan","Tej Shah","Jonathan Eicher","Michael Chen","Kushal Thaman","William Merrill","Moritz Firsching","Carter Harris","Stefan Ciobâcă","Jason Gross","Rohan Pandey","Ilya Gusev","Adam Jones","Shashank Agnihotri","Pavel Zhelnov","Siranut Usawasutsakorn","Mohammadreza Mofayezi","Alexander Piperski","Marc Carauleanu","David K. Zhang","Kostiantyn Dobarskyi","Dylan Ler","Roman Leventov","Ignat Soroko","Thorben Jansen","Scott Creighton","Pascal Lauer","Joshua Duersch","Vage Taamazyan","Dario Bezzi","Wiktor Morak","Wenjie Ma","William Held","Tran Đuc Huy","Ruicheng Xian","Armel Randy Zebaze","Mohanad Mohamed","Julian Noah Leser","Michelle X Yuan","Laila Yacar","Johannes Lengler","Katarzyna Olszewska","Hossein Shahrtash","Edson Oliveira","Joseph W. Jackson","Daniel Espinosa Gonzalez","Andy Zou","Muthu Chidambaram","Timothy Manik","Hector Haffenden","Dashiell Stander","Ali Dasouqi","Alexander Shen","Emilien Duc","Bita Golshani","David Stap","Mikalai Uzhou","Alina Borisovna Zhidkovskaya","Lukas Lewark","Miguel Orbegozo Rodriguez","Mátyás Vincze","Dustin Wehr","Colin Tang","Shaun Phillips","Fortuna Samuele","Jiang Muzhen","Fredrik Ekström","Angela Hammon","Oam Patel","Faraz Farhidi","George Medley","Forough Mohammadzadeh","Madellene Peñaflor","Haile Kassahun","Alena Friedrich","Claire Sparrow","Rayner Hernandez Perez","Taom Sakal","Omkar Dhamane","Ali Khajegili Mirabadi","Eric Hallman","Kenchi Okutsu","Mike Battaglia","Mohammad Maghsoudimehrabani","Alon Amit","Dave Hulbert","Roberto Pereira","Simon Weber"," Handoko","Anton Peristyy","Stephen Malina","Samuel Albanie","Will Cai","Mustafa Mehkary","Rami Aly","Frank Reidegeld","Anna-Katharina Dick","Cary Friday","Jasdeep Sidhu","Hassan Shapourian","Wanyoung Kim","Mariana Costa","Hubeyb Gurdogan","Brian Weber","Harsh Kumar","Tong Jiang","Arunim Agarwal","Chiara Ceconello","Warren S. Vaz","Chao Zhuang","Haon Park","Andrew R. Tawfeek","Daattavya Aggarwal","Michael Kirchhof","Linjie Dai","Evan Kim","Johan Ferret","Yuzhou Wang","Minghao Yan","Krzysztof Burdzy","Lixin Zhang","Antonio Franca","Diana T. Pham","Kang Yong Loh","Joshua Robinson","Abram Jackson","Shreen Gul","Gunjan Chhablani","Zhehang Du","Adrian Cosma","Jesus Colino","Colin White","Jacob Votava","Vladimir Vinnikov","Ethan Delaney","Petr Spelda","Vit Stritecky","Syed M. Shahid","Jean-Christophe Mourrat","Lavr Vetoshkin","Koen Sponselee","Renas Bacho","Florencia de la Rosa","Xiuyu Li","Guillaume Malod","Leon Lang","Julien Laurendeau","Dmitry Kazakov","Fatimah Adesanya","Julien Portier","Lawrence Hollom","Victor Souza","Yuchen Anna Zhou","Julien Degorre","Yiğit Yalın","Gbenga Daniel Obikoya","Luca Arnaboldi"," Rai","Filippo Bigi","M. C. Boscá","Oleg Shumar","Kaniuar Bacho","Pierre Clavier","Gabriel Recchia","Mara Popescu","Nikita Shulga","Ngefor Mildred Tanwie","Denis Peskoff","Thomas C. H. Lux","Ben Rank","Colin Ni","Matthew Brooks","Alesia Yakimchyk"," Huanxu"," Liu","Olle Häggström","Emil Verkama","Hans Gundlach","Leonor Brito-Santana","Brian Amaro","Vivek Vajipey","Rynaa Grover","Yiyang Fan","Gabriel Poesia Reis e Silva","Linwei Xin","Yosi Kratish","Jakub Łucki","Wen-Ding Li","Sivakanth Gopi","Andrea Caciolai","Justin Xu","Kevin Joseph Scaria","Freddie Vargus","Farzad Habibi"," Long"," Lian","Emanuele Rodolà","Jules Robins","Vincent Cheng","Tony Fruhauff","Brad Raynor","Hao Qi","Xi Jiang","Ben Segev","Jingxuan Fan","Sarah Martinson","Erik Y. Wang","Kaylie Hausknecht","Michael P. Brenner","Mao Mao","Xinyu Zhang","David Avagian","Eshawn Jessica Scipio","Alon Ragoler","Justin Tan","Blake Sims","Rebeka Plecnik","Aaron Kirtland","Omer Faruk Bodur","D. P. Shinde","Zahra Adoul","Mohamed Zekry","Ali Karakoc","Tania C. B. Santos","Samir Shamseldeen","Loukmane Karim","Anna Liakhovitskaia","Nate Resman","Nicholas Farina","Juan Carlos Gonzalez","Gabe Maayan","Sarah Hoback","Rodrigo De Oliveira Pena","Glen Sherman","Elizabeth Kelley","Hodjat Mariji","Rasoul Pouriamanesh","Wentao Wu","Sandra Mendoza","Ismail Alarab","Joshua Cole","Danyelle Ferreira","Bryan Johnson","Mohammad Safdari","Liangti Dai","Siriphan Arthornthurasuk","Alexey Pronin","Jing Fan","Angel Ramirez-Trinidad","Ashley Cartwright","Daphiny Pottmaier","Omid Taheri","David Outevsky","Stanley Stepanic","Samuel Perry","Luke Askew","Raúl Adrián Huerta Rodríguez","Ali M. R. Minissi","Sam Ali","Ricardo Lorena","Krishnamurthy Iyer","Arshad Anil Fasiludeen","Sk Md Salauddin","Murat Islam","Juan Gonzalez","Josh Ducey","Maja Somrak","Vasilios Mavroudis","Eric Vergo","Juehang Qin","Benjámin Borbás","Eric Chu","Jack Lindsey","Anil Radhakrishnan","Antoine Jallon","I. M. J. McInnis","Pawan Kumar","Laxman Prasad Goswami","Daniel Bugas","Nasser Heydari","Ferenc Jeanplong","Archimedes Apronti","Abdallah Galal","Ng Ze-An","Ankit Singh","Joan of Arc Xavier","Kanu Priya Agarwal","Mohammed Berkani","Benedito Alves de Oliveira Junior","Dmitry Malishev","Nicolas Remy","Taylor D. Hartman","Tim Tarver","Stephen Mensah","Javier Gimenez","Roselynn Grace Montecillo","Russell Campbell","Asankhaya Sharma","Khalida Meer","Xavier Alapont","Deepakkumar Patil","Rajat Maheshwari","Abdelkader Dendane","Priti Shukla","Sergei Bogdanov","Sören Möller","Muhammad Rehan Siddiqi","Prajvi Saxena","Himanshu Gupta","Innocent Enyekwe","Ragavendran P V","Zienab EL-Wasif","Aleksandr Maksapetyan","Vivien Rossbach","Chris Harjadi","Mohsen Bahaloohoreh","Song Bian","John Lai","Justine Leon Uro","Greg Bateman","Mohamed Sayed","Ahmed Menshawy","Darling Duclosel","Yashaswini Jain","Ashley Aaron","Murat Tiryakioglu","Sheeshram Siddh","Keith Krenek","Alex Hoover","Joseph McGowan","Tejal Patwardhan","Summer Yue","Alexandr Wang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2501.14249v3.pdf","comment":"26 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.08130v1","updated":"2025-02-12T05:24:21Z","published":"2025-02-12T05:24:21Z","title":"Selective Self-to-Supervised Fine-Tuning for Generalization in Large\n  Language Models","summary":"  Fine-tuning Large Language Models (LLMs) on specific datasets is a common\npractice to improve performance on target tasks. However, this performance gain\noften leads to overfitting, where the model becomes too specialized in either\nthe task or the characteristics of the training data, resulting in a loss of\ngeneralization. This paper introduces Selective Self-to-Supervised Fine-Tuning\n(S3FT), a fine-tuning approach that achieves better performance than the\nstandard supervised fine-tuning (SFT) while improving generalization. S3FT\nleverages the existence of multiple valid responses to a query. By utilizing\nthe model's correct responses, S3FT reduces model specialization during the\nfine-tuning stage. S3FT first identifies the correct model responses from the\ntraining set by deploying an appropriate judge. Then, it fine-tunes the model\nusing the correct model responses and the gold response (or its paraphrase) for\nthe remaining samples. The effectiveness of S3FT is demonstrated through\nexperiments on mathematical reasoning, Python programming and reading\ncomprehension tasks. The results show that standard SFT can lead to an average\nperformance drop of up to $4.4$ on multiple benchmarks, such as MMLU and\nTruthfulQA. In contrast, S3FT reduces this drop by half, i.e. $2.5$, indicating\nbetter generalization capabilities than SFT while performing significantly\nbetter on the fine-tuning tasks.\n","authors":["Sonam Gupta","Yatin Nandwani","Asaf Yehudai","Dinesh Khandelwal","Dinesh Raghu","Sachindra Joshi"],"pdf_url":"https://arxiv.org/pdf/2502.08130v1.pdf","comment":"10 pages, Accepted to NAACL Findings 2025"},{"id":"http://arxiv.org/abs/2502.07072v2","updated":"2025-02-12T05:14:41Z","published":"2025-02-10T22:07:02Z","title":"IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large\n  Language Models","summary":"  Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair.\n","authors":["Sayem Mohammad Imtiaz","Astha Singh","Fraol Batole","Hridesh Rajan"],"pdf_url":"https://arxiv.org/pdf/2502.07072v2.pdf","comment":"Accepted as full research paper at FSE'2025"},{"id":"http://arxiv.org/abs/2502.08127v1","updated":"2025-02-12T05:13:04Z","published":"2025-02-12T05:13:04Z","title":"Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance","summary":"  Recent advancements in large language models (LLMs) have shown strong general\nreasoning abilities, yet their effectiveness in financial reasoning remains\nunderexplored. In this study, we comprehensively evaluate 16 powerful reasoning\nand general LLMs on three complex financial tasks involving financial text,\ntabular data, and equations, assessing numerical reasoning, tabular\ninterpretation, financial terminology comprehension, long-context processing,\nand equation-based problem solving. Our results show that while better datasets\nand pretraining improve financial reasoning, general enhancements like CoT\nfine-tuning do not always yield consistent gains. Moreover, all reasoning\nstrategies face challenges in improving performance on long-context and\nmulti-table tasks. To address these limitations, we develop a financial\nreasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and\nreinforcement learning with domain-specific reasoning paths. Even with simple\nfine-tuning with one financial dataset, our model achieves a consistent 10%\nperformance improvement across tasks, surpassing all 8B models and even\nLlama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight\nthe need for domain-specific adaptations in financial tasks, emphasizing future\ndirections such as multi-table reasoning, long-context processing, and\nfinancial terminology comprehension. All our datasets, models, and codes are\npublicly available. Furthermore, we introduce a leaderboard for benchmarking\nfuture datasets and models.\n","authors":["Lingfei Qian","Weipeng Zhou","Yan Wang","Xueqing Peng","Jimin Huang","Qianqian Xie"],"pdf_url":"https://arxiv.org/pdf/2502.08127v1.pdf","comment":"Ongoing work, 13 pages, 2 figures, 3 Tables"},{"id":"http://arxiv.org/abs/2501.02629v2","updated":"2025-02-12T04:55:19Z","published":"2025-01-05T19:06:03Z","title":"Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for\n  Jailbreak Attack Defense","summary":"  As large language models (LLMs) are increasingly deployed in diverse\napplications, including chatbot assistants and code generation, aligning their\nbehavior with safety and ethical standards has become paramount. However,\njailbreak attacks, which exploit vulnerabilities to elicit unintended or\nharmful outputs, threaten LLMs' safety significantly. In this paper, we\nintroduce Layer-AdvPatcher, a novel methodology designed to defend against\njailbreak attacks by utilizing an unlearning strategy to patch specific layers\nwithin LLMs through self-augmented datasets. Our insight is that certain\nlayer(s), tend to produce affirmative tokens when faced with harmful prompts.\nBy identifying these layers and adversarially exposing them to generate more\nharmful data, one can understand their inherent and diverse vulnerabilities to\nattacks. With these exposures, we then \"unlearn\" these issues, reducing the\nimpact of affirmative tokens and hence minimizing jailbreak risks while keeping\nthe model's responses to safe queries intact. We conduct extensive experiments\non two models, four benchmark datasets, and multiple state-of-the-art jailbreak\nattacks to demonstrate the efficacy of our approach. Results indicate that our\nframework reduces the harmfulness and attack success rate of jailbreak attacks\nwithout compromising utility for benign queries compared to recent defense\nmethods. Our code is publicly available at:\nhttps://github.com/oyy2000/LayerAdvPatcher\n","authors":["Yang Ouyang","Hengrui Gu","Shuhang Lin","Wenyue Hua","Jie Peng","Bhavya Kailkhura","Meijun Gao","Tianlong Chen","Kaixiong Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.02629v2.pdf","comment":"14 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2501.04961v2","updated":"2025-02-12T04:52:08Z","published":"2025-01-09T04:26:15Z","title":"Demystifying Domain-adaptive Post-training for Financial LLMs","summary":"  Domain-adaptive post-training of large language models (LLMs) has emerged as\na promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation\ncriteria and training strategies across varying data and model configurations.\nTo address these challenges, we introduce FINDAP, a systematic and fine-grained\ninvestigation into domain adaptive post-training of LLMs for the finance\ndomain. Our approach consists of four key components: FinCap, which defines the\ncore capabilities required for the target domain; FinRec, an effective training\nrecipe that jointly optimizes continual pre-training and instruction-following,\nalong with a novel preference data distillation method leveraging process\nsignals from a generative reward model; FinTrain, a curated set of training\ndatasets supporting FinRec; and FinEval, a comprehensive evaluation suite\naligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art\nperformance across a wide range of financial tasks. Our analysis also\nhighlights how each post-training stage contributes to distinct capabilities,\nuncovering specific challenges and effective solutions, providing valuable\ninsights for domain adaptation of LLMs.\n","authors":["Zixuan Ke","Yifei Ming","Xuan-Phi Nguyen","Caiming Xiong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2501.04961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02936v4","updated":"2025-02-12T04:41:34Z","published":"2024-04-03T04:25:01Z","title":"Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large\n  Language Models","summary":"  The problem of pre-training data detection for large language models (LLMs)\nhas received growing attention due to its implications in critical issues like\ncopyright violation and test data contamination. Despite improved performance,\nexisting methods (including the state-of-the-art, Min-K%) are mostly developed\nupon simple heuristics and lack solid, reasonable foundations. In this work, we\npropose a novel and theoretically motivated methodology for pre-training data\ndetection, named Min-K%++. Specifically, we present a key insight that training\nsamples tend to be local maxima of the modeled distribution along each input\ndimension through maximum likelihood training, which in turn allow us to\ninsightfully translate the problem into identification of local maxima. Then,\nwe design our method accordingly that works under the discrete distribution\nmodeled by LLMs, whose core idea is to determine whether the input forms a mode\nor has relatively high probability under the conditional categorical\ndistribution. Empirically, the proposed method achieves new SOTA performance\nacross multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the\nrunner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the\nmore challenging MIMIR benchmark, it consistently improves upon reference-free\nmethods while performing on par with reference-based method that requires an\nextra reference model.\n","authors":["Jingyang Zhang","Jingwei Sun","Eric Yeats","Yang Ouyang","Martin Kuo","Jianyi Zhang","Hao Frank Yang","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2404.02936v4.pdf","comment":"ICLR'25 Spotlight. Project page and code is available at\n  https://zjysteven.github.io/mink-plus-plus/"},{"id":"http://arxiv.org/abs/2501.17630v2","updated":"2025-02-12T04:28:04Z","published":"2025-01-29T13:08:17Z","title":"Uncertainty Quantification and Decomposition for LLM-based\n  Recommendation","summary":"  Despite the widespread adoption of large language models (LLMs) for\nrecommendation, we demonstrate that LLMs often exhibit uncertainty in their\nrecommendations. To ensure the trustworthy use of LLMs in generating\nrecommendations, we emphasize the importance of assessing the reliability of\nrecommendations generated by LLMs. We start by introducing a novel framework\nfor estimating the predictive uncertainty to quantitatively measure the\nreliability of LLM-based recommendations. We further propose to decompose the\npredictive uncertainty into recommendation uncertainty and prompt uncertainty,\nenabling in-depth analyses of the primary source of uncertainty. Through\nextensive experiments, we (1) demonstrate predictive uncertainty effectively\nindicates the reliability of LLM-based recommendations, (2) investigate the\norigins of uncertainty with decomposed uncertainty measures, and (3) propose\nuncertainty-aware prompting for a lower predictive uncertainty and enhanced\nrecommendation. Our source code and model weights are available at\nhttps://github.com/WonbinKweon/UNC_LLM_REC_WWW2025\n","authors":["Wonbin Kweon","Sanghwan Jang","SeongKu Kang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2501.17630v2.pdf","comment":"WWW 2025"},{"id":"http://arxiv.org/abs/2410.09807v2","updated":"2025-02-12T04:24:19Z","published":"2024-10-13T11:48:09Z","title":"Single Ground Truth Is Not Enough: Adding Flexibility to Aspect-Based\n  Sentiment Analysis Evaluation","summary":"  Aspect-based sentiment analysis (ABSA) is a challenging task of extracting\nsentiments along with their corresponding aspects and opinion terms from the\ntext. The inherent subjectivity of span annotation makes variability in the\nsurface forms of extracted terms, complicating the evaluation process.\nTraditional evaluation methods often constrain ground truths (GT) to a single\nterm, potentially misrepresenting the accuracy of semantically valid\npredictions that differ in surface form. To address this limitation, we propose\na novel and fully automated pipeline that expands existing evaluation sets by\nadding alternative valid terms for aspect and opinion. Our approach facilitates\nan equitable assessment of language models by accommodating multiple-answer\ncandidates, resulting in enhanced human agreement compared to single-answer\ntest sets (achieving up to a 10\\%p improvement in Kendall's Tau score).\nExperimental results demonstrate that our expanded evaluation set helps uncover\nthe capabilities of large language models (LLMs) in ABSA tasks, which is\nconcealed by the single-answer GT sets. Consequently, our work contributes to\nthe development of a flexible evaluation framework for ABSA by embracing\ndiverse surface forms to span extraction tasks in a cost-effective and\nreproducible manner. Our code and dataset is open at\nhttps://github.com/dudrrm/zoom-in-n-out-absa.\n","authors":["Soyoung Yang","Hojun Cho","Jiyoung Lee","Sohee Yoon","Edward Choi","Jaegul Choo","Won Ik Cho"],"pdf_url":"https://arxiv.org/pdf/2410.09807v2.pdf","comment":"NAACL 2025 camera-ready"},{"id":"http://arxiv.org/abs/2502.08109v1","updated":"2025-02-12T04:17:02Z","published":"2025-02-12T04:17:02Z","title":"HuDEx: Integrating Hallucination Detection and Explainability for\n  Enhancing the Reliability of LLM responses","summary":"  Recent advances in large language models (LLMs) have shown promising\nimprovements, often surpassing existing methods across a wide range of\ndownstream tasks in natural language processing. However, these models still\nface challenges, which may hinder their practical applicability. For example,\nthe phenomenon of hallucination is known to compromise the reliability of LLMs,\nespecially in fields that demand high factual precision. Current benchmarks\nprimarily focus on hallucination detection and factuality evaluation but do not\nextend beyond identification. This paper proposes an explanation enhanced\nhallucination-detection model, coined as HuDEx, aimed at enhancing the\nreliability of LLM-generated responses by both detecting hallucinations and\nproviding detailed explanations. The proposed model provides a novel approach\nto integrate detection with explanations, and enable both users and the LLM\nitself to understand and reduce errors. Our measurement results demonstrate\nthat the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in\nhallucination detection accuracy, while maintaining reliable explanations.\nFurthermore, the proposed model performs well in both zero-shot and other test\nenvironments, showcasing its adaptability across diverse benchmark datasets.\nThe proposed approach further enhances the hallucination detection research by\nintroducing a novel approach to integrating interpretability with hallucination\ndetection, which further enhances the performance and reliability of evaluating\nhallucinations in language models.\n","authors":["Sujeong Lee","Hayoung Lee","Seongsoo Heo","Wonik Choi"],"pdf_url":"https://arxiv.org/pdf/2502.08109v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2502.05589v2","updated":"2025-02-12T04:15:47Z","published":"2025-02-08T14:28:36Z","title":"On Memory Construction and Retrieval for Personalized Conversational\n  Agents","summary":"  To deliver coherent and personalized experiences in long-term conversations,\nexisting approaches typically perform retrieval augmented response generation\nby constructing memory banks from conversation history at either the\nturn-level, session-level, or through summarization techniques. In this paper,\nwe present two key findings: (1) The granularity of memory unit matters:\nTurn-level, session-level, and summarization-based methods each exhibit\nlimitations in both memory retrieval accuracy and the semantic quality of the\nretrieved content. (2) Prompt compression methods, such as\n\\textit{LLMLingua-2}, can effectively serve as a denoising mechanism, enhancing\nmemory retrieval accuracy across different granularities. Building on these\ninsights, we propose SeCom, a method that constructs a memory bank with topical\nsegments by introducing a conversation Segmentation model, while performing\nmemory retrieval based on Compressed memory units. Experimental results show\nthat SeCom outperforms turn-level, session-level, and several\nsummarization-based methods on long-term conversation benchmarks such as LOCOMO\nand Long-MT-Bench+. Additionally, the proposed conversation segmentation method\ndemonstrates superior performance on dialogue segmentation datasets such as\nDialSeg711, TIAGE, and SuperDialSeg.\n","authors":["Zhuoshi Pan","Qianhui Wu","Huiqiang Jiang","Xufang Luo","Hao Cheng","Dongsheng Li","Yuqing Yang","Chin-Yew Lin","H. Vicky Zhao","Lili Qiu","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2502.05589v2.pdf","comment":"10 pages, 5 figures, conference"},{"id":"http://arxiv.org/abs/2502.07328v2","updated":"2025-02-12T04:00:14Z","published":"2025-02-11T07:46:29Z","title":"Music for All: Exploring Multicultural Representations in Music\n  Generation Models","summary":"  The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.\n","authors":["Atharva Mehta","Shivam Chauhan","Amirbek Djanibekov","Atharva Kulkarni","Gus Xia","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07328v2.pdf","comment":"17 pages, 5 figures, accepted to NAACL'25"},{"id":"http://arxiv.org/abs/2502.07555v2","updated":"2025-02-12T03:33:06Z","published":"2025-02-11T13:48:10Z","title":"O1 Embedder: Let Retrievers Think Before Action","summary":"  The growing power of large language models (LLMs) has revolutionized how\npeople access and utilize information. Notably, the LLMs excel at performing\nfine-grained data representation, which facilitates precise retrieval of\ninformation. They also generate high-quality answers based on external\nreferences, enabling the production of useful knowledge. The recent\nintroduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another\nleap forward, highlighting LLMs' ability to think progressively before\ndelivering final answers. This breakthrough significantly improves the ability\nto address complex tasks, e.g., coding and math proofs.\n  Inspired by this progress, we aim to develop similar capabilities for\nretrieval models, which hold great promise for tackling critical challenges in\nthe field, including multi-task retrieval, zero-shot retrieval, and tasks\nrequiring intensive reasoning of complex relationships. With this motivation,\nwe propose a novel approach called O1 Embedder, which generates useful thoughts\nfor the input query before making retrieval for the target documents. To\nrealize this objective, we conquer two technical difficulties. First, we design\na data synthesis workflow, creating training signals for O1 Embedder by\ngenerating initial thoughts from an LLM-expert and subsequently refining them\nusing a retrieval committee. Second, we optimize the training process, enabling\na pre-trained model to be jointly fine-tuned to generate retrieval thoughts via\nbehavior cloning and perform dense retrieval through contrastive learning. Our\napproach is evaluated by comprehensive experiments, where substantial\nimprovements are achieved across 12 popular datasets, spanning both in-domain\nand out-of-domain scenarios. These results highlight O1 Embedder's remarkable\naccuracy and generalizability, paving the way for the development of\nnext-generation IR foundation models.\n","authors":["Ruiran Yan","Zheng Liu","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2502.07555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08092v1","updated":"2025-02-12T03:33:06Z","published":"2025-02-12T03:33:06Z","title":"GCoT: Chain-of-Thought Prompt Learning for Graphs","summary":"  Chain-of-thought (CoT) prompting has achieved remarkable success in natural\nlanguage processing (NLP). However, its vast potential remains largely\nunexplored for graphs. This raises an interesting question: How can we design\nCoT prompting for graphs to guide graph models to learn step by step? On one\nhand, unlike natural languages, graphs are non-linear and characterized by\ncomplex topological structures. On the other hand, many graphs lack textual\ndata, making it difficult to formulate language-based CoT prompting. In this\nwork, we propose the first CoT prompt learning framework for text-free graphs,\nGCoT. Specifically, we decompose the adaptation process for each downstream\ntask into a series of inference steps, with each step consisting of\nprompt-based inference, ``thought'' generation, and thought-conditioned prompt\nlearning. While the steps mimic CoT prompting in NLP, the exact mechanism\ndiffers significantly. Specifically, at each step, an input graph, along with a\nprompt, is first fed into a pre-trained graph encoder for prompt-based\ninference. We then aggregate the hidden layers of the encoder to construct a\n``thought'', which captures the working state of each node in the current step.\nConditioned on this thought, we learn a prompt specific to each node based on\nthe current state. These prompts are fed into the next inference step,\nrepeating the cycle. To evaluate and analyze the effectiveness of GCoT, we\nconduct comprehensive experiments on eight public datasets, which demonstrate\nthe advantage of our approach.\n","authors":["Xingtong Yu","Chang Zhou","Zhongwei Kuai","Xinming Zhang","Yuan Fang"],"pdf_url":"https://arxiv.org/pdf/2502.08092v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.15633v4","updated":"2025-02-12T03:32:34Z","published":"2024-10-21T04:30:53Z","title":"GATEAU: Selecting Influential Samples for Long Context Alignment","summary":"  Aligning large language models to handle instructions with extremely long\ncontexts has yet to be fully investigated. Previous studies attempt to scale up\nthe available data volume by synthesizing long instruction-following samples,\nas constructing such a dataset tends to be challenging for annotators. However,\na lack of a well-defined strategy for ensuring data quality may introduce\nlow-quality samples and restrict the model performance. Thus, we propose\nGATEAU, a novel framework to address the unique challenge of long context\nalignment by identifying the influential samples enriched with long-range\ndependency relations. Specifically, GATEAU measures the long-range dependencies\nfrom two essential aspects: the difficulty of generating target responses due\nto the long-range dependencies, and the difficulty of understanding long inputs\ndue to such dependencies. Comprehensive experiments indicate that GATEAU\neffectively identifies influential samples and the model trained on these\nselected samples exhibits better instruction-following and long-context\nunderstanding capabilities.\n","authors":["Shuzheng Si","Haozhe Zhao","Gang Chen","Yunshui Li","Kangyang Luo","Chuancheng Lv","Kaikai An","Fanchao Qi","Baobao Chang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.15633v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07677v2","updated":"2025-02-12T03:31:45Z","published":"2025-02-11T16:27:28Z","title":"Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered\n  LLM Approach","summary":"  Achieving a delicate balance between fostering trust in law en- forcement and\nprotecting the rights of both officers and civilians continues to emerge as a\npressing research and product challenge in the world today. In the pursuit of\nfairness and transparency, this study presents an innovative AI-driven system\ndesigned to generate police report drafts from complex, noisy, and multi-role\ndialogue data. Our approach intelligently extracts key elements of law\nenforcement interactions and includes them in the draft, producing structured\nnarratives that are not only high in quality but also reinforce accountability\nand procedural clarity. This frame- work holds the potential to transform the\nreporting process, ensur- ing greater oversight, consistency, and fairness in\nfuture policing practices. A demonstration video of our system can be accessed\nat\nhttps://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-Y-kpCHNO/view?usp=sharing\n","authors":["Param Kulkarni","Yingchi Liu","Hao-Ming Fu","Shaohua Yang","Isuru Gunasekara","Matt Peloquin","Noah Spitzer-Williams","Xiaotian Zhou","Xiaozhong Liu","Zhengping Ji","Yasser Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2502.07677v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09150v3","updated":"2025-02-12T03:00:20Z","published":"2024-08-17T09:49:40Z","title":"CogLM: Tracking Cognitive Development of Large Language Models","summary":"  Piaget's Theory of Cognitive Development (PTC) posits that the development of\ncognitive levels forms the foundation for human learning across various\nabilities. As Large Language Models (LLMs) have recently shown remarkable\nabilities across a wide variety of tasks, we are curious about the cognitive\nlevels of current LLMs: to what extent they have developed and how this\ndevelopment has been achieved. To this end, we construct a benchmark CogLM\n(Cognitive Ability Evaluation for Language Model) based on PTC to assess the\ncognitive levels of LLMs. CogLM comprises 1,220 questions spanning 10 cognitive\nabilities crafted by more than 20 human experts, providing a comprehensive\ntestbed for the cognitive levels of LLMs. Through extensive experiments across\nmultiple mainstream LLMs with CogLM, we find that: (1) In our testing\nframework, advanced LLMs (such as GPT-4) have demonstrated human-like cognitive\nabilities, comparable to those of a 20-year-old human. (2) The parameter size\nand optimization objective are two key factors affecting the cognitive levels\nof LLMs. (3) The performance on downstream tasks is positively correlated with\nthe level of cognitive abilities. These findings fill the gap in research on\nthe cognitive abilities of LLMs, tracing the development of LLMs from a\ncognitive perspective and guiding the future direction of their evolution.\n","authors":["Xinglin Wang","Peiwen Yuan","Shaoxiong Feng","Yiwei Li","Boyuan Pan","Heda Wang","Yao Hu","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2408.09150v3.pdf","comment":"NAACL2025 Main"},{"id":"http://arxiv.org/abs/2502.08080v1","updated":"2025-02-12T02:54:12Z","published":"2025-02-12T02:54:12Z","title":"NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals","summary":"  Decomposition of text into atomic propositions is a flexible framework\nallowing for the closer inspection of input and output text. We use atomic\ndecomposition of hypotheses in two natural language reasoning tasks,\ntraditional NLI and defeasible NLI, to form atomic sub-problems, or granular\ninferences that models must weigh when solving the overall problem. These\natomic sub-problems serve as a tool to further understand the structure of both\nNLI and defeasible reasoning, probe a model's consistency and understanding of\ndifferent inferences, and measure the diversity of examples in benchmark\ndatasets. Our results indicate that LLMs still struggle with logical\nconsistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify\ncritical atomic sub-problems of defeasible NLI examples, or those that most\ncontribute to the overall label, and propose a method to measure the\ninferential consistency of a model, a metric designed to capture the degree to\nwhich a model makes consistently correct or incorrect predictions about the\nsame fact under different contexts.\n","authors":["Neha Srikanth","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2502.08080v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2408.13457v3","updated":"2025-02-12T02:52:25Z","published":"2024-08-24T04:03:35Z","title":"Make Every Penny Count: Difficulty-Adaptive Self-Consistency for\n  Cost-Efficient Reasoning","summary":"  Self-consistency (SC), a widely used decoding strategy for chain-of-thought\nreasoning, shows significant gains across various multi-step reasoning tasks\nbut comes with a high cost due to multiple sampling with the preset size. Its\nvariants, Adaptive self-consistency (ASC) and Early-stopping self-consistency\n(ESC), dynamically adjust the number of samples based on the posterior\ndistribution of a set of pre-samples, reducing the cost of SC with minimal\nimpact on performance. Both methods, however, do not exploit the prior\ninformation about question difficulty. It often results in unnecessary repeated\nsampling for easy questions that could be accurately answered with just one\nattempt, wasting resources. To tackle this problem, we propose\nDifficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty\ninformation of batch queries from both prior and posterior perspectives to\nadaptively allocate inference resources, further reducing the overall cost of\nSC. To demonstrate the effectiveness of DSC, we conduct extensive experiments\non three popular categories of reasoning tasks: arithmetic, commonsense and\nsymbolic reasoning on six benchmarks. The empirical results show that DSC\nconsistently surpasses the strong baseline ASC and ESC in terms of costs by a\nsignificant margin, while attaining comparable performances.\n","authors":["Xinglin Wang","Shaoxiong Feng","Yiwei Li","Peiwen Yuan","Yueqi Zhang","Chuyi Tan","Boyuan Pan","Yao Hu","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2408.13457v3.pdf","comment":"NAACL2025 Findings"},{"id":"http://arxiv.org/abs/2406.10281v2","updated":"2025-02-12T02:11:10Z","published":"2024-06-12T05:13:09Z","title":"Watermarking Language Models with Error Correcting Codes","summary":"  Recent progress in large language models enables the creation of realistic\nmachine-generated content. Watermarking is a promising approach to distinguish\nmachine-generated text from human text, embedding statistical signals in the\noutput that are ideally undetectable to humans. We propose a watermarking\nframework that encodes such signals through an error correcting code. Our\nmethod, termed robust binary code (RBC) watermark, introduces no distortion\ncompared to the original probability distribution, and no noticeable\ndegradation in quality. We evaluate our watermark on base and instruction\nfine-tuned models and find our watermark is robust to edits, deletions, and\ntranslations. We provide an information-theoretic perspective on watermarking,\na powerful statistical test for detection and for generating p-values, and\ntheoretical guarantees. Our empirical findings suggest our watermark is fast,\npowerful, and robust, comparing favorably to the state-of-the-art.\n","authors":["Patrick Chao","Yan Sun","Edgar Dobriban","Hamed Hassani"],"pdf_url":"https://arxiv.org/pdf/2406.10281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15512v2","updated":"2025-02-12T02:06:35Z","published":"2024-10-20T21:17:49Z","title":"Reverse Question Answering: Can an LLM Write a Question so Hard (or Bad)\n  that it Can't Answer?","summary":"  Question answering (QA), giving correct answers to questions, is a popular\ntask, but we test reverse question answering (RQA): for an input answer, give a\nquestion with that answer. Past work tests QA and RQA separately, but we test\nthem jointly, comparing their difficulty, aiding benchmark design, and checking\nreasoning consistency. We run 16 LLMs on QA and RQA with trivia\nquestions/answers, revealing: 1) Versus QA, LLMs are much less accurate in RQA\nfor numerical answers, but slightly more accurate in RQA for textual answers;\n2) LLMs often answer their own invalid questions from RQA accurately in QA, so\nRQA errors are not from knowledge gaps alone; 3) RQA errors correlate with\nquestion difficulty and inversely correlate with answer frequencies in the\nDolma corpus; and 4) LLMs struggle to provide valid multi-hop questions. By\nfinding question and answer types that lead to RQA errors, we suggest\nimprovements for LLM reasoning.\n","authors":["Nishant Balepur","Feng Gu","Abhilasha Ravichander","Shi Feng","Jordan Boyd-Graber","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2410.15512v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.08059v1","updated":"2025-02-12T01:54:21Z","published":"2025-02-12T01:54:21Z","title":"On Mechanistic Circuits for Extractive Question-Answering","summary":"  Large language models are increasingly used to process documents and\nfacilitate question-answering on them. In our paper, we extract mechanistic\ncircuits for this real-world language modeling task: context-augmented language\nmodeling for extractive question-answering (QA) tasks and understand the\npotential benefits of circuits towards downstream applications such as data\nattribution to context information. We extract circuits as a function of\ninternal model components (e.g., attention heads, MLPs) using causal mediation\nanalysis techniques. Leveraging the extracted circuits, we first understand the\ninterplay between the model's usage of parametric memory and retrieved context\ntowards a better mechanistic understanding of context-augmented language\nmodels. We then identify a small set of attention heads in our circuit which\nperforms reliable data attribution by default, thereby obtaining attribution\nfor free in just the model's forward pass. Using this insight, we then\nintroduce ATTNATTRIB, a fast data attribution algorithm which obtains\nstate-of-the-art attribution results across various extractive QA benchmarks.\nFinally, we show the possibility to steer the language model towards answering\nfrom the context, instead of the parametric memory by using the attribution\nfrom ATTNATTRIB as an additional signal during the forward pass. Beyond\nmechanistic understanding, our paper provides tangible applications of circuits\nin the form of reliable data attribution and model steering.\n","authors":["Samyadeep Basu","Vlad Morariu","Zichao Wang","Ryan Rossi","Cherry Zhao","Soheil Feizi","Varun Manjunatha"],"pdf_url":"https://arxiv.org/pdf/2502.08059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08045v1","updated":"2025-02-12T01:04:13Z","published":"2025-02-12T01:04:13Z","title":"Break the Checkbox: Challenging Closed-Style Evaluations of Cultural\n  Alignment in LLMs","summary":"  A large number of studies rely on closed-style multiple-choice surveys to\nevaluate cultural alignment in Large Language Models (LLMs). In this work, we\nchallenge this constrained evaluation paradigm and explore more realistic,\nunconstrained approaches. Using the World Values Survey (WVS) and Hofstede\nCultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger\ncultural alignment in less constrained settings, where responses are not\nforced. Additionally, we show that even minor changes, such as reordering\nsurvey choices, lead to inconsistent outputs, exposing the limitations of\nclosed-style evaluations. Our findings advocate for more robust and flexible\nevaluation frameworks that focus on specific cultural proxies, encouraging more\nnuanced and accurate assessments of cultural alignment in LLMs.\n","authors":["Mohsinul Kabir","Ajwad Abrar","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2502.08045v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.08037v1","updated":"2025-02-12T00:38:11Z","published":"2025-02-12T00:38:11Z","title":"Franken-Adapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery","summary":"  The capabilities of Large Language Models (LLMs) in low-resource languages\nlag far behind those in English, making their universal accessibility a\nsignificant challenge. To alleviate this, we present\n$\\textit{Franken-Adapter}$, a modular language adaptation approach for\ndecoder-only LLMs with embedding surgery. Our method begins by creating\ncustomized vocabularies for target languages and performing language adaptation\nthrough embedding tuning on multilingual data. These pre-trained embeddings are\nsubsequently integrated with LLMs that have been instruction-tuned on English\nalignment data to enable zero-shot cross-lingual transfer. Our experiments on\n$\\texttt{Gemma2}$ models with up to 27B parameters demonstrate improvements of\nup to 20% across 96 languages, spanning both discriminative and generative\ntasks, with minimal regressions ($<$1%) in English. Further in-depth analysis\nreveals the critical role of customizing tokenizers in enhancing language\nadaptation, while boosting inference efficiency. Additionally, we show the\nversatility of our method by achieving a 14% improvement over a math-optimized\nLLM across 20 languages, offering a modular solution to transfer reasoning\nabilities across languages post hoc.\n","authors":["Fan Jiang","Honglin Yu","Grace Chung","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2502.08037v1.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2502.08026v1","updated":"2025-02-12T00:00:37Z","published":"2025-02-12T00:00:37Z","title":"Contextual Subspace Manifold Projection for Structural Refinement of\n  Large Language Model Representations","summary":"  Internal representations within deep neural architectures encode\nhigh-dimensional abstractions of linguistic structures, yet they often exhibit\ninefficiencies in feature distribution, limiting expressiveness and\nadaptability. Contextual Subspace Manifold Projection introduces a structured\nrefinement technique that selectively reconfigures token embeddings through\ncontrolled subspace constraints, ensuring more stable and geometrically\nwell-defined feature distributions. Empirical evaluations demonstrated that the\nstructured intervention reduced anisotropy, leading to improved representation\ncompactness while preserving semantic fidelity across transformer layers.\nClustering analyses indicated that token embeddings exhibited greater feature\nseparability, reinforcing the hypothesis that structured projection techniques\nenhance internal representation organization without sacrificing linguistic\ncoherence. Gradient magnitude distributions suggested that the method\nintroduced a smoother optimization trajectory, potentially contributing to more\nstable parameter updates throughout training. Computational overhead associated\nwith the projection operations remained minimal, ensuring that the refinements\ndid not introduce significant trade-offs in model efficiency or inference\nspeed. Comparisons with standard embedding refinement techniques highlighted\nthat structured manifold constraints provided a direct mechanism for improving\nrepresentation quality without requiring additional gradient-based\noptimization. Perplexity evaluations confirmed that the adjustments did not\nnegatively impact sequence coherence, further validating the effectiveness of\nthe proposed approach.\n","authors":["Alistair Wren","Beatrice Loxley","Hamish Cadwallader","Simon Beckwith","Fabian Pargeter","James Blades"],"pdf_url":"https://arxiv.org/pdf/2502.08026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06453v2","updated":"2025-02-12T23:16:27Z","published":"2025-02-10T13:31:46Z","title":"MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard\n  Perturbations","summary":"  Large language models have demonstrated impressive performance on challenging\nmathematical reasoning tasks, which has triggered the discussion of whether the\nperformance is achieved by true reasoning capability or memorization. To\ninvestigate this question, prior work has constructed mathematical benchmarks\nwhen questions undergo simple perturbations -- modifications that still\npreserve the underlying reasoning patterns of the solutions. However, no work\nhas explored hard perturbations, which fundamentally change the nature of the\nproblem so that the original solution steps do not apply. To bridge the gap, we\nconstruct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard\nperturbation, respectively. Each consists of 279 perturbed math problems\nderived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.\nal., 2021). We observe significant performance drops on MATH-P-Hard across\nvarious models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking\n(-12.9%). We also raise concerns about a novel form of memorization where\nmodels blindly apply learned problem-solving skills without assessing their\napplicability to modified contexts. This issue is amplified when using original\nproblems for in-context learning. We call for research efforts to address this\nchallenge, which is critical for developing more robust and reliable reasoning\nmodels.\n","authors":["Kaixuan Huang","Jiacheng Guo","Zihao Li","Xiang Ji","Jiawei Ge","Wenzhe Li","Yingqing Guo","Tianle Cai","Hui Yuan","Runzhe Wang","Yue Wu","Ming Yin","Shange Tang","Yangsibo Huang","Chi Jin","Xinyun Chen","Chiyuan Zhang","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06453v2.pdf","comment":"v2: fix bugs in Fig. 1"},{"id":"http://arxiv.org/abs/2406.11278v3","updated":"2025-02-12T23:08:21Z","published":"2024-06-17T07:30:40Z","title":"Do Not Design, Learn: A Trainable Scoring Function for Uncertainty\n  Estimation in Generative LLMs","summary":"  Uncertainty estimation (UE) of generative large language models (LLMs) is\ncrucial for evaluating the reliability of generated sequences. A significant\nsubset of UE methods utilize token probabilities to assess uncertainty,\naggregating multiple token probabilities into a single UE score using a scoring\nfunction. Existing scoring functions for probability-based UE, such as\nlength-normalized scoring and semantic contribution-based weighting, are\ndesigned to solve certain aspects of the problem but exhibit limitations,\nincluding the inability to handle biased probabilities and complex semantic\ndependencies between tokens. To address these issues, in this work, we propose\nLearnable Response Scoring (LARS) function, a novel scoring function that\nleverages supervised data to capture complex dependencies between tokens and\nprobabilities, thereby producing more reliable and calibrated response scores\nin computing the uncertainty of LLM generations. Our comprehensive experiments\nacross question-answering and arithmetical reasoning tasks with various\ndatasets demonstrate that LARS significantly outperforms existing scoring\nfunctions, achieving improvements of up to 16\\% AUROC score.\n","authors":["Duygu Nur Yaldiz","Yavuz Faruk Bakman","Baturalp Buyukates","Chenyang Tao","Anil Ramakrishna","Dimitrios Dimitriadis","Jieyu Zhao","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2406.11278v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08826v1","updated":"2025-02-12T22:33:41Z","published":"2025-02-12T22:33:41Z","title":"Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation","summary":"  Large Language Models (LLMs) struggle with hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation enhancing factual and updated grounding. Recent advances in\nmultimodal learning have led to the development of Multimodal RAG,\nincorporating multiple modalities such as text, images, audio, and video to\nenhance the generated outputs. However, cross-modal alignment and reasoning\nintroduce unique challenges to Multimodal RAG, distinguishing it from\ntraditional unimodal RAG. This survey offers a structured and comprehensive\nanalysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,\nevaluation, methodologies, and innovations in retrieval, fusion, augmentation,\nand generation. We precisely review training strategies, robustness\nenhancements, and loss functions, while also exploring the diverse Multimodal\nRAG scenarios. Furthermore, we discuss open challenges and future research\ndirections to support advancements in this evolving field. This survey lays the\nfoundation for developing more capable and reliable AI systems that effectively\nleverage multimodal dynamic external knowledge bases. Resources are available\nat https://github.com/llm-lab-org/Multimodal-RAG-Survey.\n","authors":["Mohammad Mahdi Abootorabi","Amirhosein Zobeiri","Mahdi Dehghani","Mohammadali Mohammadkhani","Bardia Mohammadi","Omid Ghahroodi","Mahdieh Soleymani Baghshah","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2502.08826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08825v1","updated":"2025-02-12T22:30:18Z","published":"2025-02-12T22:30:18Z","title":"Examining and Adapting Time for Multilingual Classification via Mixture\n  of Temporal Experts","summary":"  Time is implicitly embedded in classification process: classifiers are\nusually built on existing data while to be applied on future data whose\ndistributions (e.g., label and token) may change. However, existing\nstate-of-the-art classification models merely consider the temporal variations\nand primarily focus on English corpora, which leaves temporal studies less\nexplored, let alone under multilingual settings. In this study, we fill the gap\nby treating time as domains (e.g., 2024 vs. 2025), examining temporal effects,\nand developing a domain adaptation framework to generalize classifiers over\ntime on multiple languages. Our framework proposes Mixture of Temporal Experts\n(MoTE) to leverage both semantic and data distributional shifts to learn and\nadapt temporal trends into classification models. Our analysis shows\nclassification performance varies over time across different languages, and we\nexperimentally demonstrate that MoTE can enhance classifier generalizability\nover temporal data shifts. Our study provides analytic insights and addresses\nthe need for time-aware models that perform robustly in multilingual scenarios.\n","authors":["Weisi Liu","Guangzeng Han","Xiaolei Huang"],"pdf_url":"https://arxiv.org/pdf/2502.08825v1.pdf","comment":"accept to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.04397v2","updated":"2025-02-12T22:26:50Z","published":"2025-02-06T06:58:09Z","title":"Multimodal Medical Code Tokenizer","summary":"  Foundation models trained on patient electronic health records (EHRs) require\ntokenizing medical data into sequences of discrete vocabulary items. Existing\ntokenizers treat medical codes from EHRs as isolated textual tokens. However,\neach medical code is defined by its textual description, its position in\nontological hierarchies, and its relationships to other codes, such as disease\nco-occurrences and drug-treatment associations. Medical vocabularies contain\nmore than 600,000 codes with critical information for clinical reasoning. We\nintroduce MedTok, a multimodal medical code tokenizer that uses the text\ndescriptions and relational context of codes. MedTok processes text using a\nlanguage model encoder and encodes the relational structure with a graph\nencoder. It then quantizes both modalities into a unified token space,\npreserving modality-specific and cross-modality information. We integrate\nMedTok into five EHR models and evaluate it on operational and clinical tasks\nacross in-patient and out-patient datasets, including outcome prediction,\ndiagnosis classification, drug recommendation, and risk stratification.\nSwapping standard EHR tokenizers with MedTok improves AUPRC across all EHR\nmodels, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.30% on EHRShot, with\nthe largest gains in drug recommendation. Beyond EHR modeling, we demonstrate\nusing MedTok tokenizer with medical QA systems. Our results demonstrate the\npotential of MedTok as a unified tokenizer for medical codes, improving\ntokenization for medical foundation models.\n","authors":["Xiaorui Su","Shvat Messica","Yepeng Huang","Ruth Johnson","Lukas Fesser","Shanghua Gao","Faryad Sahneh","Marinka Zitnik"],"pdf_url":"https://arxiv.org/pdf/2502.04397v2.pdf","comment":"conference"},{"id":"http://arxiv.org/abs/2502.08820v1","updated":"2025-02-12T22:18:34Z","published":"2025-02-12T22:18:34Z","title":"Can a Single Model Master Both Multi-turn Conversations and Tool Use?\n  CALM: A Unified Conversational Agentic Language Model","summary":"  Large Language Models (LLMs) with API-calling capabilities enabled building\neffective Language Agents (LA), while also revolutionizing the conventional\ntask-oriented dialogue (TOD) paradigm. However, current approaches face a\ncritical dilemma: TOD systems are often trained on a limited set of target\nAPIs, requiring new data to maintain their quality when interfacing with new\nservices, while LAs are not trained to maintain user intent over multi-turn\nconversations. Because both robust multi-turn management and advanced function\ncalling are crucial for effective conversational agents, we evaluate these\nskills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and\nAPI-Bank (LA), and our analyses reveal that specialized approaches excel in one\ndomain but underperform in the other. To bridge this chasm, we introduce CALM\n(Conversational Agentic Language Model), a unified approach that integrates\nboth conversational and agentic capabilities. We created CALM-IT, a carefully\nconstructed multi-task dataset that interleave multi-turn ReAct reasoning with\ncomplex API usage. Using CALM-IT, we train three models CALM 8B, CALM 70B, and\nCALM 405B, which outperform top domain-specific models, including GPT-4o,\nacross all three benchmarks.\n","authors":["Emre Can Acikgoz","Jeremiah Greer","Akul Datta","Ze Yang","William Zeng","Oussama Elachqar","Emmanouil Koukoumidis","Dilek Hakkani-Tür","Gokhan Tur"],"pdf_url":"https://arxiv.org/pdf/2502.08820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08818v1","updated":"2025-02-12T22:11:07Z","published":"2025-02-12T22:11:07Z","title":"Lexical Manifold Reconfiguration in Large Language Models: A Novel\n  Architectural Approach for Contextual Modulation","summary":"  Contextual adaptation in token embeddings plays a central role in determining\nhow well language models maintain coherence and retain semantic relationships\nover extended text sequences. Static embeddings often impose constraints on\nlexical flexibility, leading to suboptimal performance when faced with complex\nsentence structures or domain-specific terminology shifts. To address this\nlimitation, a structured approach was developed for dynamically reconfiguring\ntoken embeddings through continuous geometric transformations, ensuring that\nrepresentations evolved in response to evolving discourse structures. A\nmanifold-based transformation mechanism was integrated to regulate lexical\npositioning, allowing embeddings to undergo controlled shifts while preserving\nlinguistic relationships across varying textual contexts. Empirical evaluations\ndemonstrated that embedding reconfiguration contributed to reductions in\nperplexity, improved lexical coherence, and enhanced sentence-level continuity,\nparticularly in structured and domain-adaptive text generation tasks.\nComparative analyses of embedding drift indicated that dynamically restructured\nrepresentations maintained stronger contextual consistency, reducing\nmisalignment in token dependencies while preserving fluency in language\nmodeling outputs. Computational overhead assessments confirmed that while\ntraining complexity increased due to the iterative refinement of embeddings,\ninference remained efficient, ensuring practical feasibility for real-time\ngeneration. Evaluations across multiple datasets further demonstrated that\ndynamically modulated embeddings exhibited broader lexical diversity, reducing\nrepetitive token patterns and enabling a more adaptable representation learning\nprocess.\n","authors":["Koinis Vassilis","Godfrey Milbourne","Harriet Featherstone","Xanthe Peverell","Yorick Bletchley","Zachary Montford"],"pdf_url":"https://arxiv.org/pdf/2502.08818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09298v4","updated":"2025-02-12T21:57:06Z","published":"2024-07-12T14:31:05Z","title":"Transformer Layers as Painters","summary":"  Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel.\n","authors":["Qi Sun","Marc Pickett","Aakash Kumar Nain","Llion Jones"],"pdf_url":"https://arxiv.org/pdf/2407.09298v4.pdf","comment":"13 pages total, including references and appendices"},{"id":"http://arxiv.org/abs/2407.12101v2","updated":"2025-02-12T21:48:22Z","published":"2024-07-16T18:09:21Z","title":"Better RAG using Relevant Information Gain","summary":"  A common way to extend the memory of large language models (LLMs) is by\nretrieval augmented generation (RAG), which inserts text retrieved from a\nlarger memory into an LLM's context window. However, the context window is\ntypically limited to several thousand tokens, which limits the number of\nretrieved passages that can inform a model's response. For this reason, it's\nimportant to avoid occupying context window space with redundant information by\nensuring a degree of diversity among retrieved passages. At the same time, the\ninformation should also be relevant to the current task. Most prior methods\nthat encourage diversity among retrieved results, such as Maximal Marginal\nRelevance (MMR), do so by incorporating an objective that explicitly trades off\ndiversity and relevance. We propose a novel simple optimization metric based on\nrelevant information gain, a probabilistic measure of the total information\nrelevant to a query for a set of retrieved results. By optimizing this metric,\ndiversity organically emerges from our system. When used as a drop-in\nreplacement for the retrieval component of a RAG system, this method yields\nstate-of-the-art performance on question answering tasks from the Retrieval\nAugmented Generation Benchmark (RGB), outperforming existing metrics that\ndirectly optimize for relevance and diversity.\n","authors":["Marc Pickett","Jeremy Hartman","Ayan Kumar Bhowmick","Raquib-ul Alam","Aditya Vempaty"],"pdf_url":"https://arxiv.org/pdf/2407.12101v2.pdf","comment":"4 page paper submitted to EMNLP"},{"id":"http://arxiv.org/abs/2502.08796v1","updated":"2025-02-12T21:19:30Z","published":"2025-02-12T21:19:30Z","title":"A Systematic Review on the Evaluation of Large Language Models in Theory\n  of Mind Tasks","summary":"  In recent years, evaluating the Theory of Mind (ToM) capabilities of large\nlanguage models (LLMs) has received significant attention within the research\ncommunity. As the field rapidly evolves, navigating the diverse approaches and\nmethodologies has become increasingly complex. This systematic review\nsynthesizes current efforts to assess LLMs' ability to perform ToM tasks, an\nessential aspect of human cognition involving the attribution of mental states\nto oneself and others. Despite notable advancements, the proficiency of LLMs in\nToM remains a contentious issue. By categorizing benchmarks and tasks through a\ntaxonomy rooted in cognitive science, this review critically examines\nevaluation techniques, prompting strategies, and the inherent limitations of\nLLMs in replicating human-like mental state reasoning. A recurring theme in the\nliterature reveals that while LLMs demonstrate emerging competence in ToM\ntasks, significant gaps persist in their emulation of human cognitive\nabilities.\n","authors":["Karahan Sarıtaş","Kıvanç Tezören","Yavuz Durmazkeser"],"pdf_url":"https://arxiv.org/pdf/2502.08796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08788v1","updated":"2025-02-12T21:01:10Z","published":"2025-02-12T21:01:10Z","title":"If Multi-Agent Debate is the Answer, What is the Question?","summary":"  Multi-agent debate (MAD) has emerged as a promising approach to enhance the\nfactual accuracy and reasoning quality of large language models (LLMs) by\nengaging multiple agents in iterative discussions during inference. Despite its\npotential, we argue that current MAD research suffers from critical\nshortcomings in evaluation practices, including limited dataset overlap and\ninconsistent baselines, raising significant concerns about generalizability.\nCorrespondingly, this paper presents a systematic evaluation of five\nrepresentative MAD methods across nine benchmarks using four foundational\nmodels. Surprisingly, our findings reveal that MAD methods fail to reliably\noutperform simple single-agent baselines such as Chain-of-Thought and\nSelf-Consistency, even when consuming additional inference-time computation.\nFrom our analysis, we found that model heterogeneity can significantly improve\nMAD frameworks. We propose Heter-MAD enabling a single LLM agent to access the\noutput from heterogeneous foundation models, which boosts the performance of\ncurrent MAD frameworks. Finally, we outline potential directions for advancing\nMAD, aiming to spark a broader conversation and inspire future work in this\narea.\n","authors":["Hangfan Zhang","Zhiyao Cui","Xinrun Wang","Qiaosheng Zhang","Zhen Wang","Dinghao Wu","Shuyue Hu"],"pdf_url":"https://arxiv.org/pdf/2502.08788v1.pdf","comment":"This position paper takes a critical view of the status quo of MAD\n  research, and outline multiple potential directions to improve MAD"},{"id":"http://arxiv.org/abs/2502.08777v1","updated":"2025-02-12T20:39:01Z","published":"2025-02-12T20:39:01Z","title":"Zero-Shot Belief: A Hard Problem for LLMs","summary":"  We present two LLM-based approaches to zero-shot source-and-target belief\nprediction on FactBank: a unified system that identifies events, sources, and\nbelief labels in a single pass, and a hybrid approach that uses a fine-tuned\nDeBERTa tagger for event detection. We show that multiple open-sourced,\nclosed-source, and reasoning-based LLMs struggle with the task. Using the\nhybrid approach, we achieve new state-of-the-art results on FactBank and offer\na detailed error analysis. Our approach is then tested on the Italian belief\ncorpus ModaFact.\n","authors":["John Murzaku","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2502.08777v1.pdf","comment":"Submitted to ACL 2025"},{"id":"http://arxiv.org/abs/2502.08773v1","updated":"2025-02-12T20:30:28Z","published":"2025-02-12T20:30:28Z","title":"Universal Model Routing for Efficient LLM Inference","summary":"  Large language models' significant advances in capabilities are accompanied\nby significant increases in inference costs. Model routing is a simple\ntechnique for reducing inference cost, wherein one maintains a pool of\ncandidate LLMs, and learns to route each prompt to the smallest feasible LLM.\nExisting works focus on learning a router for a fixed pool of LLMs. In this\npaper, we consider the problem of dynamic routing, where new, previously\nunobserved LLMs are available at test time. We propose a new approach to this\nproblem that relies on representing each LLM as a feature vector, derived based\non predictions on a set of representative prompts. Based on this, we detail two\neffective strategies, relying on cluster-based routing and a learned cluster\nmap respectively. We prove that these strategies are estimates of a\ntheoretically optimal routing rule, and provide an excess risk bound to\nquantify their errors. Experiments on a range of public benchmarks show the\neffectiveness of the proposed strategies in routing amongst more than 30 unseen\nLLMs.\n","authors":["Wittawat Jitkrittum","Harikrishna Narasimhan","Ankit Singh Rawat","Jeevesh Juneja","Zifeng Wang","Chen-Yu Lee","Pradeep Shenoy","Rina Panigrahy","Aditya Krishna Menon","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.08773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02362v3","updated":"2025-02-12T20:21:55Z","published":"2025-02-04T14:44:58Z","title":"Premise-Augmented Reasoning Chains Improve Error Identification in Math\n  reasoning with LLMs","summary":"  Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large\nlanguage models (LLMs) by enabling detailed step-by-step solutions. However,\ndue to the verbosity of LLMs, the resulting reasoning chains can be long,\nmaking it harder to verify the reasoning steps and trace issues resulting from\ndependencies between the steps that may be farther away in the sequence of\nsteps. Importantly, mathematical reasoning allows each step to be derived from\na small set of premises, which are a subset of the preceding steps in the\nreasoning chain. In this paper, we present a framework that identifies the\npremises for each step, to improve the evaluation of reasoning. We restructure\nconventional linear reasoning chains into Premise Augmented Reasoning Chains\n(PARC) by introducing premise links, resulting in a directed acyclic graph\nwhere the nodes are the steps and the edges are the premise links. Through\nexperiments with a PARC-based dataset that we built, namely PERL (Premises and\nERrors identification in LLMs), we demonstrate that LLMs can reliably identify\npremises within complex reasoning chains. In particular, even open-source LLMs\nachieve 90% recall in premise identification. We also show that PARC helps to\nidentify errors in reasoning chains more reliably. The accuracy of error\nidentification improves by 6% to 16% absolute when step-by-step verification is\ncarried out in PARC under the premises. Our findings highlight the utility of\npremise-centric representations in addressing complex problem-solving tasks and\nopen new avenues for improving the reliability of LLM-based reasoning\nevaluations.\n","authors":["Sagnik Mukherjee","Abhinav Chinta","Takyoung Kim","Tarun Anoop Sharma","Dilek Hakkani-Tür"],"pdf_url":"https://arxiv.org/pdf/2502.02362v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08767v1","updated":"2025-02-12T20:13:56Z","published":"2025-02-12T20:13:56Z","title":"SelfElicit: Your Language Model Secretly Knows Where is the Relevant\n  Evidence","summary":"  Providing Language Models (LMs) with relevant evidence in the context (either\nvia retrieval or user-provided) can significantly improve their ability to\nprovide factually correct grounded responses. However, recent studies have\nfound that LMs often struggle to fully comprehend and utilize key evidence from\nthe context, especially when it contains noise and irrelevant information - an\nissue common in real-world scenarios. To address this, we propose SelfElicit,\nan inference-time approach that helps LMs focus on key contextual evidence\nthrough self-guided explicit highlighting. By leveraging the inherent\nevidence-finding capabilities of LMs using the attention scores of deeper\nlayers, our method automatically identifies and emphasizes key evidence within\nthe input context, facilitating more accurate and factually grounded responses\nwithout additional training or iterative prompting. We demonstrate that\nSelfElicit brings consistent and significant improvement on multiple\nevidence-based QA tasks for various LM families while maintaining computational\nefficiency. Our code and documentation are available at\nhttps://github.com/ZhiningLiu1998/SelfElicit.\n","authors":["Zhining Liu","Rana Ali Amjad","Ravinarayana Adkathimar","Tianxin Wei","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2502.08767v1.pdf","comment":"16 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2502.08745v1","updated":"2025-02-12T19:35:28Z","published":"2025-02-12T19:35:28Z","title":"IHEval: Evaluating Language Models on Following the Instruction\n  Hierarchy","summary":"  The instruction hierarchy, which establishes a priority order from system\nmessages to user messages, conversation history, and tool outputs, is essential\nfor ensuring consistent and safe behavior in language models (LMs). Despite its\nimportance, this topic receives limited attention, and there is a lack of\ncomprehensive benchmarks for evaluating models' ability to follow the\ninstruction hierarchy. We bridge this gap by introducing IHEval, a novel\nbenchmark comprising 3,538 examples across nine tasks, covering cases where\ninstructions in different priorities either align or conflict. Our evaluation\nof popular LMs highlights their struggle to recognize instruction priorities.\nAll evaluated models experience a sharp performance decline when facing\nconflicting instructions, compared to their original instruction-following\nperformance. Moreover, the most competitive open-source model only achieves 48%\naccuracy in resolving such conflicts. Our results underscore the need for\ntargeted optimization in the future development of LMs.\n","authors":["Zhihan Zhang","Shiyang Li","Zixuan Zhang","Xin Liu","Haoming Jiang","Xianfeng Tang","Yifan Gao","Zheng Li","Haodong Wang","Zhaoxuan Tan","Yichuan Li","Qingyu Yin","Bing Yin","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.08745v1.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2502.08744v1","updated":"2025-02-12T19:35:15Z","published":"2025-02-12T19:35:15Z","title":"Are Expressions for Music Emotions the Same Across Cultures?","summary":"  Music evokes profound emotions, yet the universality of emotional descriptors\nacross languages remains debated. A key challenge in cross-cultural research on\nmusic emotion is biased stimulus selection and manual curation of taxonomies,\npredominantly relying on Western music and languages. To address this, we\npropose a balanced experimental design with nine online experiments in Brazil,\nthe US, and South Korea, involving N=672 participants. First, we sample a\nbalanced set of popular music from these countries. Using an open-ended tagging\npipeline, we then gather emotion terms to create culture-specific taxonomies.\nFinally, using these bottom-up taxonomies, participants rate emotions of each\nsong. This allows us to map emotional similarities within and across cultures.\nResults show consistency in high arousal, high valence emotions but greater\nvariability in others. Notably, machine translations were often inadequate to\ncapture music-specific meanings. These findings together highlight the need for\na domain-sensitive, open-ended, bottom-up emotion elicitation approach to\nreduce cultural biases in emotion research.\n","authors":["Elif Celen","Pol van Rijn","Harin Lee","Nori Jacoby"],"pdf_url":"https://arxiv.org/pdf/2502.08744v1.pdf","comment":"Submitted to CogSci"},{"id":"http://arxiv.org/abs/2408.11081v2","updated":"2025-02-12T19:34:51Z","published":"2024-08-20T11:19:06Z","title":"What can Large Language Models Capture about Code Functional\n  Equivalence?","summary":"  Code-LLMs, LLMs pre-trained on large code corpora, have shown great progress\nin learning rich representations of the structure and syntax of code,\nsuccessfully using it to generate or classify code fragments. At the same time,\nunderstanding if they are able to do so because they capture code semantics,\nand how well, is still an open question. In this paper, we tackle this problem\nby introducing SeqCoBench, a benchmark for systematically assessing how\nCode-LLMs can capture code functional equivalence. SeqCoBench contains over 20\ncode transformations that either preserve or alter the semantics of Python\nprograms. We conduct extensive evaluations in different settings, including\nzero-shot and parameter-efficient finetuning methods on state-of-the-art\n(Code)-LLMs to see if they can discern semantically equivalent or different\npairs of programs in SeqCoBench. We find that the performance gap between these\nLLMs and classical match-based retrieval scores is minimal, with both\napproaches showing a concerning lack of depth in understanding code semantics.\n","authors":["Nickil Maveli","Antonio Vergari","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2408.11081v2.pdf","comment":"Accepted to Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2305.03092v3","updated":"2025-02-12T19:30:14Z","published":"2023-05-04T18:15:45Z","title":"Curating corpora with classifiers: A case study of clean energy\n  sentiment online","summary":"  Well curated, large-scale corpora of social media posts containing broad\npublic opinion offer an alternative data source to complement traditional\nsurveys. While surveys are effective at collecting representative samples and\nare capable of achieving high accuracy, they can be both expensive to run and\nlag public opinion by days or weeks. Both of these drawbacks could be overcome\nwith a real-time, high volume data stream and fast analysis pipeline. A central\nchallenge in orchestrating such a data pipeline is devising an effective method\nfor rapidly selecting the best corpus of relevant documents for analysis.\nQuerying with keywords alone often includes irrelevant documents that are not\neasily disambiguated with bag-of-words natural language processing methods.\nHere, we explore methods of corpus curation to filter irrelevant tweets using\npre-trained transformer-based models, fine-tuned for our binary classification\ntask on hand-labeled tweets. We are able to achieve F1 scores of up to 0.95.\nThe low cost and high performance of fine-tuning such a model suggests that our\napproach could be of broad benefit as a pre-processing step for social media\ndatasets with uncertain corpus boundaries.\n","authors":["Michael V. Arnold","Peter Sheridan Dodds","Christopher M. Danforth"],"pdf_url":"https://arxiv.org/pdf/2305.03092v3.pdf","comment":"12 pages, 6 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.08646v1","updated":"2025-02-12T18:59:43Z","published":"2025-02-12T18:59:43Z","title":"Poly-Autoregressive Prediction for Modeling Interactions","summary":"  We introduce a simple framework for predicting the behavior of an agent in\nmulti-agent settings. In contrast to autoregressive (AR) tasks, such as\nlanguage processing, our focus is on scenarios with multiple agents whose\ninteractions are shaped by physical constraints and internal motivations. To\nthis end, we propose Poly-Autoregressive (PAR) modeling, which forecasts an ego\nagent's future behavior by reasoning about the ego agent's state history and\nthe past and current states of other interacting agents. At its core, PAR\nrepresents the behavior of all agents as a sequence of tokens, each\nrepresenting an agent's state at a specific timestep. With minimal data\npre-processing changes, we show that PAR can be applied to three different\nproblems: human action forecasting in social situations, trajectory prediction\nfor autonomous vehicles, and object pose forecasting during hand-object\ninteraction. Using a small proof-of-concept transformer backbone, PAR\noutperforms AR across these three scenarios. The project website can be found\nat https://neerja.me/PAR/.\n","authors":["Neerja Thakkar","Tara Sadjadpour","Jathushan Rajasegaran","Shiry Ginosar","Jitendra Malik"],"pdf_url":"https://arxiv.org/pdf/2502.08646v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2502.08643v1","updated":"2025-02-12T18:57:22Z","published":"2025-02-12T18:57:22Z","title":"A Real-to-Sim-to-Real Approach to Robotic Manipulation with\n  VLM-Generated Iterative Keypoint Rewards","summary":"  Task specification for robotic manipulation in open-world environments is\nchallenging, requiring flexible and adaptive objectives that align with human\nintentions and can evolve through iterative feedback. We introduce Iterative\nKeypoint Reward (IKER), a visually grounded, Python-based reward function that\nserves as a dynamic task specification. Our framework leverages VLMs to\ngenerate and refine these reward functions for multi-step manipulation tasks.\nGiven RGB-D observations and free-form language instructions, we sample\nkeypoints in the scene and generate a reward function conditioned on these\nkeypoints. IKER operates on the spatial relationships between keypoints,\nleveraging commonsense priors about the desired behaviors, and enabling precise\nSE(3) control. We reconstruct real-world scenes in simulation and use the\ngenerated rewards to train reinforcement learning (RL) policies, which are then\ndeployed into the real world-forming a real-to-sim-to-real loop. Our approach\ndemonstrates notable capabilities across diverse scenarios, including both\nprehensile and non-prehensile tasks, showcasing multi-step task execution,\nspontaneous error recovery, and on-the-fly strategy adjustments. The results\nhighlight IKER's effectiveness in enabling robots to perform multi-step tasks\nin dynamic environments through iterative reward shaping.\n","authors":["Shivansh Patel","Xinchen Yin","Wenlong Huang","Shubham Garg","Hooshang Nayyeri","Li Fei-Fei","Svetlana Lazebnik","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2502.08643v1.pdf","comment":"ICRA 2025, Project Page: https://iker-robot.github.io/"},{"id":"http://arxiv.org/abs/2502.08642v1","updated":"2025-02-12T18:57:12Z","published":"2025-02-12T18:57:12Z","title":"SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation","summary":"  Recent advancements in large vision-language models have enabled highly\nexpressive and diverse vector sketch generation. However, state-of-the-art\nmethods rely on a time-consuming optimization process involving repeated\nfeedback from a pretrained model to determine stroke placement. Consequently,\ndespite producing impressive sketches, these methods are limited in practical\napplications. In this work, we introduce SwiftSketch, a diffusion model for\nimage-conditioned vector sketch generation that can produce high-quality\nsketches in less than a second. SwiftSketch operates by progressively denoising\nstroke control points sampled from a Gaussian distribution. Its\ntransformer-decoder architecture is designed to effectively handle the discrete\nnature of vector representation and capture the inherent global dependencies\nbetween strokes. To train SwiftSketch, we construct a synthetic dataset of\nimage-sketch pairs, addressing the limitations of existing sketch datasets,\nwhich are often created by non-artists and lack professional quality. For\ngenerating these synthetic sketches, we introduce ControlSketch, a method that\nenhances SDS-based techniques by incorporating precise spatial control through\na depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across\ndiverse concepts, efficiently producing sketches that combine high fidelity\nwith a natural and visually appealing style.\n","authors":["Ellie Arar","Yarden Frenkel","Daniel Cohen-Or","Ariel Shamir","Yael Vinker"],"pdf_url":"https://arxiv.org/pdf/2502.08642v1.pdf","comment":"https://swiftsketch.github.io/"},{"id":"http://arxiv.org/abs/2502.08640v1","updated":"2025-02-12T18:55:43Z","published":"2025-02-12T18:55:43Z","title":"Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs","summary":"  As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.\n","authors":["Mantas Mazeika","Xuwang Yin","Rishub Tamirisa","Jaehyuk Lim","Bruce W. Lee","Richard Ren","Long Phan","Norman Mu","Adam Khoja","Oliver Zhang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08639v1","updated":"2025-02-12T18:55:36Z","published":"2025-02-12T18:55:36Z","title":"CineMaster: A 3D-Aware and Controllable Framework for Cinematic\n  Text-to-Video Generation","summary":"  In this work, we present CineMaster, a novel framework for 3D-aware and\ncontrollable text-to-video generation. Our goal is to empower users with\ncomparable controllability as professional film directors: precise placement of\nobjects within the scene, flexible manipulation of both objects and camera in\n3D space, and intuitive layout control over the rendered frames. To achieve\nthis, CineMaster operates in two stages. In the first stage, we design an\ninteractive workflow that allows users to intuitively construct 3D-aware\nconditional signals by positioning object bounding boxes and defining camera\nmovements within the 3D space. In the second stage, these control\nsignals--comprising rendered depth maps, camera trajectories and object class\nlabels--serve as the guidance for a text-to-video diffusion model, ensuring to\ngenerate the user-intended video content. Furthermore, to overcome the scarcity\nof in-the-wild datasets with 3D object motion and camera pose annotations, we\ncarefully establish an automated data annotation pipeline that extracts 3D\nbounding boxes and camera trajectories from large-scale video data. Extensive\nqualitative and quantitative experiments demonstrate that CineMaster\nsignificantly outperforms existing methods and implements prominent 3D-aware\ntext-to-video generation. Project page: https://cinemaster-dev.github.io/.\n","authors":["Qinghe Wang","Yawen Luo","Xiaoyu Shi","Xu Jia","Huchuan Lu","Tianfan Xue","Xintao Wang","Pengfei Wan","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2502.08639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08636v1","updated":"2025-02-12T18:53:20Z","published":"2025-02-12T18:53:20Z","title":"PulseCheck457: A Diagnostic Benchmark for Comprehensive Spatial\n  Reasoning of Large Multimodal Models","summary":"  Although large multimodal models (LMMs) have demonstrated remarkable\ncapabilities in visual scene interpretation and reasoning, their capacity for\ncomplex and precise 3-dimensional spatial reasoning remains uncertain. Existing\nbenchmarks focus predominantly on 2D spatial understanding and lack a framework\nto comprehensively evaluate 6D spatial reasoning across varying complexities.\nTo address this limitation, we present PulseCheck457, a scalable and unbiased\nsynthetic dataset designed with 4 key capability for spatial reasoning:\nmulti-object recognition, 2D location, 3D location, and 3D orientation. We\ndevelop a cascading evaluation structure, constructing 7 question types across\n5 difficulty levels that range from basic single object recognition to our new\nproposed complex 6D spatial reasoning tasks. We evaluated various large\nmultimodal models (LMMs) on PulseCheck457, observing a general decline in\nperformance as task complexity increases, particularly in 3D reasoning and 6D\nspatial tasks. To quantify these challenges, we introduce the Relative\nPerformance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning\ncapabilities. Leveraging the unbiased attribute design of our dataset, we also\nuncover prediction biases across different attributes, with similar patterns\nobserved in real-world image settings.\n","authors":["Xingrui Wang","Wufei Ma","Tiezheng Zhang","Celso M de Melo","Jieneng Chen","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2502.08636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08634v1","updated":"2025-02-12T18:48:12Z","published":"2025-02-12T18:48:12Z","title":"Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale\n  Implicit Neural Representation","summary":"  Purpose: To develop and validate a novel image reconstruction technique using\nimplicit neural representations (INR) for multi-view thick-slice acquisitions\nwhile reducing the scan time but maintaining high signal-to-noise ratio (SNR).\nMethods: We propose Rotating-view super-resolution (ROVER)-MRI, an unsupervised\nneural network-based algorithm designed to reconstruct MRI data from multi-view\nthick slices, effectively reducing scan time by 2-fold while maintaining fine\nanatomical details. We compare our method to both bicubic interpolation and the\ncurrent state-of-the-art regularized least-squares super-resolution\nreconstruction (LS-SRR) technique. Validation is performed using ground-truth\nex-vivo monkey brain data, and we demonstrate superior reconstruction quality\nacross several in-vivo human datasets. Notably, we achieve the reconstruction\nof a whole human brain in-vivo T2-weighted image with an unprecedented\n180{\\mu}m isotropic spatial resolution, accomplished in just 17 minutes of scan\ntime on a 7T MRI scanner. Results: ROVER-MRI outperformed LS-SRR method in\nterms of reconstruction quality with 22.4% lower relative error (RE) and 7.5%\nlower full-width half maximum (FWHM) indicating better preservation of fine\nstructural details in nearly half the scan time. Conclusion: ROVER-MRI offers\nan efficient and robust approach for mesoscale MR imaging, enabling rapid,\nhigh-resolution whole-brain scans. Its versatility holds great promise for\nresearch applications requiring anatomical details and time-efficient imaging.\n","authors":["Jun Lyu","Lipeng Ning","William Consagra","Qiang Liu","Richard J. Rushmore","Berkin Bilgic","Yogesh Rathi"],"pdf_url":"https://arxiv.org/pdf/2502.08634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04328v2","updated":"2025-02-12T18:40:46Z","published":"2025-02-06T18:59:55Z","title":"Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment","summary":"  Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.\n","authors":["Zuyan Liu","Yuhao Dong","Jiahui Wang","Ziwei Liu","Winston Hu","Jiwen Lu","Yongming Rao"],"pdf_url":"https://arxiv.org/pdf/2502.04328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08625v1","updated":"2025-02-12T18:25:13Z","published":"2025-02-12T18:25:13Z","title":"Randomness of Low-Layer Parameters Determines Confusing Samples in Terms\n  of Interaction Representations of a DNN","summary":"  In this paper, we find that the complexity of interactions encoded by a deep\nneural network (DNN) can explain its generalization power. We also discover\nthat the confusing samples of a DNN, which are represented by non-generalizable\ninteractions, are determined by its low-layer parameters. In comparison, other\nfactors, such as high-layer parameters and network architecture, have much less\nimpact on the composition of confusing samples. Two DNNs with different\nlow-layer parameters usually have fully different sets of confusing samples,\neven though they have similar performance. This finding extends the\nunderstanding of the lottery ticket hypothesis, and well explains distinctive\nrepresentation power of different DNNs.\n","authors":["Junpeng Zhang","Lei Cheng","Qing Li","Liang Lin","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13147v2","updated":"2025-02-12T18:15:16Z","published":"2024-01-23T23:50:04Z","title":"Deep Spatiotemporal Clutter Filtering of Transthoracic Echocardiographic\n  Images: Leveraging Contextual Attention and Residual Learning","summary":"  This study presents a deep convolutional autoencoder network for filtering\nreverberation clutter from transthoracic echocardiographic (TTE) image\nsequences. Given the spatiotemporal nature of this type of clutter, the\nfiltering network employs 3D convolutional layers to suppress it throughout the\ncardiac cycle. The design of the network incorporates two key features that\ncontribute to the effectiveness of the filter: 1) an attention mechanism for\nfocusing on cluttered regions and leveraging contextual information, and 2)\nresidual learning for preserving fine image structures. To train the network, a\ndiverse set of artifact patterns was simulated and superimposed onto\nultra-realistic synthetic TTE sequences from six ultrasound vendors, generating\ninput for the filtering network. The artifact-free sequences served as\nground-truth. Performance of the filtering network was evaluated using unseen\nsynthetic and in vivo artifactual sequences. Results from the in vivo dataset\nconfirmed the network's strong generalization capabilities, despite being\ntrained solely on synthetic data and simulated artifacts. The suitability of\nthe filtered sequences for downstream processing was assessed by computing\nsegmental strain curves. A significant reduction in the discrepancy between\nstrain profiles computed from cluttered and clutter-free segments was observed\nafter filtering the cluttered sequences with the proposed network. The trained\nnetwork processes a TTE sequence in a fraction of a second, enabling real-time\nclutter filtering and potentially improving the precision of clinically\nrelevant indices derived from TTE sequences. The source code of the proposed\nmethod and example video files of the filtering results are available at:\n\\href{https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main}{https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main}.\n","authors":["Mahdi Tabassian","Somayeh Akbari","Sandro Queirós","Jan D'hooge"],"pdf_url":"https://arxiv.org/pdf/2401.13147v2.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.05771v3","updated":"2025-02-12T17:43:56Z","published":"2024-11-08T18:33:03Z","title":"Sketched Equivariant Imaging Regularization and Deep Internal Learning\n  for Inverse Problems","summary":"  Equivariant Imaging (EI) regularization has become the de-facto technique for\nunsupervised training of deep imaging networks, without any need of\nground-truth data. Observing that the EI-based unsupervised training paradigm\ncurrently has significant computational redundancy leading to inefficiency in\nhigh-dimensional applications, we propose a sketched EI regularization which\nleverages the randomized sketching techniques for acceleration. We then extend\nour sketched EI regularization to develop an accelerated deep internal learning\nframework, Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can be\nefficiently applied for single-image and task-adapted reconstruction.\nAdditionally, for network adaptation tasks, we propose a parameter-efficient\napproach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only the\nnormalization layers. Our numerical study on X-ray CT and multi-coil MRI image\nreconstruction tasks demonstrate that our approach can achieve significant\ncomputational acceleration over standard EI-based counterpart in single-input\nsetting and network adaptation at test time.\n","authors":["Guixian Xu","Jinglai Li","Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2411.05771v3.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2502.08590v1","updated":"2025-02-12T17:24:19Z","published":"2025-02-12T17:24:19Z","title":"Light-A-Video: Training-free Video Relighting via Progressive Light\n  Fusion","summary":"  Recent advancements in image relighting models, driven by large-scale\ndatasets and pre-trained diffusion models, have enabled the imposition of\nconsistent lighting. However, video relighting still lags, primarily due to the\nexcessive training costs and the scarcity of diverse, high-quality video\nrelighting datasets. A simple application of image relighting models on a\nframe-by-frame basis leads to several issues: lighting source inconsistency and\nrelighted appearance inconsistency, resulting in flickers in the generated\nvideos. In this work, we propose Light-A-Video, a training-free approach to\nachieve temporally smooth video relighting. Adapted from image relighting\nmodels, Light-A-Video introduces two key techniques to enhance lighting\nconsistency. First, we design a Consistent Light Attention (CLA) module, which\nenhances cross-frame interactions within the self-attention layers to stabilize\nthe generation of the background lighting source. Second, leveraging the\nphysical principle of light transport independence, we apply linear blending\nbetween the source video's appearance and the relighted appearance, using a\nProgressive Light Fusion (PLF) strategy to ensure smooth temporal transitions\nin illumination. Experiments show that Light-A-Video improves the temporal\nconsistency of relighted video while maintaining the image quality, ensuring\ncoherent lighting transitions across frames. Project page:\nhttps://bujiazi.github.io/light-a-video.github.io/.\n","authors":["Yujie Zhou","Jiazi Bu","Pengyang Ling","Pan Zhang","Tong Wu","Qidong Huang","Jinsong Li","Xiaoyi Dong","Yuhang Zang","Yuhang Cao","Anyi Rao","Jiaqi Wang","Li Niu"],"pdf_url":"https://arxiv.org/pdf/2502.08590v1.pdf","comment":"Project Page: https://bujiazi.github.io/light-a-video.github.io/"},{"id":"http://arxiv.org/abs/2502.08580v1","updated":"2025-02-12T17:11:58Z","published":"2025-02-12T17:11:58Z","title":"Ultrasound Image Generation using Latent Diffusion Models","summary":"  Diffusion models for image generation have been a subject of increasing\ninterest due to their ability to generate diverse, high-quality images. Image\ngeneration has immense potential in medical imaging because open-source medical\nimages are difficult to obtain compared to natural images, especially for rare\nconditions. The generated images can be used later to train classification and\nsegmentation models. In this paper, we propose simulating realistic ultrasound\n(US) images by successive fine-tuning of large diffusion models on different\npublicly available databases. To do so, we fine-tuned Stable Diffusion, a\nstate-of-the-art latent diffusion model, on BUSI (Breast US Images) an\nultrasound breast image dataset. We successfully generated high-quality US\nimages of the breast using simple prompts that specify the organ and pathology,\nwhich appeared realistic to three experienced US scientists and a US\nradiologist. Additionally, we provided user control by conditioning the model\nwith segmentations through ControlNet. We will release the source code at\nhttp://code.sonography.ai/ to allow fast US image generation to the scientific\ncommunity.\n","authors":["Benoit Freiche","Anthony El-Khoury","Ali Nasiri-Sarvi","Mahdi S. Hosseini","Damien Garcia","Adrian Basarab","Mathieu Boily","Hassan Rivaz"],"pdf_url":"https://arxiv.org/pdf/2502.08580v1.pdf","comment":"6 pages conference paper for SPIE medical imaging"},{"id":"http://arxiv.org/abs/2502.08573v1","updated":"2025-02-12T17:07:43Z","published":"2025-02-12T17:07:43Z","title":"A Novel Approach to for Multimodal Emotion Recognition : Multimodal\n  semantic information fusion","summary":"  With the advancement of artificial intelligence and computer vision\ntechnologies, multimodal emotion recognition has become a prominent research\ntopic. However, existing methods face challenges such as heterogeneous data\nfusion and the effective utilization of modality correlations. This paper\nproposes a novel multimodal emotion recognition approach, DeepMSI-MER, based on\nthe integration of contrastive learning and visual sequence compression. The\nproposed method enhances cross-modal feature fusion through contrastive\nlearning and reduces redundancy in the visual modality by leveraging visual\nsequence compression. Experimental results on two public datasets, IEMOCAP and\nMELD, demonstrate that DeepMSI-MER significantly improves the accuracy and\nrobustness of emotion recognition, validating the effectiveness of multimodal\nfeature fusion and the proposed approach.\n","authors":["Wei Dai","Dequan Zheng","Feng Yu","Yanrong Zhang","Yaohui Hou"],"pdf_url":"https://arxiv.org/pdf/2502.08573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08566v1","updated":"2025-02-12T16:56:07Z","published":"2025-02-12T16:56:07Z","title":"AR Glulam: Accurate Augmented Reality Using Multiple Fiducial Markers\n  for Glulam Fabrication","summary":"  Recent advancements in Augmented Reality (AR) have demonstrated applications\nin architecture, design, and fabrication. Compared to conventional 2D\nconstruction drawings, AR can be used to superimpose contextual instructions,\ndisplay 3D spatial information and enable on-site engagement. Despite the\npotential of AR, the widespread adoption of the technology in the industry is\nlimited by its precision. Precision is important for projects requiring strict\nconstruction tolerances, design fidelity, and fabrication feedback. For\nexample, the manufacturing of glulam beams requires tolerances of less than\n2mm. The goal of this project is to explore the industrial application of using\nmultiple fiducial markers for high-precision AR fabrication. While the method\nhas been validated in lab settings with a precision of 0.97, this paper focuses\non fabricating glulam beams in a factory setting with an industry manufacturer,\nUnalam Factory.\n","authors":["Alexander Htet Kyaw","Arvin Xu","Sasa Zivkovic","Gwyllim Jahn","Cameron Newnham","Nick Van Den Berg"],"pdf_url":"https://arxiv.org/pdf/2502.08566v1.pdf","comment":"10 Figures, Project Paper for Association for Computer Aided Design\n  in Architecture"},{"id":"http://arxiv.org/abs/2502.08560v1","updated":"2025-02-12T16:47:41Z","published":"2025-02-12T16:47:41Z","title":"Brain Latent Progression: Individual-based Spatiotemporal Disease\n  Progression on 3D Brain MRIs via Latent Diffusion","summary":"  The growing availability of longitudinal Magnetic Resonance Imaging (MRI)\ndatasets has facilitated Artificial Intelligence (AI)-driven modeling of\ndisease progression, making it possible to predict future medical scans for\nindividual patients. However, despite significant advancements in AI, current\nmethods continue to face challenges including achieving patient-specific\nindividualization, ensuring spatiotemporal consistency, efficiently utilizing\nlongitudinal data, and managing the substantial memory demands of 3D scans. To\naddress these challenges, we propose Brain Latent Progression (BrLP), a novel\nspatiotemporal model designed to predict individual-level disease progression\nin 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates\nin a small latent space, mitigating the computational challenges posed by\nhigh-dimensional imaging data; (ii) it explicitly integrates subject metadata\nto enhance the individualization of predictions; (iii) it incorporates prior\nknowledge of disease dynamics through an auxiliary model, facilitating the\nintegration of longitudinal data; and (iv) it introduces the Latent Average\nStabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in\nthe predicted progression at inference time and (b) allows us to derive a\nmeasure of the uncertainty for the prediction. We train and evaluate BrLP on\n11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its\ngeneralizability on an external test set comprising 2,257 MRIs from 962\nsubjects. Our experiments compare BrLP-generated MRI scans with real follow-up\nMRIs, demonstrating state-of-the-art accuracy compared to existing methods. The\ncode is publicly available at: https://github.com/LemuelPuglisi/BrLP.\n","authors":["Lemuel Puglisi","Daniel C. Alexander","Daniele Ravì"],"pdf_url":"https://arxiv.org/pdf/2502.08560v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.03328"},{"id":"http://arxiv.org/abs/2410.19702v2","updated":"2025-02-12T16:47:30Z","published":"2024-10-25T17:19:55Z","title":"TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.\n","authors":["Xiangyu Zeng","Kunchang Li","Chenting Wang","Xinhao Li","Tianxiang Jiang","Ziang Yan","Songze Li","Yansong Shi","Zhengrong Yue","Yi Wang","Yali Wang","Yu Qiao","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19702v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.08556v1","updated":"2025-02-12T16:38:40Z","published":"2025-02-12T16:38:40Z","title":"Human-Centric Foundation Models: Perception, Generation and Agentic\n  Modeling","summary":"  Human understanding and generation are critical for modeling digital humans\nand humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)\ninspired by the success of generalist models, such as large language and vision\nmodels, have emerged to unify diverse human-centric tasks into a single\nframework, surpassing traditional task-specific approaches. In this survey, we\npresent a comprehensive overview of HcFMs by proposing a taxonomy that\ncategorizes current approaches into four groups: (1) Human-centric Perception\nFoundation Models that capture fine-grained features for multi-modal 2D and 3D\nunderstanding. (2) Human-centric AIGC Foundation Models that generate\nhigh-fidelity, diverse human-related content. (3) Unified Perception and\nGeneration Models that integrate these capabilities to enhance both human\nunderstanding and synthesis. (4) Human-centric Agentic Foundation Models that\nextend beyond perception and generation to learn human-like intelligence and\ninteractive behaviors for humanoid embodied tasks. We review state-of-the-art\ntechniques, discuss emerging challenges and future research directions. This\nsurvey aims to serve as a roadmap for researchers and practitioners working\ntowards more robust, versatile, and intelligent digital human and embodiments\nmodeling.\n","authors":["Shixiang Tang","Yizhou Wang","Lu Chen","Yuan Wang","Sida Peng","Dan Xu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2502.08556v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.08549v1","updated":"2025-02-12T16:30:39Z","published":"2025-02-12T16:30:39Z","title":"Copula-based mixture model identification for subgroup clustering with\n  imaging applications","summary":"  Model-based clustering techniques have been widely applied to various\napplication areas, while most studies focus on canonical mixtures with unique\ncomponent distribution form. However, this strict assumption is often hard to\nsatisfy. In this paper, we consider the more flexible Copula-Based Mixture\nModels (CBMMs) for clustering, which allow heterogeneous component\ndistributions composed by flexible choices of marginal and copula forms. More\nspecifically, we propose an adaptation of the Generalized Iterative Conditional\nEstimation (GICE) algorithm to identify the CBMMs in an unsupervised manner,\nwhere the marginal and copula forms and their parameters are estimated\niteratively. GICE is adapted from its original version developed for switching\nMarkov model identification with the choice of realization time. Our CBMM-GICE\nclustering method is then tested on synthetic two-cluster data (N=2000 samples)\nwith discussion of the factors impacting its convergence. Finally, it is\ncompared to the Expectation Maximization identified mixture models with unique\ncomponent form on the entire MNIST database (N=70000), and on real cardiac\nmagnetic resonance data (N=276) to illustrate its value for imaging\napplications.\n","authors":["Fei Zheng","Nicolas Duchateau"],"pdf_url":"https://arxiv.org/pdf/2502.08549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08544v1","updated":"2025-02-12T16:28:21Z","published":"2025-02-12T16:28:21Z","title":"Moment of Untruth: Dealing with Negative Queries in Video Moment\n  Retrieval","summary":"  Video Moment Retrieval is a common task to evaluate the performance of\nvisual-language models - it involves localising start and end times of moments\nin videos from query sentences. The current task formulation assumes that the\nqueried moment is present in the video, resulting in false positive moment\npredictions when irrelevant query sentences are provided.\n  In this paper we propose the task of Negative-Aware Video Moment Retrieval\n(NA-VMR), which considers both moment retrieval accuracy and negative query\nrejection accuracy. We make the distinction between In-Domain and Out-of-Domain\nnegative queries and provide new evaluation benchmarks for two popular video\nmoment retrieval datasets: QVHighlights and Charades-STA. We analyse the\nability of current SOTA video moment retrieval approaches to adapt to\nNegative-Aware Video Moment Retrieval and propose UniVTG-NA, an adaptation of\nUniVTG designed to tackle NA-VMR. UniVTG-NA achieves high negative rejection\naccuracy (avg. $98.4\\%$) scores while retaining moment retrieval scores to\nwithin $3.87\\%$ Recall@1. Dataset splits and code are available at\nhttps://github.com/keflanagan/MomentofUntruth\n","authors":["Kevin Flanagan","Dima Damen","Michael Wray"],"pdf_url":"https://arxiv.org/pdf/2502.08544v1.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.08540v1","updated":"2025-02-12T16:24:22Z","published":"2025-02-12T16:24:22Z","title":"A Survey on Image Quality Assessment: Insights, Analysis, and Future\n  Outlook","summary":"  Image quality assessment (IQA) represents a pivotal challenge in\nimage-focused technologies, significantly influencing the advancement\ntrajectory of image processing and computer vision. Recently, IQA has witnessed\na notable surge in innovative research efforts, driven by the emergence of\nnovel architectural paradigms and sophisticated computational techniques. This\nsurvey delivers an extensive analysis of contemporary IQA methodologies,\norganized according to their application scenarios, serving as a beneficial\nreference for both beginners and experienced researchers. We analyze the\nadvantages and limitations of current approaches and suggest potential future\nresearch pathways. The survey encompasses both general and specific IQA\nmethodologies, including conventional statistical measures, machine learning\ntechniques, and cutting-edge deep learning models such as convolutional neural\nnetworks (CNNs) and Transformer models. The analysis within this survey\nhighlights the necessity for distortion-specific IQA methods tailored to\nvarious application scenarios, emphasizing the significance of practicality,\ninterpretability, and ease of implementation in future developments.\n","authors":["Chengqian Ma","Zhengyi Shi","Zhiqiang Lu","Shenghao Xie","Fei Chao","Yao Sui"],"pdf_url":"https://arxiv.org/pdf/2502.08540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11172v3","updated":"2025-02-12T16:23:02Z","published":"2024-09-17T13:26:17Z","title":"Annealed Winner-Takes-All for Motion Forecasting","summary":"  In autonomous driving, motion prediction aims at forecasting the future\ntrajectories of nearby agents, helping the ego vehicle to anticipate behaviors\nand drive safely. A key challenge is generating a diverse set of future\npredictions, commonly addressed using data-driven models with Multiple Choice\nLearning (MCL) architectures and Winner-Takes-All (WTA) training objectives.\nHowever, these methods face initialization sensitivity and training\ninstabilities. Additionally, to compensate for limited performance, some\napproaches rely on training with a large set of hypotheses, requiring a\npost-selection step during inference to significantly reduce the number of\npredictions. To tackle these issues, we take inspiration from annealed MCL, a\nrecently introduced technique that improves the convergence properties of MCL\nmethods through an annealed Winner-Takes-All loss (aWTA). In this paper, we\ndemonstrate how the aWTA loss can be integrated with state-of-the-art motion\nforecasting models to enhance their performance using only a minimal set of\nhypotheses, eliminating the need for the cumbersome post-selection step. Our\napproach can be easily incorporated into any trajectory prediction model\nnormally trained using WTA and yields significant improvements. To facilitate\nthe application of our approach to future motion forecasting models, the code\nis made publicly available: https://github.com/valeoai/MF_aWTA.\n","authors":["Yihong Xu","Victor Letzelter","Mickaël Chen","Éloi Zablocki","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2409.11172v3.pdf","comment":"7 pages, 6 figures, Accepted to ICRA2025"},{"id":"http://arxiv.org/abs/2502.08528v1","updated":"2025-02-12T16:05:46Z","published":"2025-02-12T16:05:46Z","title":"BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image\n  Generation","summary":"  The properties of black holes and accretion flows can be inferred by fitting\nEvent Horizon Telescope (EHT) data to simulated images generated through\ngeneral relativistic ray tracing (GRRT). However, due to the computationally\nintensive nature of GRRT, the efficiency of generating specific radiation flux\nimages needs to be improved. This paper introduces the Branch Correction\nDenoising Diffusion Model (BCDDM), which uses a branch correction mechanism and\na weighted mixed loss function to improve the accuracy of generated black hole\nimages based on seven physical parameters of the radiatively inefficient\naccretion flow (RIAF) model. Our experiments show a strong correlation between\nthe generated images and their physical parameters. By enhancing the GRRT\ndataset with BCDDM-generated images and using ResNet50 for parameter\nregression, we achieve significant improvements in parameter prediction\nperformance. This approach reduces computational costs and provides a faster,\nmore efficient method for dataset expansion, parameter estimation, and model\nfitting.\n","authors":["Ao liu","Zelin Zhang","Songbai Chen","Cuihong Wen"],"pdf_url":"https://arxiv.org/pdf/2502.08528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14679v2","updated":"2025-02-12T15:37:22Z","published":"2025-01-24T17:57:06Z","title":"Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation","summary":"  Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.\n","authors":["Rongzhao He","Weihao Zheng","Leilei Zhao","Ying Wang","Dalin Zhu","Dan Wu","Bin Hu"],"pdf_url":"https://arxiv.org/pdf/2501.14679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08486v1","updated":"2025-02-12T15:21:18Z","published":"2025-02-12T15:21:18Z","title":"Referring Remote Sensing Image Segmentation via Bidirectional Alignment\n  Guided Joint Prediction","summary":"  Referring Remote Sensing Image Segmentation (RRSIS) is critical for\necological monitoring, urban planning, and disaster management, requiring\nprecise segmentation of objects in remote sensing imagery guided by textual\ndescriptions. This task is uniquely challenging due to the considerable\nvision-language gap, the high spatial resolution and broad coverage of remote\nsensing imagery with diverse categories and small targets, and the presence of\nclustered, unclear targets with blurred edges. To tackle these issues, we\npropose \\ours, a novel framework designed to bridge the vision-language gap,\nenhance multi-scale feature interaction, and improve fine-grained object\ndifferentiation. Specifically, \\ours introduces: (1) the Bidirectional Spatial\nCorrelation (BSC) for improved vision-language feature alignment, (2) the\nTarget-Background TwinStream Decoder (T-BTD) for precise distinction between\ntargets and non-targets, and (3) the Dual-Modal Object Learning Strategy\n(D-MOLS) for robust multimodal feature reconstruction. Extensive experiments on\nthe benchmark datasets RefSegRS and RRSIS-D demonstrate that \\ours achieves\nstate-of-the-art performance. Specifically, \\ours improves the overall IoU\n(oIoU) by 3.76 percentage points (80.57) and 1.44 percentage points (79.23) on\nthe two datasets, respectively. Additionally, it outperforms previous methods\nin the mean IoU (mIoU) by 5.37 percentage points (67.95) and 1.84 percentage\npoints (66.04), effectively addressing the core challenges of RRSIS with\nenhanced precision and robustness.\n","authors":["Tianxiang Zhang","Zhaokun Wen","Bo Kong","Kecheng Liu","Yisi Zhang","Peixian Zhuang","Jiangyun Li"],"pdf_url":"https://arxiv.org/pdf/2502.08486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08468v1","updated":"2025-02-12T15:03:33Z","published":"2025-02-12T15:03:33Z","title":"mmE5: Improving Multimodal Multilingual Embeddings via High-quality\n  Synthetic Data","summary":"  Multimodal embedding models have gained significant attention for their\nability to map data from different modalities, such as text and images, into a\nunified representation space. However, the limited labeled multimodal data\noften hinders embedding performance. Recent approaches have leveraged data\nsynthesis to address this problem, yet the quality of synthetic data remains a\ncritical bottleneck. In this work, we identify three criteria for high-quality\nsynthetic multimodal data. First, broad scope ensures that the generated data\ncovers diverse tasks and modalities, making it applicable to various downstream\nscenarios. Second, robust cross-modal alignment makes different modalities\nsemantically consistent. Third, high fidelity ensures that the synthetic data\nmaintains realistic details to enhance its reliability. Guided by these\nprinciples, we synthesize datasets that: (1) cover a wide range of tasks,\nmodality combinations, and languages, (2) are generated via a deep thinking\nprocess within a single pass of a multimodal large language model, and (3)\nincorporate real-world images with accurate and relevant texts, ensuring\nfidelity through self-evaluation and refinement. Leveraging these high-quality\nsynthetic and labeled datasets, we train a multimodal multilingual E5 model\nmmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art\nperformance on the MMEB Benchmark and superior multilingual performance on the\nXTD benchmark. Our codes, datasets and models are released in\nhttps://github.com/haon-chen/mmE5.\n","authors":["Haonan Chen","Liang Wang","Nan Yang","Yutao Zhu","Ziliang Zhao","Furu Wei","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2502.08468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07737v2","updated":"2025-02-12T14:50:50Z","published":"2025-02-11T17:57:53Z","title":"Next Block Prediction: Video Generation via Semi-Autoregressive Modeling","summary":"  Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)\nvideo generation, but it suffers from suboptimal unidirectional dependencies\nand slow inference speed. In this work, we propose a semi-autoregressive\n(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.\nBy uniformly decomposing video content into equal-sized blocks (e.g., rows or\nframes), we shift the generation unit from individual tokens to blocks,\nallowing each token in the current block to simultaneously predict the\ncorresponding token in the next block. Unlike traditional AR modeling, our\nframework employs bidirectional attention within each block, enabling tokens to\ncapture more robust spatial dependencies. By predicting multiple tokens in\nparallel, NBP models significantly reduce the number of generation steps,\nleading to faster and more efficient inference. Our model achieves FVD scores\nof 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an\naverage of 4.4. Furthermore, thanks to the reduced number of inference steps,\nthe NBP model generates 8.89 frames (128x128 resolution) per second, achieving\nan 11x speedup. We also explored model scales ranging from 700M to 3B\nparameters, observing significant improvements in generation quality, with FVD\nscores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,\ndemonstrating the scalability of our approach.\n","authors":["Shuhuai Ren","Shuming Ma","Xu Sun","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.07737v2.pdf","comment":"project page: https://renshuhuai-andy.github.io/NBP-project/"},{"id":"http://arxiv.org/abs/2502.05240v2","updated":"2025-02-12T14:43:02Z","published":"2025-02-07T12:18:20Z","title":"Survey on AI-Generated Media Detection: From Non-MLLM to MLLM","summary":"  The proliferation of AI-generated media poses significant challenges to\ninformation authenticity and social trust, making reliable detection methods\nhighly demanded. Methods for detecting AI-generated media have evolved rapidly,\nparalleling the advancement of Multimodal Large Language Models (MLLMs).\nCurrent detection approaches can be categorized into two main groups:\nNon-MLLM-based and MLLM-based methods. The former employs high-precision,\ndomain-specific detectors powered by deep learning techniques, while the latter\nutilizes general-purpose detectors based on MLLMs that integrate authenticity\nverification, explainability, and localization capabilities. Despite\nsignificant progress in this field, there remains a gap in literature regarding\na comprehensive survey that examines the transition from domain-specific to\ngeneral-purpose detection methods. This paper addresses this gap by providing a\nsystematic review of both approaches, analyzing them from single-modal and\nmulti-modal perspectives. We present a detailed comparative analysis of these\ncategories, examining their methodological similarities and differences.\nThrough this analysis, we explore potential hybrid approaches and identify key\nchallenges in forgery detection, providing direction for future research.\nAdditionally, as MLLMs become increasingly prevalent in detection tasks,\nethical and security considerations have emerged as critical global concerns.\nWe examine the regulatory landscape surrounding Generative AI (GenAI) across\nvarious jurisdictions, offering valuable insights for researchers and\npractitioners in this field.\n","authors":["Yueying Zou","Peipei Li","Zekun Li","Huaibo Huang","Xing Cui","Xuannan Liu","Chenghanyu Zhang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2502.05240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2110.14731v3","updated":"2025-02-12T14:06:45Z","published":"2021-10-27T19:33:23Z","title":"Vision Transformer for Classification of Breast Ultrasound Images","summary":"  Medical ultrasound (US) imaging has become a prominent modality for breast\ncancer imaging due to its ease-of-use, low-cost and safety. In the past decade,\nconvolutional neural networks (CNNs) have emerged as the method of choice in\nvision applications and have shown excellent potential in automatic\nclassification of US images. Despite their success, their restricted local\nreceptive field limits their ability to learn global context information.\nRecently, Vision Transformer (ViT) designs that are based on self-attention\nbetween image patches have shown great potential to be an alternative to CNNs.\nIn this study, for the first time, we utilize ViT to classify breast US images\nusing different augmentation strategies. The results are provided as\nclassification accuracy and Area Under the Curve (AUC) metrics, and the\nperformance is compared with the state-of-the-art CNNs. The results indicate\nthat the ViT models have comparable efficiency with or even better than the\nCNNs in classification of US breast images.\n","authors":["Behnaz Gheflati","Hassan Rivaz"],"pdf_url":"https://arxiv.org/pdf/2110.14731v3.pdf","comment":"5 pages, 2 figures, Published in EMBC 2022"},{"id":"http://arxiv.org/abs/2502.08417v1","updated":"2025-02-12T13:59:37Z","published":"2025-02-12T13:59:37Z","title":"Handwritten Text Recognition: A Survey","summary":"  Handwritten Text Recognition (HTR) has become an essential field within\npattern recognition and machine learning, with applications spanning historical\ndocument preservation to modern data entry and accessibility solutions. The\ncomplexity of HTR lies in the high variability of handwriting, which makes it\nchallenging to develop robust recognition systems. This survey examines the\nevolution of HTR models, tracing their progression from early heuristic-based\napproaches to contemporary state-of-the-art neural models, which leverage deep\nlearning techniques. The scope of the field has also expanded, with models\ninitially capable of recognizing only word-level content progressing to recent\nend-to-end document-level approaches. Our paper categorizes existing work into\ntwo primary levels of recognition: (1) \\emph{up to line-level}, encompassing\nword and line recognition, and (2) \\emph{beyond line-level}, addressing\nparagraph- and document-level challenges. We provide a unified framework that\nexamines research methodologies, recent advances in benchmarking, key datasets\nin the field, and a discussion of the results reported in the literature.\nFinally, we identify pressing research challenges and outline promising future\ndirections, aiming to equip researchers and practitioners with a roadmap for\nadvancing the field.\n","authors":["Carlos Garrido-Munoz","Antonio Rios-Vila","Jorge Calvo-Zaragoza"],"pdf_url":"https://arxiv.org/pdf/2502.08417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19604v2","updated":"2025-02-12T13:46:07Z","published":"2024-04-30T14:53:07Z","title":"X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image\n  Using Cross-Sectional Diffusion Models","summary":"  Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but\nhigh-resolution scans are often slow and expensive due to extensive data\nacquisition requirements. Traditional MRI reconstruction methods aim to\nexpedite this process by filling in missing frequency components in the\nK-space, performing 3D-to-3D reconstructions that demand full 3D scans. In\ncontrast, we introduce X-Diffusion, a novel cross-sectional diffusion model\nthat reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain\ninputs, achieving 2D-to-3D reconstruction from as little as a single 2D MRI\nslice or few slices. A key aspect of X-Diffusion is that it models MRI data as\nholistic 3D volumes during the cross-sectional training and inference, unlike\nprevious learning approaches that treat MRI scans as collections of 2D slices\nin standard planes (coronal, axial, sagittal). We evaluated X-Diffusion on\nbrain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank\ndataset. Our results demonstrate that X-Diffusion not only surpasses\nstate-of-the-art methods in quantitative accuracy (PSNR) on unseen data but\nalso preserves critical anatomical features such as tumor profiles, spine\ncurvature, and brain volume. Remarkably, the model generalizes beyond the\ntraining domain, successfully reconstructing knee MRIs despite being trained\nexclusively on brain data. Medical expert evaluations further confirm the\nclinical relevance and fidelity of the generated images.To our knowledge,\nX-Diffusion is the first method capable of producing detailed 3D MRIs from\nhighly limited 2D input data, potentially accelerating MRI acquisition and\nreducing associated costs. The code is available on the project website\nhttps://emmanuelleb985.github.io/XDiffusion/ .\n","authors":["Emmanuelle Bourigault","Abdullah Hamdi","Amir Jamaludin"],"pdf_url":"https://arxiv.org/pdf/2404.19604v2.pdf","comment":"preprint, project website:\n  https://emmanuelleb985.github.io/XDiffusion/"},{"id":"http://arxiv.org/abs/2502.08391v1","updated":"2025-02-12T13:28:46Z","published":"2025-02-12T13:28:46Z","title":"ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for\n  Whole Slide Image Classification","summary":"  Multiple instance learning (MIL)-based framework has become the mainstream\nfor processing the whole slide image (WSI) with giga-pixel size and\nhierarchical image context in digital pathology. However, these methods heavily\ndepend on a substantial number of bag-level labels and solely learn from the\noriginal slides, which are easily affected by variations in data distribution.\nRecently, vision language model (VLM)-based methods introduced the language\nprior by pre-training on large-scale pathological image-text pairs. However,\nthe previous text prompt lacks the consideration of pathological prior\nknowledge, therefore does not substantially boost the model's performance.\nMoreover, the collection of such pairs and the pre-training process are very\ntime-consuming and source-intensive.To solve the above problems, we propose a\ndual-scale vision-language multiple instance learning (ViLa-MIL) framework for\nwhole slide image classification. Specifically, we propose a dual-scale visual\ndescriptive text prompt based on the frozen large language model (LLM) to boost\nthe performance of VLM effectively. To transfer the VLM to process WSI\nefficiently, for the image branch, we propose a prototype-guided patch decoder\nto aggregate the patch features progressively by grouping similar patches into\nthe same prototype; for the text branch, we introduce a context-guided text\ndecoder to enhance the text features by incorporating the multi-granular image\ncontexts. Extensive studies on three multi-cancer and multi-center subtyping\ndatasets demonstrate the superiority of ViLa-MIL.\n","authors":["Jiangbo Shi","Chen Li","Tieliang Gong","Yefeng Zheng","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2502.08391v1.pdf","comment":"CVPR 2024 (Updated version with corrections for typos and errors.)"},{"id":"http://arxiv.org/abs/2502.06581v2","updated":"2025-02-12T13:25:22Z","published":"2025-02-10T15:48:11Z","title":"A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems","summary":"  The explosive growth of video data has driven the development of distributed\nvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enabling\nefficient video processing, real-time inference, and privacy-preserving\nanalysis. Among multiple advantages, CETC systems can distribute video\nprocessing tasks and enable adaptive analytics across cloud, edge, and terminal\ndevices, leading to breakthroughs in video surveillance, autonomous driving,\nand smart cities. In this survey, we first analyze fundamental architectural\ncomponents, including hierarchical, distributed, and hybrid frameworks,\nalongside edge computing platforms and resource management mechanisms. Building\nupon these foundations, edge-centric approaches emphasize on-device processing,\nedge-assisted offloading, and edge intelligence, while cloud-centric methods\nleverage powerful computational capabilities for complex video understanding\nand model training. Our investigation also covers hybrid video analytics\nincorporating adaptive task offloading and resource-aware scheduling techniques\nthat optimize performance across the entire system. Beyond conventional\napproaches, recent advances in large language models and multimodal integration\nreveal both opportunities and challenges in platform scalability, data\nprotection, and system reliability. Future directions also encompass\nexplainable systems, efficient processing mechanisms, and advanced video\nanalytics, offering valuable insights for researchers and practitioners in this\ndynamic field.\n","authors":["Linxiao Gong","Hao Yang","Gaoyun Fang","Bobo Ju","Juncen Guo","Xiaoguang Zhu","Yan Wang","Xiping Hu","Peng Sun","Azzedine Boukerche"],"pdf_url":"https://arxiv.org/pdf/2502.06581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11959v2","updated":"2025-02-12T13:25:10Z","published":"2024-12-16T16:41:51Z","title":"Gramian Multimodal Representation Learning and Alignment","summary":"  Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modalities and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.\n","authors":["Giordano Cicchetti","Eleonora Grassucci","Luigi Sigillo","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.11959v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.15981v2","updated":"2025-02-12T13:22:54Z","published":"2024-10-21T13:06:38Z","title":"Robust Visual Representation Learning with Multi-modal Prior Knowledge\n  for Image Classification Under Distribution Shift","summary":"  Despite the remarkable success of deep neural networks (DNNs) in computer\nvision, they fail to remain high-performing when facing distribution shifts\nbetween training and testing data. In this paper, we propose Knowledge-Guided\nVisual representation learning (KGV) - a distribution-based learning approach\nleveraging multi-modal prior knowledge - to improve generalization under\ndistribution shift. It integrates knowledge from two distinct modalities: 1) a\nknowledge graph (KG) with hierarchical and association relationships; and 2)\ngenerated synthetic images of visual elements semantically represented in the\nKG. The respective embeddings are generated from the given modalities in a\ncommon latent space, i.e., visual embeddings from original and synthetic images\nas well as knowledge graph embeddings (KGEs). These embeddings are aligned via\na novel variant of translation-based KGE methods, where the node and relation\nembeddings of the KG are modeled as Gaussian distributions and translations,\nrespectively. We claim that incorporating multi-model prior knowledge enables\nmore regularized learning of image representations. Thus, the models are able\nto better generalize across different data distributions. We evaluate KGV on\ndifferent image classification tasks with major or minor distribution shifts,\nnamely road sign classification across datasets from Germany, China, and\nRussia, image classification with the mini-ImageNet dataset and its variants,\nas well as the DVM-CAR dataset. The results demonstrate that KGV consistently\nexhibits higher accuracy and data efficiency across all experiments.\n","authors":["Hongkuan Zhou","Lavdim Halilaj","Sebastian Monka","Stefan Schmid","Yuqicheng Zhu","Bo Xiong","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2410.15981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08377v1","updated":"2025-02-12T13:08:35Z","published":"2025-02-12T13:08:35Z","title":"Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling\n  Dynamic-Static Features","summary":"  Recently, the generation of dynamic 3D objects from a video has shown\nimpressive results. Existing methods directly optimize Gaussians using whole\ninformation in frames. However, when dynamic regions are interwoven with static\nregions within frames, particularly if the static regions account for a large\nproportion, existing methods often overlook information in dynamic regions and\nare prone to overfitting on static regions. This leads to producing results\nwith blurry textures. We consider that decoupling dynamic-static features to\nenhance dynamic representations can alleviate this issue. Thus, we propose a\ndynamic-static feature decoupling module (DSFD). Along temporal axes, it\nregards the portions of current frame features that possess significant\ndifferences relative to reference frame features as dynamic features.\nConversely, the remaining parts are the static features. Then, we acquire\ndecoupled features driven by dynamic features and current frame features.\nMoreover, to further enhance the dynamic representation of decoupled features\nfrom different viewpoints and ensure accurate motion prediction, we design a\ntemporal-spatial similarity fusion module (TSSF). Along spatial axes, it\nadaptively selects a similar information of dynamic regions. Hinging on the\nabove, we construct a novel approach, DS4D. Experimental results verify our\nmethod achieves state-of-the-art (SOTA) results in video-to-4D. In addition,\nthe experiments on a real-world scenario dataset demonstrate its effectiveness\non the 4D scene. Our code will be publicly available.\n","authors":["Liying Yang","Chen Liu","Zhenwei Zhu","Ajian Liu","Hui Ma","Jian Nong","Yanyan Liang"],"pdf_url":"https://arxiv.org/pdf/2502.08377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08374v1","updated":"2025-02-12T13:05:35Z","published":"2025-02-12T13:05:35Z","title":"AdvSwap: Covert Adversarial Perturbation with High Frequency\n  Info-swapping for Autonomous Driving Perception","summary":"  Perception module of Autonomous vehicles (AVs) are increasingly susceptible\nto be attacked, which exploit vulnerabilities in neural networks through\nadversarial inputs, thereby compromising the AI safety. Some researches focus\non creating covert adversarial samples, but existing global noise techniques\nare detectable and difficult to deceive the human visual system. This paper\nintroduces a novel adversarial attack method, AdvSwap, which creatively\nutilizes wavelet-based high-frequency information swapping to generate covert\nadversarial samples and fool the camera. AdvSwap employs invertible neural\nnetwork for selective high-frequency information swapping, preserving both\nforward propagation and data integrity. The scheme effectively removes the\noriginal label data and incorporates the guidance image data, producing\nconcealed and robust adversarial samples. Experimental evaluations and\ncomparisons on the GTSRB and nuScenes datasets demonstrate that AdvSwap can\nmake concealed attacks on common traffic targets. The generates adversarial\nsamples are also difficult to perceive by humans and algorithms. Meanwhile, the\nmethod has strong attacking robustness and attacking transferability.\n","authors":["Yuanhao Huang","Qinfan Zhang","Jiandong Xing","Mengyue Cheng","Haiyang Yu","Yilong Ren","Xiao Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.08374v1.pdf","comment":"27th IEEE International Conference on Intelligent Transportation\n  Systems (ITSC)"},{"id":"http://arxiv.org/abs/2502.08373v1","updated":"2025-02-12T13:05:24Z","published":"2025-02-12T13:05:24Z","title":"Uncertainty Aware Human-machine Collaboration in Camouflaged Object\n  Detection","summary":"  Camouflaged Object Detection (COD), the task of identifying objects concealed\nwithin their environments, has seen rapid growth due to its wide range of\npractical applications. A key step toward developing trustworthy COD systems is\nthe estimation and effective utilization of uncertainty. In this work, we\npropose a human-machine collaboration framework for classifying the presence of\ncamouflaged objects, leveraging the complementary strengths of computer vision\n(CV) models and noninvasive brain-computer interfaces (BCIs). Our approach\nintroduces a multiview backbone to estimate uncertainty in CV model\npredictions, utilizes this uncertainty during training to improve efficiency,\nand defers low-confidence cases to human evaluation via RSVP-based BCIs during\ntesting for more reliable decision-making. We evaluated the framework in the\nCAMO dataset, achieving state-of-the-art results with an average improvement of\n4.56\\% in balanced accuracy (BA) and 3.66\\% in the F1 score compared to\nexisting methods. For the best-performing participants, the improvements\nreached 7.6\\% in BA and 6.66\\% in the F1 score. Analysis of the training\nprocess revealed a strong correlation between our confidence measures and\nprecision, while an ablation study confirmed the effectiveness of the proposed\ntraining policy and the human-machine collaboration strategy. In general, this\nwork reduces human cognitive load, improves system reliability, and provides a\nstrong foundation for advancements in real-world COD applications and\nhuman-computer interaction. Our code and data are available at:\nhttps://github.com/ziyuey/Uncertainty-aware-human-machine-collaboration-in-camouflaged-object-identification.\n","authors":["Ziyue Yang","Kehan Wang","Yuhang Ming","Yong Peng","Han Yang","Qiong Chen","Wanzeng Kong"],"pdf_url":"https://arxiv.org/pdf/2502.08373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08352v1","updated":"2025-02-12T12:27:32Z","published":"2025-02-12T12:27:32Z","title":"Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images\n  with Depth and Normal Supervision","summary":"  With advancements in satellite imaging technology, acquiring high-resolution\nmulti-view satellite imagery has become increasingly accessible, enabling rapid\nand location-independent ground model reconstruction. However, traditional\nstereo matching methods struggle to capture fine details, and while neural\nradiance fields (NeRFs) achieve high-quality reconstructions, their training\ntime is prohibitively long. Moreover, challenges such as low visibility of\nbuilding facades, illumination and style differences between pixels, and weakly\ntextured regions in satellite imagery further make it hard to reconstruct\nreasonable terrain geometry and detailed building facades. To address these\nissues, we propose Sat-DN, a novel framework leveraging a progressively trained\nmulti-resolution hash grid reconstruction architecture with explicit depth\nguidance and surface normal consistency constraints to enhance reconstruction\nquality. The multi-resolution hash grid accelerates training, while the\nprogressive strategy incrementally increases the learning frequency, using\ncoarse low-frequency geometry to guide the reconstruction of fine\nhigh-frequency details. The depth and normal constraints ensure a clear\nbuilding outline and correct planar distribution. Extensive experiments on the\nDFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving\nstate-of-the-art results in both qualitative and quantitative evaluations. The\ncode is available at https://github.com/costune/SatDN.\n","authors":["Tianle Liu","Shuangming Zhao","Wanshou Jiang","Bingxuan Guo"],"pdf_url":"https://arxiv.org/pdf/2502.08352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08347v1","updated":"2025-02-12T12:14:02Z","published":"2025-02-12T12:14:02Z","title":"Hi-End-MAE: Hierarchical encoder-driven masked autoencoders are stronger\n  vision learners for medical image segmentation","summary":"  Medical image segmentation remains a formidable challenge due to the label\nscarcity. Pre-training Vision Transformer (ViT) through masked image modeling\n(MIM) on large-scale unlabeled medical datasets presents a promising solution,\nproviding both computational efficiency and model generalization for various\ndownstream tasks. However, current ViT-based MIM pre-training frameworks\npredominantly emphasize local aggregation representations in output layers and\nfail to exploit the rich representations across different ViT layers that\nbetter capture fine-grained semantic information needed for more precise\nmedical downstream tasks. To fill the above gap, we hereby present Hierarchical\nEncoder-driven MAE (Hi-End-MAE), a simple yet effective ViT-based pre-training\nsolution, which centers on two key innovations: (1) Encoder-driven\nreconstruction, which encourages the encoder to learn more informative features\nto guide the reconstruction of masked patches; and (2) Hierarchical dense\ndecoding, which implements a hierarchical decoding structure to capture rich\nrepresentations across different layers. We pre-train Hi-End-MAE on a\nlarge-scale dataset of 10K CT scans and evaluated its performance across seven\npublic medical image segmentation benchmarks. Extensive experiments demonstrate\nthat Hi-End-MAE achieves superior transfer learning capabilities across various\ndownstream tasks, revealing the potential of ViT in medical imaging\napplications. The code is available at:\nhttps://github.com/FengheTan9/Hi-End-MAE\n","authors":["Fenghe Tang","Qingsong Yao","Wenxin Ma","Chenxu Wu","Zihang Jiang","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08347v1.pdf","comment":"19 pages, Code: https://github.com/FengheTan9/Hi-End-MAE"},{"id":"http://arxiv.org/abs/2502.08333v1","updated":"2025-02-12T11:57:11Z","published":"2025-02-12T11:57:11Z","title":"Foundation Models in Computational Pathology: A Review of Challenges,\n  Opportunities, and Impact","summary":"  From self-supervised, vision-only models to contrastive visual-language\nframeworks, computational pathology has rapidly evolved in recent years.\nGenerative AI \"co-pilots\" now demonstrate the ability to mine subtle,\nsub-visual tissue cues across the cellular-to-pathology spectrum, generate\ncomprehensive reports, and respond to complex user queries. The scale of data\nhas surged dramatically, growing from tens to millions of multi-gigapixel\ntissue images, while the number of trainable parameters in these models has\nrisen to several billion. The critical question remains: how will this new wave\nof generative and multi-purpose AI transform clinical diagnostics? In this\narticle, we explore the true potential of these innovations and their\nintegration into clinical practice. We review the rapid progress of foundation\nmodels in pathology, clarify their applications and significance. More\nprecisely, we examine the very definition of foundational models, identifying\nwhat makes them foundational, general, or multipurpose, and assess their impact\non computational pathology. Additionally, we address the unique challenges\nassociated with their development and evaluation. These models have\ndemonstrated exceptional predictive and generative capabilities, but\nestablishing global benchmarks is crucial to enhancing evaluation standards and\nfostering their widespread clinical adoption. In computational pathology, the\nbroader impact of frontier AI ultimately depends on widespread adoption and\nsocietal acceptance. While direct public exposure is not strictly necessary, it\nremains a powerful tool for dispelling misconceptions, building trust, and\nsecuring regulatory support.\n","authors":["Mohsin Bilal"," Aadam","Manahil Raza","Youssef Altherwy","Anas Alsuhaibani","Abdulrahman Abduljabbar","Fahdah Almarshad","Paul Golding","Nasir Rajpoot"],"pdf_url":"https://arxiv.org/pdf/2502.08333v1.pdf","comment":"63 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.08321v1","updated":"2025-02-12T11:37:35Z","published":"2025-02-12T11:37:35Z","title":"Screener: Self-supervised Pathology Segmentation Model for 3D Medical\n  Images","summary":"  Accurate segmentation of all pathological findings in 3D medical images\nremains a significant challenge, as supervised models are limited to detecting\nonly the few pathology classes annotated in existing datasets. To address this,\nwe frame pathology segmentation as an unsupervised visual anomaly segmentation\n(UVAS) problem, leveraging the inherent rarity of pathological patterns\ncompared to healthy ones. We enhance the existing density-based UVAS framework\nwith two key innovations: (1) dense self-supervised learning (SSL) for feature\nextraction, eliminating the need for supervised pre-training, and (2) learned,\nmasking-invariant dense features as conditioning variables, replacing\nhand-crafted positional encodings. Trained on over 30,000 unlabeled 3D CT\nvolumes, our model, Screener, outperforms existing UVAS methods on four\nlarge-scale test datasets comprising 1,820 scans with diverse pathologies. Code\nand pre-trained models will be made publicly available.\n","authors":["Mikhail Goncharov","Eugenia Soboleva","Mariia Donskova","Ivan Oseledets","Marina Munkhoeva","Maxim Panov"],"pdf_url":"https://arxiv.org/pdf/2502.08321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08317v1","updated":"2025-02-12T11:32:19Z","published":"2025-02-12T11:32:19Z","title":"Mitigating Hallucinations in Multimodal Spatial Relations through\n  Constraint-Aware Prompting","summary":"  Spatial relation hallucinations pose a persistent challenge in large\nvision-language models (LVLMs), leading to generate incorrect predictions about\nobject positions and spatial configurations within an image. To address this\nissue, we propose a constraint-aware prompting framework designed to reduce\nspatial relation hallucinations. Specifically, we introduce two types of\nconstraints: (1) bidirectional constraint, which ensures consistency in\npairwise object relations, and (2) transitivity constraint, which enforces\nrelational dependence across multiple objects. By incorporating these\nconstraints, LVLMs can produce more spatially coherent and consistent outputs.\nWe evaluate our method on three widely-used spatial relation datasets,\ndemonstrating performance improvements over existing approaches. Additionally,\na systematic analysis of various bidirectional relation analysis choices and\ntransitivity reference selections highlights greater possibilities of our\nmethods in incorporating constraints to mitigate spatial relation\nhallucinations.\n","authors":["Jiarui Wu","Zhuo Liu","Hangfeng He"],"pdf_url":"https://arxiv.org/pdf/2502.08317v1.pdf","comment":"19 pages, accepted to NAACL Findings"},{"id":"http://arxiv.org/abs/2405.08431v5","updated":"2025-02-12T11:21:50Z","published":"2024-05-14T08:51:16Z","title":"Similarity and Quality Metrics for MR Image-To-Image Translation","summary":"  Image-to-image translation can create large impact in medical imaging, as\nimages can be synthetically transformed to other modalities, sequence types,\nhigher resolutions or lower noise levels. To ensure patient safety, these\nmethods should be validated by human readers, which requires a considerable\namount of time and costs. Quantitative metrics can effectively complement such\nstudies and provide reproducible and objective assessment of synthetic images.\nIf a reference is available, the similarity of MR images is frequently\nevaluated by SSIM and PSNR metrics, even though these metrics are not or too\nsensitive regarding specific distortions. When reference images to compare with\nare not available, non-reference quality metrics can reliably detect specific\ndistortions, such as blurriness. To provide an overview on distortion\nsensitivity, we quantitatively analyze 11 similarity (reference) and 12 quality\n(non-reference) metrics for assessing synthetic images. We additionally include\na metric on a downstream segmentation task. We investigate the sensitivity\nregarding 11 kinds of distortions and typical MR artifacts, and analyze the\ninfluence of different normalization methods on each metric and distortion.\nFinally, we derive recommendations for effective usage of the analyzed\nsimilarity and quality metrics for evaluation of image-to-image translation\nmodels.\n","authors":["Melanie Dohmen","Mark A. Klemens","Ivo M. Baltruschat","Tuan Truong","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2405.08431v5.pdf","comment":"44 pages (main: 22 pages, 3 figures, supplement: 22 pages, 15\n  figures)"},{"id":"http://arxiv.org/abs/2407.21416v3","updated":"2025-02-12T11:15:25Z","published":"2024-07-31T08:04:32Z","title":"VIPeR: Visual Incremental Place Recognition with Adaptive Mining and\n  Continual Learning","summary":"  Visual place recognition (VPR) is an essential component of many autonomous\nand augmented/virtual reality systems. It enables the systems to robustly\nlocalize themselves in large-scale environments. Existing VPR methods\ndemonstrate attractive performance at the cost of heavy pre-training and\nlimited generalizability. When deployed in unseen environments, these methods\nexhibit significant performance drops. Targeting this issue, we present VIPeR,\na novel approach for visual incremental place recognition with the ability to\nadapt to new environments while retaining the performance of previous\nenvironments. We first introduce an adaptive mining strategy that balances the\nperformance within a single environment and the generalizability across\nmultiple environments. Then, to prevent catastrophic forgetting in lifelong\nlearning, we draw inspiration from human memory systems and design a novel\nmemory bank for our VIPeR. Our memory bank contains a sensory memory, a working\nmemory and a long-term memory, with the first two focusing on the current\nenvironment and the last one for all previously visited environments.\nAdditionally, we propose a probabilistic knowledge distillation to explicitly\nsafeguard the previously learned knowledge. We evaluate our proposed VIPeR on\nthree large-scale datasets, namely Oxford Robotcar, Nordland, and TartanAir.\nFor comparison, we first set a baseline performance with naive finetuning.\nThen, several more recent lifelong learning methods are compared. Our VIPeR\nachieves better performance in almost all aspects with the biggest improvement\nof 13.65% in average performance.\n","authors":["Yuhang Ming","Minyang Xu","Xingrui Yang","Weicai Ye","Weihan Wang","Yong Peng","Weichen Dai","Wanzeng Kong"],"pdf_url":"https://arxiv.org/pdf/2407.21416v3.pdf","comment":"8 pages, 4 figures. In IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2502.08299v1","updated":"2025-02-12T10:59:45Z","published":"2025-02-12T10:59:45Z","title":"When do they StOP?: A First Step Towards Automatically Identifying Team\n  Communication in the Operating Room","summary":"  Purpose: Surgical performance depends not only on surgeons' technical skills\nbut also on team communication within and across the different professional\ngroups present during the operation. Therefore, automatically identifying team\ncommunication in the OR is crucial for patient safety and advances in the\ndevelopment of computer-assisted surgical workflow analysis and intra-operative\nsupport systems. To take the first step, we propose a new task of detecting\ncommunication briefings involving all OR team members, i.e. the team Time-out\nand the StOP?-protocol, by localizing their start and end times in video\nrecordings of surgical operations. Methods: We generate an OR dataset of real\nsurgeries, called Team-OR, with more than one hundred hours of surgical videos\ncaptured by the multi-view camera system in the OR. The dataset contains\ntemporal annotations of 33 Time-out and 22 StOP?-protocol activities in total.\nWe then propose a novel group activity detection approach, where we encode both\nscene context and action features, and use an efficient neural network model to\noutput the results. Results: The experimental results on the Team-OR dataset\nshow that our approach outperforms existing state-of-the-art temporal action\ndetection approaches. It also demonstrates the lack of research on group\nactivities in the OR, proving the significance of our dataset. Conclusion: We\ninvestigate the Team Time-Out and the StOP?-protocol in the OR, by presenting\nthe first OR dataset with temporal annotations of group activities protocols,\nand introducing a novel group activity detection approach that outperforms\nexisting approaches. Code is available at\nhttps://github.com/CAMMA-public/Team-OR .\n","authors":["Keqi Chen","Lilien Schewski","Vinkle Srivastav","Joël Lavanchy","Didier Mutter","Guido Beldi","Sandra Keller","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2502.08299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08297v1","updated":"2025-02-12T10:58:09Z","published":"2025-02-12T10:58:09Z","title":"BEAM: Bridging Physically-based Rendering and Gaussian Modeling for\n  Relightable Volumetric Video","summary":"  Volumetric video enables immersive experiences by capturing dynamic 3D\nscenes, enabling diverse applications for virtual reality, education, and\ntelepresence. However, traditional methods struggle with fixed lighting\nconditions, while neural approaches face trade-offs in efficiency, quality, or\nadaptability for relightable scenarios. To address these limitations, we\npresent BEAM, a novel pipeline that bridges 4D Gaussian representations with\nphysically-based rendering (PBR) to produce high-quality, relightable\nvolumetric videos from multi-view RGB footage. BEAM recovers detailed geometry\nand PBR properties via a series of available Gaussian-based techniques. It\nfirst combines Gaussian-based performance tracking with geometry-aware\nrasterization in a coarse-to-fine optimization framework to recover spatially\nand temporally consistent geometries. We further enhance Gaussian attributes by\nincorporating PBR properties step by step. We generate roughness via a\nmulti-view-conditioned diffusion model, and then derive AO and base color using\na 2D-to-3D strategy, incorporating a tailored Gaussian-based ray tracer for\nefficient visibility computation. Once recovered, these dynamic, relightable\nassets integrate seamlessly into traditional CG pipelines, supporting real-time\nrendering with deferred shading and offline rendering with ray tracing. By\noffering realistic, lifelike visualizations under diverse lighting conditions,\nBEAM opens new possibilities for interactive entertainment, storytelling, and\ncreative visualization.\n","authors":["Yu Hong","Yize Wu","Zhehao Shen","Chengcheng Guo","Yuheng Jiang","Yingliang Zhang","Jingyi Yu","Lan Xu"],"pdf_url":"https://arxiv.org/pdf/2502.08297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09388v2","updated":"2025-02-12T10:55:54Z","published":"2024-12-12T15:56:20Z","title":"All You Need in Knowledge Distillation Is a Tailored Coordinate System","summary":"  Knowledge Distillation (KD) is essential in transferring dark knowledge from\na large teacher to a small student network, such that the student can be much\nmore efficient than the teacher but with comparable accuracy. Existing KD\nmethods, however, rely on a large teacher trained specifically for the target\ntask, which is both very inflexible and inefficient. In this paper, we argue\nthat a SSL-pretrained model can effectively act as the teacher and its dark\nknowledge can be captured by the coordinate system or linear subspace where the\nfeatures lie in. We then need only one forward pass of the teacher, and then\ntailor the coordinate system (TCS) for the student network. Our TCS method is\nteacher-free and applies to diverse architectures, works well for KD and\npractical few-shot learning, and allows cross-architecture distillation with\nlarge capacity gap. Experiments show that TCS achieves significantly higher\naccuracy than state-of-the-art KD methods, while only requiring roughly half of\ntheir training time and GPU memory costs.\n","authors":["Junjie Zhou","Ke Zhu","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2412.09388v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2501.04304v2","updated":"2025-02-12T10:49:40Z","published":"2025-01-08T06:30:31Z","title":"DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion\n  Models","summary":"  Despite the widespread use of text-to-image diffusion models across various\ntasks, their computational and memory demands limit practical applications. To\nmitigate this issue, quantization of diffusion models has been explored. It\nreduces memory usage and computational costs by compressing weights and\nactivations into lower-bit formats. However, existing methods often struggle to\npreserve both image quality and text-image alignment, particularly in\nlower-bit($<$ 8bits) quantization. In this paper, we analyze the challenges\nassociated with quantizing text-to-image diffusion models from a distributional\nperspective. Our analysis reveals that activation outliers play a crucial role\nin determining image quality. Additionally, we identify distinctive patterns in\ncross-attention scores, which significantly affects text-image alignment. To\naddress these challenges, we propose Distribution-aware Group Quantization\n(DGQ), a method that identifies and adaptively handles pixel-wise and\nchannel-wise outliers to preserve image quality. Furthermore, DGQ applies\nprompt-specific logarithmic quantization scales to maintain text-image\nalignment. Our method demonstrates remarkable performance on datasets such as\nMS-COCO and PartiPrompts. We are the first to successfully achieve low-bit\nquantization of text-to-image diffusion models without requiring additional\nfine-tuning of weight quantization parameters. Code is available at\nhttps://github.com/ugonfor/DGQ.\n","authors":["Hyogon Ryu","NaHyeon Park","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2501.04304v2.pdf","comment":"Accepted ICLR 2025. Project page: https://ugonfor.kr/DGQ"},{"id":"http://arxiv.org/abs/2502.08287v1","updated":"2025-02-12T10:44:45Z","published":"2025-02-12T10:44:45Z","title":"CRISP: A Framework for Cryo-EM Image Segmentation and Processing with\n  Conditional Random Field","summary":"  Differentiating signals from the background in micrographs is a critical\ninitial step for cryogenic electron microscopy (cryo-EM), yet it remains\nlaborious due to low signal-to-noise ratio (SNR), the presence of contaminants\nand densely packed particles of varying sizes. Although image segmentation has\nrecently been introduced to distinguish particles at the pixel level, the low\nSNR complicates the automated generation of accurate annotations for training\nsupervised models. Moreover, platforms for systematically comparing different\ndesign choices in pipeline construction are lacking. Thus, a modular framework\nis essential to understand the advantages and limitations of this approach and\ndrive further development. To address these challenges, we present a pipeline\nthat automatically generates high-quality segmentation maps from cryo-EM data\nto serve as ground truth labels. Our modular framework enables the selection of\nvarious segmentation models and loss functions. We also integrate Conditional\nRandom Fields (CRFs) with different solvers and feature sets to refine coarse\npredictions, thereby producing fine-grained segmentation. This flexibility\nfacilitates optimal configurations tailored to cryo-EM datasets. When trained\non a limited set of micrographs, our approach achieves over 90% accuracy,\nrecall, precision, Intersection over Union (IoU), and F1-score on synthetic\ndata. Furthermore, to demonstrate our framework's efficacy in downstream\nanalyses, we show that the particles extracted by our pipeline produce 3D\ndensity maps with higher resolution than those generated by existing particle\npickers on real experimental datasets, while achieving performance comparable\nto that of manually curated datasets from experts.\n","authors":["Szu-Chi Chung","Po-Cheng Chou"],"pdf_url":"https://arxiv.org/pdf/2502.08287v1.pdf","comment":"31 pages, 28 Figures"},{"id":"http://arxiv.org/abs/2502.08285v1","updated":"2025-02-12T10:44:36Z","published":"2025-02-12T10:44:36Z","title":"Fully-Geometric Cross-Attention for Point Cloud Registration","summary":"  Point cloud registration approaches often fail when the overlap between point\nclouds is low due to noisy point correspondences. This work introduces a novel\ncross-attention mechanism tailored for Transformer-based architectures that\ntackles this problem, by fusing information from coordinates and features at\nthe super-point level between point clouds. This formulation has remained\nunexplored primarily because it must guarantee rotation and translation\ninvariance since point clouds reside in different and independent reference\nframes. We integrate the Gromov-Wasserstein distance into the cross-attention\nformulation to jointly compute distances between points across different point\nclouds and account for their geometric structure. By doing so, points from two\ndistinct point clouds can attend to each other under arbitrary rigid\ntransformations. At the point level, we also devise a self-attention mechanism\nthat aggregates the local geometric structure information into point features\nfor fine matching. Our formulation boosts the number of inlier correspondences,\nthereby yielding more precise registration results compared to state-of-the-art\napproaches. We have conducted an extensive evaluation on 3DMatch, 3DLoMatch,\nKITTI, and 3DCSR datasets.\n","authors":["Weijie Wang","Guofeng Mei","Jian Zhang","Nicu Sebe","Bruno Lepri","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2502.08285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19270v2","updated":"2025-02-12T10:37:04Z","published":"2023-05-30T17:59:32Z","title":"Learning without Forgetting for Vision-Language Models","summary":"  Class-Incremental Learning (CIL) or continual learning is a desired\ncapability in the real world, which requires a learning system to adapt to new\ntasks without forgetting former ones. While traditional CIL methods focus on\nvisual information to grasp core features, recent advances in Vision-Language\nModels (VLM) have shown promising capabilities in learning generalizable\nrepresentations with the aid of textual information. However, when continually\ntrained with new classes, VLMs often suffer from catastrophic forgetting of\nformer knowledge. Applying VLMs to CIL poses two major challenges: 1) how to\nadapt the model without forgetting; and 2) how to make full use of the\nmulti-modal information. To this end, we propose PROjectiOn Fusion (PROOF) that\nenables VLMs to learn without forgetting. To handle the first challenge, we\npropose training task-specific projections based on the frozen image/text\nencoders. When facing new tasks, new projections are expanded and former\nprojections are fixed, alleviating the forgetting of old concepts. For the\nsecond challenge, we propose the fusion module to better utilize the\ncross-modality information. By jointly adjusting visual and textual features,\nthe model can capture semantic information with stronger representation\nability. Extensive experiments on nine benchmark datasets validate PROOF\nachieves state-of-the-art performance. Code is available at\nhttps://github.com/zhoudw-zdw/PROOF\n","authors":["Da-Wei Zhou","Yuanhan Zhang","Yan Wang","Jingyi Ning","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2305.19270v2.pdf","comment":"Accepted to TPAMI. Code is available at\n  https://github.com/zhoudw-zdw/PROOF"},{"id":"http://arxiv.org/abs/2502.08279v1","updated":"2025-02-12T10:36:55Z","published":"2025-02-12T10:36:55Z","title":"What Is That Talk About? A Video-to-Text Summarization Dataset for\n  Scientific Presentations","summary":"  Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of scientific video summarization.\n","authors":["Dongqi Liu","Chenxi Whitehouse","Xi Yu","Louis Mahon","Rohit Saxena","Zheng Zhao","Yifu Qiu","Mirella Lapata","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.08279v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2306.02873 by other authors"},{"id":"http://arxiv.org/abs/2309.17170v2","updated":"2025-02-12T10:09:27Z","published":"2023-09-29T12:07:08Z","title":"Robotic Grasping of Harvested Tomato Trusses Using Vision and Online\n  Learning","summary":"  Currently, truss tomato weighing and packaging require significant manual\nwork. The main obstacle to automation lies in the difficulty of developing a\nreliable robotic grasping system for already harvested trusses. We propose a\nmethod to grasp trusses that are stacked in a crate with considerable clutter,\nwhich is how they are commonly stored and transported after harvest. The method\nconsists of a deep learning-based vision system to first identify the\nindividual trusses in the crate and then determine a suitable grasping location\non the stem. To this end, we have introduced a grasp pose ranking algorithm\nwith online learning capabilities. After selecting the most promising grasp\npose, the robot executes a pinch grasp without needing touch sensors or\ngeometric models. Lab experiments with a robotic manipulator equipped with an\neye-in-hand RGB-D camera showed a 100% clearance rate when tasked to pick all\ntrusses from a pile. 93% of the trusses were successfully grasped on the first\ntry, while the remaining 7% required more attempts.\n","authors":["Luuk van den Bent","Tomás Coleman","Robert Babuška"],"pdf_url":"https://arxiv.org/pdf/2309.17170v2.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.06782v2","updated":"2025-02-12T10:07:07Z","published":"2025-02-10T18:58:11Z","title":"Lumina-Video: Efficient and Flexible Video Generation with Multi-scale\n  Next-DiT","summary":"  Recent advancements have established Diffusion Transformers (DiTs) as a\ndominant framework in generative modeling. Building on this success,\nLumina-Next achieves exceptional performance in the generation of\nphotorealistic images with Next-DiT. However, its potential for video\ngeneration remains largely untapped, with significant challenges in modeling\nthe spatiotemporal complexity inherent to video data. To address this, we\nintroduce Lumina-Video, a framework that leverages the strengths of Next-DiT\nwhile introducing tailored solutions for video synthesis. Lumina-Video\nincorporates a Multi-scale Next-DiT architecture, which jointly learns multiple\npatchifications to enhance both efficiency and flexibility. By incorporating\nthe motion score as an explicit condition, Lumina-Video also enables direct\ncontrol of generated videos' dynamic degree. Combined with a progressive\ntraining scheme with increasingly higher resolution and FPS, and a multi-source\ntraining scheme with mixed natural and synthetic data, Lumina-Video achieves\nremarkable aesthetic quality and motion smoothness at high training and\ninference efficiency. We additionally propose Lumina-V2A, a video-to-audio\nmodel based on Next-DiT, to create synchronized sounds for generated videos.\nCodes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.\n","authors":["Dongyang Liu","Shicheng Li","Yutong Liu","Zhen Li","Kai Wang","Xinyue Li","Qi Qin","Yufei Liu","Yi Xin","Zhongyu Li","Bin Fu","Chenyang Si","Yuewen Cao","Conghui He","Ziwei Liu","Yu Qiao","Qibin Hou","Hongsheng Li","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2502.06782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10957v2","updated":"2025-02-12T10:06:09Z","published":"2025-01-19T06:11:02Z","title":"MARIO: A Mixed Annotation Framework For Polyp Segmentation","summary":"  Existing polyp segmentation models are limited by high labeling costs and the\nsmall size of datasets. Additionally, vast polyp datasets remain underutilized\nbecause these models typically rely on a single type of annotation. To address\nthis dilemma, we introduce MARIO, a mixed supervision model designed to\naccommodate various annotation types, significantly expanding the range of\nusable data. MARIO learns from underutilized datasets by incorporating five\nforms of supervision: pixel-level, box-level, polygon-level, scribblelevel, and\npoint-level. Each form of supervision is associated with a tailored loss that\neffectively leverages the supervision labels while minimizing the noise. This\nallows MARIO to move beyond the constraints of relying on a single annotation\ntype. Furthermore, MARIO primarily utilizes dataset with weak and cheap\nannotations, reducing the dependence on large-scale, fully annotated ones.\nExperimental results across five benchmark datasets demonstrate that MARIO\nconsistently outperforms existing methods, highlighting its efficacy in\nbalancing trade-offs between different forms of supervision and maximizing\npolyp segmentation performance\n","authors":["Haoyang Li","Yiwen Hu","Jun Wei","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2501.10957v2.pdf","comment":"Accepted by IEEE ISBI 2025 4-page paper"},{"id":"http://arxiv.org/abs/2411.01969v2","updated":"2025-02-12T09:58:15Z","published":"2024-11-04T10:44:46Z","title":"Toddlers' Active Gaze Behavior Supports Self-Supervised Object Learning","summary":"  Toddlers learn to recognize objects from different viewpoints with almost no\nsupervision. Recent works argue that toddlers develop this ability by mapping\nclose-in-time visual inputs to similar representations while interacting with\nobjects. High acuity vision is only available in the central visual field,\nwhich May explain why toddlers (much like adults) constantly move around their\ngaze during such interactions. It is unclear whether/how much toddlers curate\ntheir visual experience through these eye movements to support their learning\nof object representations. In this work, we explore whether a bio-inspired\nvisual learning model can harness toddlers' gaze behavior during a play session\nto develop view-invariant object recognition. Exploiting head-mounted eye\ntracking during dyadic play, we simulate toddlers' central visual field\nexperience by cropping image regions centered on the gaze location. This visual\nstream feeds time-based self-supervised learning algorithms. Our experiments\ndemonstrate that toddlers' gaze strategy supports the learning of invariant\nobject representations. Our analysis also reveals that the limited size of the\ncentral visual field where acuity is high is crucial for this. We further find\nthat toddlers' visual experience elicits more robust representations compared\nto adults', mostly because toddlers look at objects they hold themselves for\nlonger bouts. Overall, our work reveals how toddlers' gaze behavior supports\nself-supervised learning of view-invariant object recognition.\n","authors":["Zhengyang Yu","Arthur Aubret","Marcel C. Raabe","Jane Yang","Chen Yu","Jochen Triesch"],"pdf_url":"https://arxiv.org/pdf/2411.01969v2.pdf","comment":"20 pages, 15 figures"},{"id":"http://arxiv.org/abs/2404.02544v3","updated":"2025-02-12T09:52:45Z","published":"2024-04-03T08:01:00Z","title":"Semi-Supervised Unconstrained Head Pose Estimation in the Wild","summary":"  Existing research on unconstrained in-the-wild head pose estimation suffers\nfrom the flaws of its datasets, which consist of either numerous samples by\nnon-realistic synthesis or constrained collection, or small-scale natural\nimages yet with plausible manual annotations. This makes fully-supervised\nsolutions compromised due to the reliance on generous labels. To alleviate it,\nwe propose the first semi-supervised unconstrained head pose estimation method\nSemiUHPE, which can leverage abundant easily available unlabeled head images.\nTechnically, we choose semi-supervised rotation regression and adapt it to the\nerror-sensitive and label-scarce problem of unconstrained head pose. Our method\nis based on the observation that the aspect-ratio invariant cropping of wild\nheads is superior to previous landmark-based affine alignment given that\nlandmarks of unconstrained human heads are usually unavailable, especially for\nunderexplored non-frontal heads. Instead of using a pre-fixed threshold to\nfilter out pseudo labeled heads, we propose dynamic entropy based filtering to\nadaptively remove unlabeled outliers as training progresses by updating the\nthreshold in multiple stages. We then revisit the design of weak-strong\naugmentations and improve it by devising two novel head-oriented strong\naugmentations, termed pose-irrelevant cut-occlusion and pose-altering rotation\nconsistency respectively. Extensive experiments and ablation studies show that\nSemiUHPE outperforms its counterparts greatly on public benchmarks under both\nthe front-range and full-range settings. Furthermore, our proposed method is\nalso beneficial for solving other closely related problems, including generic\nobject rotation regression and 3D head reconstruction, demonstrating good\nversatility and extensibility. Code is in https://github.com/hnuzhy/SemiUHPE.\n","authors":["Huayi Zhou","Fei Jiang","Jin Yuan","Yong Rui","Hongtao Lu","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2404.02544v3.pdf","comment":"under review. Semi-Supervised Unconstrained Head Pose Estimation"},{"id":"http://arxiv.org/abs/2502.08254v1","updated":"2025-02-12T09:49:43Z","published":"2025-02-12T09:49:43Z","title":"UniCoRN: Unified Commented Retrieval Network with LMMs","summary":"  Multimodal retrieval methods have limitations in handling complex,\ncompositional queries that require reasoning about the visual content of both\nthe query and the retrieved entities. On the other hand, Large Multimodal\nModels (LMMs) can answer with language to more complex visual questions, but\nwithout the inherent ability to retrieve relevant entities to support their\nanswers. We aim to address these limitations with UniCoRN, a Unified Commented\nRetrieval Network that combines the strengths of composed multimodal retrieval\nmethods and generative language approaches, going beyond Retrieval-Augmented\nGeneration (RAG). We introduce an entity adapter module to inject the retrieved\nmultimodal entities back into the LMM, so it can attend to them while\ngenerating answers and comments. By keeping the base LMM frozen, UniCoRN\npreserves its original capabilities while being able to perform both retrieval\nand text generation tasks under a single integrated framework. To assess these\nnew abilities, we introduce the Commented Retrieval task (CoR) and a\ncorresponding dataset, with the goal of retrieving an image that accurately\nanswers a given question and generate an additional textual response that\nprovides further clarification and details about the visual information. We\ndemonstrate the effectiveness of UniCoRN on several datasets showing\nimprovements of +4.5% recall over the state of the art for composed multimodal\nretrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.\n","authors":["Maximilian Jaritz","Matthieu Guillaumin","Sabine Sternig","Loris Bazzani"],"pdf_url":"https://arxiv.org/pdf/2502.08254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02483v5","updated":"2025-02-12T09:39:06Z","published":"2024-09-04T07:20:01Z","title":"TASAR: Transfer-based Attack on Skeletal Action Recognition","summary":"  Skeletal sequence data, as a widely employed representation of human actions,\nare crucial in Human Activity Recognition (HAR). Recently, adversarial attacks\nhave been proposed in this area, which exposes potential security concerns, and\nmore importantly provides a good tool for model robustness test. Within this\nresearch, transfer-based attack is an important tool as it mimics the\nreal-world scenario where an attacker has no knowledge of the target model, but\nis under-explored in Skeleton-based HAR (S-HAR). Consequently, existing S-HAR\nattacks exhibit weak adversarial transferability and the reason remains largely\nunknown. In this paper, we investigate this phenomenon via the characterization\nof the loss function. We find that one prominent indicator of poor\ntransferability is the low smoothness of the loss function. Led by this\nobservation, we improve the transferability by properly smoothening the loss\nwhen computing the adversarial examples. This leads to the first Transfer-based\nAttack on Skeletal Action Recognition, TASAR. TASAR explores the smoothened\nmodel posterior of pre-trained surrogates, which is achieved by a new\npost-train Dual Bayesian optimization strategy. Furthermore, unlike existing\ntransfer-based methods which overlook the temporal coherence within sequences,\nTASAR incorporates motion dynamics into the Bayesian attack, effectively\ndisrupting the spatial-temporal coherence of S-HARs. For exhaustive evaluation,\nwe build the first large-scale robust S-HAR benchmark, comprising 7 S-HAR\nmodels, 10 attack methods, 3 S-HAR datasets and 2 defense models. Extensive\nresults demonstrate the superiority of TASAR. Our benchmark enables easy\ncomparisons for future studies, with the code available in the\nhttps://github.com/yunfengdiao/Skeleton-Robustness-Benchmark.\n","authors":["Yunfeng Diao","Baiqi Wu","Ruixuan Zhang","Ajian Liu","Xiaoshuai Hao","Xingxing Wei","Meng Wang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02483v5.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08244v1","updated":"2025-02-12T09:38:41Z","published":"2025-02-12T09:38:41Z","title":"FloVD: Optical Flow Meets Video Diffusion Model for Enhanced\n  Camera-Controlled Video Synthesis","summary":"  This paper presents FloVD, a novel optical-flow-based video diffusion model\nfor camera-controllable video generation. FloVD leverages optical flow maps to\nrepresent motions of the camera and moving objects. This approach offers two\nkey benefits. Since optical flow can be directly estimated from videos, our\napproach allows for the use of arbitrary training videos without ground-truth\ncamera parameters. Moreover, as background optical flow encodes 3D correlation\nacross different viewpoints, our method enables detailed camera control by\nleveraging the background motion. To synthesize natural object motion while\nsupporting detailed camera control, our framework adopts a two-stage video\nsynthesis pipeline consisting of optical flow generation and flow-conditioned\nvideo synthesis. Extensive experiments demonstrate the superiority of our\nmethod over previous approaches in terms of accurate camera control and natural\nobject motion synthesis.\n","authors":["Wonjoon Jin","Qi Dai","Chong Luo","Seung-Hwan Baek","Sunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2502.08244v1.pdf","comment":"Project website: https://jinwonjoon.github.io/flovd_site/"},{"id":"http://arxiv.org/abs/2406.06230v2","updated":"2025-02-12T09:27:59Z","published":"2024-06-10T13:00:22Z","title":"UEMM-Air: A Synthetic Multi-modal Dataset for Unmanned Aerial Vehicle\n  Object Detection","summary":"  The development of multi-modal learning for Unmanned Aerial Vehicles (UAVs)\ntypically relies on a large amount of pixel-aligned multi-modal image data.\nHowever, existing datasets face challenges such as limited modalities, high\nconstruction costs, and imprecise annotations. To this end, we propose a\nsynthetic multi-modal UAV-based multi-task dataset, UEMM-Air. Specifically, we\nsimulate various UAV flight scenarios and object types using the Unreal Engine\n(UE). Then we design the UAV's flight logic to automatically collect data from\ndifferent scenarios, perspectives, and altitudes. Furthermore, we propose a\nnovel heuristic automatic annotation algorithm to generate accurate object\ndetection labels. Finally, we utilize labels to generate text descriptions of\nimages to make our UEMM-Air support more cross-modality tasks. In total, our\nUEMM-Air consists of 120k pairs of images with 6 modalities and precise\nannotations. Moreover, we conduct numerous experiments and establish new\nbenchmark results on our dataset. We also found that models pre-trained on\nUEMM-Air exhibit better performance on downstream tasks compared to other\nsimilar datasets. The dataset is publicly available\n(https://github.com/1e12Leon/UEMM-Air) to support the research of multi-modal\ntasks on UAVs.\n","authors":["Liang Yao","Fan Liu","Shengxiang Xu","Chuanyi Zhang","Xing Ma","Jianyu Jiang","Zequan Wang","Shimin Di","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.06230v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01993v2","updated":"2025-02-12T09:25:56Z","published":"2025-02-04T04:11:29Z","title":"One Diffusion Step to Real-World Super-Resolution via Flow Trajectory\n  Distillation","summary":"  Diffusion models (DMs) have significantly advanced the development of\nreal-world image super-resolution (Real-ISR), but the computational cost of\nmulti-step diffusion models limits their application. One-step diffusion models\ngenerate high-quality images in a one sampling step, greatly reducing\ncomputational overhead and inference latency. However, most existing one-step\ndiffusion methods are constrained by the performance of the teacher model,\nwhere poor teacher performance results in image artifacts. To address this\nlimitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique\nbased on flow matching models. We use the state-of-the-art diffusion model\nFLUX.1-dev as both the teacher model and the base model. First, we introduce\nFlow Trajectory Distillation (FTD) to distill a multi-step flow matching model\ninto a one-step Real-ISR. Second, to improve image realism and address\nhigh-frequency artifact issues in generated images, we propose TV-LPIPS as a\nperceptual loss and introduce Attention Diversification Loss (ADL) as a\nregularization term to reduce token similarity in transformer, thereby\neliminating high-frequency artifacts. Comprehensive experiments demonstrate\nthat our method outperforms existing one-step diffusion-based Real-ISR methods.\nThe code and model will be released at https://github.com/JianzeLi-114/FluxSR.\n","authors":["Jianze Li","Jiezhang Cao","Yong Guo","Wenbo Li","Yulun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.01993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08234v1","updated":"2025-02-12T09:21:40Z","published":"2025-02-12T09:21:40Z","title":"Learning Human Skill Generators at Key-Step Levels","summary":"  We are committed to learning human skill generators at key-step levels. The\ngeneration of skills is a challenging endeavor, but its successful\nimplementation could greatly facilitate human skill learning and provide more\nexperience for embodied intelligence. Although current video generation models\ncan synthesis simple and atomic human operations, they struggle with human\nskills due to their complex procedure process. Human skills involve multi-step,\nlong-duration actions and complex scene transitions, so the existing naive\nauto-regressive methods for synthesizing long videos cannot generate human\nskills. To address this, we propose a novel task, the Key-step Skill Generation\n(KS-Gen), aimed at reducing the complexity of generating human skill videos.\nGiven the initial state and a skill description, the task is to generate video\nclips of key steps to complete the skill, rather than a full-length video. To\nsupport this task, we introduce a carefully curated dataset and define multiple\nevaluation metrics to assess performance. Considering the complexity of KS-Gen,\nwe propose a new framework for this task. First, a multimodal large language\nmodel (MLLM) generates descriptions for key steps using retrieval argument.\nSubsequently, we use a Key-step Image Generator (KIG) to address the\ndiscontinuity between key steps in skill videos. Finally, a video generation\nmodel uses these descriptions and key-step images to generate video clips of\nthe key steps with high temporal consistency. We offer a detailed analysis of\nthe results, hoping to provide more insights on human skill generation. All\nmodels and data are available at https://github.com/MCG-NJU/KS-Gen.\n","authors":["Yilu Wu","Chenhui Zhu","Shuai Wang","Hanlin Wang","Jing Wang","Zhaoxiang Zhang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08233v1","updated":"2025-02-12T09:21:16Z","published":"2025-02-12T09:21:16Z","title":"Plantation Monitoring Using Drone Images: A Dataset and Performance\n  Review","summary":"  Automatic monitoring of tree plantations plays a crucial role in agriculture.\nFlawless monitoring of tree health helps farmers make informed decisions\nregarding their management by taking appropriate action. Use of drone images\nfor automatic plantation monitoring can enhance the accuracy of the monitoring\nprocess, while still being affordable to small farmers in developing countries\nsuch as India. Small, low cost drones equipped with an RGB camera can capture\nhigh-resolution images of agricultural fields, allowing for detailed analysis\nof the well-being of the plantations. Existing methods of automated plantation\nmonitoring are mostly based on satellite images, which are difficult to get for\nthe farmers. We propose an automated system for plantation health monitoring\nusing drone images, which are becoming easier to get for the farmers. We\npropose a dataset of images of trees with three categories: ``Good health\",\n``Stunted\", and ``Dead\". We annotate the dataset using CVAT annotation tool,\nfor use in research purposes. We experiment with different well-known CNN\nmodels to observe their performance on the proposed dataset. The initial low\naccuracy levels show the complexity of the proposed dataset. Further, our study\nrevealed that, depth-wise convolution operation embedded in a deep CNN model,\ncan enhance the performance of the model on drone dataset. Further, we apply\nstate-of-the-art object detection models to identify individual trees to better\nmonitor them automatically.\n","authors":["Yashwanth Karumanchi","Gudala Laxmi Prasanna","Snehasis Mukherjee","Nagesh Kolagani"],"pdf_url":"https://arxiv.org/pdf/2502.08233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08226v1","updated":"2025-02-12T09:12:30Z","published":"2025-02-12T09:12:30Z","title":"TRISHUL: Towards Region Identification and Screen Hierarchy\n  Understanding for Large VLM based GUI Agents","summary":"  Recent advancements in Large Vision Language Models (LVLMs) have enabled the\ndevelopment of LVLM-based Graphical User Interface (GUI) agents under various\nparadigms. Training-based approaches, such as CogAgent and SeeClick, struggle\nwith cross-dataset and cross-platform generalization due to their reliance on\ndataset-specific training. Generalist LVLMs, such as GPT-4V, employ\nSet-of-Marks (SoM) for action grounding, but obtaining SoM labels requires\nmetadata like HTML source, which is not consistently available across\nplatforms. Moreover, existing methods often specialize in singular GUI tasks\nrather than achieving comprehensive GUI understanding. To address these\nlimitations, we introduce TRISHUL, a novel, training-free agentic framework\nthat enhances generalist LVLMs for holistic GUI comprehension. Unlike prior\nworks that focus on either action grounding (mapping instructions to GUI\nelements) or GUI referring (describing GUI elements given a location), TRISHUL\nseamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen\nParsing (HSP) and the Spatially Enhanced Element Description (SEED) module,\nwhich work synergistically to provide multi-granular, spatially, and\nsemantically enriched representations of GUI elements. Our results demonstrate\nTRISHUL's superior performance in action grounding across the ScreenSpot,\nVisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring,\nTRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new\nstandard for robust and adaptable GUI comprehension.\n","authors":["Kunal Singh","Shreyas Singh","Mukund Khanna"],"pdf_url":"https://arxiv.org/pdf/2502.08226v1.pdf","comment":"Under review at ICML 2025, 8 pages 5 figures"},{"id":"http://arxiv.org/abs/2502.07183v2","updated":"2025-02-12T09:07:32Z","published":"2025-02-11T02:14:49Z","title":"Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog\n  Robots Assisting the Visually Impaired","summary":"  Guide dog robots offer promising solutions to enhance mobility and safety for\nvisually impaired individuals, addressing the limitations of traditional guide\ndogs, particularly in perceptual intelligence and communication. With the\nemergence of Vision-Language Models (VLMs), robots are now capable of\ngenerating natural language descriptions of their surroundings, aiding in safer\ndecision-making. However, existing VLMs often struggle to accurately interpret\nand convey spatial relationships, which is crucial for navigation in complex\nenvironments such as street crossings. We introduce the Space-Aware Instruction\nTuning (SAIT) dataset and the Space-Aware Benchmark (SA-Bench) to address the\nlimitations of current VLMs in understanding physical environments. Our\nautomated data generation pipeline focuses on the virtual path to the\ndestination in 3D space and the surroundings, enhancing environmental\ncomprehension and enabling VLMs to provide more accurate guidance to visually\nimpaired individuals. We also propose an evaluation protocol to assess VLM\neffectiveness in delivering walking guidance. Comparative experiments\ndemonstrate that our space-aware instruction-tuned model outperforms\nstate-of-the-art algorithms. We have fully open-sourced the SAIT dataset and\nSA-Bench, along with the related code, at\nhttps://github.com/byungokhan/Space-awareVLM\n","authors":["ByungOk Han","Woo-han Yun","Beom-Su Seo","Jaehong Kim"],"pdf_url":"https://arxiv.org/pdf/2502.07183v2.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2502.08221v1","updated":"2025-02-12T09:01:25Z","published":"2025-02-12T09:01:25Z","title":"Take What You Need: Flexible Multi-Task Semantic Communications with\n  Channel Adaptation","summary":"  The growing demand for efficient semantic communication systems capable of\nmanaging diverse tasks and adapting to fluctuating channel conditions has\ndriven the development of robust, resource-efficient frameworks. This article\nintroduces a novel channel-adaptive and multi-task-aware semantic communication\nframework based on a masked auto-encoder architecture. Our framework optimizes\nthe transmission of meaningful information by incorporating a multi-task-aware\nscoring mechanism that identifies and prioritizes semantically significant data\nacross multiple concurrent tasks. A channel-aware extractor is employed to\ndynamically select relevant information in response to real-time channel\nconditions. By jointly optimizing semantic relevance and transmission\nefficiency, the framework ensures minimal performance degradation under\nresource constraints. Experimental results demonstrate the superior performance\nof our framework compared to conventional methods in tasks such as image\nreconstruction and object detection. These results underscore the framework's\nadaptability to heterogeneous channel environments and its scalability for\nmulti-task applications, positioning it as a promising solution for\nnext-generation semantic communication networks.\n","authors":["Xiang Chen","Shuying Gan","Chenyuan Feng","Xijun Wang","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2502.08221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08216v1","updated":"2025-02-12T08:51:33Z","published":"2025-02-12T08:51:33Z","title":"Deepfake Detection with Spatio-Temporal Consistency and Attention","summary":"  Deepfake videos are causing growing concerns among communities due to their\never-increasing realism. Naturally, automated detection of forged Deepfake\nvideos is attracting a proportional amount of interest of researchers. Current\nmethods for detecting forged videos mainly rely on global frame features and\nunder-utilize the spatio-temporal inconsistencies found in the manipulated\nvideos. Moreover, they fail to attend to manipulation-specific subtle and\nwell-localized pattern variations along both spatial and temporal dimensions.\nAddressing these gaps, we propose a neural Deepfake detector that focuses on\nthe localized manipulative signatures of the forged videos at individual frame\nlevel as well as frame sequence level. Using a ResNet backbone, it strengthens\nthe shallow frame-level feature learning with a spatial attention mechanism.\nThe spatial stream of the model is further helped by fusing texture enhanced\nshallow features with the deeper features. Simultaneously, the model processes\nframe sequences with a distance attention mechanism that further allows fusion\nof temporal attention maps with the learned features at the deeper layers. The\noverall model is trained to detect forged content as a classifier. We evaluate\nour method on two popular large data sets and achieve significant performance\nover the state-of-the-art methods.Moreover, our technique also provides memory\nand computational advantages over the competitive techniques.\n","authors":["Yunzhuo Chen","Naveed Akhtar","Nur Al Hasan Haldar","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2502.08216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06606v2","updated":"2025-02-12T08:49:38Z","published":"2025-02-10T16:04:33Z","title":"MaterialFusion: High-Quality, Zero-Shot, and Controllable Material\n  Transfer with Diffusion Models","summary":"  Manipulating the material appearance of objects in images is critical for\napplications like augmented reality, virtual prototyping, and digital content\ncreation. We present MaterialFusion, a novel framework for high-quality\nmaterial transfer that allows users to adjust the degree of material\napplication, achieving an optimal balance between new material properties and\nthe object's original features. MaterialFusion seamlessly integrates the\nmodified object into the scene by maintaining background consistency and\nmitigating boundary artifacts. To thoroughly evaluate our approach, we have\ncompiled a dataset of real-world material transfer examples and conducted\ncomplex comparative analyses. Through comprehensive quantitative evaluations\nand user studies, we demonstrate that MaterialFusion significantly outperforms\nexisting methods in terms of quality, user control, and background\npreservation. Code is available at\nhttps://github.com/ControlGenAI/MaterialFusion.\n","authors":["Kamil Garifullin","Maxim Nikolaev","Andrey Kuznetsov","Aibek Alanov"],"pdf_url":"https://arxiv.org/pdf/2502.06606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08200v1","updated":"2025-02-12T08:24:36Z","published":"2025-02-12T08:24:36Z","title":"ActiveSSF: An Active-Learning-Guided Self-Supervised Framework for\n  Long-Tailed Megakaryocyte Classification","summary":"  Precise classification of megakaryocytes is crucial for diagnosing\nmyelodysplastic syndromes. Although self-supervised learning has shown promise\nin medical image analysis, its application to classifying megakaryocytes in\nstained slides faces three main challenges: (1) pervasive background noise that\nobscures cellular details, (2) a long-tailed distribution that limits data for\nrare subtypes, and (3) complex morphological variations leading to high\nintra-class variability. To address these issues, we propose the ActiveSSF\nframework, which integrates active learning with self-supervised pretraining.\nSpecifically, our approach employs Gaussian filtering combined with K-means\nclustering and HSV analysis (augmented by clinical prior knowledge) for\naccurate region-of-interest extraction; an adaptive sample selection mechanism\nthat dynamically adjusts similarity thresholds to mitigate class imbalance; and\nprototype clustering on labeled samples to overcome morphological complexity.\nExperimental results on clinical megakaryocyte datasets demonstrate that\nActiveSSF not only achieves state-of-the-art performance but also significantly\nimproves recognition accuracy for rare subtypes. Moreover, the integration of\nthese advanced techniques further underscores the practical potential of\nActiveSSF in clinical settings. To foster further research, the code and\ndatasets will be publicly released in the future.\n","authors":["Linghao Zhuang","Ying Zhang","Gege Yuan","Xingyue Zhao","Zhiping Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.08200v1.pdf","comment":"6 pages, submitted to EMBC 2025"},{"id":"http://arxiv.org/abs/2501.10967v2","updated":"2025-02-12T08:10:06Z","published":"2025-01-19T07:00:46Z","title":"Advancing General Multimodal Capability of Vision-language Models with\n  Pyramid-descent Visual Position Encoding","summary":"  Vision-language Models (VLMs) have shown remarkable capabilities in advancing\ngeneral artificial intelligence, yet the irrational encoding of visual\npositions persists in inhibiting the models' comprehensive perception\nperformance across different levels of granularity. In this work, we propose\nPyramid-descent Visual Position Encoding (PyPE), a novel approach designed to\nenhance the perception of visual tokens within VLMs. By assigning visual\nposition indexes from the periphery to the center and expanding the central\nreceptive field incrementally, PyPE addresses the limitations of traditional\nraster-scan methods and mitigates the long-term decay effects induced by Rotary\nPosition Embedding (RoPE). Our method reduces the relative distance between\ninterrelated visual elements and instruction tokens, promoting a more rational\nallocation of attention weights and allowing for a multi-granularity perception\nof visual elements and countering the over-reliance on anchor tokens. Extensive\nexperimental evaluations demonstrate that PyPE consistently improves the\ngeneral capabilities of VLMs across various sizes. Code is available at\nhttps://github.com/SakuraTroyChen/PyPE.\n","authors":["Zhanpeng Chen","Mingxiao Li","Ziyang Chen","Nan Du","Xiaolong Li","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2501.10967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08189v1","updated":"2025-02-12T07:59:41Z","published":"2025-02-12T07:59:41Z","title":"AnyCharV: Bootstrap Controllable Character Video Generation with\n  Fine-to-Coarse Guidance","summary":"  Character video generation is a significant real-world application focused on\nproducing high-quality videos featuring specific characters. Recent\nadvancements have introduced various control signals to animate static\ncharacters, successfully enhancing control over the generation process.\nHowever, these methods often lack flexibility, limiting their applicability and\nmaking it challenging for users to synthesize a source character into a desired\ntarget scene. To address this issue, we propose a novel framework, AnyCharV,\nthat flexibly generates character videos using arbitrary source characters and\ntarget scenes, guided by pose information. Our approach involves a two-stage\ntraining process. In the first stage, we develop a base model capable of\nintegrating the source character with the target scene using pose guidance. The\nsecond stage further bootstraps controllable generation through a self-boosting\nmechanism, where we use the generated video in the first stage and replace the\nfine mask with the coarse one, enabling training outcomes with better\npreservation of character details. Experimental results demonstrate the\neffectiveness and robustness of our proposed method. Our project page is\nhttps://anycharv.github.io.\n","authors":["Zhao Wang","Hao Wen","Lingting Zhu","Chenming Shang","Yujiu Yang","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2502.08189v1.pdf","comment":"15 pages, 9 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.02385v2","updated":"2025-02-12T07:55:10Z","published":"2025-01-04T21:23:36Z","title":"Guiding Medical Vision-Language Models with Explicit Visual Prompts:\n  Framework Design and Comprehensive Exploration of Prompt Variations","summary":"  While mainstream vision-language models (VLMs) have advanced rapidly in\nunderstanding image level information, they still lack the ability to focus on\nspecific areas designated by humans. Rather, they typically rely on large\nvolumes of high-quality image-text paired data to learn and generate posterior\nattention maps. To address this critical issue, we propose leveraging visual\nprompts:simple visual markers in various forms to guide and enhance the\nformation of region-specific attention. Thus, we introduce MedVP, a pioneering\nframework that integrates medical entity extraction, visual prompt generation,\nand dataset adaptation for visual prompt guided fine-tuning. We successfully\noutperform recent state-of-the-art large models across multiple medical VQA\ndatasets. Extensive experiments and Human evaluation are conducted to analyze\nthe impact of different visual prompt forms and how they contribute to\nperformance improvement. The results demonstrate both the effectiveness and\nclinical significance of our approach.\n","authors":["Kangyu Zhu","Ziyuan Qin","Huahui Yi","Zekun Jiang","Qicheng Lao","Shaoting Zhang","Kang Li"],"pdf_url":"https://arxiv.org/pdf/2501.02385v2.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.08181v1","updated":"2025-02-12T07:39:44Z","published":"2025-02-12T07:39:44Z","title":"Latest Advancements Towards Catastrophic Forgetting under Data Scarcity:\n  A Comprehensive Survey on Few-Shot Class Incremental Learning","summary":"  Data scarcity significantly complicates the continual learning problem, i.e.,\nhow a deep neural network learns in dynamic environments with very few samples.\nHowever, the latest progress of few-shot class incremental learning (FSCIL)\nmethods and related studies show insightful knowledge on how to tackle the\nproblem. This paper presents a comprehensive survey on FSCIL that highlights\nseveral important aspects i.e. comprehensive and formal objectives of FSCIL\napproaches, the importance of prototype rectifications, the new learning\nparadigms based on pre-trained model and language-guided mechanism, the deeper\nanalysis of FSCIL performance metrics and evaluation, and the practical\ncontexts of FSCIL in various areas. Our extensive discussion presents the open\nchallenges, potential solutions, and future directions of FSCIL.\n","authors":["M. Anwar Ma'sum","Mahardhika Pratama","Igor Skrjanc"],"pdf_url":"https://arxiv.org/pdf/2502.08181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07381v2","updated":"2025-02-12T07:37:30Z","published":"2025-02-11T08:57:45Z","title":"Spatial Degradation-Aware and Temporal Consistent Diffusion Model for\n  Compressed Video Super-Resolution","summary":"  Due to limitations of storage and bandwidth, videos stored and transmitted on\nthe Internet are usually low-quality with low-resolution and compression noise.\nAlthough video super-resolution (VSR) is an efficient technique to enhance\nvideo resolution, relatively VSR methods focus on compressed videos. Directly\napplying general VSR approaches leads to the failure of improving practical\nvideos, especially when frames are highly compressed at a low bit rate.\nRecently, diffusion models have achieved superior performance in low-level\nvisual tasks, and their high-realism generation capability enables them to be\napplied in VSR. To synthesize more compression-lost details and refine temporal\nconsistency, we propose a novel Spatial Degradation-Aware and Temporal\nConsistent (SDATC) diffusion model for compressed VSR. Specifically, we\nintroduce a distortion Control module (DCM) to modulate diffusion model inputs\nand guide the generation. Next, the diffusion model executes the denoising\nprocess for texture generation with fine-tuned spatial prompt-based\ncompression-aware module (PCAM) and spatio-temporal attention module (STAM).\nPCAM extracts features to encode specific compression information dynamically.\nSTAM extends the spatial attention mechanism to a spatio-temporal dimension for\ncapturing temporal correlation. Extensive experimental results on benchmark\ndatasets demonstrate the effectiveness of the proposed modules in enhancing\ncompressed videos.\n","authors":["Hongyu An","Xinfeng Zhang","Shijie Zhao","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07381v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07531v2","updated":"2025-02-12T07:35:56Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08169v1","updated":"2025-02-12T07:23:26Z","published":"2025-02-12T07:23:26Z","title":"CoDynTrust: Robust Asynchronous Collaborative Perception via Dynamic\n  Feature Trust Modulus","summary":"  Collaborative perception, fusing information from multiple agents, can extend\nperception range so as to improve perception performance. However, temporal\nasynchrony in real-world environments, caused by communication delays, clock\nmisalignment, or sampling configuration differences, can lead to information\nmismatches. If this is not well handled, then the collaborative performance is\npatchy, and what's worse safety accidents may occur. To tackle this challenge,\nwe propose CoDynTrust, an uncertainty-encoded asynchronous fusion perception\nframework that is robust to the information mismatches caused by temporal\nasynchrony. CoDynTrust generates dynamic feature trust modulus (DFTM) for each\nregion of interest by modeling aleatoric and epistemic uncertainty as well as\nselectively suppressing or retaining single-vehicle features, thereby\nmitigating information mismatches. We then design a multi-scale fusion module\nto handle multi-scale feature maps processed by DFTM. Compared to existing\nworks that also consider asynchronous collaborative perception, CoDynTrust\ncombats various low-quality information in temporally asynchronous scenarios\nand allows uncertainty to be propagated to downstream tasks such as planning\nand control. Experimental results demonstrate that CoDynTrust significantly\nreduces performance degradation caused by temporal asynchrony across multiple\ndatasets, achieving state-of-the-art detection performance even with temporal\nasynchrony. The code is available at https://github.com/CrazyShout/CoDynTrust.\n","authors":["Yunjiang Xu","Lingzhi Li","Jin Wang","Benyuan Yang","Zhiwen Wu","Xinhong Chen","Jianping Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08169v1.pdf","comment":"7 pages, 5 figures, conference"},{"id":"http://arxiv.org/abs/2410.01521v2","updated":"2025-02-12T07:22:41Z","published":"2024-10-02T13:10:57Z","title":"MiraGe: Editable 2D Images using Gaussian Splatting","summary":"  Implicit Neural Representations (INRs) approximate discrete data through\ncontinuous functions and are commonly used for encoding 2D images. Traditional\nimage-based INRs employ neural networks to map pixel coordinates to RGB values,\ncapturing shapes, colors, and textures within the network's weights. Recently,\nGaussianImage has been proposed as an alternative, using Gaussian functions\ninstead of neural networks to achieve comparable quality and compression. Such\na solution obtains a quality and compression ratio similar to classical INR\nmodels but does not allow image modification. In contrast, our work introduces\na novel method, MiraGe, which uses mirror reflections to perceive 2D images in\n3D space and employs flat-controlled Gaussians for precise 2D image editing.\nOur approach improves the rendering quality and allows realistic image\nmodifications, including human-inspired perception of photos in the 3D world.\nThanks to modeling images in 3D space, we obtain the illusion of 3D-based\nmodification in 2D images. We also show that our Gaussian representation can be\neasily combined with a physics engine to produce physics-based modification of\n2D images. Consequently, MiraGe allows for better quality than the standard\napproach and natural modification of 2D images\n","authors":["Joanna Waczyńska","Tomasz Szczepanik","Piotr Borycki","Sławomir Tadeja","Thomas Bohné","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2410.01521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08167v1","updated":"2025-02-12T07:14:54Z","published":"2025-02-12T07:14:54Z","title":"DNNs May Determine Major Properties of Their Outputs Early, with Timing\n  Possibly Driven by Bias","summary":"  This paper argues that deep neural networks (DNNs) mostly determine their\noutputs during the early stages of inference, where biases inherent in the\nmodel play a crucial role in shaping this process. We draw a parallel between\nthis phenomenon and human decision-making, which often relies on fast,\nintuitive heuristics. Using diffusion models (DMs) as a case study, we\ndemonstrate that DNNs often make early-stage decision-making influenced by the\ntype and extent of bias in their design and training. Our findings offer a new\nperspective on bias mitigation, efficient inference, and the interpretation of\nmachine learning systems. By identifying the temporal dynamics of\ndecision-making in DNNs, this paper aims to inspire further discussion and\nresearch within the machine learning community.\n","authors":["Song Park","Sanghyuk Chun","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2502.08167v1.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2309.14054v2","updated":"2025-02-12T07:00:51Z","published":"2023-09-25T11:36:20Z","title":"Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning\n  in Generative Adversarial Networks","summary":"  Owing to the growing concerns about privacy and regulatory compliance, it is\ndesirable to regulate the output of generative models. To that end, the\nobjective of this work is to prevent the generation of outputs containing\nundesired features from a pre-trained Generative Adversarial Network (GAN)\nwhere the underlying training data set is inaccessible. Our approach is\ninspired by the observation that the parameter space of GANs exhibits\nmeaningful directions that can be leveraged to suppress specific undesired\nfeatures. However, such directions usually result in the degradation of the\nquality of generated samples. Our proposed two-stage method, known as\n'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also\nmaintaining the quality of generated samples. In the initial stage, we adapt a\npre-trained GAN on a set of negative samples (containing undesired features)\nprovided by the user. Subsequently, we train the original pre-trained GAN using\npositive samples, along with a repulsion regularizer. This regularizer\nencourages the learned model parameters to move away from the parameters of the\nadapted model (first stage) while not degrading the generation quality. We\nprovide theoretical insights into the proposed method. To the best of our\nknowledge, our approach stands as the first method addressing unlearning within\nthe realm of high-fidelity GANs (such as StyleGAN). We validate the\neffectiveness of our method through comprehensive experiments, encompassing\nboth class-level unlearning on the MNIST and AFHQ dataset and feature-level\nunlearning tasks on the CelebA-HQ dataset. Our code and implementation is\navailable at: https://github.com/atriguha/Adapt_Unlearn.\n","authors":["Piyush Tiwary","Atri Guha","Subhodip Panda","Prathosh A. P"],"pdf_url":"https://arxiv.org/pdf/2309.14054v2.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2409.11744v2","updated":"2025-02-12T06:53:14Z","published":"2024-09-18T06:56:06Z","title":"Exploring Gaze Pattern Differences Between ASD and TD Children Using\n  Internal Cluster Validity Indices","summary":"  Autism Spectrum Disorder (ASD) affects children's social and communication\nabilities, with eye-tracking widely used to identify atypical gaze patterns.\nWhile unsupervised clustering can automate the creation of areas of interest\nfor gaze feature extraction, the use of internal cluster validity indices, like\nSilhouette Coefficient, to distinguish gaze pattern differences between ASD and\ntypically developing (TD) children remains underexplored. We explore whether\ninternal cluster validity indices can distinguish ASD from TD children.\nSpecifically, we apply seven clustering algorithms to gaze points and extract\n63 internal cluster validity indices to reveal correlations with ASD diagnosis.\nUsing these indices, we train predictive models for ASD diagnosis. Experiments\non three datasets demonstrate high predictive accuracy (81\\% AUC), validating\nthe effectiveness of these indices.\n","authors":["Weiyan Shi","Haihong Zhang","Ruiqing Ding","YongWei Zhu","Wei Wang","Kenny Tsu Wei Choo"],"pdf_url":"https://arxiv.org/pdf/2409.11744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14316v2","updated":"2025-02-12T06:48:03Z","published":"2025-01-24T08:21:35Z","title":"PAID: A Framework of Product-Centric Advertising Image Design","summary":"  Creating visually appealing advertising images is often a labor-intensive and\ntime-consuming process. Is it possible to automatically generate such images\nusing only basic product information--specifically, a product foreground image,\ntaglines, and a target size? Existing methods mainly focus on parts of the\nproblem and fail to provide a comprehensive solution. To address this gap, we\npropose a novel multistage framework called Product-Centric Advertising Image\nDesign (PAID). It consists of four sequential stages to highlight product\nforegrounds and taglines while achieving overall image aesthetics: prompt\ngeneration, layout generation, background image generation, and graphics\nrendering. Different expert models are designed and trained for the first three\nstages: First, we use a visual language model (VLM) to generate background\nprompts that match the products. Next, a VLM-based layout generation model\narranges the placement of product foregrounds, graphic elements (taglines and\ndecorative underlays), and various nongraphic elements (objects from the\nbackground prompt). Following this, we train an SDXL-based image generation\nmodel that can simultaneously accept prompts, layouts, and foreground controls.\nTo support the PAID framework, we create corresponding datasets with over\n50,000 labeled images. Extensive experimental results and online A/B tests\ndemonstrate that PAID can produce more visually appealing advertising images.\n","authors":["Hongyu Chen","Min Zhou","Jing Jiang","Jiale Chen","Yang Lu","Bo Xiao","Tiezheng Ge","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.14316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17133v2","updated":"2025-02-12T06:47:37Z","published":"2024-02-27T01:57:02Z","title":"SAM-DiffSR: Structure-Modulated Diffusion Model for Image\n  Super-Resolution","summary":"  Diffusion-based super-resolution (SR) models have recently garnered\nsignificant attention due to their potent restoration capabilities. But\nconventional diffusion models perform noise sampling from a single\ndistribution, constraining their ability to handle real-world scenes and\ncomplex textures across semantic regions. With the success of segment anything\nmodel (SAM), generating sufficiently fine-grained region masks can enhance the\ndetail recovery of diffusion-based SR model. However, directly integrating SAM\ninto SR models will result in much higher computational cost. In this paper, we\npropose the SAM-DiffSR model, which can utilize the fine-grained structure\ninformation from SAM in the process of sampling noise to improve the image\nquality without additional computational cost during inference. In the process\nof training, we encode structural position information into the segmentation\nmask from SAM. Then the encoded mask is integrated into the forward diffusion\nprocess by modulating it to the sampled noise. This adjustment allows us to\nindependently adapt the noise mean within each corresponding segmentation area.\nThe diffusion model is trained to estimate this modulated noise. Crucially, our\nproposed framework does NOT change the reverse diffusion process and does NOT\nrequire SAM at inference. Experimental results demonstrate the effectiveness of\nour proposed method, showcasing superior performance in suppressing artifacts,\nand surpassing existing diffusion-based methods by 0.74 dB at the maximum in\nterms of PSNR on DIV2K dataset. The code and dataset are available at\nhttps://github.com/lose4578/SAM-DiffSR.\n","authors":["Chengcheng Wang","Zhiwei Hao","Yehui Tang","Jianyuan Guo","Yujie Yang","Kai Han","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17133v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16769v2","updated":"2025-02-12T06:39:07Z","published":"2024-11-25T04:17:24Z","title":"In-Context Experience Replay Facilitates Safety Red-Teaming of\n  Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) models have shown remarkable progress, but their\npotential to generate harmful content remains a critical concern in the ML\ncommunity. While various safety mechanisms have been developed, the field lacks\nsystematic tools for evaluating their effectiveness against real-world misuse\nscenarios. In this work, we propose ICER, a novel red-teaming framework that\nleverages Large Language Models (LLMs) and a bandit optimization-based\nalgorithm to generate interpretable and semantic meaningful problematic prompts\nby learning from past successful red-teaming attempts. Our ICER efficiently\nprobes safety mechanisms across different T2I models without requiring internal\naccess or additional training, making it broadly applicable to deployed\nsystems. Through extensive experiments, we demonstrate that ICER significantly\noutperforms existing prompt attack methods in identifying model vulnerabilities\nwhile maintaining high semantic similarity with intended content. By uncovering\nthat successful jailbreaking instances can systematically facilitate the\ndiscovery of new vulnerabilities, our work provides crucial insights for\ndeveloping more robust safety mechanisms in T2I systems.\n","authors":["Zhi-Yi Chin","Mario Fritz","Pin-Yu Chen","Wei-Chen Chiu"],"pdf_url":"https://arxiv.org/pdf/2411.16769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08150v1","updated":"2025-02-12T06:30:01Z","published":"2025-02-12T06:30:01Z","title":"Force Matching with Relativistic Constraints: A Physics-Inspired\n  Approach to Stable and Efficient Generative Modeling","summary":"  This paper introduces Force Matching (ForM), a novel framework for generative\nmodeling that represents an initial exploration into leveraging special\nrelativistic mechanics to enhance the stability of the sampling process. By\nincorporating the Lorentz factor, ForM imposes a velocity constraint, ensuring\nthat sample velocities remain bounded within a constant limit. This constraint\nserves as a fundamental mechanism for stabilizing the generative dynamics,\nleading to a more robust and controlled sampling process. We provide a rigorous\ntheoretical analysis demonstrating that the velocity constraint is preserved\nthroughout the sampling procedure within the ForM framework. To validate the\neffectiveness of our approach, we conduct extensive empirical evaluations. On\nthe \\textit{half-moons} dataset, ForM significantly outperforms baseline\nmethods, achieving the lowest Euclidean distance loss of \\textbf{0.714}, in\ncontrast to vanilla first-order flow matching (5.853) and first- and\nsecond-order flow matching (5.793). Additionally, we perform an ablation study\nto further investigate the impact of our velocity constraint, reaffirming the\nsuperiority of ForM in stabilizing the generative process. The theoretical\nguarantees and empirical results underscore the potential of integrating\nspecial relativity principles into generative modeling. Our findings suggest\nthat ForM provides a promising pathway toward achieving stable, efficient, and\nflexible generative processes. This work lays the foundation for future\nadvancements in high-dimensional generative modeling, opening new avenues for\nthe application of physical principles in machine learning.\n","authors":["Yang Cao","Bo Chen","Xiaoyu Li","Yingyu Liang","Zhizhou Sha","Zhenmei Shi","Zhao Song","Mingda Wan"],"pdf_url":"https://arxiv.org/pdf/2502.08150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02252v4","updated":"2025-02-12T06:27:34Z","published":"2024-07-02T13:17:49Z","title":"GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion\n  Models and Large Language Models","summary":"  Posters play a crucial role in marketing and advertising by enhancing visual\ncommunication and brand visibility, making significant contributions to\nindustrial design. With the latest advancements in controllable T2I diffusion\nmodels, increasing research has focused on rendering text within synthesized\nimages. Despite improvements in text rendering accuracy, the field of automatic\nposter generation remains underexplored. In this paper, we propose an automatic\nposter generation framework with text rendering capabilities leveraging LLMs,\nutilizing a triple-cross attention mechanism based on alignment learning. This\nframework aims to create precise poster text within a detailed contextual\nbackground. Additionally, the framework supports controllable fonts, adjustable\nimage resolution, and the rendering of posters with descriptions and text in\nboth English and Chinese.Furthermore, we introduce a high-resolution font\ndataset and a poster dataset with resolutions exceeding 1024 pixels. Our\napproach leverages the SDXL architecture. Extensive experiments validate our\nmethod's capability in generating poster images with complex and contextually\nrich backgrounds.Codes is available at\nhttps://github.com/OPPO-Mente-Lab/GlyphDraw2.\n","authors":["Jian Ma","Yonglin Deng","Chen Chen","Nanyang Du","Haonan Lu","Zhenyu Yang"],"pdf_url":"https://arxiv.org/pdf/2407.02252v4.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2408.15239v2","updated":"2025-02-12T06:26:29Z","published":"2024-08-27T17:57:14Z","title":"Generative Inbetweening: Adapting Image-to-Video Models for Keyframe\n  Interpolation","summary":"  We present a method for generating video sequences with coherent motion\nbetween a pair of input key frames. We adapt a pretrained large-scale\nimage-to-video diffusion model (originally trained to generate videos moving\nforward in time from a single input image) for key frame interpolation, i.e.,\nto produce a video in between two input frames. We accomplish this adaptation\nthrough a lightweight fine-tuning technique that produces a version of the\nmodel that instead predicts videos moving backwards in time from a single input\nimage. This model (along with the original forward-moving model) is\nsubsequently used in a dual-directional diffusion sampling process that\ncombines the overlapping model estimates starting from each of the two\nkeyframes. Our experiments show that our method outperforms both existing\ndiffusion-based methods and traditional frame interpolation techniques.\n","authors":["Xiaojuan Wang","Boyang Zhou","Brian Curless","Ira Kemelmacher-Shlizerman","Aleksander Holynski","Steven M. Seitz"],"pdf_url":"https://arxiv.org/pdf/2408.15239v2.pdf","comment":"Published at ICLR 2025; Project page:\n  https://svd-keyframe-interpolation.github.io/"},{"id":"http://arxiv.org/abs/2502.08149v1","updated":"2025-02-12T06:26:05Z","published":"2025-02-12T06:26:05Z","title":"Generalized Class Discovery in Instance Segmentation","summary":"  This work addresses the task of generalized class discovery (GCD) in instance\nsegmentation. The goal is to discover novel classes and obtain a model capable\nof segmenting instances of both known and novel categories, given labeled and\nunlabeled data. Since the real world contains numerous objects with long-tailed\ndistributions, the instance distribution for each class is inherently\nimbalanced. To address the imbalanced distributions, we propose an\ninstance-wise temperature assignment (ITA) method for contrastive learning and\nclass-wise reliability criteria for pseudo-labels. The ITA method relaxes\ninstance discrimination for samples belonging to head classes to enhance GCD.\nThe reliability criteria are to avoid excluding most pseudo-labels for tail\nclasses when training an instance segmentation network using pseudo-labels from\nGCD. Additionally, we propose dynamically adjusting the criteria to leverage\ndiverse samples in the early stages while relying only on reliable\npseudo-labels in the later stages. We also introduce an efficient soft\nattention module to encode object-specific representations for GCD. Finally, we\nevaluate our proposed method by conducting experiments on two settings:\nCOCO$_{half}$ + LVIS and LVIS + Visual Genome. The experimental results\ndemonstrate that the proposed method outperforms previous state-of-the-art\nmethods.\n","authors":["Cuong Manh Hoang","Yeejin Lee","Byeongkeun Kang"],"pdf_url":"https://arxiv.org/pdf/2502.08149v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2502.05206v2","updated":"2025-02-12T06:16:00Z","published":"2025-02-02T05:14:22Z","title":"Safety at Scale: A Comprehensive Survey of Large Model Safety","summary":"  The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.\n","authors":["Xingjun Ma","Yifeng Gao","Yixu Wang","Ruofan Wang","Xin Wang","Ye Sun","Yifan Ding","Hengyuan Xu","Yunhao Chen","Yunhan Zhao","Hanxun Huang","Yige Li","Jiaming Zhang","Xiang Zheng","Yang Bai","Zuxuan Wu","Xipeng Qiu","Jingfeng Zhang","Yiming Li","Jun Sun","Cong Wang","Jindong Gu","Baoyuan Wu","Siheng Chen","Tianwei Zhang","Yang Liu","Mingming Gong","Tongliang Liu","Shirui Pan","Cihang Xie","Tianyu Pang","Yinpeng Dong","Ruoxi Jia","Yang Zhang","Shiqing Ma","Xiangyu Zhang","Neil Gong","Chaowei Xiao","Sarah Erfani","Bo Li","Masashi Sugiyama","Dacheng Tao","James Bailey","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.05206v2.pdf","comment":"47 pages, 3 figures, 11 tables GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety"},{"id":"http://arxiv.org/abs/2410.20971v2","updated":"2025-02-12T05:52:11Z","published":"2024-10-28T12:43:47Z","title":"BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against\n  Jailbreak Attacks","summary":"  In this paper, we focus on black-box defense for VLMs against jailbreak\nattacks. Existing black-box defense methods are either unimodal or bimodal.\nUnimodal methods enhance either the vision or language module of the VLM, while\nbimodal methods robustify the model through text-image representation\nrealignment. However, these methods suffer from two limitations: 1) they fail\nto fully exploit the cross-modal information, or 2) they degrade the model\nperformance on benign inputs. To address these limitations, we propose a novel\nblue-team method BlueSuffix that defends target VLMs against jailbreak attacks\nwithout compromising its performance under black-box setting. BlueSuffix\nincludes three key components: 1) a visual purifier against jailbreak images,\n2) a textual purifier against jailbreak texts, and 3) a blue-team suffix\ngenerator using reinforcement fine-tuning for enhancing cross-modal robustness.\nWe empirically show on four VLMs (LLaVA, MiniGPT-4, InstructionBLIP, and\nGemini) and four safety benchmarks (Harmful Instruction, AdvBench,\nMM-SafetyBench, and RedTeam-2K) that BlueSuffix outperforms the baseline\ndefenses by a significant margin. Our BlueSuffix opens up a promising direction\nfor defending VLMs against jailbreak attacks. Code is available at\nhttps://github.com/Vinsonzyh/BlueSuffix.\n","authors":["Yunhan Zhao","Xiang Zheng","Lin Luo","Yige Li","Xingjun Ma","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.20971v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14415v2","updated":"2025-02-12T05:41:42Z","published":"2024-12-19T00:06:09Z","title":"DriveGPT: Scaling Autoregressive Behavior Models for Driving","summary":"  We present DriveGPT, a scalable behavior model for autonomous driving. We\nmodel driving as a sequential decision-making task, and learn a transformer\nmodel to predict future agent states as tokens in an autoregressive fashion. We\nscale up our model parameters and training data by multiple orders of\nmagnitude, enabling us to explore the scaling properties in terms of dataset\nsize, model parameters, and compute. We evaluate DriveGPT across different\nscales in a planning task, through both quantitative metrics and qualitative\nexamples, including closed-loop driving in complex real-world scenarios. In a\nseparate prediction task, DriveGPT outperforms state-of-the-art baselines and\nexhibits improved performance by pretraining on a large-scale dataset, further\nvalidating the benefits of data scaling.\n","authors":["Xin Huang","Eric M. Wolff","Paul Vernaza","Tung Phan-Minh","Hongge Chen","David S. Hayden","Mark Edmonds","Brian Pierce","Xinxin Chen","Pratik Elias Jacob","Xiaobai Chen","Chingiz Tairbekov","Pratik Agarwal","Tianshi Gao","Yuning Chai","Siddhartha Srinivasa"],"pdf_url":"https://arxiv.org/pdf/2412.14415v2.pdf","comment":"13 pages, 16 figures, 8 tables, and 1 video link"},{"id":"http://arxiv.org/abs/2502.08137v1","updated":"2025-02-12T05:41:25Z","published":"2025-02-12T05:41:25Z","title":"Riemannian Complex Hermit Positive Definite Convolution Network for\n  Polarimetric SAR Image Classification","summary":"  Deep learning can learn high-level semantic features in Euclidean space\neffectively for PolSAR images, while they need to covert the complex covariance\nmatrix into a feature vector or complex-valued vector as the network input.\nHowever, the complex covariance matrices are essentially a complex Hermit\npositive definite (HPD) matrix endowed in Riemannian manifold rather than\nEuclidean space. The matrix's real and imagery parts are with the same\nsignificance, as the imagery part represents the phase information. The matrix\nvectorization will destroy the geometric structure and manifold characteristics\nof complex covariance matrices. To learn complex HPD matrices directly, we\npropose a Riemannian complex HPD convolution network(HPD\\_CNN) for PolSAR\nimages. This method consists of a complex HPD unfolding network(HPDnet) and a\nCV-3DCNN enhanced network. The proposed complex HPDnet defines the HPD mapping,\nrectifying and the logEig layers to learn geometric features of complex\nmatrices. In addition, a fast eigenvalue decomposition method is designed to\nreduce computation burden. Finally, a Riemannian-to-Euclidean enhanced network\nis defined to enhance contextual information for classification. Experimental\nresults on two real PolSSAR datasets demonstrate the proposed method can\nachieve superior performance than the state-of-the-art methods especially in\nheterogeneous regions.\n","authors":["Junfei Shi","Mengmeng Nie","Yuke Li","Haiyan Jin","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2502.08137v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.14891v3","updated":"2025-02-12T05:38:07Z","published":"2024-09-23T10:38:20Z","title":"Observe Then Act: Asynchronous Active Vision-Action Model for Robotic\n  Manipulation","summary":"  In real-world scenarios, many robotic manipulation tasks are hindered by\nocclusions and limited fields of view, posing significant challenges for\npassive observation-based models that rely on fixed or wrist-mounted cameras.\nIn this paper, we investigate the problem of robotic manipulation under limited\nvisual observation and propose a task-driven asynchronous active vision-action\nmodel.Our model serially connects a camera Next-Best-View (NBV) policy with a\ngripper Next-Best Pose (NBP) policy, and trains them in a sensor-motor\ncoordination framework using few-shot reinforcement learning. This approach\nallows the agent to adjust a third-person camera to actively observe the\nenvironment based on the task goal, and subsequently infer the appropriate\nmanipulation actions.We trained and evaluated our model on 8\nviewpoint-constrained tasks in RLBench. The results demonstrate that our model\nconsistently outperforms baseline algorithms, showcasing its effectiveness in\nhandling visual constraints in manipulation tasks.\n","authors":["Guokang Wang","Hang Li","Shuyuan Zhang","Di Guo","Yanhong Liu","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2409.14891v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08134v1","updated":"2025-02-12T05:34:48Z","published":"2025-02-12T05:34:48Z","title":"A Survey on Data Curation for Visual Contrastive Learning: Why Crafting\n  Effective Positive and Negative Pairs Matters","summary":"  Visual contrastive learning aims to learn representations by contrasting\nsimilar (positive) and dissimilar (negative) pairs of data samples. The design\nof these pairs significantly impacts representation quality, training\nefficiency, and computational cost. A well-curated set of pairs leads to\nstronger representations and faster convergence. As contrastive pre-training\nsees wider adoption for solving downstream tasks, data curation becomes\nessential for optimizing its effectiveness. In this survey, we attempt to\ncreate a taxonomy of existing techniques for positive and negative pair\ncuration in contrastive learning, and describe them in detail.\n","authors":["Shasvat Desai","Debasmita Ghose","Deep Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2502.08134v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.09591v3","updated":"2025-02-12T05:19:36Z","published":"2024-04-15T09:01:47Z","title":"3D Gaussian Splatting as Markov Chain Monte Carlo","summary":"  While 3D Gaussian Splatting has recently become popular for neural rendering,\ncurrent methods rely on carefully engineered cloning and splitting strategies\nfor placing Gaussians, which can lead to poor-quality renderings, and reliance\non a good initialization. In this work, we rethink the set of 3D Gaussians as a\nrandom sample drawn from an underlying probability distribution describing the\nphysical representation of the scene-in other words, Markov Chain Monte Carlo\n(MCMC) samples. Under this view, we show that the 3D Gaussian updates can be\nconverted as Stochastic Gradient Langevin Dynamics (SGLD) updates by simply\nintroducing noise. We then rewrite the densification and pruning strategies in\n3D Gaussian Splatting as simply a deterministic state transition of MCMC\nsamples, removing these heuristics from the framework. To do so, we revise the\n'cloning' of Gaussians into a relocalization scheme that approximately\npreserves sample probability. To encourage efficient use of Gaussians, we\nintroduce a regularizer that promotes the removal of unused Gaussians. On\nvarious standard evaluation scenes, we show that our method provides improved\nrendering quality, easy control over the number of Gaussians, and robustness to\ninitialization.\n","authors":["Shakiba Kheradmand","Daniel Rebain","Gopal Sharma","Weiwei Sun","Jeff Tseng","Hossam Isack","Abhishek Kar","Andrea Tagliasacchi","Kwang Moo Yi"],"pdf_url":"https://arxiv.org/pdf/2404.09591v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11811v3","updated":"2025-02-12T05:16:17Z","published":"2024-08-21T17:57:06Z","title":"EmbodiedSAM: Online Segment Any 3D Thing in Real Time","summary":"  Embodied tasks require the agent to fully understand 3D scenes simultaneously\nwith its exploration, so an online, real-time, fine-grained and\nhighly-generalized 3D perception model is desperately needed. Since\nhigh-quality 3D data is limited, directly training such a model in 3D is almost\ninfeasible. Meanwhile, vision foundation models (VFM) has revolutionized the\nfield of 2D computer vision with superior performance, which makes the use of\nVFM to assist embodied 3D perception a promising direction. However, most\nexisting VFM-assisted 3D perception methods are either offline or too slow that\ncannot be applied in practical embodied tasks. In this paper, we aim to\nleverage Segment Anything Model (SAM) for real-time 3D instance segmentation in\nan online setting. This is a challenging problem since future frames are not\navailable in the input streaming RGB-D video, and an instance may be observed\nin several frames so object matching between frames is required. To address\nthese challenges, we first propose a geometric-aware query lifting module to\nrepresent the 2D masks generated by SAM by 3D-aware queries, which is then\niteratively refined by a dual-level query decoder. In this way, the 2D masks\nare transferred to fine-grained shapes on 3D point clouds. Benefit from the\nquery representation for 3D masks, we can compute the similarity matrix between\nthe 3D masks from different views by efficient matrix operation, which enables\nreal-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan\nshow our method achieves leading performance even compared with offline\nmethods. Our method also demonstrates great generalization ability in several\nzero-shot dataset transferring experiments and show great potential in\nopen-vocabulary and data-efficient setting. Code and demo are available at\nhttps://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for\ntraining and evaluation.\n","authors":["Xiuwei Xu","Huangxing Chen","Linqing Zhao","Ziwei Wang","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2408.11811v3.pdf","comment":"ICLR25 Oral. Project page: https://xuxw98.github.io/ESAM/"},{"id":"http://arxiv.org/abs/2404.13016v3","updated":"2025-02-12T04:59:47Z","published":"2024-04-19T17:25:43Z","title":"Optimizing Calibration by Gaining Aware of Prediction Correctness","summary":"  Model calibration aims to align confidence with prediction correctness. The\nCross-Entropy (CE) loss is widely used for calibrator training, which enforces\nthe model to increase confidence on the ground truth class. However, we find\nthe CE loss has intrinsic limitations. For example, for a narrow\nmisclassification (e.g., a test sample is wrongly classified and its softmax\nscore on the ground truth class is 0.4), a calibrator trained by the CE loss\noften produces high confidence on the wrongly predicted class, which is\nundesirable. In this paper, we propose a new post-hoc calibration objective\nderived from the aim of calibration. Intuitively, the proposed objective\nfunction asks that the calibrator decrease model confidence on wrongly\npredicted samples and increase confidence on correctly predicted samples.\nBecause a sample itself has insufficient ability to indicate correctness, we\nuse its transformed versions (e.g., rotated, greyscaled, and color-jittered)\nduring calibrator training. Trained on an in-distribution validation set and\ntested with isolated, individual test samples, our method achieves competitive\ncalibration performance on both in-distribution and out-of-distribution test\nsets compared with the state of the art. Further, our analysis points out the\ndifference between our method and commonly used objectives such as CE loss and\nMean Square Error (MSE) loss, where the latters sometimes deviates from the\ncalibration aim.\n","authors":["Yuchi Liu","Lei Wang","Yuli Zou","James Zou","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.13016v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01162v2","updated":"2025-02-12T04:35:28Z","published":"2024-09-02T10:49:10Z","title":"Sparsity Meets Similarity: Leveraging Long-Tail Distribution for Dynamic\n  Optimized Token Representation in Multimodal Large Language Models","summary":"  Recently, multimodal large language models (MM-LLMs) have achieved\nsignificant success in various tasks, but their high computational costs limit\nwidespread application. The main computational burden arises from processing\nconcatenated text and visual tokens in the LLM layer, where input token length\ndirectly affects efficiency. Our analysis of visual tokens reveals that their\nsimilarity to the CLS token follows a long-tail distribution, with only a few\nshowing high similarity. To address this, we propose a dynamic pruning\nalgorithm that identifies the inflection point in the visual CLS token\nsimilarity curve, enabling effective trimming of visual markers to accelerate\nmodel performance. Additionally, we perform a second round of pruning in the\nLLM layer, filtering out low-correlation tokens through the interaction between\nvisual and textual features. Experimental results demonstrate that our method\nachieves performance comparable to the original while utilizing only 22% of the\noriginal token quantity. Our source code will be made publicly available upon\nacceptance.\n","authors":["Gaotong Yu","Yi Chen","Jian Xu"],"pdf_url":"https://arxiv.org/pdf/2409.01162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06114v2","updated":"2025-02-12T04:33:28Z","published":"2025-02-10T02:48:56Z","title":"A Novel Multi-Teacher Knowledge Distillation for Real-Time Object\n  Detection using 4D Radar","summary":"  Accurate 3D object detection is crucial for safe autonomous navigation,\nrequiring reliable performance across diverse weather conditions. While LiDAR\nperformance deteriorates in challenging weather, Radar systems maintain their\nreliability. Traditional Radars have limitations due to their lack of elevation\ndata, but the recent 4D Radars overcome this by measuring elevation alongside\nrange, azimuth, and Doppler velocity, making them invaluable for autonomous\nvehicles. The primary challenge in utilizing 4D Radars is the sparsity of their\npoint clouds. Previous works address this by developing architectures that\nbetter capture semantics and context in sparse point cloud, largely drawing\nfrom LiDAR-based approaches. However, these methods often overlook a unique\nadvantage of 4D Radars: the dense Radar tensor, which encapsulates power\nmeasurements across three spatial dimensions and the Doppler dimension. Our\npaper leverages this tensor to tackle the sparsity issue. We introduce a novel\nknowledge distillation framework that enables a student model to densify its\nsparse input in the latent space by emulating an ensemble of teacher models.\nOur experiments demonstrate a 25% performance improvement over the\nstate-of-the-art RTNH model on the K-Radar dataset. Notably, this improvement\nis achieved while still maintaining a real-time inference speed.\n","authors":["Seung-Hyun Song","Dong-Hee Paek","Minh-Quan Dao","Ezio Malis","Seung-Hyun Kong"],"pdf_url":"https://arxiv.org/pdf/2502.06114v2.pdf","comment":"Arxiv preprint"},{"id":"http://arxiv.org/abs/2310.07916v3","updated":"2025-02-12T04:19:43Z","published":"2023-10-11T22:04:33Z","title":"Dynamic Appearance Particle Neural Radiance Field","summary":"  Neural Radiance Fields (NeRFs) have shown great potential in modeling 3D\nscenes. Dynamic NeRFs extend this model by capturing time-varying elements,\ntypically using deformation fields. The existing dynamic NeRFs employ a similar\nEulerian representation for both light radiance and deformation fields. This\nleads to a close coupling of appearance and motion and lacks a physical\ninterpretation. In this work, we propose Dynamic Appearance Particle Neural\nRadiance Field (DAP-NeRF), which introduces particle-based representation to\nmodel the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists\nof the superposition of a static field and a dynamic field. The dynamic field\nis quantized as a collection of appearance particles, which carries the visual\ninformation of a small dynamic element in the scene and is equipped with a\nmotion model. All components, including the static field, the visual features\nand the motion models of particles, are learned from monocular videos without\nany prior geometric knowledge of the scene. We develop an efficient\ncomputational framework for the particle-based model. We also construct a new\ndataset to evaluate motion modeling. Experimental results show that DAP-NeRF is\nan effective technique to capture not only the appearance but also the\nphysically meaningful motions in a 3D dynamic scene. Code is available at:\nhttps://github.com/Cenbylin/DAP-NeRF.\n","authors":["Ancheng Lin","Yusheng Xiang","Jun Li","Mukesh Prasad"],"pdf_url":"https://arxiv.org/pdf/2310.07916v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08106v1","updated":"2025-02-12T04:07:14Z","published":"2025-02-12T04:07:14Z","title":"PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced\n  Text-to-Image Generation","summary":"  Diffusion models have made significant advancements in recent years. However,\ntheir performance often deteriorates when trained or fine-tuned on imbalanced\ndatasets. This degradation is largely due to the disproportionate\nrepresentation of majority and minority data in image-text pairs. In this\npaper, we propose a general fine-tuning approach, dubbed PoGDiff, to address\nthis challenge. Rather than directly minimizing the KL divergence between the\npredicted and ground-truth distributions, PoGDiff replaces the ground-truth\ndistribution with a Product of Gaussians (PoG), which is constructed by\ncombining the original ground-truth targets with the predicted distribution\nconditioned on a neighboring text embedding. Experiments on real-world datasets\ndemonstrate that our method effectively addresses the imbalance problem in\ndiffusion models, improving both generation accuracy and quality.\n","authors":["Ziyan Wang","Sizhe Wei","Xiaoming Huo","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08097v1","updated":"2025-02-12T03:52:36Z","published":"2025-02-12T03:52:36Z","title":"ID-Cloak: Crafting Identity-Specific Cloaks Against Personalized\n  Text-to-Image Generation","summary":"  Personalized text-to-image models allow users to generate images of new\nconcepts from several reference photos, thereby leading to critical concerns\nregarding civil privacy. Although several anti-personalization techniques have\nbeen developed, these methods typically assume that defenders can afford to\ndesign a privacy cloak corresponding to each specific image. However, due to\nextensive personal images shared online, image-specific methods are limited by\nreal-world practical applications. To address this issue, we are the first to\ninvestigate the creation of identity-specific cloaks (ID-Cloak) that safeguard\nall images belong to a specific identity. Specifically, we first model an\nidentity subspace that preserves personal commonalities and learns diverse\ncontexts to capture the image distribution to be protected. Then, we craft\nidentity-specific cloaks with the proposed novel objective that encourages the\ncloak to guide the model away from its normal output within the subspace.\nExtensive experiments show that the generated universal cloak can effectively\nprotect the images. We believe our method, along with the proposed\nidentity-specific cloak setting, marks a notable advance in realistic privacy\nprotection.\n","authors":["Qianrui Teng","Xing Cui","Xuannan Liu","Peipei Li","Zekun Li","Huaibo Huang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2502.08097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17207v2","updated":"2025-02-12T03:15:52Z","published":"2024-08-30T11:22:09Z","title":"NanoMVG: USV-Centric Low-Power Multi-Task Visual Grounding based on\n  Prompt-Guided Camera and 4D mmWave Radar","summary":"  Recently, visual grounding and multi-sensors setting have been incorporated\ninto perception system for terrestrial autonomous driving systems and Unmanned\nSurface Vehicles (USVs), yet the high complexity of modern learning-based\nvisual grounding model using multi-sensors prevents such model to be deployed\non USVs in the real-life. To this end, we design a low-power multi-task model\nnamed NanoMVG for waterway embodied perception, guiding both camera and 4D\nmillimeter-wave radar to locate specific object(s) through natural language.\nNanoMVG can perform both box-level and mask-level visual grounding tasks\nsimultaneously. Compared to other visual grounding models, NanoMVG achieves\nhighly competitive performance on the WaterVG dataset, particularly in harsh\nenvironments and boasts ultra-low power consumption for long endurance.\n","authors":["Runwei Guan","Jianan Liu","Liye Jia","Haocheng Zhao","Shanliang Yao","Xiaohui Zhu","Ka Lok Man","Eng Gee Lim","Jeremy Smith","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2408.17207v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.18862v3","updated":"2025-02-12T03:13:48Z","published":"2024-12-25T10:16:57Z","title":"WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via\n  Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) has gained significant attention for 3D scene\nreconstruction, but still suffers from complex outdoor environments, especially\nunder adverse weather. This is because 3DGS treats the artifacts caused by\nadverse weather as part of the scene and will directly reconstruct them,\nlargely reducing the clarity of the reconstructed scene. To address this\nchallenge, we propose WeatherGS, a 3DGS-based framework for reconstructing\nclear scenes from multi-view images under different weather conditions.\nSpecifically, we explicitly categorize the multi-weather artifacts into the\ndense particles and lens occlusions that have very different characters, in\nwhich the former are caused by snowflakes and raindrops in the air, and the\nlatter are raised by the precipitation on the camera lens. In light of this, we\npropose a dense-to-sparse preprocess strategy, which sequentially removes the\ndense particles by an Atmospheric Effect Filter (AEF) and then extracts the\nrelatively sparse occlusion masks with a Lens Effect Detector (LED). Finally,\nwe train a set of 3D Gaussians by the processed images and generated masks for\nexcluding occluded areas, and accurately recover the underlying clear scene by\nGaussian splatting. We conduct a diverse and challenging benchmark to\nfacilitate the evaluation of 3D reconstruction under complex weather scenarios.\nExtensive experiments on this benchmark demonstrate that our WeatherGS\nconsistently produces high-quality, clean scenes across various weather\nscenarios, outperforming existing state-of-the-art methods. See project\npage:https://jumponthemoon.github.io/weather-gs.\n","authors":["Chenghao Qian","Yuhu Guo","Wenjing Li","Gustav Markkula"],"pdf_url":"https://arxiv.org/pdf/2412.18862v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08079v1","updated":"2025-02-12T02:53:27Z","published":"2025-02-12T02:53:27Z","title":"MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained\n  Models","summary":"  Current adversarial attacks for evaluating the robustness of vision-language\npre-trained (VLP) models in multi-modal tasks suffer from limited\ntransferability, where attacks crafted for a specific model often struggle to\ngeneralize effectively across different models, limiting their utility in\nassessing robustness more broadly. This is mainly attributed to the\nover-reliance on model-specific features and regions, particularly in the image\nmodality. In this paper, we propose an elegant yet highly effective method\ntermed Meticulous Adversarial Attack (MAA) to fully exploit model-independent\ncharacteristics and vulnerabilities of individual samples, achieving enhanced\ngeneralizability and reduced model dependence. MAA emphasizes fine-grained\noptimization of adversarial images by developing a novel resizing and sliding\ncrop (RScrop) technique, incorporating a multi-granularity similarity\ndisruption (MGSD) strategy. Extensive experiments across diverse VLP models,\nmultiple benchmark datasets, and a variety of downstream tasks demonstrate that\nMAA significantly enhances the effectiveness and transferability of adversarial\nattacks. A large cohort of performance studies is conducted to generate\ninsights into the effectiveness of various model configurations, guiding future\nadvancements in this domain.\n","authors":["Peng-Fei Zhang","Guangdong Bai","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2502.08079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13385v5","updated":"2025-02-12T02:50:01Z","published":"2023-11-22T13:27:36Z","title":"SegVol: Universal and Interactive Volumetric Medical Image Segmentation","summary":"  Precise image segmentation provides clinical study with instructive\ninformation. Despite the remarkable progress achieved in medical image\nsegmentation, there is still an absence of a 3D foundation segmentation model\nthat can segment a wide range of anatomical categories with easy user\ninteraction. In this paper, we propose a 3D foundation segmentation model,\nnamed SegVol, supporting universal and interactive volumetric medical image\nsegmentation. By scaling up training data to 90K unlabeled Computed Tomography\n(CT) volumes and 6K labeled CT volumes, this foundation model supports the\nsegmentation of over 200 anatomical categories using semantic and spatial\nprompts. To facilitate efficient and precise inference on volumetric images, we\ndesign a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical\nsegmentation tasks verify that SegVol outperforms the competitors in 19 tasks,\nwith improvements up to 37.24% compared to the runner-up methods. We\ndemonstrate the effectiveness and importance of specific designs by ablation\nstudy. We expect this foundation model can promote the development of\nvolumetric medical image analysis. The model and code are publicly available\nat: https://github.com/BAAI-DCAI/SegVol.\n","authors":["Yuxin Du","Fan Bai","Tiejun Huang","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.13385v5.pdf","comment":"NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2502.08075v1","updated":"2025-02-12T02:37:16Z","published":"2025-02-12T02:37:16Z","title":"Knowledge Swapping via Learning and Unlearning","summary":"  We introduce \\textbf{Knowledge Swapping}, a novel task designed to\nselectively regulate knowledge of a pretrained model by enabling the forgetting\nof user\\-specified information, retaining essential knowledge, and acquiring\nnew knowledge simultaneously. By delving into the analysis of knock-on feature\nhierarchy, we find that incremental learning typically progresses from\nlow\\-level representations to higher\\-level semantics, whereas forgetting tends\nto occur in the opposite direction\\-starting from high-level semantics and\nmoving down to low-level features. Building upon this, we propose to benchmark\nthe knowledge swapping task with the strategy of \\textit{Learning Before\nForgetting}. Comprehensive experiments on various tasks like image\nclassification, object detection, and semantic segmentation validate the\neffectiveness of the proposed strategy. The source code is available at\n\\href{https://github.com/xingmingyu123456/KnowledgeSwapping}{https://github.com/xingmingyu123456/KnowledgeSwapping}.\n","authors":["Mingyu Xing","Lechao Cheng","Shenggeng Tang","Yaxiong Wang","Zhun Zhong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08075v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2211.10580v3","updated":"2025-02-12T02:27:20Z","published":"2022-11-19T03:55:09Z","title":"Normal Transformer: Extracting Surface Geometry from LiDAR Points\n  Enhanced by Visual Semantics","summary":"  High-quality surface normal can help improve geometry estimation in problems\nfaced by autonomous vehicles, such as collision avoidance and occlusion\ninference. While a considerable volume of literature focuses on densely scanned\nindoor scenarios, normal estimation during autonomous driving remains an\nintricate problem due to the sparse, non-uniform, and noisy nature of\nreal-world LiDAR scans. In this paper, we introduce a multi-modal technique\nthat leverages 3D point clouds and 2D colour images obtained from LiDAR and\ncamera sensors for surface normal estimation. We present the Hybrid Geometric\nTransformer (HGT), a novel transformer-based neural network architecture that\nproficiently fuses visual semantic and 3D geometric information. Furthermore,\nwe developed an effective learning strategy for the multi-modal data.\nExperimental results demonstrate the superior effectiveness of our information\nfusion approach compared to existing methods. It has also been verified that\nthe proposed model can learn from a simulated 3D environment that mimics a\ntraffic scene. The learned geometric knowledge is transferable and can be\napplied to real-world 3D scenes in the KITTI dataset. Further tasks built upon\nthe estimated normal vectors in the KITTI dataset show that the proposed\nestimator has an advantage over existing methods.\n","authors":["Ancheng Lin","Jun Li","Yusheng Xiang","Wei Bian","Mukesh Prasad"],"pdf_url":"https://arxiv.org/pdf/2211.10580v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11836v7","updated":"2025-02-12T02:12:47Z","published":"2024-08-06T22:09:50Z","title":"Analysis of Unstructured High-Density Crowded Scenes for Crowd\n  Monitoring","summary":"  We are interested in developing an automated system for detection of\norganized movements in human crowds. Computer vision algorithms can extract\ninformation from videos of crowded scenes and automatically detect and track\ngroups of individuals undergoing organized motion that represents an anomalous\nbehavior in the context of conflict aversion. Our system can detect organized\ncohorts against the background of randomly moving objects and we can estimate\nthe number of participants in an organized cohort, the speed and direction of\nmotion in real time, within three to four video frames, which is less than one\nsecond from the onset of motion captured on a CCTV. We have performed\npreliminary analysis in this context in biological cell data containing up to\nfour thousand objects per frame and will extend this numerically to a\nhundred-fold for public safety applications.\n  We envisage using the existing infrastructure of video cameras for acquiring\nimage datasets on-the-fly and deploying an easy-to-use data-driven software\nsystem for parsing of significant events by analyzing image sequences taken\ninside and outside of sports stadiums or other public venues. Other prospective\nusers are organizers of political rallies, civic and wildlife organizations,\nsecurity firms, and the military. We will optimize the performance of the\nsoftware by implementing a classification method able to distinguish between\nactivities posing a threat and those not posing a threat.\n","authors":["Alexandre Matov"],"pdf_url":"https://arxiv.org/pdf/2408.11836v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19243v3","updated":"2025-02-12T00:08:30Z","published":"2024-03-28T08:58:20Z","title":"Efficient Learning With Sine-Activated Low-rank Matrices","summary":"  Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.\n","authors":["Yiping Ji","Hemanth Saratchandran","Cameron Gordon","Zeyu Zhang","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19243v3.pdf","comment":"The first two authors contributed equally. Paper accepted at ICLR\n  2025"},{"id":"http://arxiv.org/abs/2502.08836v1","updated":"2025-02-12T22:57:06Z","published":"2025-02-12T22:57:06Z","title":"Survey on Single-Image Reflection Removal using Deep Learning Techniques","summary":"  The phenomenon of reflection is quite common in digital images, posing\nsignificant challenges for various applications such as computer vision,\nphotography, and image processing. Traditional methods for reflection removal\noften struggle to achieve clean results while maintaining high fidelity and\nrobustness, particularly in real-world scenarios. Over the past few decades,\nnumerous deep learning-based approaches for reflection removal have emerged,\nyielding impressive results. In this survey, we conduct a comprehensive review\nof the current literature by focusing on key venues such as ICCV, ECCV, CVPR,\nNeurIPS, etc., as these conferences and journals have been central to advances\nin the field. Our review follows a structured paper selection process, and we\ncritically assess both single-stage and two-stage deep learning methods for\nreflection removal. The contribution of this survey is three-fold: first, we\nprovide a comprehensive summary of the most recent work on single-image\nreflection removal; second, we outline task hypotheses, current deep learning\ntechniques, publicly available datasets, and relevant evaluation metrics; and\nthird, we identify key challenges and opportunities in deep learning-based\nreflection removal, highlighting the potential of this rapidly evolving\nresearch area.\n","authors":["Kangning Yang","Huiming Sun","Jie Cai","Lan Fu","Jiaming Ding","Jinlong Li","Chiu Man Ho","Zibo Meng"],"pdf_url":"https://arxiv.org/pdf/2502.08836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08821v1","updated":"2025-02-12T22:24:49Z","published":"2025-02-12T22:24:49Z","title":"DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with\n  Saliency Maps","summary":"  The recent surge in advanced generative models, such as diffusion models and\ngenerative adversarial networks (GANs), has led to an alarming rise in\nAI-generated images across various domains on the web. While such technologies\noffer benefits such as democratizing artistic creation, they also pose\nchallenges in misinformation, digital forgery, and authenticity verification.\nAdditionally, the uncredited use of AI-generated images in media and marketing\nhas sparked significant backlash from online communities. In response to this,\nwe introduce DejAIvu, a Chrome Web extension that combines real-time\nAI-generated image detection with saliency-based explainability while users\nbrowse the web. Using an ONNX-optimized deep learning model, DejAIvu\nautomatically analyzes images on websites such as Google Images, identifies\nAI-generated content using model inference, and overlays a saliency heatmap to\nhighlight AI-related artifacts. Our approach integrates efficient in-browser\ninference, gradient-based saliency analysis, and a seamless user experience,\nensuring that AI detection is both transparent and interpretable. We also\nevaluate DejAIvu across multiple pretrained architectures and benchmark\ndatasets, demonstrating high accuracy and low latency, making it a practical\nand deployable tool for enhancing AI image accountability. The code for this\nsystem can be found at https://github.com/Noodulz/dejAIvu.\n","authors":["Jocelyn Dzuong"],"pdf_url":"https://arxiv.org/pdf/2502.08821v1.pdf","comment":"5 pages, 3 figures, submitted to IJCAI 2025 demo track"},{"id":"http://arxiv.org/abs/2502.08822v1","updated":"2025-02-12T22:24:49Z","published":"2025-02-12T22:24:49Z","title":"$\\mathsf{CSMAE~}$:~Cataract Surgical Masked Autoencoder (MAE) based\n  Pre-training","summary":"  Automated analysis of surgical videos is crucial for improving surgical\ntraining, workflow optimization, and postoperative assessment. We introduce a\nCSMAE, Masked Autoencoder (MAE)-based pretraining approach, specifically\ndeveloped for Cataract Surgery video analysis, where instead of randomly\nselecting tokens for masking, they are selected based on the spatiotemporal\nimportance of the token. We created a large dataset of cataract surgery videos\nto improve the model's learning efficiency and expand its robustness in\nlow-data regimes. Our pre-trained model can be easily adapted for specific\ndownstream tasks via fine-tuning, serving as a robust backbone for further\nanalysis. Through rigorous testing on a downstream step-recognition task on two\nCataract Surgery video datasets, D99 and Cataract-101, our approach surpasses\ncurrent state-of-the-art self-supervised pretraining and adapter-based transfer\nlearning methods by a significant margin. This advancement not only\ndemonstrates the potential of our MAE-based pretraining in the field of\nsurgical video analysis but also sets a new benchmark for future research.\n","authors":["Nisarg A. Shah","Wele Gedara Chaminda Bandara","Shameema Skider","S. Swaroop Vedula","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2502.08822v1.pdf","comment":"5 pages, Accepted to IEEE International Symposium on Biomedical\n  Imaging (ISBI 2025)"},{"id":"http://arxiv.org/abs/2502.08813v1","updated":"2025-02-12T21:55:26Z","published":"2025-02-12T21:55:26Z","title":"Measuring Anxiety Levels with Head Motion Patterns in Severe Depression\n  Population","summary":"  Depression and anxiety are prevalent mental health disorders that frequently\ncooccur, with anxiety significantly influencing both the manifestation and\ntreatment of depression. An accurate assessment of anxiety levels in\nindividuals with depression is crucial to develop effective and personalized\ntreatment plans. This study proposes a new noninvasive method for quantifying\nanxiety severity by analyzing head movements -specifically speed, acceleration,\nand angular displacement - during video-recorded interviews with patients\nsuffering from severe depression. Using data from a new CALYPSO Depression\nDataset, we extracted head motion characteristics and applied regression\nanalysis to predict clinically evaluated anxiety levels. Our results\ndemonstrate a high level of precision, achieving a mean absolute error (MAE) of\n0.35 in predicting the severity of psychological anxiety based on head movement\npatterns. This indicates that our approach can enhance the understanding of\nanxiety's role in depression and assist psychiatrists in refining treatment\nstrategies for individuals.\n","authors":["Fouad Boualeb","Emery Pierson","Nicolas Doudeau","Clémence Nineuil","Ali Amad","Mohamed Daoudi"],"pdf_url":"https://arxiv.org/pdf/2502.08813v1.pdf","comment":"19th IEEE International Conference on Automatic Face and Gesture\n  Recognition (FG), 2025"},{"id":"http://arxiv.org/abs/2502.08786v1","updated":"2025-02-12T20:56:54Z","published":"2025-02-12T20:56:54Z","title":"MRUCT: Mixed Reality Assistance for Acupuncture Guided by Ultrasonic\n  Computed Tomography","summary":"  Chinese acupuncture practitioners primarily depend on muscle memory and\ntactile feedback to insert needles and accurately target acupuncture points, as\nthe current workflow lacks imaging modalities and visual aids. Consequently,\nnew practitioners often learn through trial and error, requiring years of\nexperience to become proficient and earn the trust of patients. Medical\nstudents face similar challenges in mastering this skill. To address these\nchallenges, we developed an innovative system, MRUCT, that integrates\nultrasonic computed tomography (UCT) with mixed reality (MR) technology to\nvisualize acupuncture points in real-time. This system offers offline image\nregistration and real-time guidance during needle insertion, enabling them to\naccurately position needles based on anatomical structures such as bones,\nmuscles, and auto-generated reference points, with the potential for clinical\nimplementation. In this paper, we outline the non-rigid registration methods\nused to reconstruct anatomical structures from UCT data, as well as the key\ndesign considerations of the MR system. We evaluated two different 3D user\ninterface (3DUI) designs and compared the performance of our system to\ntraditional workflows for both new practitioners and medical students. The\nresults highlight the potential of MR to enhance therapeutic medical practices\nand demonstrate the effectiveness of the system we developed.\n","authors":["Yue Yang","Xinkai Wang","Kehong Zhou","Xue Xie","Lifeng Zhu","Aiguo Song","Bruce Daniel"],"pdf_url":"https://arxiv.org/pdf/2502.08786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08779v1","updated":"2025-02-12T20:41:53Z","published":"2025-02-12T20:41:53Z","title":"SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models","summary":"  Stereotype biases in Large Multimodal Models (LMMs) perpetuate harmful\nsocietal prejudices, undermining the fairness and equity of AI applications. As\nLMMs grow increasingly influential, addressing and mitigating inherent biases\nrelated to stereotypes, harmful generations, and ambiguous assumptions in\nreal-world scenarios has become essential. However, existing datasets\nevaluating stereotype biases in LMMs often lack diversity and rely on synthetic\nimages, leaving a gap in bias evaluation for real-world visual contexts. To\naddress this, we introduce the Stereotype Bias Benchmark (SB-bench), the most\ncomprehensive framework to date for assessing stereotype biases across nine\ndiverse categories with non-synthetic images. SB-bench rigorously evaluates\nLMMs through carefully curated, visually grounded scenarios, challenging them\nto reason accurately about visual stereotypes. It offers a robust evaluation\nframework featuring real-world visual samples, image variations, and\nmultiple-choice question formats. By introducing visually grounded queries that\nisolate visual biases from textual ones, SB-bench enables a precise and nuanced\nassessment of a model's reasoning capabilities across varying levels of\ndifficulty. Through rigorous testing of state-of-the-art open-source and\nclosed-source LMMs, SB-bench provides a systematic approach to assessing\nstereotype biases in LMMs across key social dimensions. This benchmark\nrepresents a significant step toward fostering fairness in AI systems and\nreducing harmful biases, laying the groundwork for more equitable and socially\nresponsible LMMs. Our code and dataset are publicly available.\n","authors":["Vishal Narnaware","Ashmal Vayani","Rohit Gupta","Swetha Sirnam","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2502.08779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08774v1","updated":"2025-02-12T20:31:47Z","published":"2025-02-12T20:31:47Z","title":"Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal\n  Brain in 3D Ultrasound","summary":"  Monitoring the growth of subcortical regions of the fetal brain in ultrasound\n(US) images can help identify the presence of abnormal development. Manually\nsegmenting these regions is a challenging task, but recent work has shown that\nit can be automated using deep learning. However, applying pretrained models to\nunseen freehand US volumes often leads to a degradation of performance due to\nthe vast differences in acquisition and alignment. In this work, we first\ndemonstrate that test time adaptation (TTA) can be used to improve model\nperformance in the presence of both real and simulated domain shifts. We\nfurther propose a novel TTA method by incorporating a normative atlas as a\nprior for anatomy. In the presence of various types of domain shifts, we\nbenchmark the performance of different TTA methods and demonstrate the\nimprovements brought by our proposed approach, which may further facilitate\nautomated monitoring of fetal brain development. Our code is available at\nhttps://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation.\n","authors":["Joshua Omolegan","Pak Hei Yeung","Madeleine K. Wyburd","Linde Hesse","Monique Haak","Intergrowth-21st Consortium","Ana I. L. Namburete","Nicola K. Dinsdale"],"pdf_url":"https://arxiv.org/pdf/2502.08774v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.08769v1","updated":"2025-02-12T20:17:10Z","published":"2025-02-12T20:17:10Z","title":"Cluster and Predict Latents Patches for Improved Masked Image Modeling","summary":"  Masked Image Modeling (MIM) offers a promising approach to self-supervised\nrepresentation learning, however existing MIM models still lag behind the\nstate-of-the-art. In this paper, we systematically analyze target\nrepresentations, loss functions, and architectures, to introduce CAPI - a novel\npure-MIM framework that relies on the prediction of latent clusterings. Our\napproach leverages a clustering-based loss, which is stable to train, and\nexhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8%\naccuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes,\nsubstantially outperforming previous MIM methods and approaching the\nperformance of the current state-of-the-art, DINOv2. We release all our code\nand models.\n","authors":["Timothée Darcet","Federico Baldassarre","Maxime Oquab","Julien Mairal","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2502.08769v1.pdf","comment":"13 pages, 7 figures, submitted to TMLR"},{"id":"http://arxiv.org/abs/2409.11456v3","updated":"2025-02-12T20:10:41Z","published":"2024-09-17T17:48:12Z","title":"Two Stage Segmentation of Cervical Tumors using PocketNet","summary":"  Cervical cancer remains the fourth most common malignancy amongst women\nworldwide.1 Concurrent chemoradiotherapy (CRT) serves as the mainstay\ndefinitive treatment regimen for locally advanced cervical cancers and includes\nexternal beam radiation followed by brachytherapy.2 Integral to radiotherapy\ntreatment planning is the routine contouring of both the target tumor at the\nlevel of the cervix, associated gynecologic anatomy and the adjacent organs at\nrisk (OARs). However, manual contouring of these structures is both time and\nlabor intensive and associated with known interobserver variability that can\nimpact treatment outcomes. While multiple tools have been developed to\nautomatically segment OARs and the high-risk clinical tumor volume (HR-CTV)\nusing computed tomography (CT) images,3,4,5,6 the development of deep\nlearning-based tumor segmentation tools using routine T2-weighted (T2w)\nmagnetic resonance imaging (MRI) addresses an unmet clinical need to improve\nthe routine contouring of both anatomical structures and cervical cancers,\nthereby increasing quality and consistency of radiotherapy planning. This work\napplied a novel deep-learning model (PocketNet) to segment the cervix, vagina,\nuterus, and tumor(s) on T2w MRI. The performance of the PocketNet architecture\nwas evaluated, when trained on data via five-fold cross validation. PocketNet\nachieved a mean Dice-Sorensen similarity coefficient (DSC) exceeding 70% for\ntumor segmentation and 80% for organ segmentation. Validation on a publicly\navailable dataset from The Cancer Imaging Archive (TCIA) demonstrated the\nmodels robustness, achieving DSC scores of 67.3% for tumor segmentation and\n80.8% for organ segmentation. These results suggest that PocketNet is robust to\nvariations in contrast protocols, providing reliable segmentation of the\nregions of interest.\n","authors":["Awj Twam","Adrian E. Celaya","Megan C. Jacobsen","Rachel Glenn","Peng Wei","Jia Sun","Ann Klopp","Aradhana M. Venkatesan","David Fuentes"],"pdf_url":"https://arxiv.org/pdf/2409.11456v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08754v1","updated":"2025-02-12T19:51:41Z","published":"2025-02-12T19:51:41Z","title":"HistoSmith: Single-Stage Histology Image-Label Generation via\n  Conditional Latent Diffusion for Enhanced Cell Segmentation and\n  Classification","summary":"  Precise segmentation and classification of cell instances are vital for\nanalyzing the tissue microenvironment in histology images, supporting medical\ndiagnosis, prognosis, treatment planning, and studies of brain\ncytoarchitecture. However, the creation of high-quality annotated datasets for\ntraining remains a major challenge. This study introduces a novel single-stage\napproach (HistoSmith) for generating image-label pairs to augment histology\ndatasets. Unlike state-of-the-art methods that utilize diffusion models with\nseparate components for label and image generation, our approach employs a\nlatent diffusion model to learn the joint distribution of cellular layouts,\nclassification masks, and histology images. This model enables tailored data\ngeneration by conditioning on user-defined parameters such as cell types,\nquantities, and tissue types. Trained on the Conic H&E histopathology dataset\nand the Nissl-stained CytoDArk0 dataset, the model generates realistic and\ndiverse labeled samples. Experimental results demonstrate improvements in cell\ninstance segmentation and classification, particularly for underrepresented\ncell types like neutrophils in the Conic dataset. These findings underscore the\npotential of our approach to address data scarcity challenges.\n","authors":["Valentina Vadori","Jean-Marie Graïc","Antonella Peruffo","Livio Finos","Ujwala Kiran Chaudhari","Enrico Grisan"],"pdf_url":"https://arxiv.org/pdf/2502.08754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06034v2","updated":"2025-02-12T19:36:57Z","published":"2025-02-09T21:14:27Z","title":"Traveling Waves Integrate Spatial Information Into Spectral\n  Representations","summary":"  Traveling waves are widely observed in the brain, but their precise\ncomputational function remains unclear. One prominent hypothesis is that they\nenable the transfer and integration of spatial information across neural\npopulations. However, few computational models have explored how traveling\nwaves might be harnessed to perform such integrative processing. Drawing\ninspiration from the famous ``Can one hear the shape of a drum?'' problem --\nwhich highlights how spectral modes encode geometric information -- we\nintroduce a set of convolutional recurrent neural networks that learn to\nproduce traveling waves in their hidden states in response to visual stimuli.\nBy applying a spectral decomposition to these wave-like activations, we obtain\na powerful new representational space that outperforms equivalently local\nfeed-forward networks on tasks requiring global spatial context. In particular,\nwe observe that traveling waves effectively expand the receptive field of\nlocally connected neurons, supporting long-range encoding and communication of\ninformation. We demonstrate that models equipped with this mechanism and\nspectral readouts solve visual semantic segmentation tasks demanding global\nintegration, where local feed-forward models fail. As a first step toward\ntraveling-wave-based representations in artificial networks, our findings\nsuggest potential efficiency benefits and offer a new framework for connecting\nto biological recordings of neural activity.\n","authors":["Mozes Jacobs","Roberto C. Budzinski","Lyle Muller","Demba Ba","T. Anderson Keller"],"pdf_url":"https://arxiv.org/pdf/2502.06034v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21144v4","updated":"2025-02-12T19:20:49Z","published":"2024-10-28T15:44:35Z","title":"Enhancing Learned Image Compression via Cross Window-based Attention","summary":"  In recent years, learned image compression methods have demonstrated superior\nrate-distortion performance compared to traditional image compression methods.\nRecent methods utilize convolutional neural networks (CNN), variational\nautoencoders (VAE), invertible neural networks (INN), and transformers. Despite\ntheir significant contributions, a main drawback of these models is their poor\nperformance in capturing local redundancy. Therefore, to leverage global\nfeatures along with local redundancy, we propose a CNN-based solution\nintegrated with a feature encoding module. The feature encoding module encodes\nimportant features before feeding them to the CNN and then utilizes cross-scale\nwindow-based attention, which further captures local redundancy. Cross-scale\nwindow-based attention is inspired by the attention mechanism in transformers\nand effectively enlarges the receptive field. Both the feature encoding module\nand the cross-scale window-based attention module in our architecture are\nflexible and can be incorporated into any other network architecture. We\nevaluate our method on the Kodak and CLIC datasets and demonstrate that our\napproach is effective and on par with state-of-the-art methods. Our code is\npublicly available at https://github.com/prmudgal/CWAM_IC_ISVC. .\n","authors":["Priyanka Mudgal","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21144v4.pdf","comment":"Paper accepted and presented in ISVC'24. Copyrights stay with ISVC\n  Our code is available at: https://github.com/prmudgal/CWAM_IC_ISVC"},{"id":"http://arxiv.org/abs/2406.04341v3","updated":"2025-02-12T19:02:07Z","published":"2024-06-06T17:59:52Z","title":"Interpreting the Second-Order Effects of Neurons in CLIP","summary":"  We interpret the function of individual neurons in CLIP by automatically\ndescribing them using text. Analyzing the direct effects (i.e. the flow from a\nneuron through the residual stream to the output) or the indirect effects\n(overall contribution) fails to capture the neurons' function in CLIP.\nTherefore, we present the \"second-order lens\", analyzing the effect flowing\nfrom a neuron through the later attention heads, directly to the output. We\nfind that these effects are highly selective: for each neuron, the effect is\nsignificant for <2% of the images. Moreover, each effect can be approximated by\na single direction in the text-image space of CLIP. We describe neurons by\ndecomposing these directions into sparse sets of text representations. The sets\nreveal polysemantic behavior - each neuron corresponds to multiple, often\nunrelated, concepts (e.g. ships and cars). Exploiting this neuron polysemy, we\nmass-produce \"semantic\" adversarial examples by generating images with concepts\nspuriously correlated to the incorrect class. Additionally, we use the\nsecond-order effects for zero-shot segmentation, outperforming previous\nmethods. Our results indicate that an automated interpretation of neurons can\nbe used for model deception and for introducing new model capabilities.\n","authors":["Yossi Gandelsman","Alexei A. Efros","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2406.04341v3.pdf","comment":"project page:\n  https://yossigandelsman.github.io/clip_neurons/index.html"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2412.20163v3","updated":"2025-02-12T16:49:56Z","published":"2024-12-28T14:27:45Z","title":"Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems","summary":"  The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.\n","authors":["Minhye Jeon","Seokho Ahn","Young-Duk Seo"],"pdf_url":"https://arxiv.org/pdf/2412.20163v3.pdf","comment":"Accepted by The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2025"},{"id":"http://arxiv.org/abs/2502.08557v1","updated":"2025-02-12T16:39:06Z","published":"2025-02-12T16:39:06Z","title":"QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval","summary":"  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n","authors":["Wonduk Seo","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08557v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.08496v1","updated":"2025-02-12T15:31:16Z","published":"2025-02-12T15:31:16Z","title":"Fine-Tuning Topics through Weighting Aspect Keywords","summary":"  Topic modeling often requires examining topics from multiple perspectives to\nuncover hidden patterns, especially in less explored areas. This paper presents\nan approach to address this need, utilizing weighted keywords from various\naspects derived from a domain knowledge. The research method starts with\nstandard topic modeling. Then, it adds a process consisting of four key steps.\nFirst, it defines keywords for each aspect. Second, it gives weights to these\nkeywords based on their relevance. Third, it calculates relevance scores for\naspect-weighted keywords and topic keywords to create aspect-topic models.\nFourth, it uses these scores to tune relevant new documents. Finally, the\ngenerated topic models are interpreted and validated. The findings show that\ntop-scoring documents are more likely to be about the same aspect of a topic.\nThis highlights the model's effectiveness in finding the related documents to\nthe aspects.\n","authors":["Ali Nazari","Michael Weiss"],"pdf_url":"https://arxiv.org/pdf/2502.08496v1.pdf","comment":"17 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2502.08346v1","updated":"2025-02-12T12:13:51Z","published":"2025-02-12T12:13:51Z","title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","summary":"  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n","authors":["Bin Wu","Yihang Wang","Yuanhao Zeng","Jiawei Liu","Jiashu Zhao","Cheng Yang","Yawen Li","Long Xia","Dawei Yin","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.08346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08326v1","updated":"2025-02-12T11:48:15Z","published":"2025-02-12T11:48:15Z","title":"Model-Free Counterfactual Subset Selection at Scale","summary":"  Ensuring transparency in AI decision-making requires interpretable\nexplanations, particularly at the instance level. Counterfactual explanations\nare a powerful tool for this purpose, but existing techniques frequently depend\non synthetic examples, introducing biases from unrealistic assumptions, flawed\nmodels, or skewed data. Many methods also assume full dataset availability, an\nimpractical constraint in real-time environments where data flows continuously.\nIn contrast, streaming explanations offer adaptive, real-time insights without\nrequiring persistent storage of the entire dataset. This work introduces a\nscalable, model-free approach to selecting diverse and relevant counterfactual\nexamples directly from observed data. Our algorithm operates efficiently in\nstreaming settings, maintaining $O(\\log k)$ update complexity per item while\nensuring high-quality counterfactual selection. Empirical evaluations on both\nreal-world and synthetic datasets demonstrate superior performance over\nbaseline methods, with robust behavior even under adversarial conditions.\n","authors":["Minh Hieu Nguyen","Viet Hung Doan","Anh Tuan Nguyen","Jun Jo","Quoc Viet Hung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.08326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08309v1","updated":"2025-02-12T11:23:46Z","published":"2025-02-12T11:23:46Z","title":"Unlocking Scaling Law in Industrial Recommendation Systems with a\n  Three-step Paradigm based Large User Model","summary":"  Recent advancements in autoregressive Large Language Models (LLMs) have\nachieved significant milestones, largely attributed to their scalability, often\nreferred to as the \"scaling law\". Inspired by these achievements, there has\nbeen a growing interest in adapting LLMs for Recommendation Systems (RecSys) by\nreformulating RecSys tasks into generative problems. However, these End-to-End\nGenerative Recommendation (E2E-GR) methods tend to prioritize idealized goals,\noften at the expense of the practical advantages offered by traditional Deep\nLearning based Recommendation Models (DLRMs) in terms of in features,\narchitecture, and practices. This disparity between idealized goals and\npractical needs introduces several challenges and limitations, locking the\nscaling law in industrial RecSys. In this paper, we introduce a large user\nmodel (LUM) that addresses these limitations through a three-step paradigm,\ndesigned to meet the stringent requirements of industrial settings while\nunlocking the potential for scalable recommendations. Our extensive\nexperimental evaluations demonstrate that LUM outperforms both state-of-the-art\nDLRMs and E2E-GR approaches. Notably, LUM exhibits excellent scalability, with\nperformance improvements observed as the model scales up to 7 billion\nparameters. Additionally, we have successfully deployed LUM in an industrial\napplication, where it achieved significant gains in an A/B test, further\nvalidating its effectiveness and practicality.\n","authors":["Bencheng Yan","Shilei Liu","Zhiyuan Zeng","Zihao Wang","Yizhen Zhang","Yujin Yuan","Langming Liu","Jiaqi Liu","Di Wang","Wenbo Su","Wang Pengjie","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.08309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02231v3","updated":"2025-02-12T10:53:30Z","published":"2024-03-04T17:21:19Z","title":"CODE-ACCORD: A Corpus of building regulatory data for rule generation\n  towards automatic compliance checking","summary":"  Automatic Compliance Checking (ACC) within the Architecture, Engineering, and\nConstruction (AEC) sector necessitates automating the interpretation of\nbuilding regulations to achieve its full potential. Converting textual rules\ninto machine-readable formats is challenging due to the complexities of natural\nlanguage and the scarcity of resources for advanced Machine Learning (ML).\nAddressing these challenges, we introduce CODE-ACCORD, a dataset of 862\nsentences from the building regulations of England and Finland. Only the\nself-contained sentences, which express complete rules without needing\nadditional context, were considered as they are essential for ACC. Each\nsentence was manually annotated with entities and relations by a team of 12\nannotators to facilitate machine-readable rule generation, followed by careful\ncuration to ensure accuracy. The final dataset comprises 4,297 entities and\n4,329 relations across various categories, serving as a robust ground truth.\nCODE-ACCORD supports a range of ML and Natural Language Processing (NLP) tasks,\nincluding text classification, entity recognition, and relation extraction. It\nenables applying recent trends, such as deep neural networks and large language\nmodels, to ACC.\n","authors":["Hansi Hettiarachchi","Amna Dridi","Mohamed Medhat Gaber","Pouyan Parsafard","Nicoleta Bocaneala","Katja Breitenfelder","Gonçal Costa","Maria Hedblom","Mihaela Juganaru-Mathieu","Thamer Mecharnia","Sumee Park","He Tan","Abdel-Rahman H. Tawil","Edlira Vakaj"],"pdf_url":"https://arxiv.org/pdf/2403.02231v3.pdf","comment":"This is a preprint of an article submitted to the Scientific Data\n  Journal"},{"id":"http://arxiv.org/abs/2502.08277v1","updated":"2025-02-12T10:31:45Z","published":"2025-02-12T10:31:45Z","title":"ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion\n  Rate Modeling","summary":"  Post-click conversion rate (CVR) estimation is a vital task in many\nrecommender systems of revenue businesses, e.g., e-commerce and advertising. In\na perspective of sample, a typical CVR positive sample usually goes through a\nfunnel of exposure to click to conversion. For lack of post-event labels for\nun-clicked samples, CVR learning task commonly only utilizes clicked samples,\nrather than all exposed samples as for click-through rate (CTR) learning task.\nHowever, during online inference, CVR and CTR are estimated on the same assumed\nexposure space, which leads to a inconsistency of sample space between training\nand inference, i.e., sample selection bias (SSB). To alleviate SSB, previous\nwisdom proposes to design novel auxiliary tasks to enable the CVR learning on\nun-click training samples, such as CTCVR and counterfactual CVR, etc. Although\nalleviating SSB to some extent, none of them pay attention to the\ndiscrimination between ambiguous negative samples (un-clicked) and factual\nnegative samples (clicked but un-converted) during modelling, which makes CVR\nmodel lacks robustness. To full this gap, we propose a novel ChorusCVR model to\nrealize debiased CVR learning in entire-space.\n","authors":["Wei Cheng","Yucheng Lu","Boyang Xia","Jiangxia Cao","Kuan Xu","Mingxing Wen","Wei Jiang","Jiaming Zhang","Zhaojie Liu","Kun Gai","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08277v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.08271v1","updated":"2025-02-12T10:24:22Z","published":"2025-02-12T10:24:22Z","title":"MoLoRec: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation","summary":"  Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm enhances domain-specific\nrecommendation tasks with parameter-efficient fine-tuning techniques, in order\nto improve models under the warm recommendation scenarios. While most previous\nworks treat these two paradigms separately, we argue that they have\ncomplementary advantages, and combining them together would be helpful.\n  To that end, in this paper, we propose a generalizable and efficient\nLLM-based recommendation framework MoLoRec. Our approach starts by\nparameter-efficient fine-tuning a domain-general module with general\nrecommendation instruction data, to align LLM with recommendation knowledge.\nThen, given users' behavior of a specific domain, we construct a\ndomain-specific instruction dataset and apply efficient fine-tuning to the\npre-trained LLM. After that, we provide approaches to integrate the above\ndomain-general part and domain-specific part with parameters mixture. Please\nnote that, MoLoRec is efficient with plug and play, as the domain-general\nmodule is trained only once, and any domain-specific plug-in can be efficiently\nmerged with only domain-specific fine-tuning. Extensive experiments on multiple\ndatasets under both warm and cold-start recommendation scenarios validate the\neffectiveness and generality of the proposed MoLoRec.\n","authors":["Min Hou","Chenxi Bai","Le Wu","Hao Liu","Kun Zhang","Kai Zhang","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08205v1","updated":"2025-02-12T08:35:10Z","published":"2025-02-12T08:35:10Z","title":"Wisdom of the Crowds in Forecasting: Forecast Summarization for\n  Supporting Future Event Prediction","summary":"  Future Event Prediction (FEP) is an essential activity whose demand and\napplication range across multiple domains. While traditional methods like\nsimulations, predictive and time-series forecasting have demonstrated promising\noutcomes, their application in forecasting complex events is not entirely\nreliable due to the inability of numerical data to accurately capture the\nsemantic information related to events. One forecasting way is to gather and\naggregate collective opinions on the future to make predictions as cumulative\nperspectives carry the potential to help estimating the likelihood of upcoming\nevents. In this work, we organize the existing research and frameworks that aim\nto support future event prediction based on crowd wisdom through aggregating\nindividual forecasts. We discuss the challenges involved, available datasets,\nas well as the scope of improvement and future research directions for this\ntask. We also introduce a novel data model to represent individual forecast\nstatements.\n","authors":["Anisha Saha","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03307v3","updated":"2025-02-12T08:16:44Z","published":"2025-02-05T16:08:05Z","title":"Intent Alignment between Interaction and Language Spaces for\n  Recommendation","summary":"  Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.\n","authors":["Yu Wang","Lei Sang","Yi Zhang","Yiwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.03307v3.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.05806v2","updated":"2025-02-12T07:38:08Z","published":"2024-10-08T08:39:15Z","title":"A Parameter Update Balancing Algorithm for Multi-task Ranking Models in\n  Recommendation Systems","summary":"  Multi-task ranking models have become essential for modern real-world\nrecommendation systems. While most recommendation researches focus on designing\nsophisticated models for specific scenarios, achieving performance improvement\nfor multi-task ranking models across various scenarios still remains a\nsignificant challenge. Training all tasks naively can result in inconsistent\nlearning, highlighting the need for the development of multi-task optimization\n(MTO) methods to tackle this challenge. Conventional methods assume that the\noptimal joint gradient on shared parameters leads to optimal parameter updates.\nHowever, the actual update on model parameters may deviates significantly from\ngradients when using momentum based optimizers such as Adam, and we design and\nexecute statistical experiments to support the observation. In this paper, we\npropose a novel Parameter Update Balancing algorithm for multi-task\noptimization, denoted as PUB. In contrast to traditional MTO method which are\nbased on gradient level tasks fusion or loss level tasks fusion, PUB is the\nfirst work to optimize multiple tasks through parameter update balancing.\nComprehensive experiments on benchmark multi-task ranking datasets demonstrate\nthat PUB consistently improves several multi-task backbones and achieves\nstate-of-the-art performance. Additionally, experiments on benchmark computer\nvision datasets show the great potential of PUB in various multi-task learning\nscenarios. Furthermore, we deployed our method for an industrial evaluation on\nthe real-world commercial platform, HUAWEI AppGallery, where PUB significantly\nenhances the online multi-task ranking model, efficiently managing the primary\ntraffic of a crucial channel.\n","authors":["Jun Yuan","Guohao Cai","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2410.05806v2.pdf","comment":"Accepted by ICDM'24"},{"id":"http://arxiv.org/abs/2502.08161v1","updated":"2025-02-12T07:05:59Z","published":"2025-02-12T07:05:59Z","title":"MixDec Sampling: A Soft Link-based Sampling Method of Graph Neural\n  Network for Recommendation","summary":"  Graph neural networks have been widely used in recent recommender systems,\nwhere negative sampling plays an important role. Existing negative sampling\nmethods restrict the relationship between nodes as either hard positive pairs\nor hard negative pairs. This leads to the loss of structural information, and\nlacks the mechanism to generate positive pairs for nodes with few neighbors. To\novercome limitations, we propose a novel soft link-based sampling method,\nnamely MixDec Sampling, which consists of Mixup Sampling module and Decay\nSampling module. The Mixup Sampling augments node features by synthesizing new\nnodes and soft links, which provides sufficient number of samples for nodes\nwith few neighbors. The Decay Sampling strengthens the digestion of graph\nstructure information by generating soft links for node embedding learning. To\nthe best of our knowledge, we are the first to model sampling relationships\nbetween nodes by soft links in GNN-based recommender systems. Extensive\nexperiments demonstrate that the proposed MixDec Sampling can significantly and\nconsistently improve the recommendation performance of several representative\nGNN-based models on various recommendation benchmarks.\n","authors":["Xiangjin Xie","Yuxin Chen","Ruipeng Wang","Kai Ouyang","Zihan Zhang","Hai-Tao Zheng","Buyue Qian","Hansen Zheng","Bo Hu","Chengxiang Zhuo","Zang Li"],"pdf_url":"https://arxiv.org/pdf/2502.08161v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.00784v2","updated":"2025-02-12T06:34:11Z","published":"2024-10-17T06:44:18Z","title":"FIRE: Fact-checking with Iterative Retrieval and Verification","summary":"  Fact-checking long-form text is challenging, and it is therefore common\npractice to break it down into multiple atomic claims. The typical approach to\nfact-checking these atomic claims involves retrieving a fixed number of pieces\nof evidence, followed by a verification step. However, this method is usually\nnot cost-effective, as it underutilizes the verification model's internal\nknowledge of the claim and fails to replicate the iterative reasoning process\nin human search strategies. To address these limitations, we propose FIRE, a\nnovel agent-based framework that integrates evidence retrieval and claim\nverification in an iterative manner. Specifically, FIRE employs a unified\nmechanism to decide whether to provide a final answer or generate a subsequent\nsearch query, based on its confidence in the current judgment. We compare FIRE\nwith other strong fact-checking frameworks and find that it achieves slightly\nbetter performance while reducing large language model (LLM) costs by an\naverage of 7.6 times and search costs by 16.5 times. These results indicate\nthat FIRE holds promise for application in large-scale fact-checking\noperations. Our code is available at https://github.com/mbzuai-nlp/fire.git.\n","authors":["Zhuohan Xie","Rui Xing","Yuxia Wang","Jiahui Geng","Hasan Iqbal","Dhruv Sahnan","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2411.00784v2.pdf","comment":"4 figures, 8 tables, accepted to Findings of NAACL"},{"id":"http://arxiv.org/abs/2502.08132v1","updated":"2025-02-12T05:28:08Z","published":"2025-02-12T05:28:08Z","title":"SS4Rec: Continuous-Time Sequential Recommendation with State Space\n  Models","summary":"  Sequential recommendation is a key area in the field of recommendation\nsystems aiming to model user interest based on historical interaction sequences\nwith irregular intervals. While previous recurrent neural network-based and\nattention-based approaches have achieved significant results, they have\nlimitations in capturing system continuity due to the discrete characteristics.\nIn the context of continuous-time modeling, state space model (SSM) offers a\npotential solution, as it can effectively capture the dynamic evolution of user\ninterest over time. However, existing SSM-based approaches ignore the impact of\nirregular time intervals within historical user interactions, making it\ndifficult to model complexed user-item transitions in sequences. To address\nthis issue, we propose a hybrid SSM-based model called SS4Rec for\ncontinuous-time sequential recommendation. SS4Rec integrates a time-aware SSM\nto handle irregular time intervals and a relation-aware SSM to model contextual\ndependencies, enabling it to infer user interest from both temporal and\nsequential perspectives. In the training process, the time-aware SSM and the\nrelation-aware SSM are discretized by variable stepsizes according to user\ninteraction time intervals and input data, respectively. This helps capture the\ncontinuous dependency from irregular time intervals and provides time-specific\npersonalized recommendations. Experimental studies on five benchmark datasets\ndemonstrate the superiority and effectiveness of SS4Rec.\n","authors":["Wei Xiao","Huiying Wang","Qifeng Zhou","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15005v3","updated":"2025-02-12T04:50:07Z","published":"2024-12-19T16:20:42Z","title":"DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start\n  Cross-Domain Recommendation","summary":"  Recommender systems are widely used in various real-world applications, but\nthey often encounter the persistent challenge of the user cold-start problem.\nCross-domain recommendation (CDR), which leverages user interactions from one\ndomain to improve prediction performance in another, has emerged as a promising\nsolution. However, users with similar preferences in the source domain may\nexhibit different interests in the target domain. Therefore, directly\ntransferring embeddings may introduce irrelevant source-domain collaborative\ninformation. In this paper, we propose a novel graph-based disentangled\ncontrastive learning framework to capture fine-grained user intent and filter\nout irrelevant collaborative information, thereby avoiding negative transfer.\nSpecifically, for each domain, we use a multi-channel graph encoder to capture\ndiverse user intents. We then construct the affinity graph in the embedding\nspace and perform multi-step random walks to capture high-order user similarity\nrelationships. Treating one domain as the target, we propose a disentangled\nintent-wise contrastive learning approach, guided by user similarity, to refine\nthe bridging of user intents across domains. Extensive experiments on four\nbenchmark CDR datasets demonstrate that DisCo consistently outperforms existing\nstate-of-the-art baselines, thereby validating the effectiveness of both DisCo\nand its components.\n","authors":["Hourun Li","Yifan Wang","Zhiping Xiao","Jia Yang","Changling Zhou","Ming Zhang","Wei Ju"],"pdf_url":"https://arxiv.org/pdf/2412.15005v3.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2501.17630v2","updated":"2025-02-12T04:28:04Z","published":"2025-01-29T13:08:17Z","title":"Uncertainty Quantification and Decomposition for LLM-based\n  Recommendation","summary":"  Despite the widespread adoption of large language models (LLMs) for\nrecommendation, we demonstrate that LLMs often exhibit uncertainty in their\nrecommendations. To ensure the trustworthy use of LLMs in generating\nrecommendations, we emphasize the importance of assessing the reliability of\nrecommendations generated by LLMs. We start by introducing a novel framework\nfor estimating the predictive uncertainty to quantitatively measure the\nreliability of LLM-based recommendations. We further propose to decompose the\npredictive uncertainty into recommendation uncertainty and prompt uncertainty,\nenabling in-depth analyses of the primary source of uncertainty. Through\nextensive experiments, we (1) demonstrate predictive uncertainty effectively\nindicates the reliability of LLM-based recommendations, (2) investigate the\norigins of uncertainty with decomposed uncertainty measures, and (3) propose\nuncertainty-aware prompting for a lower predictive uncertainty and enhanced\nrecommendation. Our source code and model weights are available at\nhttps://github.com/WonbinKweon/UNC_LLM_REC_WWW2025\n","authors":["Wonbin Kweon","Sanghwan Jang","SeongKu Kang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2501.17630v2.pdf","comment":"WWW 2025"},{"id":"http://arxiv.org/abs/2408.09380v4","updated":"2025-02-12T04:00:41Z","published":"2024-08-18T06:41:46Z","title":"ELASTIC: Efficient Linear Attention for Sequential Interest Compression","summary":"  State-of-the-art sequential recommendation models heavily rely on\ntransformer's attention mechanism. However, the quadratic computational and\nmemory complexities of self attention have limited its scalability for modeling\nusers' long range behaviour sequences. To address this problem, we propose\nELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,\nrequiring only linear time complexity and decoupling model capacity from\ncomputational cost. Specifically, ELASTIC introduces a fixed length interest\nexperts with linear dispatcher attention mechanism which compresses the\nlong-term behaviour sequences to a significantly more compact representation\nwhich reduces up to 90% GPU memory usage with x2.7 inference speed up. The\nproposed linear dispatcher attention mechanism significantly reduces the\nquadratic complexity and makes the model feasible for adequately modeling\nextremely long sequences. Moreover, in order to retain the capacity for\nmodeling various user interests, ELASTIC initializes a vast learnable interest\nmemory bank and sparsely retrieves compressed user's interests from the memory\nwith a negligible computational overhead. The proposed interest memory\nretrieval technique significantly expands the cardinality of available interest\nspace while keeping the same computational cost, thereby striking a trade-off\nbetween recommendation accuracy and efficiency. To validate the effectiveness\nof our proposed ELASTIC, we conduct extensive experiments on various public\ndatasets and compare it with several strong sequential recommenders.\nExperimental results demonstrate that ELASTIC consistently outperforms\nbaselines by a significant margin and also highlight the computational\nefficiency of ELASTIC when modeling long sequences. We will make our\nimplementation code publicly available.\n","authors":["Jiaxin Deng","Shiyao Wang","Song Lu","Yinfeng Li","Xinchen Luo","Yuanjun Liu","Peixing Xu","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.09380v4.pdf","comment":"We hereby withdraw this paper from arXiv due to incomplete\n  experiments. Upon further review, we have determined that additional\n  experimental work is necessary to fully validate our findings and conclusions"},{"id":"http://arxiv.org/abs/2502.08071v1","updated":"2025-02-12T02:24:26Z","published":"2025-02-12T02:24:26Z","title":"Collaborative Filtering Meets Spectrum Shift: Connecting User-Item\n  Interaction with Graph-Structured Side Information","summary":"  Graph Neural Network (GNN) has demonstrated their superiority in\ncollaborative filtering, where the user-item (U-I) interaction bipartite graph\nserves as the fundamental data format. However, when graph-structured side\ninformation (e.g., multimodal similarity graphs or social networks) is\nintegrated into the U-I bipartite graph, existing graph collaborative filtering\nmethods fall short of achieving satisfactory performance. We quantitatively\nanalyze this problem from a spectral perspective. Recall that a bipartite graph\npossesses a full spectrum within the range of [-1, 1], with the highest\nfrequency exactly achievable at -1 and the lowest frequency at 1; however, we\nobserve as more side information is incorporated, the highest frequency of the\naugmented adjacency matrix progressively shifts rightward. This spectrum shift\nphenomenon has caused previous approaches built for the full spectrum [-1, 1]\nto assign mismatched importance to different frequencies. To this end, we\npropose Spectrum Shift Correction (dubbed SSC), incorporating shifting and\nscaling factors to enable spectral GNNs to adapt to the shifted spectrum.\nUnlike previous paradigms of leveraging side information, which necessitate\ntailored designs for diverse data types, SSC directly connects traditional\ngraph collaborative filtering with any graph-structured side information.\nExperiments on social and multimodal recommendation demonstrate the\neffectiveness of SSC, achieving relative improvements of up to 23% without\nincurring any additional computational overhead.\n","authors":["Yunhang He","Cong Xu","Jun Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07219v2","updated":"2025-02-12T01:58:22Z","published":"2025-02-11T03:25:42Z","title":"DOGR: Leveraging Document-Oriented Contrastive Learning in Generative\n  Retrieval","summary":"  Generative retrieval constitutes an innovative approach in information\nretrieval, leveraging generative language models (LM) to generate a ranked list\nof document identifiers (docid) for a given query. It simplifies the retrieval\npipeline by replacing the large external index with model parameters. However,\nexisting works merely learned the relationship between queries and document\nidentifiers, which is unable to directly represent the relevance between\nqueries and documents. To address the above problem, we propose a novel and\ngeneral generative retrieval framework, namely Leveraging Document-Oriented\nContrastive Learning in Generative Retrieval (DOGR), which leverages\ncontrastive learning to improve generative retrieval tasks. It adopts a\ntwo-stage learning strategy that captures the relationship between queries and\ndocuments comprehensively through direct interactions. Furthermore, negative\nsampling methods and corresponding contrastive learning objectives are\nimplemented to enhance the learning of semantic representations, thereby\npromoting a thorough comprehension of the relationship between queries and\ndocuments. Experimental results demonstrate that DOGR achieves state-of-the-art\nperformance compared to existing generative retrieval methods on two public\nbenchmark datasets. Further experiments have shown that our framework is\ngenerally effective for common identifier construction techniques.\n","authors":["Penghao Lu","Xin Dong","Yuansheng Zhou","Lei Cheng","Chuan Yuan","Linjian Mo"],"pdf_url":"https://arxiv.org/pdf/2502.07219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08845v1","updated":"2025-02-12T23:32:09Z","published":"2025-02-12T23:32:09Z","title":"Optimal Dataset Size for Recommender Systems: Evaluating Algorithms'\n  Performance via Downsampling","summary":"  This thesis investigates dataset downsampling as a strategy to optimize\nenergy efficiency in recommender systems while maintaining competitive\nperformance. With increasing dataset sizes posing computational and\nenvironmental challenges, this study explores the trade-offs between energy\nefficiency and recommendation quality in Green Recommender Systems, which aim\nto reduce environmental impact. By applying two downsampling approaches to\nseven datasets, 12 algorithms, and two levels of core pruning, the research\ndemonstrates significant reductions in runtime and carbon emissions. For\nexample, a 30% downsampling portion can reduce runtime by 52% compared to the\nfull dataset, leading to a carbon emission reduction of up to 51.02 KgCO2e\nduring the training of a single algorithm on a single dataset. The analysis\nreveals that algorithm performance under different downsampling portions\ndepends on factors like dataset characteristics, algorithm complexity, and the\nspecific downsampling configuration (scenario dependent). Some algorithms,\nwhich showed lower nDCG@10 scores compared to higher-performing ones, exhibited\nlower sensitivity to the amount of training data, offering greater potential\nfor efficiency in lower downsampling portions. On average, these algorithms\nretained 81% of full-size performance using only 50% of the training set. In\ncertain downsampling configurations, where more users were progressively\nincluded while keeping the test set size fixed, they even showed higher nDCG@10\nscores than when using the full dataset. These findings highlight the\nfeasibility of balancing sustainability and effectiveness, providing insights\nfor designing energy-efficient recommender systems and promoting sustainable AI\npractices.\n","authors":["Ardalan Arabzadeh","Joeran Beel","Tobias Vente"],"pdf_url":"https://arxiv.org/pdf/2502.08845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08826v1","updated":"2025-02-12T22:33:41Z","published":"2025-02-12T22:33:41Z","title":"Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation","summary":"  Large Language Models (LLMs) struggle with hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation enhancing factual and updated grounding. Recent advances in\nmultimodal learning have led to the development of Multimodal RAG,\nincorporating multiple modalities such as text, images, audio, and video to\nenhance the generated outputs. However, cross-modal alignment and reasoning\nintroduce unique challenges to Multimodal RAG, distinguishing it from\ntraditional unimodal RAG. This survey offers a structured and comprehensive\nanalysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,\nevaluation, methodologies, and innovations in retrieval, fusion, augmentation,\nand generation. We precisely review training strategies, robustness\nenhancements, and loss functions, while also exploring the diverse Multimodal\nRAG scenarios. Furthermore, we discuss open challenges and future research\ndirections to support advancements in this evolving field. This survey lays the\nfoundation for developing more capable and reliable AI systems that effectively\nleverage multimodal dynamic external knowledge bases. Resources are available\nat https://github.com/llm-lab-org/Multimodal-RAG-Survey.\n","authors":["Mohammad Mahdi Abootorabi","Amirhosein Zobeiri","Mahdi Dehghani","Mohammadali Mohammadkhani","Bardia Mohammadi","Omid Ghahroodi","Mahdieh Soleymani Baghshah","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2502.08826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18870v2","updated":"2025-02-12T21:37:54Z","published":"2024-10-24T15:57:17Z","title":"End-to-end Training for Recommendation with Language-based User Profiles","summary":"  There is a growing interest in natural language-based user profiles for\nrecommender systems, which aims to enhance transparency and scrutability\ncompared with embedding-based methods. Existing studies primarily generate\nthese profiles using zero-shot inference from large language models (LLMs), but\ntheir quality remains insufficient, leading to suboptimal recommendation\nperformance. In this paper, we introduce LangPTune, the first end-to-end\ntraining framework to optimize LLM-generated user profiles. Our method\nsignificantly outperforms zero-shot approaches by explicitly training the LLM\nfor the recommendation objective. Through extensive evaluations across diverse\ntraining configurations and benchmarks, we demonstrate that LangPTune not only\nsurpasses zero-shot baselines but can also matches the performance of\nstate-of-the-art embedding-based methods. Finally, we investigate whether the\ntraining procedure preserves the interpretability of these profiles compared to\nzero-shot inference through both GPT-4 simulations and crowdworker user\nstudies. Implementation of LangPTune can be found at\nhttps://github.com/ZhaolinGao/LangPTune.\n","authors":["Zhaolin Gao","Joyce Zhou","Yijia Dai","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2410.18870v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.08644v1","updated":"2025-02-12T18:58:34Z","published":"2025-02-12T18:58:34Z","title":"Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and\n  learning in neural networks","summary":"  The brain can rapidly adapt to new contexts and learn from limited data, a\ncoveted characteristic that artificial intelligence algorithms have struggled\nto mimic. Inspired by oscillatory rhythms of the mechanical structures of\nneural cells, we developed a learning paradigm that is based on oscillations in\nlink strengths and associates learning with the coordination of these\noscillations. We find that this paradigm yields rapid adaptation and learning\nin artificial neural networks. Link oscillations can rapidly change\ncoordination, endowing the network with the ability to sense subtle context\nchanges in an unsupervised manner. In other words, the network generates the\nmissing contextual tokens required to perform as a generalist AI architecture\ncapable of predicting dynamics in multiple contexts. Oscillations also allow\nthe network to extrapolate dynamics to never-seen-before contexts. These\ncapabilities make our learning paradigm a powerful starting point for novel\nmodels of learning and cognition. Furthermore, learning through link\ncoordination is agnostic to the specifics of the neural network architecture,\nhence our study opens the door for introducing rapid adaptation and learning\ncapabilities into leading AI models.\n","authors":["Hoony Kang","Wolfgang Losert"],"pdf_url":"https://arxiv.org/pdf/2502.08644v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.08640v1","updated":"2025-02-12T18:55:43Z","published":"2025-02-12T18:55:43Z","title":"Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs","summary":"  As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.\n","authors":["Mantas Mazeika","Xuwang Yin","Rishub Tamirisa","Jaehyuk Lim","Bruce W. Lee","Richard Ren","Long Phan","Norman Mu","Adam Khoja","Oliver Zhang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08637v1","updated":"2025-02-12T18:54:10Z","published":"2025-02-12T18:54:10Z","title":"Joint Transmit and Pinching Beamforming for PASS: Optimization-Based or\n  Learning-Based?","summary":"  A novel pinching antenna system (PASS)-enabled downlink multi-user\nmultiple-input single-output (MISO) framework is proposed. PASS consists of\nmultiple waveguides spanning over thousands of wavelength, which equip numerous\nlow-cost dielectric particles, named pinching antennas (PAs), to radiate\nsignals into free space. The positions of PAs can be reconfigured to change\nboth the large-scale path losses and phases of signals, thus facilitating the\nnovel pinching beamforming design. A sum rate maximization problem is\nformulated, which jointly optimizes the transmit and pinching beamforming to\nadaptively achieve constructive signal enhancement and destructive interference\nmitigation. To solve this highly coupled and nonconvex problem, both\noptimization-based and learning-based methods are proposed. 1) For the\noptimization-based method, a majorization-minimization and penalty dual\ndecomposition (MM-PDD) algorithm is developed, which handles the nonconvex\ncomplex exponential component using a Lipschitz surrogate function and then\ninvokes PDD for problem decoupling. 2) For the learning-based method, a novel\nKarush-Kuhn-Tucker (KKT)-guided dual learning (KDL) approach is proposed, which\nenables KKT solutions to be reconstructed in a data-driven manner by learning\ndual variables. Following this idea, a KDL-Tranformer algorithm is developed,\nwhich captures both inter-PA/inter-user dependencies and\nchannel-state-information (CSI)-beamforming dependencies by attention\nmechanisms. Simulation results demonstrate that: i) The proposed PASS framework\nsignificantly outperforms conventional massive multiple input multiple output\n(MIMO) system even with a few PAs. ii) The proposed KDL-Transformer can improve\nover 30% system performance than MM-PDD algorithm, while achieving a\nmillisecond-level response on modern GPUs.\n","authors":["Xiaoxia Xu","Xidong Mu","Yuanwei Liu","Arumugam Nallanathan"],"pdf_url":"https://arxiv.org/pdf/2502.08637v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2502.08634v1","updated":"2025-02-12T18:48:12Z","published":"2025-02-12T18:48:12Z","title":"Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale\n  Implicit Neural Representation","summary":"  Purpose: To develop and validate a novel image reconstruction technique using\nimplicit neural representations (INR) for multi-view thick-slice acquisitions\nwhile reducing the scan time but maintaining high signal-to-noise ratio (SNR).\nMethods: We propose Rotating-view super-resolution (ROVER)-MRI, an unsupervised\nneural network-based algorithm designed to reconstruct MRI data from multi-view\nthick slices, effectively reducing scan time by 2-fold while maintaining fine\nanatomical details. We compare our method to both bicubic interpolation and the\ncurrent state-of-the-art regularized least-squares super-resolution\nreconstruction (LS-SRR) technique. Validation is performed using ground-truth\nex-vivo monkey brain data, and we demonstrate superior reconstruction quality\nacross several in-vivo human datasets. Notably, we achieve the reconstruction\nof a whole human brain in-vivo T2-weighted image with an unprecedented\n180{\\mu}m isotropic spatial resolution, accomplished in just 17 minutes of scan\ntime on a 7T MRI scanner. Results: ROVER-MRI outperformed LS-SRR method in\nterms of reconstruction quality with 22.4% lower relative error (RE) and 7.5%\nlower full-width half maximum (FWHM) indicating better preservation of fine\nstructural details in nearly half the scan time. Conclusion: ROVER-MRI offers\nan efficient and robust approach for mesoscale MR imaging, enabling rapid,\nhigh-resolution whole-brain scans. Its versatility holds great promise for\nresearch applications requiring anatomical details and time-efficient imaging.\n","authors":["Jun Lyu","Lipeng Ning","William Consagra","Qiang Liu","Richard J. Rushmore","Berkin Bilgic","Yogesh Rathi"],"pdf_url":"https://arxiv.org/pdf/2502.08634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08632v1","updated":"2025-02-12T18:47:13Z","published":"2025-02-12T18:47:13Z","title":"Necessary and Sufficient Oracles: Toward a Computational Taxonomy For\n  Reinforcement Learning","summary":"  Algorithms for reinforcement learning (RL) in large state spaces crucially\nrely on supervised learning subroutines to estimate objects such as value\nfunctions or transition probabilities. Since only the simplest supervised\nlearning problems can be solved provably and efficiently, practical performance\nof an RL algorithm depends on which of these supervised learning \"oracles\" it\nassumes access to (and how they are implemented). But which oracles are better\nor worse? Is there a minimal oracle?\n  In this work, we clarify the impact of the choice of supervised learning\noracle on the computational complexity of RL, as quantified by the oracle\nstrength. First, for the task of reward-free exploration in Block MDPs in the\nstandard episodic access model -- a ubiquitous setting for RL with function\napproximation -- we identify two-context regression as a minimal oracle, i.e.\nan oracle that is both necessary and sufficient (under a mild regularity\nassumption). Second, we identify one-context regression as a near-minimal\noracle in the stronger reset access model, establishing a provable\ncomputational benefit of resets in the process. Third, we broaden our focus to\nLow-Rank MDPs, where we give cryptographic evidence that the analogous oracle\nfrom the Block MDP setting is insufficient.\n","authors":["Dhruv Rohatgi","Dylan J. Foster"],"pdf_url":"https://arxiv.org/pdf/2502.08632v1.pdf","comment":"84 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.04689v2","updated":"2025-02-12T18:36:24Z","published":"2025-02-07T06:30:33Z","title":"ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning","summary":"  Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.\n","authors":["Yuwei Yin","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.04689v2.pdf","comment":"20 pages. Code: https://github.com/YuweiYin/ARR"},{"id":"http://arxiv.org/abs/2501.18823v2","updated":"2025-02-12T18:35:15Z","published":"2025-01-31T00:36:30Z","title":"Transcoders Beat Sparse Autoencoders for Interpretability","summary":"  Sparse autoencoders (SAEs) extract human-interpretable features from deep\nneural networks by transforming their activations into a sparse, higher\ndimensional latent space, and then reconstructing the activations from these\nlatents. Transcoders are similar to SAEs, but they are trained to reconstruct\nthe output of a component of a deep network given its input. In this work, we\ncompare the features found by transcoders and SAEs trained on the same model\nand data, finding that transcoder features are significantly more\ninterpretable. We also propose skip transcoders, which add an affine skip\nconnection to the transcoder architecture, and show that these achieve lower\nreconstruction loss with no effect on interpretability.\n","authors":["Gonçalo Paulo","Stepan Shabalin","Nora Belrose"],"pdf_url":"https://arxiv.org/pdf/2501.18823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13734v3","updated":"2025-02-12T18:32:37Z","published":"2025-01-23T15:10:51Z","title":"Sample complexity of data-driven tuning of model hyperparameters in\n  neural networks with structured parameter-dependent dual function","summary":"  Modern machine learning algorithms, especially deep learning based\ntechniques, typically involve careful hyperparameter tuning to achieve the best\nperformance. Despite the surge of intense interest in practical techniques like\nBayesian optimization and random search based approaches to automating this\nlaborious and compute intensive task, the fundamental learning theoretic\ncomplexity of tuning hyperparameters for deep neural networks is poorly\nunderstood. Inspired by this glaring gap, we initiate the formal study of\nhyperparameter tuning complexity in deep learning through a recently introduced\ndata driven setting. We assume that we have a series of deep learning tasks,\nand we have to tune hyperparameters to do well on average over the distribution\nof tasks. A major difficulty is that the utility function as a function of the\nhyperparameter is very volatile and furthermore, it is given implicitly by an\noptimization problem over the model parameters. To tackle this challenge, we\nintroduce a new technique to characterize the discontinuities and oscillations\nof the utility function on any fixed problem instance as we vary the\nhyperparameter; our analysis relies on subtle concepts including tools from\ndifferential/algebraic geometry and constrained optimization. This can be used\nto show that the learning theoretic complexity of the corresponding family of\nutility functions is bounded. We instantiate our results and provide sample\ncomplexity bounds for concrete applications tuning a hyperparameter that\ninterpolates neural activation functions and setting the kernel parameter in\ngraph neural networks.\n","authors":["Maria-Florina Balcan","Anh Tuan Nguyen","Dravyansh Sharma"],"pdf_url":"https://arxiv.org/pdf/2501.13734v3.pdf","comment":"50 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.08628v1","updated":"2025-02-12T18:30:36Z","published":"2025-02-12T18:30:36Z","title":"Concentration Inequalities for the Stochastic Optimization of Unbounded\n  Objectives with Application to Denoising Score Matching","summary":"  We derive novel concentration inequalities that bound the statistical error\nfor a large class of stochastic optimization problems, focusing on the case of\nunbounded objective functions. Our derivations utilize the following tools: 1)\nA new form of McDiarmid's inequality that is based on sample dependent one\ncomponent difference bounds and which leads to a novel uniform law of large\nnumbers result for unbounded functions. 2) A Rademacher complexity bound for\nfamilies of functions that satisfy an appropriate local Lipschitz property. As\nan application of these results, we derive statistical error bounds for\ndenoising score matching (DSM), an application that inherently requires one to\nconsider unbounded objective functions, even when the data distribution has\nbounded support. In addition, our results establish the benefit of sample reuse\nin algorithms that employ easily sampled auxiliary random variables in addition\nto the training data, e.g., as in DSM, which uses auxiliary Gaussian random\nvariables.\n","authors":["Jeremiah Birrell"],"pdf_url":"https://arxiv.org/pdf/2502.08628v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2410.06976v2","updated":"2025-02-12T18:27:29Z","published":"2024-10-09T15:15:40Z","title":"Matcha: Mitigating Graph Structure Shifts with Test-Time Adaptation","summary":"  Powerful as they are, graph neural networks (GNNs) are known to be vulnerable\nto distribution shifts. Recently, test-time adaptation (TTA) has attracted\nattention due to its ability to adapt a pre-trained model to a target domain,\nwithout re-accessing the source domain. However, existing TTA algorithms are\nprimarily designed for attribute shifts in vision tasks, where samples are\nindependent. These methods perform poorly on graph data that experience\nstructure shifts, where node connectivity differs between source and target\ngraphs. We attribute this performance gap to the distinct impact of node\nattribute shifts versus graph structure shifts: the latter significantly\ndegrades the quality of node representations and blurs the boundaries between\ndifferent node categories. To address structure shifts in graphs, we propose\nMatcha, an innovative framework designed for effective and efficient adaptation\nto structure shifts by adjusting the htop-aggregation parameters in GNNs. To\nenhance the representation quality, we design a prediction-informed clustering\nloss to encourage the formation of distinct clusters for different node\ncategories. Additionally, Matcha seamlessly integrates with existing TTA\nalgorithms, allowing it to handle attribute shifts effectively while improving\noverall performance under combined structure and attribute shifts. We validate\nthe effectiveness of Matcha on both synthetic and real-world datasets,\ndemonstrating its robustness across various combinations of structure and\nattribute shifts. Our code is available at https://github.com/baowenxuan/Matcha .\n","authors":["Wenxuan Bao","Zhichen Zeng","Zhining Liu","Hanghang Tong","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2410.06976v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08625v1","updated":"2025-02-12T18:25:13Z","published":"2025-02-12T18:25:13Z","title":"Randomness of Low-Layer Parameters Determines Confusing Samples in Terms\n  of Interaction Representations of a DNN","summary":"  In this paper, we find that the complexity of interactions encoded by a deep\nneural network (DNN) can explain its generalization power. We also discover\nthat the confusing samples of a DNN, which are represented by non-generalizable\ninteractions, are determined by its low-layer parameters. In comparison, other\nfactors, such as high-layer parameters and network architecture, have much less\nimpact on the composition of confusing samples. Two DNNs with different\nlow-layer parameters usually have fully different sets of confusing samples,\neven though they have similar performance. This finding extends the\nunderstanding of the lottery ticket hypothesis, and well explains distinctive\nrepresentation power of different DNNs.\n","authors":["Junpeng Zhang","Lei Cheng","Qing Li","Liang Lin","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13312v2","updated":"2025-02-12T18:22:42Z","published":"2025-01-23T01:43:31Z","title":"Tensor-Var: Variational Data Assimilation in Tensor Product Feature\n  Space","summary":"  Variational data assimilation estimates the dynamical system states by\nminimizing a cost function that fits the numerical models with observational\ndata. The widely used method, four-dimensional variational assimilation\n(4D-Var), has two primary challenges: (1) computationally demanding for complex\nnonlinear systems and (2) relying on state-observation mappings, which are\noften not perfectly known. Deep learning (DL) has been used as a more\nexpressive class of efficient model approximators to address these challenges.\nHowever, integrating such models into 4D-Var remains challenging due to their\ninherent nonlinearities and the lack of theoretical guarantees for consistency\nin assimilation results. In this paper, we propose Tensor-Var to address these\nchallenges using kernel Conditional Mean Embedding (CME). Tensor-Var improves\noptimization efficiency by characterizing system dynamics and state-observation\nmappings as linear operators, leading to a convex cost function in the feature\nspace. Furthermore, our method provides a new perspective to incorporate CME\ninto 4D-Var, offering theoretical guarantees of consistent assimilation results\nbetween the original and feature spaces. To improve scalability, we propose a\nmethod to learn deep features (DFs) using neural networks within the Tensor-Var\nframework. Experiments on chaotic systems and global weather prediction with\nreal-time observations show that Tensor-Var outperforms conventional and DL\nhybrid 4D-Var baselines in accuracy while achieving efficiency comparable to\nthe static 3D-Var method.\n","authors":["Yiming Yang","Xiaoyuan Cheng","Daniel Giles","Sibo Cheng","Yi He","Xiao Xue","Boli Chen","Yukun Hu"],"pdf_url":"https://arxiv.org/pdf/2501.13312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08622v1","updated":"2025-02-12T18:20:41Z","published":"2025-02-12T18:20:41Z","title":"Forecasting Drought Using Machine Learning in California","summary":"  Drought is a frequent and costly natural disaster in California, with major\nnegative impacts on agricultural production and water resource availability,\nparticularly groundwater. This study investigated the performance of applying\ndifferent machine learning approaches to predicting the U.S. Drought Monitor\nclassification in California. Four approaches were used: a convolutional neural\nnetwork (CNN), random forest, XGBoost, and long short term memory (LSTM)\nrecurrent neural network, and compared to a baseline persistence model. We\nevaluated the models' performance in predicting severe drought (USDM drought\ncategory D2 or higher) using a macro F1 binary classification metric. The LSTM\nmodel emerged as the top performer, followed by XGBoost, CNN, and random\nforest. Further evaluation of our results at the county level suggested that\nthe LSTM model would perform best in counties with more consistent drought\npatterns and where severe drought was more common, and the LSTM model would\nperform worse where drought scores increased rapidly. Utilizing 30 weeks of\nhistorical data, the LSTM model successfully forecasted drought scores for a\n12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less\nthan half a drought category on a scale of 0 to 5. Additionally, the LSTM\nachieved a macro F1 score of 0.9, indicating high accuracy in binary\nclassification for severe drought conditions. Evaluation of different window\nand future horizon sizes in weeks suggested that at least 24 weeks of data\nwould result in the best performance, with best performance for shorter horizon\nsizes, particularly less than eight weeks.\n","authors":["Nan K. Li","Angela Chang","David Sherman"],"pdf_url":"https://arxiv.org/pdf/2502.08622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08620v1","updated":"2025-02-12T18:15:35Z","published":"2025-02-12T18:15:35Z","title":"Mathematical Data Science","summary":"  Can machine learning help discover new mathematical structures? In this\narticle we discuss an approach to doing this which one can call \"mathematical\ndata science\". In this paradigm, one studies mathematical objects collectively\nrather than individually, by creating datasets and doing machine learning\nexperiments and interpretations. After an overview, we present two case\nstudies: murmurations in number theory and loadings of partitions related to\nKronecker coefficients in representation theory and combinatorics.\n","authors":["Michael R. Douglas","Kyu-Hwan Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01512v2","updated":"2025-02-12T18:11:46Z","published":"2025-02-03T16:46:46Z","title":"Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices","summary":"  Circular and non-flat data distributions are prevalent across diverse domains\nof data science, yet their specific geometric structures often remain\nunderutilized in machine learning frameworks. A principled approach to\naccounting for the underlying geometry of such data is pivotal, particularly\nwhen extending statistical models, like the pervasive Gaussian distribution. In\nthis work, we tackle those issue by focusing on the manifold of symmetric\npositive definite matrices, a key focus in information geometry. We introduced\na non-isotropic wrapped Gaussian by leveraging the exponential map, we derive\ntheoretical properties of this distribution and propose a maximum likelihood\nframework for parameter estimation. Furthermore, we reinterpret established\nclassifiers on SPD through a probabilistic lens and introduce new classifiers\nbased on the wrapped Gaussian model. Experiments on synthetic and real-world\ndatasets demonstrate the robustness and flexibility of this geometry-aware\ndistribution, underscoring its potential to advance manifold-based data\nanalysis. This work lays the groundwork for extending classical machine\nlearning and statistical methods to more complex and structured data.\n","authors":["Thibault de Surrel","Fabien Lotte","Sylvain Chevallier","Florian Yger"],"pdf_url":"https://arxiv.org/pdf/2502.01512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08612v1","updated":"2025-02-12T18:01:04Z","published":"2025-02-12T18:01:04Z","title":"Continuous Cardiac Arrest Prediction in ICU using PPG Foundation Model","summary":"  Non-invasive patient monitoring for tracking and predicting adverse acute\nhealth events is an emerging area of research. We pursue in-hospital cardiac\narrest (IHCA) prediction using only single-channel finger photoplethysmography\n(PPG) signals. Our proposed two-stage model Feature Extractor-Aggregator\nNetwork (FEAN) leverages powerful representations from pre-trained PPG\nfoundation models (PPG-GPT of size up to 1 Billion) stacked with sequential\nclassification models. We propose two FEAN variants (\"1H\", \"FH\") which use the\nlatest one-hour and (max) 24-hour history to make decisions respectively. Our\nstudy is the first to present IHCA prediction results in ICU patients using\nonly unimodal (continuous PPG signal) waveform deep representations. With our\nbest model, we obtain an average of 0.79 AUROC over 24~h prediction window\nbefore CA event onset with our model peaking performance at 0.82 one hour\nbefore CA. We also provide a comprehensive analysis of our model through\narchitectural tuning and PaCMAP visualization of patient health trajectory in\nlatent space.\n","authors":["Saurabh Kataria","Ran Xiao","Timothy Ruchti","Matthew Clark","Jiaying Lu","Randall J. Lee","Jocelyn Grunwell","Xiao Hu"],"pdf_url":"https://arxiv.org/pdf/2502.08612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08611v1","updated":"2025-02-12T17:59:21Z","published":"2025-02-12T17:59:21Z","title":"Robustly Learning Monotone Generalized Linear Models via Data\n  Augmentation","summary":"  We study the task of learning Generalized Linear models (GLMs) in the\nagnostic model under the Gaussian distribution. We give the first\npolynomial-time algorithm that achieves a constant-factor approximation for\n\\textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners\nsucceed for a substantially smaller class of activations. Our work resolves a\nwell-known open problem, by developing a robust counterpart to the classical\nGLMtron algorithm (Kakade et al., 2011). Our robust learner applies more\ngenerally, encompassing all monotone activations with bounded\n$(2+\\zeta)$-moments, for any fixed $\\zeta>0$ -- a condition that is essentially\nnecessary. To obtain our results, we leverage a novel data augmentation\ntechnique with decreasing Gaussian noise injection and prove a number of\nstructural results that may be useful in other settings.\n","authors":["Nikos Zarifis","Puqian Wang","Ilias Diakonikolas","Jelena Diakonikolas"],"pdf_url":"https://arxiv.org/pdf/2502.08611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15537v3","updated":"2025-02-12T17:59:14Z","published":"2024-02-23T04:52:08Z","title":"Evaluating the Performance of ChatGPT for Spam Email Detection","summary":"  Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction with (or without) a\nfew demonstrations. We also investigate how the number of demonstrations in the\nprompt affects the performance of ChatGPT. For comparison, we also implement\nfive popular benchmark methods, including naive Bayes, support vector machines\n(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and\nBERT classifiers. Through extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset. This study provides insights into the potential and limitations of\nChatGPT for spam identification, highlighting its potential as a viable\nsolution for resource-constrained language domains.\n","authors":["Shijing Si","Yuwei Wu","Le Tang","Yugui Zhang","Jedrek Wosik","Qinliang Su"],"pdf_url":"https://arxiv.org/pdf/2402.15537v3.pdf","comment":"12 pages, 4 figures; Accepted by Pacific Journal of Optimization\n  (PJO)"},{"id":"http://arxiv.org/abs/2409.05655v2","updated":"2025-02-12T17:55:53Z","published":"2024-09-09T14:22:19Z","title":"Interactive incremental learning of generalizable skills with local\n  trajectory modulation","summary":"  The problem of generalization in learning from demonstration (LfD) has\nreceived considerable attention over the years, particularly within the context\nof movement primitives, where a number of approaches have emerged. Recently,\ntwo important approaches have gained recognition. While one leverages\nvia-points to adapt skills locally by modulating demonstrated trajectories,\nanother relies on so-called task-parameterized models that encode movements\nwith respect to different coordinate systems, using a product of probabilities\nfor generalization. While the former are well-suited to precise, local\nmodulations, the latter aim at generalizing over large regions of the workspace\nand often involve multiple objects. Addressing the quality of generalization by\nleveraging both approaches simultaneously has received little attention. In\nthis work, we propose an interactive imitation learning framework that\nsimultaneously leverages local and global modulations of trajectory\ndistributions. Building on the kernelized movement primitives (KMP) framework,\nwe introduce novel mechanisms for skill modulation from direct human corrective\nfeedback. Our approach particularly exploits the concept of via-points to\nincrementally and interactively 1) improve the model accuracy locally, 2) add\nnew objects to the task during execution and 3) extend the skill into regions\nwhere demonstrations were not provided. We evaluate our method on a bearing\nring-loading task using a torque-controlled, 7-DoF, DLR SARA robot.\n","authors":["Markus Knauer","Alin Albu-Schäffer","Freek Stulp","João Silvério"],"pdf_url":"https://arxiv.org/pdf/2409.05655v2.pdf","comment":"Accepted at IEEE Robotics and Automation Letters (RA-L), 16 pages, 19\n  figures, 6 tables. See\n  https://github.com/DLR-RM/interactive-incremental-learning for further\n  information and video"},{"id":"http://arxiv.org/abs/2502.08606v1","updated":"2025-02-12T17:52:47Z","published":"2025-02-12T17:52:47Z","title":"Distillation Scaling Laws","summary":"  We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.\n","authors":["Dan Busbridge","Amitis Shidani","Floris Weers","Jason Ramapuram","Etai Littwin","Russ Webb"],"pdf_url":"https://arxiv.org/pdf/2502.08606v1.pdf","comment":"67 pages, 54 figures, 13 tables"},{"id":"http://arxiv.org/abs/2502.08605v1","updated":"2025-02-12T17:49:46Z","published":"2025-02-12T17:49:46Z","title":"CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection","summary":"  Does the intrinsic curvature of complex networks hold the key to unveiling\ngraph anomalies that conventional approaches overlook? Reconstruction-based\ngraph anomaly detection (GAD) methods overlook such geometric outliers,\nfocusing only on structural and attribute-level anomalies. To this end, we\npropose CurvGAD - a mixed-curvature graph autoencoder that introduces the\nnotion of curvature-based geometric anomalies. CurvGAD introduces two parallel\npipelines for enhanced anomaly interpretability: (1) Curvature-equivariant\ngeometry reconstruction, which focuses exclusively on reconstructing the edge\ncurvatures using a mixed-curvature, Riemannian encoder and Gaussian\nkernel-based decoder; and (2) Curvature-invariant structure and attribute\nreconstruction, which decouples structural and attribute anomalies from\ngeometric irregularities by regularizing graph curvature under discrete\nOllivier-Ricci flow, thereby isolating the non-geometric anomalies. By\nleveraging curvature, CurvGAD refines the existing anomaly classifications and\nidentifies new curvature-driven anomalies. Extensive experimentation over 10\nreal-world datasets (both homophilic and heterophilic) demonstrates an\nimprovement of up to 6.5% over state-of-the-art GAD methods.\n","authors":["Karish Grover","Geoffrey J. Gordon","Christos Faloutsos"],"pdf_url":"https://arxiv.org/pdf/2502.08605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08603v1","updated":"2025-02-12T17:44:40Z","published":"2025-02-12T17:44:40Z","title":"Scalable Thermodynamic Second-order Optimization","summary":"  Many hardware proposals have aimed to accelerate inference in AI workloads.\nLess attention has been paid to hardware acceleration of training, despite the\nenormous societal impact of rapid training of AI models. Physics-based\ncomputers, such as thermodynamic computers, offer an efficient means to solve\nkey primitives in AI training algorithms. Optimizers that normally would be\ncomputationally out-of-reach (e.g., due to expensive matrix inversions) on\ndigital hardware could be unlocked with physics-based hardware. In this work,\nwe propose a scalable algorithm for employing thermodynamic computers to\naccelerate a popular second-order optimizer called Kronecker-factored\napproximate curvature (K-FAC). Our asymptotic complexity analysis predicts\nincreasing advantage with our algorithm as $n$, the number of neurons per\nlayer, increases. Numerical experiments show that even under significant\nquantization noise, the benefits of second-order optimization can be preserved.\nFinally, we predict substantial speedups for large-scale vision and graph\nproblems based on realistic hardware characteristics.\n","authors":["Kaelan Donatella","Samuel Duffield","Denis Melanson","Maxwell Aifer","Phoebe Klett","Rajath Salegame","Zach Belateche","Gavin Crooks","Antonio J. Martinez","Patrick J. Coles"],"pdf_url":"https://arxiv.org/pdf/2502.08603v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.05771v3","updated":"2025-02-12T17:43:56Z","published":"2024-11-08T18:33:03Z","title":"Sketched Equivariant Imaging Regularization and Deep Internal Learning\n  for Inverse Problems","summary":"  Equivariant Imaging (EI) regularization has become the de-facto technique for\nunsupervised training of deep imaging networks, without any need of\nground-truth data. Observing that the EI-based unsupervised training paradigm\ncurrently has significant computational redundancy leading to inefficiency in\nhigh-dimensional applications, we propose a sketched EI regularization which\nleverages the randomized sketching techniques for acceleration. We then extend\nour sketched EI regularization to develop an accelerated deep internal learning\nframework, Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can be\nefficiently applied for single-image and task-adapted reconstruction.\nAdditionally, for network adaptation tasks, we propose a parameter-efficient\napproach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only the\nnormalization layers. Our numerical study on X-ray CT and multi-coil MRI image\nreconstruction tasks demonstrate that our approach can achieve significant\ncomputational acceleration over standard EI-based counterpart in single-input\nsetting and network adaptation at test time.\n","authors":["Guixian Xu","Jinglai Li","Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2411.05771v3.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2501.18715v2","updated":"2025-02-12T17:41:57Z","published":"2025-01-30T19:26:05Z","title":"chebgreen: Learning and Interpolating Continuous Empirical Green's\n  Functions from Data","summary":"  In this work, we present a mesh-independent, data-driven library, chebgreen,\nto mathematically model one-dimensional systems, possessing an associated\ncontrol parameter, and whose governing partial differential equation is\nunknown. The proposed method learns an Empirical Green's Function for the\nassociated, but hidden, boundary value problem, in the form of a Rational\nNeural Network from which we subsequently construct a bivariate representation\nin a Chebyshev basis. We uncover the Green's function, at an unseen control\nparameter value, by interpolating the left and right singular functions within\na suitable library, expressed as points on a manifold of Quasimatrices, while\nthe associated singular values are interpolated with Lagrange polynomials.\n","authors":["Harshwardhan Praveen","Jacob Brown","Christopher Earls"],"pdf_url":"https://arxiv.org/pdf/2501.18715v2.pdf","comment":"Code is available at https://github.com/hsharsh/chebgreen"},{"id":"http://arxiv.org/abs/2501.07602v2","updated":"2025-02-12T17:41:23Z","published":"2025-01-10T23:33:15Z","title":"An Explainable Pipeline for Machine Learning with Functional Data","summary":"  Machine learning (ML) models have shown success in applications with an\nobjective of prediction, but the algorithmic complexity of some models makes\nthem difficult to interpret. Methods have been proposed to provide insight into\nthese \"black-box\" models, but there is little research that focuses on\nsupervised ML when the model inputs are functional data. In this work, we\nconsider two applications from high-consequence spaces with objectives of\nmaking predictions using functional data inputs. One application aims to\nclassify material types to identify explosive materials given hyperspectral\ncomputed tomography scans of the materials. The other application considers the\nforensics science task of connecting an inkjet printed document to the source\nprinter using color signatures extracted by Raman spectroscopy. An instinctive\nroute to consider for analyzing these data is a data driven ML model for\nclassification, but due to the high consequence nature of the applications, we\nargue it is important to appropriately account for the nature of the data in\nthe analysis to not obscure or misrepresent patterns. As such, we propose the\nVariable importance Explainable Elastic Shape Analysis (VEESA) pipeline for\ntraining ML models with functional data that (1) accounts for the vertical and\nhorizontal variability in the functional data and (2) provides an explanation\nin the original data space of how the model uses variability in the functional\ndata for prediction. The pipeline makes use of elastic functional principal\ncomponents analysis (efPCA) to generate uncorrelated model inputs and\npermutation feature importance (PFI) to identify the principal components\nimportant for prediction. The variability captured by the important principal\ncomponents in visualized the original data space. We ultimately discuss ideas\nfor natural extensions of the VEESA pipeline and challenges for future\nresearch.\n","authors":["Katherine Goode","J. Derek Tucker","Daniel Ries","Heike Hofmann"],"pdf_url":"https://arxiv.org/pdf/2501.07602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08600v1","updated":"2025-02-12T17:39:02Z","published":"2025-02-12T17:39:02Z","title":"Two-stage hybrid models for enhancing forecasting accuracy on\n  heterogeneous time series","summary":"  Compared to local models built in a series-by-series manner, global models\nleverage relevant information across time series, resulting in improved\nforecasting performance and generalization capacity. Constructing global models\non a set of time series is becoming mainstream in the field of time series\nforecasting. However, the advantages of global models may not always be\nrealized when dealing with heterogeneous data. While they can adapt to\nheterogeneous datasets by increasing the model complexity, the model cannot be\ninfinitely complex due to the finite sample size, which poses challenges for\nthe application of global models. Additionally, determining whether the time\nseries data is homogeneous or heterogeneous can be ambiguous in practice. To\naddress these research gaps, this paper argues that the heterogeneity of the\ndata should be defined by the global model used, and for each series, the\nportion not modelled by the global model represents heterogeneity. It further\nproposes two-stage hybrid models, which include a second stage to identify and\nmodel heterogeneous patterns. In this second stage, we can estimate either all\nlocal models or sub-global models across different domains divided based on\nheterogeneity. Experiments on four open datasets reveal that the proposed\nmethods significantly outperform five existing models, indicating they\ncontribute to fully unleash the potential of global models on heterogeneous\ndatasets.\n","authors":["Junru Ren","Shaomin Wu"],"pdf_url":"https://arxiv.org/pdf/2502.08600v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.08598v1","updated":"2025-02-12T17:35:43Z","published":"2025-02-12T17:35:43Z","title":"Enhancing Diffusion Models Efficiency by Disentangling Total-Variance\n  and Signal-to-Noise Ratio","summary":"  The long sampling time of diffusion models remains a significant bottleneck,\nwhich can be mitigated by reducing the number of diffusion time steps. However,\nthe quality of samples with fewer steps is highly dependent on the noise\nschedule, i.e., the specific manner in which noise is introduced and the signal\nis reduced at each step. Although prior work has improved upon the original\nvariance-preserving and variance-exploding schedules, these approaches\n$\\textit{passively}$ adjust the total variance, without direct control over it.\nIn this work, we propose a novel total-variance/signal-to-noise-ratio\ndisentangled (TV/SNR) framework, where TV and SNR can be controlled\nindependently. Our approach reveals that different existing schedules, where\nthe TV explodes exponentially, can be $\\textit{improved}$ by setting a constant\nTV schedule while preserving the same SNR schedule. Furthermore, generalizing\nthe SNR schedule of the optimal transport flow matching significantly improves\nthe performance in molecular structure generation, achieving few step\ngeneration of stable molecules. A similar tendency is observed in image\ngeneration, where our approach with a uniform diffusion time grid performs\ncomparably to the highly tailored EDM sampler.\n","authors":["Khaled Kahouli","Winfried Ripken","Stefan Gugler","Oliver T. Unke","Klaus-Robert Müller","Shinichi Nakajima"],"pdf_url":"https://arxiv.org/pdf/2502.08598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08593v1","updated":"2025-02-12T17:32:23Z","published":"2025-02-12T17:32:23Z","title":"Toward Universal Laws of Outlier Propagation","summary":"  We argue that Algorithmic Information Theory (AIT) admits a principled way to\nquantify outliers in terms of so-called randomness deficiency. For the\nprobability distribution generated by a causal Bayesian network, we show that\nthe randomness deficiency of the joint state decomposes into randomness\ndeficiencies of each causal mechanism, subject to the Independence of\nMechanisms Principle. Accordingly, anomalous joint observations can be\nquantitatively attributed to their root causes, i.e., the mechanisms that\nbehaved anomalously. As an extension of Levin's law of randomness conservation,\nwe show that weak outliers cannot cause strong ones when Independence of\nMechanisms holds. We show how these information theoretic laws provide a better\nunderstanding of the behaviour of outliers defined with respect to existing\nscores.\n","authors":["Yuhao Wang","Aram Ebtekar","Dominik Janzing"],"pdf_url":"https://arxiv.org/pdf/2502.08593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03943v2","updated":"2025-02-12T17:29:54Z","published":"2024-10-04T22:00:13Z","title":"Oscillatory State-Space Models","summary":"  We propose Linear Oscillatory State-Space models (LinOSS) for efficiently\nlearning on long sequences. Inspired by cortical dynamics of biological neural\nnetworks, we base our proposed LinOSS model on a system of forced harmonic\noscillators. A stable discretization, integrated over time using fast\nassociative parallel scans, yields the proposed state-space model. We prove\nthat LinOSS produces stable dynamics only requiring nonnegative diagonal state\nmatrix. This is in stark contrast to many previous state-space models relying\nheavily on restrictive parameterizations. Moreover, we rigorously show that\nLinOSS is universal, i.e., it can approximate any continuous and causal\noperator mapping between time-varying functions, to desired accuracy. In\naddition, we show that an implicit-explicit discretization of LinOSS perfectly\nconserves the symmetry of time reversibility of the underlying dynamics.\nTogether, these properties enable efficient modeling of long-range\ninteractions, while ensuring stable and accurate long-horizon forecasting.\nFinally, our empirical results, spanning a wide range of time-series tasks from\nmid-range to very long-range classification and regression, as well as\nlong-horizon forecasting, demonstrate that our proposed LinOSS model\nconsistently outperforms state-of-the-art sequence models. Notably, LinOSS\noutperforms Mamba by nearly 2x and LRU by 2.5x on a sequence modeling task with\nsequences of length 50k.\n","authors":["T. Konstantin Rusch","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2410.03943v2.pdf","comment":"ICLR (Oral)"},{"id":"http://arxiv.org/abs/2408.05486v2","updated":"2025-02-12T17:29:28Z","published":"2024-08-10T08:27:58Z","title":"Topological Blindspots: Understanding and Extending Topological Deep\n  Learning Through the Lens of Expressivity","summary":"  Topological deep learning (TDL) is a rapidly growing field that seeks to\nleverage topological structure in data and facilitate learning from data\nsupported on topological objects, ranging from molecules to 3D shapes. Most TDL\narchitectures can be unified under the framework of higher-order\nmessage-passing (HOMP), which generalizes graph message-passing to higher-order\ndomains. In the first part of the paper, we explore HOMP's expressive power\nfrom a topological perspective, demonstrating the framework's inability to\ncapture fundamental topological and metric invariants such as diameter,\norientability, planarity, and homology. In addition, we demonstrate HOMP's\nlimitations in fully leveraging lifting and pooling methods on graphs. To the\nbest of our knowledge, this is the first work to study the expressivity of TDL\nfrom a \\emph{topological} perspective. In the second part of the paper, we\ndevelop two new classes of architectures -- multi-cellular networks (MCN) and\nscalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can\nreach full expressivity, but scaling it to large data objects can be\ncomputationally expansive. Designed as a more scalable alternative, SMCN still\nmitigates many of HOMP's expressivity limitations. Finally, we create new\nbenchmarks for evaluating models based on their ability to learn topological\nproperties of complexes. We then evaluate SMCN on these benchmarks and on\nreal-world graph datasets, demonstrating improvements over both HOMP baselines\nand expressive graph methods, highlighting the value of expressively leveraging\ntopological information. Code and data are available at\nhttps://github.com/yoavgelberg/SMCN.\n","authors":["Yam Eitan","Yoav Gelberg","Guy Bar-Shalom","Fabrizio Frasca","Michael Bronstein","Haggai Maron"],"pdf_url":"https://arxiv.org/pdf/2408.05486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08586v1","updated":"2025-02-12T17:19:36Z","published":"2025-02-12T17:19:36Z","title":"Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous\n  Attacks","summary":"  A high volume of recent ML security literature focuses on attacks against\naligned large language models (LLMs). These attacks may extract private\ninformation or coerce the model into producing harmful outputs. In real-world\ndeployments, LLMs are often part of a larger agentic pipeline including memory\nsystems, retrieval, web access, and API calling. Such additional components\nintroduce vulnerabilities that make these LLM-powered agents much easier to\nattack than isolated LLMs, yet relatively little work focuses on the security\nof LLM agents. In this paper, we analyze security and privacy vulnerabilities\nthat are unique to LLM agents. We first provide a taxonomy of attacks\ncategorized by threat actors, objectives, entry points, attacker observability,\nattack strategies, and inherent vulnerabilities of agent pipelines. We then\nconduct a series of illustrative attacks on popular open-source and commercial\nagents, demonstrating the immediate practical implications of their\nvulnerabilities. Notably, our attacks are trivial to implement and require no\nunderstanding of machine learning.\n","authors":["Ang Li","Yin Zhou","Vethavikashini Chithrra Raghuram","Tom Goldstein","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2502.08586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08585v1","updated":"2025-02-12T17:18:14Z","published":"2025-02-12T17:18:14Z","title":"Scalable Bilevel Loss Balancing for Multi-Task Learning","summary":"  Multi-task learning (MTL) has been widely adopted for its ability to\nsimultaneously learn multiple tasks. While existing gradient manipulation\nmethods often yield more balanced solutions than simple scalarization-based\napproaches, they typically incur a significant computational overhead of\n$\\mathcal{O}(K)$ in both time and memory, where $K$ is the number of tasks. In\nthis paper, we propose BiLB4MTL, a simple and scalable loss balancing approach\nfor MTL, formulated from a novel bilevel optimization perspective. Our method\nincorporates three key components: (i) an initial loss normalization, (ii) a\nbilevel loss-balancing formulation, and (iii) a scalable first-order algorithm\nthat requires only $\\mathcal{O}(1)$ time and memory. Theoretically, we prove\nthat BiLB4MTL guarantees convergence not only to a stationary point of the\nbilevel loss balancing problem but also to an $\\epsilon$-accurate Pareto\nstationary point for all $K$ loss functions under mild conditions. Extensive\nexperiments on diverse multi-task datasets demonstrate that BiLB4MTL achieves\nstate-of-the-art performance in both accuracy and efficiency. Code is available\nat https://github.com/OptMN-Lab/-BiLB4MTL.\n","authors":["Peiyao Xiao","Chaosheng Dong","Shaofeng Zou","Kaiyi Ji"],"pdf_url":"https://arxiv.org/pdf/2502.08585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08582v1","updated":"2025-02-12T17:14:07Z","published":"2025-02-12T17:14:07Z","title":"A method for classification of data with uncertainty using hypothesis\n  testing","summary":"  Binary classification is a task that involves the classification of data into\none of two distinct classes. It is widely utilized in various fields. However,\nconventional classifiers tend to make overconfident predictions for data that\nbelong to overlapping regions of the two class distributions or for data\noutside the distributions (out-of-distribution data). Therefore, conventional\nclassifiers should not be applied in high-risk fields where classification\nresults can have significant consequences. In order to address this issue, it\nis necessary to quantify uncertainty and adopt decision-making approaches that\ntake it into account. Many methods have been proposed for this purpose;\nhowever, implementing these methods often requires performing resampling,\nimproving the structure or performance of models, and optimizing the thresholds\nof classifiers. We propose a new decision-making approach using two types of\nhypothesis testing. This method is capable of detecting ambiguous data that\nbelong to the overlapping regions of two class distributions, as well as\nout-of-distribution data that are not included in the training data\ndistribution. In addition, we quantify uncertainty using the empirical\ndistribution of feature values derived from the training data obtained through\nthe trained model. The classification threshold is determined by the\n$\\alpha$-quantile and ($1-\\alpha$)-quantile, where the significance level\n$\\alpha$ is set according to each specific situation.\n","authors":["Shoma Yokura","Akihisa Ichiki"],"pdf_url":"https://arxiv.org/pdf/2502.08582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08577v1","updated":"2025-02-12T17:10:53Z","published":"2025-02-12T17:10:53Z","title":"FBFL: A Field-Based Coordination Approach for Data Heterogeneity in\n  Federated Learning","summary":"  In the last years, Federated learning (FL) has become a popular solution to\ntrain machine learning models in domains with high privacy concerns. However,\nFL scalability and performance face significant challenges in real-world\ndeployments where data across devices are non-independently and identically\ndistributed (non-IID). The heterogeneity in data distribution frequently arises\nfrom spatial distribution of devices, leading to degraded model performance in\nthe absence of proper handling. Additionally, FL typical reliance on\ncentralized architectures introduces bottlenecks and single-point-of-failure\nrisks, particularly problematic at scale or in dynamic environments. To close\nthis gap, we propose Field-Based Federated Learning (FBFL), a novel approach\nleveraging macroprogramming and field coordination to address these limitations\nthrough: (i) distributed spatial-based leader election for personalization to\nmitigate non-IID data challenges; and (ii) construction of a self-organizing,\nhierarchical architecture using advanced macroprogramming patterns. Moreover,\nFBFL not only overcomes the aforementioned limitations, but also enables the\ndevelopment of more specialized models tailored to the specific data\ndistribution in each subregion. This paper formalizes FBFL and evaluates it\nextensively using MNIST, FashionMNIST, and Extended MNIST datasets. We\ndemonstrate that, when operating under IID data conditions, FBFL performs\ncomparably to the widely-used FedAvg algorithm. Furthermore, in challenging\nnon-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other\nstate-of-the-art methods, namely FedProx and Scaffold, which have been\nspecifically designed to address non-IID data distributions. Additionally, we\nshowcase the resilience of FBFL's self-organizing hierarchical architecture\nagainst server failures.\n","authors":["Davide Domini","Gianluca Aguzzi","Lukas Esterle","Mirko Viroli"],"pdf_url":"https://arxiv.org/pdf/2502.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08576v1","updated":"2025-02-12T17:10:34Z","published":"2025-02-12T17:10:34Z","title":"Mapping the Landscape of Generative AI in Network Monitoring and\n  Management","summary":"  Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management.\n","authors":["Giampaolo Bovenzi","Francesco Cerasuolo","Domenico Ciuonzo","Davide Di Monda","Idio Guarino","Antonio Montieri","Valerio Persico","Antonio Pescapè"],"pdf_url":"https://arxiv.org/pdf/2502.08576v1.pdf","comment":"32 pages, 9 figure, 10 tables"},{"id":"http://arxiv.org/abs/2502.08574v1","updated":"2025-02-12T17:09:13Z","published":"2025-02-12T17:09:13Z","title":"COAST: Intelligent Time-Adaptive Neural Operators","summary":"  We introduce Causal Operator with Adaptive Solver Transformer (COAST), a\nnovel neural operator learning method that leverages a causal language model\n(CLM) framework to dynamically adapt time steps. Our method predicts both the\nevolution of a system and its optimal time step, intelligently balancing\ncomputational efficiency and accuracy. We find that COAST generates variable\nstep sizes that correlate with the underlying system intrinsicities, both\nwithin and across dynamical systems. Within a single trajectory, smaller steps\nare taken in regions of high complexity, while larger steps are employed in\nsimpler regions. Across different systems, more complex dynamics receive more\ngranular time steps. Benchmarked on diverse systems with varied dynamics, COAST\nconsistently outperforms state-of-the-art methods, achieving superior\nperformance in both efficiency and accuracy. This work underscores the\npotential of CLM-based intelligent adaptive solvers for scalable operator\nlearning of dynamical systems.\n","authors":["Zhikai Wu","Shiyang Zhang","Sizhuang He","Sifan Wang","Min Zhu","Anran Jiao","Lu Lu","David van Dijk"],"pdf_url":"https://arxiv.org/pdf/2502.08574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18304v4","updated":"2025-02-12T17:02:06Z","published":"2023-10-27T17:53:53Z","title":"A Stability Principle for Learning under Non-Stationarity","summary":"  We develop a versatile framework for statistical learning in non-stationary\nenvironments. In each time period, our approach applies a stability principle\nto select a look-back window that maximizes the utilization of historical data\nwhile keeping the cumulative bias within an acceptable range relative to the\nstochastic error. Our theory and numerical experiments showcase the adaptivity\nof this approach to unknown non-stationarity. We prove regret bounds that are\nminimax optimal up to logarithmic factors when the population losses are\nstrongly convex, or Lipschitz only. At the heart of our analysis lie two novel\ncomponents: a measure of similarity between functions and a segmentation\ntechnique for dividing the non-stationary data sequence into quasi-stationary\npieces.\n","authors":["Chengpiao Huang","Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.18304v4.pdf","comment":"65 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.04686v3","updated":"2025-02-12T16:49:50Z","published":"2025-01-08T18:49:41Z","title":"URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics","summary":"  Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.\n","authors":["Ruilin Luo","Zhuofan Zheng","Yifan Wang","Yiyao Yu","Xinzhe Ni","Zicheng Lin","Jin Zeng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04686v3.pdf","comment":"Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io"},{"id":"http://arxiv.org/abs/2502.06914v2","updated":"2025-02-12T16:47:32Z","published":"2025-02-10T09:46:26Z","title":"UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme\n  Active-Site Knowledge","summary":"  Enzyme-catalyzed protein cleavage is essential for many biological functions.\nAccurate prediction of cleavage sites can facilitate various applications such\nas drug development, enzyme design, and a deeper understanding of biological\nmechanisms. However, most existing models are restricted to an individual\nenzyme, which neglects shared knowledge of enzymes and fails generalize to\nnovel enzymes. Thus, we introduce a unified protein cleavage site predictor\nnamed UniZyme, which can generalize across diverse enzymes. To enhance the\nenzyme encoding for the protein cleavage site prediction, UniZyme employs a\nnovel biochemically-informed model architecture along with active-site\nknowledge of proteolytic enzymes. Extensive experiments demonstrate that\nUniZyme achieves high accuracy in predicting cleavage sites across a range of\nproteolytic enzymes, including unseen enzymes. The code is available in\nhttps://anonymous.4open.science/r/UniZyme-4A67.\n","authors":["Chenao Li","Shuo Yan","Enyan Dai"],"pdf_url":"https://arxiv.org/pdf/2502.06914v2.pdf","comment":"18 pages,8 figures"},{"id":"http://arxiv.org/abs/2110.06257v3","updated":"2025-02-12T16:47:01Z","published":"2021-10-12T18:12:57Z","title":"Causal Discovery from Conditionally Stationary Time Series","summary":"  Causal discovery, i.e., inferring underlying causal relationships from\nobservational data, is highly challenging for AI systems. In a time series\nmodeling context, traditional causal discovery methods mainly consider\nconstrained scenarios with fully observed variables and/or data from stationary\ntime-series. We develop a causal discovery approach to handle a wide class of\nnonstationary time series that are conditionally stationary, where the\nnonstationary behaviour is modeled as stationarity conditioned on a set of\nlatent state variables. Named State-Dependent Causal Inference (SDCI), our\napproach is able to recover the underlying causal dependencies, with provable\nidentifiablity for the state-dependent causal structures. Empirical experiments\non nonlinear particle interaction data and gene regulatory networks demonstrate\nSDCI's superior performance over baseline causal discovery methods. Improved\nresults over non-causal RNNs on modeling NBA player movements demonstrate the\npotential of our method and motivate the use of causality-driven methods for\nforecasting.\n","authors":["Carles Balsells-Rodas","Xavier Sumba","Tanmayee Narendra","Ruibo Tu","Gabriele Schweikert","Hedvig Kjellstrom","Yingzhen Li"],"pdf_url":"https://arxiv.org/pdf/2110.06257v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08557v1","updated":"2025-02-12T16:39:06Z","published":"2025-02-12T16:39:06Z","title":"QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval","summary":"  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n","authors":["Wonduk Seo","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08557v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.08556v1","updated":"2025-02-12T16:38:40Z","published":"2025-02-12T16:38:40Z","title":"Human-Centric Foundation Models: Perception, Generation and Agentic\n  Modeling","summary":"  Human understanding and generation are critical for modeling digital humans\nand humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)\ninspired by the success of generalist models, such as large language and vision\nmodels, have emerged to unify diverse human-centric tasks into a single\nframework, surpassing traditional task-specific approaches. In this survey, we\npresent a comprehensive overview of HcFMs by proposing a taxonomy that\ncategorizes current approaches into four groups: (1) Human-centric Perception\nFoundation Models that capture fine-grained features for multi-modal 2D and 3D\nunderstanding. (2) Human-centric AIGC Foundation Models that generate\nhigh-fidelity, diverse human-related content. (3) Unified Perception and\nGeneration Models that integrate these capabilities to enhance both human\nunderstanding and synthesis. (4) Human-centric Agentic Foundation Models that\nextend beyond perception and generation to learn human-like intelligence and\ninteractive behaviors for humanoid embodied tasks. We review state-of-the-art\ntechniques, discuss emerging challenges and future research directions. This\nsurvey aims to serve as a roadmap for researchers and practitioners working\ntowards more robust, versatile, and intelligent digital human and embodiments\nmodeling.\n","authors":["Shixiang Tang","Yizhou Wang","Lu Chen","Yuan Wang","Sida Peng","Dan Xu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2502.08556v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.08555v1","updated":"2025-02-12T16:35:46Z","published":"2025-02-12T16:35:46Z","title":"A Machine Learning-Ready Data Processing Tool for Near Real-Time\n  Forecasting","summary":"  Space weather forecasting is critical for mitigating radiation risks in space\nexploration and protecting Earth-based technologies from geomagnetic\ndisturbances. This paper presents the development of a Machine Learning (ML)-\nready data processing tool for Near Real-Time (NRT) space weather forecasting.\nBy merging data from diverse NRT sources such as solar imagery, magnetic field\nmeasurements, and energetic particle fluxes, the tool addresses key gaps in\ncurrent space weather prediction capabilities. The tool processes and\nstructures the data for machine learning models, focusing on time-series\nforecasting and event detection for extreme solar events. It provides users\nwith a framework to download, process, and label data for ML applications,\nstreamlining the workflow for improved NRT space weather forecasting and\nscientific research.\n","authors":["Maher A Dayeh","Michael J Starkey","Subhamoy Chatterjee","Heather Elliott","Samuel Hart","Kimberly Moreland"],"pdf_url":"https://arxiv.org/pdf/2502.08555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08549v1","updated":"2025-02-12T16:30:39Z","published":"2025-02-12T16:30:39Z","title":"Copula-based mixture model identification for subgroup clustering with\n  imaging applications","summary":"  Model-based clustering techniques have been widely applied to various\napplication areas, while most studies focus on canonical mixtures with unique\ncomponent distribution form. However, this strict assumption is often hard to\nsatisfy. In this paper, we consider the more flexible Copula-Based Mixture\nModels (CBMMs) for clustering, which allow heterogeneous component\ndistributions composed by flexible choices of marginal and copula forms. More\nspecifically, we propose an adaptation of the Generalized Iterative Conditional\nEstimation (GICE) algorithm to identify the CBMMs in an unsupervised manner,\nwhere the marginal and copula forms and their parameters are estimated\niteratively. GICE is adapted from its original version developed for switching\nMarkov model identification with the choice of realization time. Our CBMM-GICE\nclustering method is then tested on synthetic two-cluster data (N=2000 samples)\nwith discussion of the factors impacting its convergence. Finally, it is\ncompared to the Expectation Maximization identified mixture models with unique\ncomponent form on the entire MNIST database (N=70000), and on real cardiac\nmagnetic resonance data (N=276) to illustrate its value for imaging\napplications.\n","authors":["Fei Zheng","Nicolas Duchateau"],"pdf_url":"https://arxiv.org/pdf/2502.08549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08542v1","updated":"2025-02-12T16:27:40Z","published":"2025-02-12T16:27:40Z","title":"Beyond Predictions: A Participatory Framework for Multi-Stakeholder\n  Decision-Making","summary":"  Conventional decision-support systems, primarily based on supervised\nlearning, focus on outcome prediction models to recommend actions. However,\nthey often fail to account for the complexities of multi-actor environments,\nwhere diverse and potentially conflicting stakeholder preferences must be\nbalanced. In this paper, we propose a novel participatory framework that\nredefines decision-making as a multi-stakeholder optimization problem,\ncapturing each actor's preferences through context-dependent reward functions.\nOur framework leverages $k$-fold cross-validation to fine-tune user-provided\noutcome prediction models and evaluate decision strategies, including\ncompromise functions mediating stakeholder trade-offs. We introduce a synthetic\nscoring mechanism that exploits user-defined preferences across multiple\nmetrics to rank decision-making strategies and identify the optimal\ndecision-maker. The selected decision-maker can then be used to generate\nactionable recommendations for new data. We validate our framework using two\nreal-world use cases, demonstrating its ability to deliver recommendations that\neffectively balance multiple metrics, achieving results that are often beyond\nthe scope of purely prediction-based methods. Ablation studies demonstrate that\nour framework, with its modular, model-agnostic, and inherently transparent\ndesign, integrates seamlessly with various predictive models, reward\nstructures, evaluation metrics, and sample sizes, making it particularly suited\nfor complex, high-stakes decision-making contexts.\n","authors":["Vittoria Vineis","Giuseppe Perelli","Gabriele Tolomei"],"pdf_url":"https://arxiv.org/pdf/2502.08542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07577v2","updated":"2025-02-12T16:25:44Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08536v1","updated":"2025-02-12T16:21:01Z","published":"2025-02-12T16:21:01Z","title":"Matrix Completion with Graph Information: A Provable Nonconvex\n  Optimization Approach","summary":"  We consider the problem of matrix completion with graphs as side information\ndepicting the interrelations between variables. The key challenge lies in\nleveraging the similarity structure of the graph to enhance matrix recovery.\nExisting approaches, primarily based on graph Laplacian regularization, suffer\nfrom several limitations: (1) they focus only on the similarity between\nneighboring variables, while overlooking long-range correlations; (2) they are\nhighly sensitive to false edges in the graphs and (3) they lack theoretical\nguarantees regarding statistical and computational complexities. To address\nthese issues, we propose in this paper a novel graph regularized matrix\ncompletion algorithm called GSGD, based on preconditioned projected gradient\ndescent approach. We demonstrate that GSGD effectively captures the\nhigher-order correlation information behind the graphs, and achieves superior\nrobustness and stability against the false edges. Theoretically, we prove that\nGSGD achieves linear convergence to the global optimum with near-optimal sample\ncomplexity, providing the first theoretical guarantees for both recovery\naccuracy and efficacy in the perspective of nonconvex optimization. Our\nnumerical experiments on both synthetic and real-world data further validate\nthat GSGD achieves superior recovery accuracy and scalability compared with\nseveral popular alternatives.\n","authors":["Yao Wang","Yiyang Yang","Kaidong Wang","Shanxing Gao","Xiuwu Liao"],"pdf_url":"https://arxiv.org/pdf/2502.08536v1.pdf","comment":"41 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.08531v1","updated":"2025-02-12T16:08:48Z","published":"2025-02-12T16:08:48Z","title":"On Different Notions of Redundancy in Conditional-Independence-Based\n  Discovery of Graphical Models","summary":"  The goal of conditional-independence-based discovery of graphical models is\nto find a graph that represents the independence structure of variables in a\ngiven dataset. To learn such a representation, conditional-independence-based\napproaches conduct a set of statistical tests that suffices to identify the\ngraphical representation under some assumptions on the underlying distribution\nof the data. In this work, we highlight that due to the conciseness of the\ngraphical representation, there are often many tests that are not used in the\nconstruction of the graph. These redundant tests have the potential to detect\nor sometimes correct errors in the learned model. We show that not all tests\ncontain this additional information and that such redundant tests have to be\napplied with care. Precisely, we argue that particularly those conditional\n(in)dependence statements are interesting that follow only from graphical\nassumptions but do not hold for every probability distribution.\n","authors":["Philipp M. Faller","Dominik Janzing"],"pdf_url":"https://arxiv.org/pdf/2502.08531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09262v2","updated":"2025-02-12T16:07:40Z","published":"2025-01-16T03:11:50Z","title":"On the convergence rate of noisy Bayesian Optimization with Expected\n  Improvement","summary":"  Expected improvement (EI) is one of the most widely used acquisition\nfunctions in Bayesian optimization (BO). Despite its proven success in\napplications for decades, important open questions remain on the theoretical\nconvergence behaviors and rates for EI. In this paper, we contribute to the\nconvergence theory of EI in three novel and critical areas. First, we consider\nobjective functions that fit under the Gaussian process (GP) prior assumption,\nwhereas existing works mostly focus on functions in the reproducing kernel\nHilbert space (RKHS). Second, we establish for the first time the asymptotic\nerror bound and its corresponding rate for GP-EI with noisy observations under\nthe GP prior assumption. Third, by investigating the exploration and\nexploitation properties of the non-convex EI function, we establish improved\nerror bounds of GP-EI for both the noise-free and noisy cases.\n","authors":["Jingyi Wang","Haowei Wang","Nai-Yuan Chiang","Cosmin G. Petra"],"pdf_url":"https://arxiv.org/pdf/2501.09262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08524v1","updated":"2025-02-12T16:00:11Z","published":"2025-02-12T16:00:11Z","title":"LLM Pretraining with Continuous Concepts","summary":"  Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process.\n","authors":["Jihoon Tack","Jack Lanchantin","Jane Yu","Andrew Cohen","Ilia Kulikov","Janice Lan","Shibo Hao","Yuandong Tian","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2502.08524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08518v1","updated":"2025-02-12T15:54:56Z","published":"2025-02-12T15:54:56Z","title":"FedMHO: Heterogeneous One-Shot Federated Learning Towards\n  Resource-Constrained Edge Devices","summary":"  Federated Learning (FL) is increasingly adopted in edge computing scenarios,\nwhere a large number of heterogeneous clients operate under constrained or\nsufficient resources. The iterative training process in conventional FL\nintroduces significant computation and communication overhead, which is\nunfriendly for resource-constrained edge devices. One-shot FL has emerged as a\npromising approach to mitigate communication overhead, and model-heterogeneous\nFL solves the problem of diverse computing resources across clients. However,\nexisting methods face challenges in effectively managing model-heterogeneous\none-shot FL, often leading to unsatisfactory global model performance or\nreliance on auxiliary datasets. To address these challenges, we propose a novel\nFL framework named FedMHO, which leverages deep classification models on\nresource-sufficient clients and lightweight generative models on\nresource-constrained devices. On the server side, FedMHO involves a two-stage\nprocess that includes data generation and knowledge fusion. Furthermore, we\nintroduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem\nduring the knowledge fusion stage, and an unsupervised data optimization\nsolution to improve the quality of synthetic samples. Comprehensive experiments\ndemonstrate the effectiveness of our methods, as they outperform\nstate-of-the-art baselines in various experimental setups.\n","authors":["Dezhong Yao","Yuexin Shi","Tongtong Liu","Zhiqiang Xu"],"pdf_url":"https://arxiv.org/pdf/2502.08518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08515v1","updated":"2025-02-12T15:47:48Z","published":"2025-02-12T15:47:48Z","title":"The Paradox of Stochasticity: Limited Creativity and Computational\n  Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data","summary":"  This study examines how temperature settings and model architectures affect\nthe generation of structured fictional data (names, birthdates) across three\nlarge language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest.\nBy systematically testing temperature values from 0.0 to 1.0 in increments of\n0.1, we conducted 330 trials yielding 889 structured entities, validated for\nsyntactic consistency. Key findings reveal that model architecture\nsignificantly influences computational efficiency, with mistral:latest and\nllama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to\nexpectations, temperature showed no correlation with processing time,\nchallenging assumptions about stochastic sampling costs. Output diversity\nremained limited, as models consistently defaulted to common name archetypes\n(e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names\nclustered at intermediate values (0.3-0.7). These results demonstrate that\narchitectural optimizations, rather than temperature adjustments, dominate\nperformance in structured generation tasks. The findings emphasize prioritizing\nmodel selection over hyperparameter tuning for efficiency and suggest explicit\ndiversity constraints are necessary to mitigate default output biases in\nsynthetic data pipelines.\n","authors":["Evgenii Evstafev"],"pdf_url":"https://arxiv.org/pdf/2502.08515v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.05980v3","updated":"2025-02-12T15:40:35Z","published":"2024-02-08T06:48:01Z","title":"Do Large Code Models Understand Programming Concepts? Counterfactual\n  Analysis for Code Predicates","summary":"  Large Language Models' success on text generation has also made them better\nat code generation and coding tasks. While a lot of work has demonstrated their\nremarkable performance on tasks such as code completion and editing, it is\nstill unclear as to why. We help bridge this gap by exploring to what degree\nauto-regressive models understand the logical constructs of the underlying\nprograms. We propose Counterfactual Analysis for Programming Concept Predicates\n(CACP) as a counterfactual testing framework to evaluate whether Large Code\nModels understand programming concepts. With only black-box access to the\nmodel, we use CACP to evaluate ten popular Large Code Models for four different\nprogramming concepts. Our findings suggest that current models lack\nunderstanding of concepts such as data flow and control flow.\n","authors":["Ashish Hooda","Mihai Christodorescu","Miltiadis Allamanis","Aaron Wilson","Kassem Fawaz","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2402.05980v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08505v1","updated":"2025-02-12T15:36:38Z","published":"2025-02-12T15:36:38Z","title":"Bridging Domain Adaptation and Graph Neural Networks: A Tensor-Based\n  Framework for Effective Label Propagation","summary":"  Graph Neural Networks (GNNs) have recently become the predominant tools for\nstudying graph data. Despite state-of-the-art performance on graph\nclassification tasks, GNNs are overwhelmingly trained in a single domain under\nsupervision, thus necessitating a prohibitively high demand for labels and\nresulting in poorly transferable representations. To address this challenge, we\npropose the Label-Propagation Tensor Graph Neural Network (LP-TGNN) framework\nto bridge the gap between graph data and traditional domain adaptation methods.\nIt extracts graph topological information holistically with a tensor\narchitecture and then reduces domain discrepancy through label propagation. It\nis readily compatible with general GNNs and domain adaptation techniques with\nminimal adjustment through pseudo-labeling. Experiments on various real-world\nbenchmarks show that our LP-TGNN outperforms baselines by a notable margin. We\nalso validate and analyze each component of the proposed framework in the\nablation study.\n","authors":["Tao Wen","Elynn Chen","Yuzhou Chen","Qi Lei"],"pdf_url":"https://arxiv.org/pdf/2502.08505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10229v2","updated":"2025-02-12T15:34:01Z","published":"2024-05-16T16:28:11Z","title":"Random ReLU Neural Networks as Non-Gaussian Processes","summary":"  We consider a large class of shallow neural networks with randomly\ninitialized parameters and rectified linear unit activation functions. We prove\nthat these random neural networks are well-defined non-Gaussian processes. As a\nby-product, we demonstrate that these networks are solutions to stochastic\ndifferential equations driven by impulsive white noise (combinations of random\nDirac measures). These processes are parameterized by the law of the weights\nand biases as well as the density of activation thresholds in each bounded\nregion of the input domain. We prove that these processes are isotropic and\nwide-sense self-similar with Hurst exponent 3/2. We also derive a remarkably\nsimple closed-form expression for their autocovariance function. Our results\nare fundamentally different from prior work in that we consider a\nnon-asymptotic viewpoint: The number of neurons in each bounded region of the\ninput domain (i.e., the width) is itself a random variable with a Poisson law\nwith mean proportional to the density parameter. Finally, we show that, under\nsuitable hypotheses, as the expected width tends to infinity, these processes\ncan converge in law not only to Gaussian processes, but also to non-Gaussian\nprocesses depending on the law of the weights. Our asymptotic results provide a\nnew take on several classical results (wide networks converge to Gaussian\nprocesses) as well as some new ones (wide networks can converge to non-Gaussian\nprocesses).\n","authors":["Rahul Parhi","Pakshal Bohra","Ayoub El Biari","Mehrsa Pourya","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2405.10229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08496v1","updated":"2025-02-12T15:31:16Z","published":"2025-02-12T15:31:16Z","title":"Fine-Tuning Topics through Weighting Aspect Keywords","summary":"  Topic modeling often requires examining topics from multiple perspectives to\nuncover hidden patterns, especially in less explored areas. This paper presents\nan approach to address this need, utilizing weighted keywords from various\naspects derived from a domain knowledge. The research method starts with\nstandard topic modeling. Then, it adds a process consisting of four key steps.\nFirst, it defines keywords for each aspect. Second, it gives weights to these\nkeywords based on their relevance. Third, it calculates relevance scores for\naspect-weighted keywords and topic keywords to create aspect-topic models.\nFourth, it uses these scores to tune relevant new documents. Finally, the\ngenerated topic models are interpreted and validated. The findings show that\ntop-scoring documents are more likely to be about the same aspect of a topic.\nThis highlights the model's effectiveness in finding the related documents to\nthe aspects.\n","authors":["Ali Nazari","Michael Weiss"],"pdf_url":"https://arxiv.org/pdf/2502.08496v1.pdf","comment":"17 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.08488v1","updated":"2025-02-12T15:23:29Z","published":"2025-02-12T15:23:29Z","title":"One-Shot Federated Learning with Classifier-Free Diffusion Models","summary":"  Federated learning (FL) enables collaborative learning without data\ncentralization but introduces significant communication costs due to multiple\ncommunication rounds between clients and the server. One-shot federated\nlearning (OSFL) addresses this by forming a global model with a single\ncommunication round, often relying on the server's model distillation or\nauxiliary dataset generation - often through pre-trained diffusion models\n(DMs). Existing DM-assisted OSFL methods, however, typically employ\nclassifier-guided DMs, which require training auxiliary classifier models at\neach client, introducing additional computation overhead. This work introduces\nOSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a\nnovel OSFL approach that eliminates the need for auxiliary models. OSCAR uses\nfoundation models to devise category-specific data representations at each\nclient, seamlessly integrated into a classifier-free diffusion model pipeline\nfor server-side data generation. OSCAR is a simple yet cost-effective OSFL\napproach that outperforms the state-of-the-art on four benchmarking datasets\nwhile reducing the communication load by at least 99%.\n","authors":["Obaidullah Zaland","Shutong Jin","Florian T. Pokorny","Monowar Bhuyan"],"pdf_url":"https://arxiv.org/pdf/2502.08488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07746v2","updated":"2025-02-12T15:18:45Z","published":"2024-10-10T09:23:33Z","title":"Benign Overfitting in Single-Head Attention","summary":"  The phenomenon of benign overfitting, where a trained neural network\nperfectly fits noisy training data but still achieves near-optimal test\nperformance, has been extensively studied in recent years for linear models and\nfully-connected/convolutional networks. In this work, we study benign\noverfitting in a single-head softmax attention model, which is the fundamental\nbuilding block of Transformers. We prove that under appropriate conditions, the\nmodel exhibits benign overfitting in a classification setting already after two\nsteps of gradient descent. Moreover, we show conditions where a\nminimum-norm/maximum-margin interpolator exhibits benign overfitting. We study\nhow the overfitting behavior depends on the signal-to-noise ratio (SNR) of the\ndata distribution, namely, the ratio between norms of signal and noise tokens,\nand prove that a sufficiently large SNR is both necessary and sufficient for\nbenign overfitting.\n","authors":["Roey Magen","Shuning Shang","Zhiwei Xu","Spencer Frei","Wei Hu","Gal Vardi"],"pdf_url":"https://arxiv.org/pdf/2410.07746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10949v2","updated":"2025-02-12T15:18:32Z","published":"2024-07-15T17:45:53Z","title":"Representing Rule-based Chatbots with Transformers","summary":"  What kind of internal mechanisms might Transformers use to conduct fluid,\nnatural-sounding conversations? Prior work has illustrated by construction how\nTransformers can solve various synthetic tasks, such as sorting a list or\nrecognizing formal languages, but it remains unclear how to extend this\napproach to a conversational setting. In this work, we propose using ELIZA, a\nclassic rule-based chatbot, as a setting for formal, mechanistic analysis of\nTransformer-based chatbots. ELIZA allows us to formally model key aspects of\nconversation, including local pattern matching and long-term dialogue state\ntracking. We first present a theoretical construction of a Transformer that\nimplements the ELIZA chatbot. Building on prior constructions, particularly\nthose for simulating finite-state automata, we show how simpler mechanisms can\nbe composed and extended to produce more sophisticated behavior. Next, we\nconduct a set of empirical analyses of Transformers trained on synthetically\ngenerated ELIZA conversations. Our analysis illustrates the kinds of mechanisms\nthese models tend to prefer--for example, models favor an induction head\nmechanism over a more precise, position-based copying mechanism; and using\nintermediate generations to simulate recurrent data structures, akin to an\nimplicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit\nconnection between neural chatbots and interpretable, symbolic mechanisms, our\nresults provide a new framework for the mechanistic analysis of conversational\nagents.\n","authors":["Dan Friedman","Abhishek Panigrahi","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10949v2.pdf","comment":"NAACL 2025. Code and data are available at\n  https://github.com/princeton-nlp/ELIZA-Transformer"},{"id":"http://arxiv.org/abs/2502.08482v1","updated":"2025-02-12T15:17:04Z","published":"2025-02-12T15:17:04Z","title":"Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned\n  Reasoning","summary":"  Chain-of-Thought (CoT) prompting has emerged as a powerful technique for\nenhancing language model's reasoning capabilities. However, generating long and\ncorrect CoT trajectories is challenging. Recent studies have demonstrated that\nLooped Transformers possess remarkable length generalization capabilities, but\ntheir limited generality and adaptability prevent them from serving as an\nalternative to auto-regressive solutions. To better leverage the strengths of\nLooped Transformers, we propose RELAY (REasoning through Loop Alignment\niterativelY). Specifically, we align the steps of Chain-of-Thought (CoT)\nreasoning with loop iterations and apply intermediate supervision during the\ntraining of Looped Transformers. This additional iteration-wise supervision not\nonly preserves the Looped Transformer's ability for length generalization but\nalso enables it to predict CoT reasoning steps for unseen data. Therefore, we\nleverage this Looped Transformer to generate accurate reasoning chains for\ncomplex problems that exceed the training length, which will then be used to\nfine-tune an auto-regressive model. We conduct extensive experiments, and the\nresults demonstrate the effectiveness of our approach, with significant\nimprovements in the performance of the auto-regressive model. Code will be\nreleased at https://github.com/qifanyu/RELAY.\n","authors":["Qifan Yu","Zhenyu He","Sijie Li","Xun Zhou","Jun Zhang","Jingjing Xu","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.08482v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2411.02540v3","updated":"2025-02-12T15:14:01Z","published":"2024-11-04T19:21:06Z","title":"GraphXAIN: Narratives to Explain Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose challenges in interpretability. Existing\nGNN explanation methods usually yield technical outputs, such as subgraphs and\nfeature importance scores, that are difficult for non-data scientists to\nunderstand and thereby violate the purpose of explanations. Motivated by recent\nExplainable AI (XAI) research, we propose GraphXAIN, a method that generates\nnatural language narratives explaining GNN predictions. GraphXAIN is a model-\nand explainer-agnostic method that uses Large Language Models (LLMs) to\ntranslate explanatory subgraphs and feature importance scores into coherent,\nstory-like explanations of GNN decision-making processes. Evaluations on\nreal-world datasets demonstrate GraphXAIN's ability to improve graph\nexplanations. A survey of machine learning researchers and practitioners\nreveals that GraphXAIN enhances four explainability dimensions:\nunderstandability, satisfaction, convincingness, and suitability for\ncommunicating model predictions. When combined with another graph explainer\nmethod, GraphXAIN further improves trustworthiness, insightfulness, confidence,\nand usability. Notably, 95% of participants found GraphXAIN to be a valuable\naddition to the GNN explanation method. By incorporating natural language\nnarratives, our approach serves both graph practitioners and non-expert users\nby providing clearer and more effective explanations.\n","authors":["Mateusz Cedro","David Martens"],"pdf_url":"https://arxiv.org/pdf/2411.02540v3.pdf","comment":"19 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.11759v4","updated":"2025-02-12T15:07:01Z","published":"2024-10-15T16:28:55Z","title":"LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and\n  General Noise for Global Causal Discovery","summary":"  Inferring causal relationships from observational data is crucial when\nexperiments are costly or infeasible. Additive noise models (ANMs) enable\nunique directed acyclic graph (DAG) identification, but existing ANM methods\noften rely on restrictive assumptions on the data generating process, limiting\ntheir applicability to real-world settings. We propose local search in additive\nnoise models, LoSAM, a topological ordering method for learning a unique DAG in\nANMs with mixed causal mechanisms and general noise distributions. We introduce\nnew causal substructures and criteria for identifying roots and leaves,\nenabling efficient top-down learning. We prove asymptotic consistency and\npolynomial runtime, ensuring scalability and sample efficiency. We test LoSAM\non synthetic and real-world data, demonstrating state-of-the-art performance\nacross all mixed mechanism settings.\n","authors":["Sujai Hiremath","Promit Ghosal","Kyra Gan"],"pdf_url":"https://arxiv.org/pdf/2410.11759v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11140v4","updated":"2025-02-12T15:06:14Z","published":"2023-04-21T17:22:08Z","title":"Convergence of Message Passing Graph Neural Networks with Generic\n  Aggregation On Large Random Graphs","summary":"  We study the convergence of message passing graph neural networks on random\ngraph models to their continuous counterpart as the number of nodes tends to\ninfinity. Until now, this convergence was only known for architectures with\naggregation functions in the form of normalized means, or, equivalently, of an\napplication of classical operators like the adjacency matrix or the graph\nLaplacian. We extend such results to a large class of aggregation functions,\nthat encompasses all classically used message passing graph neural networks,\nsuch as attention-based message passing, max convolutional message passing,\n(degree-normalized) convolutional message passing, or moment-based aggregation\nmessage passing. Under mild assumptions, we give non-asymptotic bounds with\nhigh probability to quantify this convergence. Our main result is based on the\nMcDiarmid inequality. Interestingly, this result does not apply to the case\nwhere the aggregation is a coordinate-wise maximum. We treat this case\nseparately and obtain a different convergence rate.\n","authors":["Matthieu Cordonnier","Nicolas Keriven","Nicolas Tremblay","Samuel Vaiter"],"pdf_url":"https://arxiv.org/pdf/2304.11140v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08470v1","updated":"2025-02-12T15:04:23Z","published":"2025-02-12T15:04:23Z","title":"Numerical Schemes for Signature Kernels","summary":"  Signature kernels have emerged as a powerful tool within kernel methods for\nsequential data. In the paper \"The Signature Kernel is the solution of a\nGoursat PDE\", the authors identify a kernel trick that demonstrates that, for\ncontinuously differentiable paths, the signature kernel satisfies a Goursat\nproblem for a hyperbolic partial differential equation (PDE) in two independent\ntime variables. While finite difference methods have been explored for this\nPDE, they face limitations in accuracy and stability when handling highly\noscillatory inputs. In this work, we introduce two advanced numerical schemes\nthat leverage polynomial representations of boundary conditions through either\napproximation or interpolation techniques, and rigorously establish the\ntheoretical convergence of the polynomial approximation scheme. Experimental\nevaluations reveal that our approaches yield improvements of several orders of\nmagnitude in mean absolute percentage error (MAPE) compared to traditional\nfinite difference schemes, without increasing computational complexity.\nFurthermore, like finite difference methods, our algorithms can be\nGPU-parallelized to reduce computational complexity from quadratic to linear in\nthe length of the input sequences, thereby improving scalability for\nhigh-frequency data. We have implemented these algorithms in a dedicated Python\nlibrary, which is publicly available at:\nhttps://github.com/FrancescoPiatti/polysigkernel.\n","authors":["Thomas Cass","Francesco Piatti","Jeffrey Pei"],"pdf_url":"https://arxiv.org/pdf/2502.08470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03667v2","updated":"2025-02-12T14:56:35Z","published":"2024-05-06T17:43:39Z","title":"Fault Detection and Monitoring using a Data-Driven Information-Based\n  Strategy: Method, Theory, and Application","summary":"  The ability to detect when a system undergoes an incipient fault is of\nparamount importance in preventing a critical failure. Classic methods for\nfault detection (including model-based and data-driven approaches) rely on\nthresholding error statistics or simple input-residual dependencies but face\ndifficulties with non-linear or non-Gaussian systems. Behavioral methods (e.g.,\nthose relying on digital twins) address these difficulties but still face\nchallenges when faulty data is scarce, decision guarantees are required, or\nworking with already-deployed models is required. In this work, we propose an\ninformation-driven fault detection method based on a novel concept drift\ndetector, addressing these challenges. The method is tailored to identifying\ndrifts in input-output relationships of additive noise models (i.e., model\ndrifts) and is based on a distribution-free mutual information (MI) estimator.\nOur scheme does not require prior faulty examples and can be applied\ndistribution-free over a large class of system models. Our core contributions\nare twofold. First, we demonstrate the connection between fault detection,\nmodel drift detection, and testing independence between two random variables.\nSecond, we prove several theoretical properties of the proposed MI-based fault\ndetection scheme: (i) strong consistency, (ii) exponentially fast detection of\nthe non-faulty case, and (iii) control of both significance levels and power of\nthe test. To conclude, we validate our theory with synthetic data and the\nbenchmark dataset N-CMAPSS of aircraft turbofan engines. These empirical\nresults support the usefulness of our methodology in many practical and\nrealistic settings, and the theoretical results show performance guarantees\nthat other methods cannot offer.\n","authors":["Camilo Ramírez","Jorge F. Silva","Ferhat Tamssaouet","Tomás Rojas","Marcos E. Orchard"],"pdf_url":"https://arxiv.org/pdf/2405.03667v2.pdf","comment":"31 pages, 15 figures. This is the accepted manuscript for publication\n  in Mechanical Systems and Signal Processing. The arXiv version has been\n  updated accordingly"},{"id":"http://arxiv.org/abs/2408.09966v2","updated":"2025-02-12T14:55:29Z","published":"2024-08-19T13:14:02Z","title":"Mask in the Mirror: Implicit Sparsification","summary":"  Continuous sparsification strategies are among the most effective methods for\nreducing the inference costs and memory demands of large-scale neural networks.\nA key factor in their success is the implicit $L_1$ regularization induced by\njointly learning both mask and weight variables, which has been shown\nexperimentally to outperform explicit $L_1$ regularization. We provide a\ntheoretical explanation for this observation by analyzing the learning\ndynamics, revealing that early continuous sparsification is governed by an\nimplicit $L_2$ regularization that gradually transitions to an $L_1$ penalty\nover time. Leveraging this insight, we propose a method to dynamically control\nthe strength of this implicit bias. Through an extension of the mirror flow\nframework, we establish convergence and optimality guarantees in the context of\nunderdetermined linear regression. Our theoretical findings may be of\nindependent interest, as we demonstrate how to enter the rich regime and show\nthat the implicit bias can be controlled via a time-dependent Bregman\npotential. To validate these insights, we introduce PILoT, a continuous\nsparsification approach with novel initialization and dynamic regularization,\nwhich consistently outperforms baselines in standard experiments.\n","authors":["Tom Jacobs","Rebekka Burkholz"],"pdf_url":"https://arxiv.org/pdf/2408.09966v2.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.08457v1","updated":"2025-02-12T14:52:04Z","published":"2025-02-12T14:52:04Z","title":"Learning Theory for Kernel Bilevel Optimization","summary":"  Bilevel optimization has emerged as a technique for addressing a wide range\nof machine learning problems that involve an outer objective implicitly\ndetermined by the minimizer of an inner problem. In this paper, we investigate\nthe generalization properties for kernel bilevel optimization problems where\nthe inner objective is optimized over a Reproducing Kernel Hilbert Space. This\nsetting enables rich function approximation while providing a foundation for\nrigorous theoretical analysis. In this context, we establish novel\ngeneralization error bounds for the bilevel problem under finite-sample\napproximation. Our approach adopts a functional perspective, inspired by\n(Petrulionyte et al., 2024), and leverages tools from empirical process theory\nand maximal inequalities for degenerate $U$-processes to derive uniform error\nbounds. These generalization error estimates allow to characterize the\nstatistical accuracy of gradient-based methods applied to the empirical\ndiscretization of the bilevel problem.\n","authors":["Fares El Khoury","Edouard Pauwels","Samuel Vaiter","Michael Arbel"],"pdf_url":"https://arxiv.org/pdf/2502.08457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08649v2","updated":"2025-02-12T14:49:16Z","published":"2024-07-11T16:28:31Z","title":"Confidence-based Estimators for Predictive Performance in Model\n  Monitoring","summary":"  After a machine learning model has been deployed into production, its\npredictive performance needs to be monitored. Ideally, such monitoring can be\ncarried out by comparing the model's predictions against ground truth labels.\nFor this to be possible, the ground truth labels must be available relatively\nsoon after inference. However, there are many use cases where ground truth\nlabels are available only after a significant delay, or in the worst case, not\nat all. In such cases, directly monitoring the model's predictive performance\nis impossible.\n  Recently, novel methods for estimating the predictive performance of a model\nwhen ground truth is unavailable have been developed. Many of these methods\nleverage model confidence or other uncertainty estimates and are experimentally\ncompared against a naive baseline method, namely Average Confidence (AC), which\nestimates model accuracy as the average of confidence scores for a given set of\npredictions. However, until now the theoretical properties of the AC method\nhave not been properly explored. In this paper, we try to fill this gap by\nreviewing the AC method and show that under certain general assumptions, it is\nan unbiased and consistent estimator of model accuracy with many desirable\nproperties. We also compare this baseline estimator against some more complex\nestimators empirically and show that in many cases the AC method is able to\nbeat the others, although the comparative quality of the different estimators\nis heavily case-dependent.\n","authors":["Juhani Kivimäki","Jakub Białek","Jukka K. Nurminen","Wojtek Kuberski"],"pdf_url":"https://arxiv.org/pdf/2407.08649v2.pdf","comment":"This version corresponds to the final published version in JAIR. The\n  published article is available at [https://doi.org/10.1613/jair.1.16709]"},{"id":"http://arxiv.org/abs/2311.00055v2","updated":"2025-02-12T14:43:07Z","published":"2023-10-31T18:03:54Z","title":"Rethinking Pre-Training in Tabular Data: A Neighborhood Embedding\n  Perspective","summary":"  Pre-training is prevalent in deep learning for vision and text data,\nleveraging knowledge from other datasets to enhance downstream tasks. However,\nfor tabular data, the inherent heterogeneity in attribute and label spaces\nacross datasets complicates the learning of shareable knowledge. We propose\nTabular data Pre-Training via Meta-representation (TabPTM), aiming to pre-train\na general tabular model over diverse datasets. The core idea is to embed data\ninstances into a shared feature space, where each instance is represented by\nits distance to a fixed number of nearest neighbors and their labels. This\n''meta-representation'' transforms heterogeneous tasks into homogeneous local\nprediction problems, enabling the model to infer labels (or scores for each\nlabel) based on neighborhood information. As a result, the pre-trained TabPTM\ncan be applied directly to new datasets, regardless of their diverse attributes\nand labels, without further fine-tuning. Extensive experiments on 101 datasets\nconfirm TabPTM's effectiveness in both classification and regression tasks,\nwith and without fine-tuning.\n","authors":["Han-Jia Ye","Qi-Le Zhou","Huai-Hong Yin","De-Chuan Zhan","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2311.00055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08448v1","updated":"2025-02-12T14:40:19Z","published":"2025-02-12T14:40:19Z","title":"Monge SAM: Robust Reparameterization-Invariant Sharpness-Aware\n  Minimization Based on Loss Geometry","summary":"  Recent studies on deep neural networks show that flat minima of the loss\nlandscape correlate with improved generalization. Sharpness-aware minimization\n(SAM) efficiently finds flat regions by updating the parameters according to\nthe gradient at an adversarial perturbation. The perturbation depends on the\nEuclidean metric, making SAM non-invariant under reparametrizations, which\nblurs sharpness and generalization. We propose Monge SAM (M-SAM), a\nreparametrization invariant version of SAM by considering a Riemannian metric\nin the parameter space induced naturally by the loss surface. Compared to\nprevious approaches, M-SAM works under any modeling choice, relies only on mild\nassumptions while being as computationally efficient as SAM. We theoretically\nargue that M-SAM varies between SAM and gradient descent (GD), which increases\nrobustness to hyperparameter selection and reduces attraction to suboptimal\nequilibria like saddle points. We demonstrate this behavior both theoretically\nand empirically on a multi-modal representation alignment task.\n","authors":["Albert Kjøller Jacobsen","Georgios Arvanitidis"],"pdf_url":"https://arxiv.org/pdf/2502.08448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08445v1","updated":"2025-02-12T14:36:25Z","published":"2025-02-12T14:36:25Z","title":"$\\texttt{LucidAtlas}$: Learning Uncertainty-Aware,\n  Covariate-Disentangled, Individualized Atlas Representations","summary":"  The goal of this work is to develop principled techniques to extract\ninformation from high dimensional data sets with complex dependencies in areas\nsuch as medicine that can provide insight into individual as well as population\nlevel variation. We develop $\\texttt{LucidAtlas}$, an approach that can\nrepresent spatially varying information, and can capture the influence of\ncovariates as well as population uncertainty. As a versatile atlas\nrepresentation, $\\texttt{LucidAtlas}$ offers robust capabilities for covariate\ninterpretation, individualized prediction, population trend analysis, and\nuncertainty estimation, with the flexibility to incorporate prior knowledge.\nAdditionally, we discuss the trustworthiness and potential risks of neural\nadditive models for analyzing dependent covariates and then introduce a\nmarginalization approach to explain the dependence of an individual predictor\non the models' response (the atlas). To validate our method, we demonstrate its\ngeneralizability on two medical datasets. Our findings underscore the critical\nrole of by-construction interpretable models in advancing scientific discovery.\nOur code will be publicly available upon acceptance.\n","authors":["Yining Jiao","Sreekalyani Bhamidi","Huaizhi Qu","Carlton Zdanski","Julia Kimbell","Andrew Prince","Cameron Worden","Samuel Kirse","Christopher Rutter","Benjamin Shields","William Dunn","Jisan Mahmud","Tianlong Chen","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2502.08445v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2404.02690v2","updated":"2025-02-12T14:32:46Z","published":"2024-04-03T12:37:34Z","title":"How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse","summary":"  Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.\n","authors":["Yichuan Deng","Zhao Song","Jing Xiong","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2404.02690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08441v1","updated":"2025-02-12T14:32:17Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.16737v2","updated":"2025-02-12T14:25:30Z","published":"2024-11-23T13:16:06Z","title":"Federated Learning in Chemical Engineering: A Tutorial on a Framework\n  for Privacy-Preserving Collaboration Across Distributed Data Sources","summary":"  Federated Learning (FL) is a decentralized machine learning approach that has\ngained attention for its potential to enable collaborative model training\nacross clients while protecting data privacy, making it an attractive solution\nfor the chemical industry. This work aims to provide the chemical engineering\ncommunity with an accessible introduction to the discipline. Supported by a\nhands-on tutorial and a comprehensive collection of examples, it explores the\napplication of FL in tasks such as manufacturing optimization, multimodal data\nintegration, and drug discovery while addressing the unique challenges of\nprotecting proprietary information and managing distributed datasets. The\ntutorial was built using key frameworks such as $\\texttt{Flower}$ and\n$\\texttt{TensorFlow Federated}$ and was designed to provide chemical engineers\nwith the right tools to adopt FL in their specific needs. We compare the\nperformance of FL against centralized learning across three different datasets\nrelevant to chemical engineering applications, demonstrating that FL will often\nmaintain or improve classification performance, particularly for complex and\nheterogeneous data. We conclude with an outlook on the open challenges in\nfederated learning to be tackled and current approaches designed to remediate\nand improve this framework.\n","authors":["Siddhant Dutta","Iago Leal de Freitas","Pedro Maciel Xavier","Claudio Miceli de Farias","David Esteban Bernal Neira"],"pdf_url":"https://arxiv.org/pdf/2411.16737v2.pdf","comment":"53 Pages, 8 figures, Under review in ACS Industrial & Engineering\n  Chemistry Research Journal"},{"id":"http://arxiv.org/abs/2502.08436v1","updated":"2025-02-12T14:20:36Z","published":"2025-02-12T14:20:36Z","title":"From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification","summary":"  We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference.\n","authors":["Nathan Vandemoortele","Bram Steenwinckel","Femke Ongenae","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2502.08436v1.pdf","comment":"Under review at ICML 2025"},{"id":"http://arxiv.org/abs/2502.08432v1","updated":"2025-02-12T14:16:45Z","published":"2025-02-12T14:16:45Z","title":"Closer through commonality: Enhancing hypergraph contrastive learning\n  with shared groups","summary":"  Hypergraphs provide a superior modeling framework for representing complex\nmultidimensional relationships in the context of real-world interactions that\noften occur in groups, overcoming the limitations of traditional homogeneous\ngraphs. However, there have been few studies on hypergraphbased contrastive\nlearning, and existing graph-based contrastive learning methods have not been\nable to fully exploit the highorder correlation information in hypergraphs.\nHere, we propose a Hypergraph Fine-grained contrastive learning (HyFi) method\ndesigned to exploit the complex high-dimensional information inherent in\nhypergraphs. While avoiding traditional graph augmentation methods that corrupt\nthe hypergraph topology, the proposed method provides a simple and efficient\nlearning augmentation function by adding noise to node features. Furthermore,\nwe expands beyond the traditional dichotomous relationship between positive and\nnegative samples in contrastive learning by introducing a new relationship of\nweak positives. It demonstrates the importance of fine-graining positive\nsamples in contrastive learning. Therefore, HyFi is able to produce highquality\nembeddings, and outperforms both supervised and unsupervised baselines in\naverage rank on node classification across 10 datasets. Our approach\neffectively exploits high-dimensional hypergraph information, shows significant\nimprovement over existing graph-based contrastive learning methods, and is\nefficient in terms of training speed and GPU memory cost. The source code is\navailable at https://github.com/Noverse0/HyFi.git.\n","authors":["Daeyoung Roh","Donghee Han","Daehee Kim","Keejun Han","Mun Yi"],"pdf_url":"https://arxiv.org/pdf/2502.08432v1.pdf","comment":"11page, 5 figures, 6 tables, 2024 IEEE International Conference on\n  Big Data"},{"id":"http://arxiv.org/abs/2502.08426v1","updated":"2025-02-12T14:09:05Z","published":"2025-02-12T14:09:05Z","title":"Semantic Learning for Molecular Communication in Internet of Bio-Nano\n  Things","summary":"  Molecular communication (MC) provides a foundational framework for\ninformation transmission in the Internet of Bio-Nano Things (IoBNT), where\nefficiency and reliability are crucial. However, the inherent limitations of\nmolecular channels, such as low transmission rates, noise, and inter-symbol\ninterference (ISI), limit their ability to support complex data transmission.\nThis paper proposes an end-to-end semantic learning framework designed to\noptimize task-oriented molecular communication, with a focus on biomedical\ndiagnostic tasks under resource-constrained conditions. The proposed framework\nemploys a deep encoder-decoder architecture to efficiently extract, quantize,\nand decode semantic features, prioritizing task-relevant semantic information\nto enhance diagnostic classification performance. Additionally, a probabilistic\nchannel network is introduced to approximate molecular propagation dynamics,\nenabling gradient-based optimization for end-to-end learning. Experimental\nresults demonstrate that the proposed semantic framework improves diagnostic\naccuracy by at least 25% compared to conventional JPEG compression with LDPC\ncoding methods under resource-constrained communication scenarios.\n","authors":["Hanlin Cai","Ozgur B. Akan"],"pdf_url":"https://arxiv.org/pdf/2502.08426v1.pdf","comment":"4 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2502.08416v1","updated":"2025-02-12T13:59:22Z","published":"2025-02-12T13:59:22Z","title":"Multifidelity Simulation-based Inference for Computationally Expensive\n  Simulators","summary":"  Across many domains of science, stochastic models are an essential tool to\nunderstand the mechanisms underlying empirically observed data. Models can be\nof different levels of detail and accuracy, with models of high-fidelity (i.e.,\nhigh accuracy) to the phenomena under study being often preferable. However,\ninferring parameters of high-fidelity models via simulation-based inference is\nchallenging, especially when the simulator is computationally expensive. We\nintroduce MF-NPE, a multifidelity approach to neural posterior estimation that\nleverages inexpensive low-fidelity simulations to infer parameters of\nhigh-fidelity simulators within a limited simulation budget. MF-NPE performs\nneural posterior estimation with limited high-fidelity resources by virtue of\ntransfer learning, with the ability to prioritize individual observations using\nactive learning. On one statistical task with analytical ground-truth and two\nreal-world tasks, MF-NPE shows comparable performance to current approaches\nwhile requiring up to two orders of magnitude fewer high-fidelity simulations.\nOverall, MF-NPE opens new opportunities to perform efficient Bayesian inference\non computationally expensive simulators.\n","authors":["Anastasia N. Krouglova","Hayden R. Johnson","Basile Confavreux","Michael Deistler","Pedro J. Gonçalves"],"pdf_url":"https://arxiv.org/pdf/2502.08416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08414v1","updated":"2025-02-12T13:57:09Z","published":"2025-02-12T13:57:09Z","title":"Sparse Estimation of Inverse Covariance and Partial Correlation Matrices\n  via Joint Partial Regression","summary":"  We present a new method for estimating high-dimensional sparse partial\ncorrelation and inverse covariance matrices, which exploits the connection\nbetween the inverse covariance matrix and linear regression. The method is a\ntwo-stage estimation method wherein each individual feature is regressed on all\nother features while positive semi-definiteness is enforced simultaneously. We\nprovide statistical rates of convergence for the proposed method which match,\nand improve upon, the state-of-the-art for inverse covariance and partial\ncorrelation matrix estimation, respectively. We also propose an efficient\nproximal splitting algorithm for numerically computing the estimate. The\neffectiveness of the proposed method is demonstrated on both synthetic and\nreal-world data.\n","authors":["Samuel Erickson","Tobias Rydén"],"pdf_url":"https://arxiv.org/pdf/2502.08414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05431v2","updated":"2025-02-12T13:54:01Z","published":"2025-02-08T03:41:16Z","title":"APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding","summary":"  Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.\n","authors":["Xinyu Yang","Tianqi Chen","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2502.05431v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08397v1","updated":"2025-02-12T13:40:00Z","published":"2025-02-12T13:40:00Z","title":"Strong bounds for large-scale Minimum Sum-of-Squares Clustering","summary":"  Clustering is a fundamental technique in data analysis and machine learning,\nused to group similar data points together. Among various clustering methods,\nthe Minimum Sum-of-Squares Clustering (MSSC) is one of the most widely used.\nMSSC aims to minimize the total squared Euclidean distance between data points\nand their corresponding cluster centroids. Due to the unsupervised nature of\nclustering, achieving global optimality is crucial, yet computationally\nchallenging. The complexity of finding the global solution increases\nexponentially with the number of data points, making exact methods impractical\nfor large-scale datasets. Even obtaining strong lower bounds on the optimal\nMSSC objective value is computationally prohibitive, making it difficult to\nassess the quality of heuristic solutions. We address this challenge by\nintroducing a novel method to validate heuristic MSSC solutions through\noptimality gaps. Our approach employs a divide-and-conquer strategy,\ndecomposing the problem into smaller instances that can be handled by an exact\nsolver. The decomposition is guided by an auxiliary optimization problem, the\n\"anticlustering problem\", for which we design an efficient heuristic.\nComputational experiments demonstrate the effectiveness of the method for\nlarge-scale instances, achieving optimality gaps below 3% in most cases while\nmaintaining reasonable computational times. These results highlight the\npracticality of our approach in assessing feasible clustering solutions for\nlarge datasets, bridging a critical gap in MSSC evaluation.\n","authors":["Anna Livia Croella","Veronica Piccialli","Antonio M. Sudoso"],"pdf_url":"https://arxiv.org/pdf/2502.08397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08707v2","updated":"2025-02-12T13:29:11Z","published":"2024-08-16T12:40:01Z","title":"Beam Prediction based on Large Language Models","summary":"  In this letter, we use large language models (LLMs) to develop a\nhigh-performing and robust beam prediction method. We formulate the millimeter\nwave (mmWave) beam prediction problem as a time series forecasting task, where\nthe historical observations are aggregated through cross-variable attention and\nthen transformed into text-based representations using a trainable tokenizer.\nBy leveraging the prompt-as-prefix (PaP) technique for contextual enrichment,\nour method harnesses the power of LLMs to predict future optimal beams.\nSimulation results demonstrate that our LLM-based approach outperforms\ntraditional learning-based models in prediction accuracy as well as robustness,\nhighlighting the significant potential of LLMs in enhancing wireless\ncommunication systems.\n","authors":["Yucheng Sheng","Kai Huang","Le Liang","Peng Liu","Shi Jin","Geoffrey Ye Li"],"pdf_url":"https://arxiv.org/pdf/2408.08707v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06581v2","updated":"2025-02-12T13:25:22Z","published":"2025-02-10T15:48:11Z","title":"A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems","summary":"  The explosive growth of video data has driven the development of distributed\nvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enabling\nefficient video processing, real-time inference, and privacy-preserving\nanalysis. Among multiple advantages, CETC systems can distribute video\nprocessing tasks and enable adaptive analytics across cloud, edge, and terminal\ndevices, leading to breakthroughs in video surveillance, autonomous driving,\nand smart cities. In this survey, we first analyze fundamental architectural\ncomponents, including hierarchical, distributed, and hybrid frameworks,\nalongside edge computing platforms and resource management mechanisms. Building\nupon these foundations, edge-centric approaches emphasize on-device processing,\nedge-assisted offloading, and edge intelligence, while cloud-centric methods\nleverage powerful computational capabilities for complex video understanding\nand model training. Our investigation also covers hybrid video analytics\nincorporating adaptive task offloading and resource-aware scheduling techniques\nthat optimize performance across the entire system. Beyond conventional\napproaches, recent advances in large language models and multimodal integration\nreveal both opportunities and challenges in platform scalability, data\nprotection, and system reliability. Future directions also encompass\nexplainable systems, efficient processing mechanisms, and advanced video\nanalytics, offering valuable insights for researchers and practitioners in this\ndynamic field.\n","authors":["Linxiao Gong","Hao Yang","Gaoyun Fang","Bobo Ju","Juncen Guo","Xiaoguang Zhu","Yan Wang","Xiping Hu","Peng Sun","Azzedine Boukerche"],"pdf_url":"https://arxiv.org/pdf/2502.06581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11959v2","updated":"2025-02-12T13:25:10Z","published":"2024-12-16T16:41:51Z","title":"Gramian Multimodal Representation Learning and Alignment","summary":"  Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modalities and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.\n","authors":["Giordano Cicchetti","Eleonora Grassucci","Luigi Sigillo","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.11959v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.15981v2","updated":"2025-02-12T13:22:54Z","published":"2024-10-21T13:06:38Z","title":"Robust Visual Representation Learning with Multi-modal Prior Knowledge\n  for Image Classification Under Distribution Shift","summary":"  Despite the remarkable success of deep neural networks (DNNs) in computer\nvision, they fail to remain high-performing when facing distribution shifts\nbetween training and testing data. In this paper, we propose Knowledge-Guided\nVisual representation learning (KGV) - a distribution-based learning approach\nleveraging multi-modal prior knowledge - to improve generalization under\ndistribution shift. It integrates knowledge from two distinct modalities: 1) a\nknowledge graph (KG) with hierarchical and association relationships; and 2)\ngenerated synthetic images of visual elements semantically represented in the\nKG. The respective embeddings are generated from the given modalities in a\ncommon latent space, i.e., visual embeddings from original and synthetic images\nas well as knowledge graph embeddings (KGEs). These embeddings are aligned via\na novel variant of translation-based KGE methods, where the node and relation\nembeddings of the KG are modeled as Gaussian distributions and translations,\nrespectively. We claim that incorporating multi-model prior knowledge enables\nmore regularized learning of image representations. Thus, the models are able\nto better generalize across different data distributions. We evaluate KGV on\ndifferent image classification tasks with major or minor distribution shifts,\nnamely road sign classification across datasets from Germany, China, and\nRussia, image classification with the mini-ImageNet dataset and its variants,\nas well as the DVM-CAR dataset. The results demonstrate that KGV consistently\nexhibits higher accuracy and data efficiency across all experiments.\n","authors":["Hongkuan Zhou","Lavdim Halilaj","Sebastian Monka","Stefan Schmid","Yuqicheng Zhu","Bo Xiong","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2410.15981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06862v2","updated":"2025-02-12T13:21:32Z","published":"2025-02-08T00:43:57Z","title":"Poincaré Inequality for Local Log-Polyak-Lojasiewicz Measures :\n  Non-asymptotic Analysis in Low-temperature Regime","summary":"  Potential functions in highly pertinent applications, such as deep learning\nin over-parameterized regime, are empirically observed to admit non-isolated\nminima. To understand the convergence behavior of stochastic dynamics in such\nlandscapes, we propose to study the class of \\logPLmeasure\\ measures\n$\\mu_\\epsilon \\propto \\exp(-V/\\epsilon)$, where the potential $V$ satisfies a\nlocal Polyak-{\\L}ojasiewicz (P\\L) inequality, and its set of local minima is\nprovably \\emph{connected}. Notably, potentials in this class can exhibit local\nmaxima and we characterize its optimal set S to be a compact $\\mathcal{C}^2$\n\\emph{embedding submanifold} of $\\mathbb{R}^d$ without boundary. The\n\\emph{non-contractibility} of S distinguishes our function class from the\nclassical convex setting topologically. Moreover, the embedding structure\ninduces a naturally defined Laplacian-Beltrami operator on S, and we show that\nits first non-trivial eigenvalue provides an \\emph{$\\epsilon$-independent}\nlower bound for the \\Poincare\\ constant in the \\Poincare\\ inequality of\n$\\mu_\\epsilon$. As a direct consequence, Langevin dynamics with such non-convex\npotential $V$ and diffusion coefficient $\\epsilon$ converges to its equilibrium\n$\\mu_\\epsilon$ at a rate of $\\tilde{\\mathcal{O}}(1/\\epsilon)$, provided\n$\\epsilon$ is sufficiently small. Here $\\tilde{\\mathcal{O}}$ hides logarithmic\nterms.\n","authors":["Yun Gong","Zebang Shen","Niao He"],"pdf_url":"https://arxiv.org/pdf/2502.06862v2.pdf","comment":"This is a replacement version of arXiv:2501.00429"},{"id":"http://arxiv.org/abs/2402.04059v2","updated":"2025-02-12T13:16:29Z","published":"2024-02-06T15:03:53Z","title":"Deep Learning for Multivariate Time Series Imputation: A Survey","summary":"  Missing values are ubiquitous in multivariate time series (MTS) data, posing\nsignificant challenges for accurate analysis and downstream applications. In\nrecent years, deep learning-based methods have successfully handled missing\ndata by leveraging complex temporal dependencies and learned data\ndistributions. In this survey, we provide a comprehensive summary of deep\nlearning approaches for multivariate time series imputation (MTSI) tasks. We\npropose a novel taxonomy that categorizes existing methods based on two key\nperspectives: imputation uncertainty and neural network architecture.\nFurthermore, we summarize existing MTSI toolkits with a particular emphasis on\nthe PyPOTS Ecosystem, which provides an integrated and standardized foundation\nfor MTSI research. Finally, we discuss key challenges and future research\ndirections, which give insight for further MTSI research. This survey aims to\nserve as a valuable resource for researchers and practitioners in the field of\ntime series analysis and missing data imputation tasks.\n","authors":["Jun Wang","Wenjie Du","Yiyuan Yang","Linglong Qian","Wei Cao","Keli Zhang","Wenjia Wang","Yuxuan Liang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2402.04059v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.13306v2","updated":"2025-02-12T13:14:53Z","published":"2024-09-20T08:04:12Z","title":"Predicting DNA fragmentation: A non-destructive analogue to chemical\n  assays using machine learning","summary":"  Globally, infertility rates are increasing, with 2.5\\% of all births being\nassisted by in vitro fertilisation (IVF) in 2022. Male infertility is the cause\nfor approximately half of these cases. The quality of sperm DNA has substantial\nimpact on the success of IVF. The assessment of sperm DNA is traditionally done\nthrough chemical assays which render sperm cells ineligible for IVF. Many\ncompounding factors lead to the population crisis, with fertility rates\ndropping globally in recent history. As such assisted reproductive technologies\n(ART) have been the focus of recent research efforts. Simultaneously,\nartificial intelligence has grown ubiquitous and is permeating more aspects of\nmodern life. With the advent of state-of-the-art machine learning and its\nexceptional performance in many sectors, this work builds on these successes\nand proposes a novel framework for the prediction of sperm cell DNA\nfragmentation from images of unstained sperm. Rendering a predictive model\nwhich preserves sperm integrity and allows for optimal selection of sperm for\nIVF.\n","authors":["Byron A Jacobs","Ifthakaar Shaik","Frando Lin"],"pdf_url":"https://arxiv.org/pdf/2409.13306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08378v1","updated":"2025-02-12T13:10:09Z","published":"2025-02-12T13:10:09Z","title":"Learning Humanoid Standing-up Control across Diverse Postures","summary":"  Standing-up control is crucial for humanoid robots, with the potential for\nintegration into current locomotion and loco-manipulation systems, such as fall\nrecovery. Existing approaches are either limited to simulations that overlook\nhardware constraints or rely on predefined ground-specific motion trajectories,\nfailing to enable standing up across postures in real-world scenes. To bridge\nthis gap, we present HoST (Humanoid Standing-up Control), a reinforcement\nlearning framework that learns standing-up control from scratch, enabling\nrobust sim-to-real transfer across diverse postures. HoST effectively learns\nposture-adaptive motions by leveraging a multi-critic architecture and\ncurriculum-based training on diverse simulated terrains. To ensure successful\nreal-world deployment, we constrain the motion with smoothness regularization\nand implicit motion speed bound to alleviate oscillatory and violent motions on\nphysical hardware, respectively. After simulation-based training, the learned\ncontrol policies are directly deployed on the Unitree G1 humanoid robot. Our\nexperimental results demonstrate that the controllers achieve smooth, stable,\nand robust standing-up motions across a wide range of laboratory and outdoor\nenvironments. Videos are available at\nhttps://taohuang13.github.io/humanoid-standingup.github.io/.\n","authors":["Tao Huang","Junli Ren","Huayi Wang","Zirui Wang","Qingwei Ben","Muning Wen","Xiao Chen","Jianan Li","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2502.08378v1.pdf","comment":"Humanoid Standing-up Control, 12 pages"},{"id":"http://arxiv.org/abs/2502.08376v1","updated":"2025-02-12T13:07:18Z","published":"2025-02-12T13:07:18Z","title":"Enhanced Load Forecasting with GAT-LSTM: Leveraging Grid and Temporal\n  Features","summary":"  Accurate power load forecasting is essential for the efficient operation and\nplanning of electrical grids, particularly given the increased variability and\ncomplexity introduced by renewable energy sources. This paper introduces\nGAT-LSTM, a hybrid model that combines Graph Attention Networks (GAT) and Long\nShort-Term Memory (LSTM) networks. A key innovation of the model is the\nincorporation of edge attributes, such as line capacities and efficiencies,\ninto the attention mechanism, enabling it to dynamically capture spatial\nrelationships grounded in grid-specific physical and operational constraints.\nAdditionally, by employing an early fusion of spatial graph embeddings and\ntemporal sequence features, the model effectively learns and predicts complex\ninteractions between spatial dependencies and temporal patterns, providing a\nrealistic representation of the dynamics of power grids. Experimental\nevaluations on the Brazilian Electricity System dataset demonstrate that the\nGAT-LSTM model significantly outperforms state-of-the-art models, achieving\nreductions of 21. 8% in MAE, 15. 9% in RMSE and 20. 2% in MAPE. These results\nunderscore the robustness and adaptability of the GAT-LSTM model, establishing\nit as a powerful tool for applications in grid management and energy planning.\n","authors":["Ugochukwu Orji","Çiçek Güven","Dan Stowell"],"pdf_url":"https://arxiv.org/pdf/2502.08376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16332v2","updated":"2025-02-12T13:07:07Z","published":"2023-05-22T01:14:46Z","title":"Continual Learning through Human-Robot Interaction: Human Perceptions of\n  a Continual Learning Robot in Repeated Interactions","summary":"  For long-term deployment in dynamic real-world environments, assistive robots\nmust continue to learn and adapt to their environments. Researchers have\ndeveloped various computational models for continual learning (CL) that can\nallow robots to continually learn from limited training data, and avoid\nforgetting previous knowledge. While these CL models can mitigate forgetting on\nstatic, systematically collected datasets, it is unclear how human users might\nperceive a robot that continually learns over multiple interactions with them.\nIn this paper, we developed a system that integrates CL models for object\nrecognition with a Fetch mobile manipulator robot and allows human participants\nto directly teach and test the robot over multiple sessions. We conducted an\nin-person study with 60 participants that interacted with our system in 300\nsessions (5 sessions per participant). We conducted a between-subject study\nwith three different CL models to understand human perceptions of continual\nlearning robots over multiple sessions. Our results suggest that participants'\nperceptions of trust, competence, and usability of a continual learning robot\nsignificantly decrease over multiple sessions if the robot forgets previously\nlearned objects. However, the perceived task load on participants for teaching\nand testing the robot remains the same over multiple sessions even if the robot\nforgets previously learned objects. Our results also indicate that\nstate-of-the-art CL models might perform unreliably when applied on robots\ninteracting with human participants. Further, continual learning robots are not\nperceived as very trustworthy or competent by human participants, regardless of\nthe underlying continual learning model or the session number.\n","authors":["Ali Ayub","Zachary De Francesco","Patrick Holthaus","Chrystopher L. Nehaniv","Kerstin Dautenhahn"],"pdf_url":"https://arxiv.org/pdf/2305.16332v2.pdf","comment":"Accepted at the International Journal of Social Robotics (SoRo), 2025"},{"id":"http://arxiv.org/abs/2502.08365v1","updated":"2025-02-12T12:51:36Z","published":"2025-02-12T12:51:36Z","title":"Towards Principled Multi-Agent Task Agnostic Exploration","summary":"  In reinforcement learning, we typically refer to task-agnostic exploration\nwhen we aim to explore the environment without access to the task specification\na priori. In a single-agent setting the problem has been extensively studied\nand mostly understood. A popular approach cast the task-agnostic objective as\nmaximizing the entropy of the state distribution induced by the agent's policy,\nfrom which principles and methods follows. In contrast, little is known about\ntask-agnostic exploration in multi-agent settings, which are ubiquitous in the\nreal world. How should different agents explore in the presence of others? In\nthis paper, we address this question through a generalization to multiple\nagents of the problem of maximizing the state distribution entropy. First, we\ninvestigate alternative formulations, highlighting respective positives and\nnegatives. Then, we present a scalable, decentralized, trust-region policy\nsearch algorithm to address the problem in practical settings. Finally, we\nprovide proof of concept experiments to both corroborate the theoretical\nfindings and pave the way for task-agnostic exploration in challenging\nmulti-agent settings.\n","authors":["Riccardo Zamboni","Mirco Mutti","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2502.08365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08364v1","updated":"2025-02-12T12:50:24Z","published":"2025-02-12T12:50:24Z","title":"A Survey on Pre-Trained Diffusion Model Distillations","summary":"  Diffusion Models~(DMs) have emerged as the dominant approach in Generative\nArtificial Intelligence (GenAI), owing to their remarkable performance in tasks\nsuch as text-to-image synthesis. However, practical DMs, such as stable\ndiffusion, are typically trained on massive datasets and thus usually require\nlarge storage. At the same time, many steps may be required, i.e., recursively\nevaluating the trained neural network, to generate a high-quality image, which\nresults in significant computational costs during sample generation. As a\nresult, distillation methods on pre-trained DM have become widely adopted\npractices to develop smaller, more efficient models capable of rapid, few-step\ngeneration in low-resource environment. When these distillation methods are\ndeveloped from different perspectives, there is an urgent need for a systematic\nsurvey, particularly from a methodological perspective. In this survey, we\nreview distillation methods through three aspects: output loss distillation,\ntrajectory distillation and adversarial distillation. We also discuss current\nchallenges and outline future research directions in the conclusion.\n","authors":["Xuhui Fan","Zhangkai Wu","Hongyu Wu"],"pdf_url":"https://arxiv.org/pdf/2502.08364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18652v7","updated":"2025-02-12T12:49:36Z","published":"2024-10-24T11:32:00Z","title":"$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation","summary":"  Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.\n","authors":["Woosung Koh","Jang Han Yoon","MinHyung Lee","Youngjin Song","Jaegwan Cho","Jaehyun Kang","Taehyeon Kim","Se-Young Yun","Youngjae Yu","Bongshin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.18652v7.pdf","comment":"NAACL 2025 Main (Long)"},{"id":"http://arxiv.org/abs/2502.07071v2","updated":"2025-02-12T12:38:13Z","published":"2025-01-31T19:43:13Z","title":"TRADES: Generating Realistic Market Simulations with Diffusion Models","summary":"  Financial markets are complex systems characterized by high statistical\nnoise, nonlinearity, and constant evolution. Thus, modeling them is extremely\nhard. We address the task of generating realistic and responsive Limit Order\nBook (LOB) market simulations, which are fundamental for calibrating and\ntesting trading strategies, performing market impact experiments, and\ngenerating synthetic market data. Previous works lack realism, usefulness, and\nresponsiveness of the generated simulations. To bridge this gap, we propose a\nnovel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB\nSimulations (TRADES). TRADES generates realistic order flows conditioned on the\nstate of the market, leveraging a transformer-based architecture that captures\nthe temporal and spatial characteristics of high-frequency market data. There\nis a notable absence of quantitative metrics for evaluating generative market\nsimulation models in the literature. To tackle this problem, we adapt the\npredictive score, a metric measured as an MAE, by training a stock price\npredictive model on synthetic data and testing it on real data. We compare\nTRADES with previous works on two stocks, reporting an x3.27 and x3.47\nimprovement over SoTA according to the predictive score, demonstrating that we\ngenerate useful synthetic market data for financial downstream tasks. We assess\nTRADES's market simulation realism and responsiveness, showing that it\neffectively learns the conditional data distribution and successfully reacts to\nan experimental agent, giving sprout to possible calibrations and evaluations\nof trading strategies and market impact experiments. We developed DeepMarket,\nthe first open-source Python framework for market simulation with deep\nlearning. Our repository includes a synthetic LOB dataset composed of TRADES's\ngenerates simulations. We release the code at\ngithub.com/LeonardoBerti00/DeepMarket.\n","authors":["Leonardo Berti","Bardh Prenkaj","Paola Velardi"],"pdf_url":"https://arxiv.org/pdf/2502.07071v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.08355v1","updated":"2025-02-12T12:30:49Z","published":"2025-02-12T12:30:49Z","title":"Loss Landscape Analysis for Reliable Quantized ML Models for Scientific\n  Sensing","summary":"  In this paper, we propose a method to perform empirical analysis of the loss\nlandscape of machine learning (ML) models. The method is applied to two ML\nmodels for scientific sensing, which necessitates quantization to be deployed\nand are subject to noise and perturbations due to experimental conditions. Our\nmethod allows assessing the robustness of ML models to such effects as a\nfunction of quantization precision and under different regularization\ntechniques -- two crucial concerns that remained underexplored so far. By\ninvestigating the interplay between performance, efficiency, and robustness by\nmeans of loss landscape analysis, we both established a strong correlation\nbetween gently-shaped landscapes and robustness to input and weight\nperturbations and observed other intriguing and non-obvious phenomena. Our\nmethod allows a systematic exploration of such trade-offs a priori, i.e.,\nwithout training and testing multiple models, leading to more efficient\ndevelopment workflows. This work also highlights the importance of\nincorporating robustness into the Pareto optimization of ML models, enabling\nmore reliable and adaptive scientific sensing systems.\n","authors":["Tommaso Baldi","Javier Campos","Olivia Weng","Caleb Geniesse","Nhan Tran","Ryan Kastner","Alessandro Biondi"],"pdf_url":"https://arxiv.org/pdf/2502.08355v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.08353v1","updated":"2025-02-12T12:28:39Z","published":"2025-02-12T12:28:39Z","title":"Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy","summary":"  With the extensive application of Graph Neural Networks (GNNs) across various\ndomains, their trustworthiness has emerged as a focal point of research. Some\nexisting studies have shown that the integration of large language models\n(LLMs) can improve the semantic understanding and generation capabilities of\nGNNs, which in turn improves the trustworthiness of GNNs from various aspects.\nOur review introduces a taxonomy that offers researchers a clear framework for\ncomprehending the principles and applications of different methods and helps\nclarify the connections and differences among various approaches. Then we\nsystematically survey representative approaches along the four categories of\nour taxonomy. Through our taxonomy, researchers can understand the applicable\nscenarios, potential advantages, and limitations of each approach for the the\ntrusted integration of GNNs with LLMs. Finally, we present some promising\ndirections of work and future trends for the integration of LLMs and GNNs to\nimprove model trustworthiness.\n","authors":["Ruizhan Xue","Huimin Deng","Fang He","Maojun Wang","Zeyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08353v1.pdf","comment":"Submitted to IJCAI 2025"},{"id":"http://arxiv.org/abs/2501.16937v3","updated":"2025-02-12T12:25:56Z","published":"2025-01-28T13:31:18Z","title":"TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models","summary":"  Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.\n","authors":["Makoto Shing","Kou Misaki","Han Bao","Sho Yokoi","Takuya Akiba"],"pdf_url":"https://arxiv.org/pdf/2501.16937v3.pdf","comment":"To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025) as a Spotlight presentation"},{"id":"http://arxiv.org/abs/2502.08346v1","updated":"2025-02-12T12:13:51Z","published":"2025-02-12T12:13:51Z","title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","summary":"  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n","authors":["Bin Wu","Yihang Wang","Yuanhao Zeng","Jiawei Liu","Jiashu Zhao","Cheng Yang","Yawen Li","Long Xia","Dawei Yin","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.08346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08340v1","updated":"2025-02-12T12:07:09Z","published":"2025-02-12T12:07:09Z","title":"Hierarchical Learning-based Graph Partition for Large-scale Vehicle\n  Routing Problems","summary":"  Neural solvers based on the divide-and-conquer approach for Vehicle Routing\nProblems (VRPs) in general, and capacitated VRP (CVRP) in particular,\nintegrates the global partition of an instance with local constructions for\neach subproblem to enhance generalization. However, during the global partition\nphase, misclusterings within subgraphs have a tendency to progressively\ncompound throughout the multi-step decoding process of the learning-based\npartition policy. This suboptimal behavior in the global partition phase, in\nturn, may lead to a dramatic deterioration in the performance of the overall\ndecomposition-based system, despite using optimal local constructions. To\naddress these challenges, we propose a versatile Hierarchical Learning-based\nGraph Partition (HLGP) framework, which is tailored to benefit the partition of\nCVRP instances by synergistically integrating global and local partition\npolicies. Specifically, the global partition policy is tasked with creating the\ncoarse multi-way partition to generate the sequence of simpler two-way\npartition subtasks. These subtasks mark the initiation of the subsequent K\nlocal partition levels. At each local partition level, subtasks exclusive for\nthis level are assigned to the local partition policy which benefits from the\ninsensitive local topological features to incrementally alleviate the\ncompounded errors. This framework is versatile in the sense that it optimizes\nthe involved partition policies towards a unified objective harmoniously\ncompatible with both reinforcement learning (RL) and supervised learning (SL).\n(*Due to the notification of arXiv \"The Abstract field cannot be longer than\n1,920 characters\", the appeared Abstract is shortened. For the full Abstract,\nplease download the Article.)\n","authors":["Yuxin Pan","Ruohong Liu","Yize Chen","Zhiguang Cao","Fangzhen Lin"],"pdf_url":"https://arxiv.org/pdf/2502.08340v1.pdf","comment":"Accepted as a Full Paper at AAMAS 2025 (24th International Conference\n  on Autonomous Agents and Multiagent Systems)"},{"id":"http://arxiv.org/abs/2502.08337v1","updated":"2025-02-12T12:00:58Z","published":"2025-02-12T12:00:58Z","title":"Hierarchical Multi-Agent Framework for Carbon-Efficient Liquid-Cooled\n  Data Center Clusters","summary":"  Reducing the environmental impact of cloud computing requires efficient\nworkload distribution across geographically dispersed Data Center Clusters\n(DCCs) and simultaneously optimizing liquid and air (HVAC) cooling with time\nshift of workloads within individual data centers (DC). This paper introduces\nGreen-DCC, which proposes a Reinforcement Learning (RL) based hierarchical\ncontroller to optimize both workload and liquid cooling dynamically in a DCC.\nBy incorporating factors such as weather, carbon intensity, and resource\navailability, Green-DCC addresses realistic constraints and interdependencies.\nWe demonstrate how the system optimizes multiple data centers synchronously,\nenabling the scope of digital twins, and compare the performance of various RL\napproaches based on carbon emissions and sustainability metrics while also\noffering a framework and benchmark simulation for broader ML research in\nsustainability.\n","authors":["Soumyendu Sarkar","Avisek Naug","Antonio Guillen","Vineet Gundecha","Ricardo Luna Gutierrez","Sahand Ghorbanpour","Sajad Mousavi","Ashwin Ramesh Babu","Desik Rengarajan","Cullen Bash"],"pdf_url":"https://arxiv.org/pdf/2502.08337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04237v3","updated":"2025-02-12T11:50:45Z","published":"2023-11-06T15:45:33Z","title":"Online Learning Quantum States with the Logarithmic Loss via VB-FTRL","summary":"  Online learning of quantum states with the logarithmic loss (LL-OLQS) is a\nquantum generalization of online portfolio selection (OPS), a classic open\nproblem in online learning for over three decades. This problem also emerges in\ndesigning stochastic optimization algorithms for maximum-likelihood quantum\nstate tomography. Recently, Jezequel et al. (arXiv:2209.13932) proposed the\nVB-FTRL algorithm, the first regret-optimal algorithm for OPS with moderate\ncomputational complexity. In this paper, we generalize VB-FTRL for LL-OLQS. Let\n$d$ denote the dimension and $T$ the number of rounds. The generalized\nalgorithm achieves a regret rate of $O ( d^2 \\log ( d + T ) )$ for LL-OLQS.\nEach iteration of the algorithm consists of solving a semidefinite program that\ncan be implemented in polynomial time by, for example, cutting-plane methods.\nFor comparison, the best-known regret rate for LL-OLQS is currently $O ( d^2\n\\log T )$, achieved by an exponential weight method. However, no explicit\nimplementation is available for the exponential weight method for LL-OLQS. To\nfacilitate the generalization, we introduce the notion of VB-convexity.\nVB-convexity is a sufficient condition for the volumetric barrier associated\nwith any function to be convex and is of independent interest.\n","authors":["Wei-Fu Tseng","Kai-Chun Chen","Zi-Hong Xiao","Yen-Huan Li"],"pdf_url":"https://arxiv.org/pdf/2311.04237v3.pdf","comment":"ALT 2025"},{"id":"http://arxiv.org/abs/2502.08326v1","updated":"2025-02-12T11:48:15Z","published":"2025-02-12T11:48:15Z","title":"Model-Free Counterfactual Subset Selection at Scale","summary":"  Ensuring transparency in AI decision-making requires interpretable\nexplanations, particularly at the instance level. Counterfactual explanations\nare a powerful tool for this purpose, but existing techniques frequently depend\non synthetic examples, introducing biases from unrealistic assumptions, flawed\nmodels, or skewed data. Many methods also assume full dataset availability, an\nimpractical constraint in real-time environments where data flows continuously.\nIn contrast, streaming explanations offer adaptive, real-time insights without\nrequiring persistent storage of the entire dataset. This work introduces a\nscalable, model-free approach to selecting diverse and relevant counterfactual\nexamples directly from observed data. Our algorithm operates efficiently in\nstreaming settings, maintaining $O(\\log k)$ update complexity per item while\nensuring high-quality counterfactual selection. Empirical evaluations on both\nreal-world and synthetic datasets demonstrate superior performance over\nbaseline methods, with robust behavior even under adversarial conditions.\n","authors":["Minh Hieu Nguyen","Viet Hung Doan","Anh Tuan Nguyen","Jun Jo","Quoc Viet Hung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.08326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00003v4","updated":"2025-02-12T11:47:03Z","published":"2024-10-15T06:53:30Z","title":"Unsupervised Training of Diffusion Models for Feasible Solution\n  Generation in Neural Combinatorial Optimization","summary":"  Recent advancements in neural combinatorial optimization (NCO) methods have\nshown promising results in generating near-optimal solutions without the need\nfor expert-crafted heuristics. However, high performance of these approaches\noften rely on problem-specific human-expertise-based search after generating\ncandidate solutions, limiting their applicability to commonly solved CO\nproblems such as Traveling Salesman Problem (TSP). In this paper, we present\nIC/DC, an unsupervised CO framework that directly trains a diffusion model from\nscratch. We train our model in a self-supervised way to minimize the cost of\nthe solution while adhering to the problem-specific constraints. IC/DC is\nspecialized in addressing CO problems involving two distinct sets of items, and\nit does not need problem-specific search processes to generate valid solutions.\nIC/DC employs a novel architecture capable of capturing the intricate\nrelationships between items, and thereby enabling effective optimization in\nchallenging CO scenarios. IC/DC achieves state-of-the-art performance relative\nto existing NCO methods on the Parallel Machine Scheduling Problem (PMSP) and\nAsymmetric Traveling Salesman Problem (ATSP).\n","authors":["Seong-Hyun Hong","Hyun-Sung Kim","Zian Jang","Deunsol Yoon","Hyungseok Song","Byung-Jun Lee"],"pdf_url":"https://arxiv.org/pdf/2411.00003v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18071v2","updated":"2025-02-12T11:31:48Z","published":"2025-01-30T00:42:43Z","title":"Towards Transparent and Accurate Diabetes Prediction Using Machine\n  Learning and Explainable Artificial Intelligence","summary":"  Diabetes mellitus (DM) is a global health issue of significance that must be\ndiagnosed as early as possible and managed well. This study presents a\nframework for diabetes prediction using Machine Learning (ML) models,\ncomplemented with eXplainable Artificial Intelligence (XAI) tools, to\ninvestigate both the predictive accuracy and interpretability of the\npredictions from ML models. Data Preprocessing is based on the Synthetic\nMinority Oversampling Technique (SMOTE) and feature scaling used on the\nDiabetes Binary Health Indicators dataset to deal with class imbalance and\nvariability of clinical features. The ensemble model provided high accuracy,\nwith a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General\nHealth, Income, and Physical Activity were the most influential predictors\nobtained from the model explanations. The results of this study suggest that ML\ncombined with XAI is a promising means of developing accurate and\ncomputationally transparent tools for use in healthcare systems.\n","authors":["Pir Bakhsh Khokhar","Viviana Pentangelo","Fabio Palomba","Carmine Gravino"],"pdf_url":"https://arxiv.org/pdf/2501.18071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08302v1","updated":"2025-02-12T11:03:51Z","published":"2025-02-12T11:03:51Z","title":"HDT: Hierarchical Discrete Transformer for Multivariate Time Series\n  Forecasting","summary":"  Generative models have gained significant attention in multivariate time\nseries forecasting (MTS), particularly due to their ability to generate\nhigh-fidelity samples. Forecasting the probability distribution of multivariate\ntime series is a challenging yet practical task. Although some recent attempts\nhave been made to handle this task, two major challenges persist: 1) some\nexisting generative methods underperform in high-dimensional multivariate time\nseries forecasting, which is hard to scale to higher dimensions; 2) the\ninherent high-dimensional multivariate attributes constrain the forecasting\nlengths of existing generative models. In this paper, we point out that\ndiscrete token representations can model high-dimensional MTS with faster\ninference time, and forecasting the target with long-term trends of itself can\nextend the forecasting length with high accuracy. Motivated by this, we propose\na vector quantized framework called Hierarchical Discrete Transformer (HDT)\nthat models time series into discrete token representations with l2\nnormalization enhanced vector quantized strategy, in which we transform the MTS\nforecasting into discrete tokens generation. To address the limitations of\ngenerative models in long-term forecasting, we propose a hierarchical discrete\nTransformer. This model captures the discrete long-term trend of the target at\nthe low level and leverages this trend as a condition to generate the discrete\nrepresentation of the target at the high level that introduces the features of\nthe target itself to extend the forecasting length in high-dimensional MTS.\nExtensive experiments on five popular MTS datasets verify the effectiveness of\nour proposed method.\n","authors":["Shibo Feng","Peilin Zhao","Liu Liu","Pengcheng Wu","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2502.08302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08298v1","updated":"2025-02-12T10:58:57Z","published":"2025-02-12T10:58:57Z","title":"Improving Existing Optimization Algorithms with LLMs","summary":"  The integration of Large Language Models (LLMs) into optimization has created\na powerful synergy, opening exciting research opportunities. This paper\ninvestigates how LLMs can enhance existing optimization algorithms. Using their\npre-trained knowledge, we demonstrate their ability to propose innovative\nheuristic variations and implementation strategies. To evaluate this, we\napplied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt\n(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that\nincorporates a heuristic in the solution construction phase. Our results show\nthat an alternative heuristic proposed by GPT-4o outperforms the\nexpert-designed heuristic of CMSA, with the performance gap widening on larger\nand denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/\n","authors":["Camilo Chacón Sartori","Christian Blum"],"pdf_url":"https://arxiv.org/pdf/2502.08298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04304v2","updated":"2025-02-12T10:49:40Z","published":"2025-01-08T06:30:31Z","title":"DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion\n  Models","summary":"  Despite the widespread use of text-to-image diffusion models across various\ntasks, their computational and memory demands limit practical applications. To\nmitigate this issue, quantization of diffusion models has been explored. It\nreduces memory usage and computational costs by compressing weights and\nactivations into lower-bit formats. However, existing methods often struggle to\npreserve both image quality and text-image alignment, particularly in\nlower-bit($<$ 8bits) quantization. In this paper, we analyze the challenges\nassociated with quantizing text-to-image diffusion models from a distributional\nperspective. Our analysis reveals that activation outliers play a crucial role\nin determining image quality. Additionally, we identify distinctive patterns in\ncross-attention scores, which significantly affects text-image alignment. To\naddress these challenges, we propose Distribution-aware Group Quantization\n(DGQ), a method that identifies and adaptively handles pixel-wise and\nchannel-wise outliers to preserve image quality. Furthermore, DGQ applies\nprompt-specific logarithmic quantization scales to maintain text-image\nalignment. Our method demonstrates remarkable performance on datasets such as\nMS-COCO and PartiPrompts. We are the first to successfully achieve low-bit\nquantization of text-to-image diffusion models without requiring additional\nfine-tuning of weight quantization parameters. Code is available at\nhttps://github.com/ugonfor/DGQ.\n","authors":["Hyogon Ryu","NaHyeon Park","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2501.04304v2.pdf","comment":"Accepted ICLR 2025. Project page: https://ugonfor.kr/DGQ"},{"id":"http://arxiv.org/abs/2502.08284v1","updated":"2025-02-12T10:42:04Z","published":"2025-02-12T10:42:04Z","title":"Data Pricing for Graph Neural Networks without Pre-purchased Inspection","summary":"  Machine learning (ML) models have become essential tools in various\nscenarios. Their effectiveness, however, hinges on a substantial volume of data\nfor satisfactory performance. Model marketplaces have thus emerged as crucial\nplatforms bridging model consumers seeking ML solutions and data owners\npossessing valuable data. These marketplaces leverage model trading mechanisms\nto properly incentive data owners to contribute their data, and return a well\nperforming ML model to the model consumers. However, existing model trading\nmechanisms often assume the data owners are willing to share their data before\nbeing paid, which is not reasonable in real world. Given that, we propose a\nnovel mechanism, named Structural Importance based Model Trading (SIMT)\nmechanism, that assesses the data importance and compensates data owners\naccordingly without disclosing the data. Specifically, SIMT procures feature\nand label data from data owners according to their structural importance, and\nthen trains a graph neural network for model consumers. Theoretically, SIMT\nensures incentive compatible, individual rational and budget feasible. The\nexperiments on five popular datasets validate that SIMT consistently\noutperforms vanilla baselines by up to $40\\%$ in both MacroF1 and MicroF1.\n","authors":["Yiping Liu","Mengxiao Zhang","Jiamou Liu","Song Yang"],"pdf_url":"https://arxiv.org/pdf/2502.08284v1.pdf","comment":"Accepted by AAMAS-2025"},{"id":"http://arxiv.org/abs/2502.08282v1","updated":"2025-02-12T10:41:21Z","published":"2025-02-12T10:41:21Z","title":"Individualised Treatment Effects Estimation with Composite Treatments\n  and Composite Outcomes","summary":"  Estimating individualised treatment effect (ITE) -- that is the causal effect\nof a set of variables (also called exposures, treatments, actions, policies, or\ninterventions), referred to as \\textit{composite treatments}, on a set of\noutcome variables of interest, referred to as \\textit{composite outcomes}, for\na unit from observational data -- remains a fundamental problem in causal\ninference with applications across disciplines, such as healthcare, economics,\neducation, social science, marketing, and computer science. Previous work in\ncausal machine learning for ITE estimation is limited to simple settings, like\nsingle treatments and single outcomes. This hinders their use in complex\nreal-world scenarios; for example, consider studying the effect of different\nICU interventions, such as beta-blockers and statins for a patient admitted for\nheart surgery, on different outcomes of interest such as atrial fibrillation\nand in-hospital mortality. The limited research into composite treatments and\noutcomes is primarily due to data scarcity for all treatments and outcomes. To\naddress the above challenges, we propose a novel and innovative\nhypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation\nunder composite treatments and composite outcomes, which tackles the data\nscarcity issue by dynamically sharing information across treatments and\noutcomes. Our empirical analysis with binary and arbitrary composite treatments\nand outcomes demonstrates the effectiveness of the proposed approach compared\nto existing methods.\n","authors":["Vinod Kumar Chauhan","Lei Clifton","Gaurav Nigam","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2502.08282v1.pdf","comment":"6 pages (double column), 4 figures"},{"id":"http://arxiv.org/abs/2305.19270v2","updated":"2025-02-12T10:37:04Z","published":"2023-05-30T17:59:32Z","title":"Learning without Forgetting for Vision-Language Models","summary":"  Class-Incremental Learning (CIL) or continual learning is a desired\ncapability in the real world, which requires a learning system to adapt to new\ntasks without forgetting former ones. While traditional CIL methods focus on\nvisual information to grasp core features, recent advances in Vision-Language\nModels (VLM) have shown promising capabilities in learning generalizable\nrepresentations with the aid of textual information. However, when continually\ntrained with new classes, VLMs often suffer from catastrophic forgetting of\nformer knowledge. Applying VLMs to CIL poses two major challenges: 1) how to\nadapt the model without forgetting; and 2) how to make full use of the\nmulti-modal information. To this end, we propose PROjectiOn Fusion (PROOF) that\nenables VLMs to learn without forgetting. To handle the first challenge, we\npropose training task-specific projections based on the frozen image/text\nencoders. When facing new tasks, new projections are expanded and former\nprojections are fixed, alleviating the forgetting of old concepts. For the\nsecond challenge, we propose the fusion module to better utilize the\ncross-modality information. By jointly adjusting visual and textual features,\nthe model can capture semantic information with stronger representation\nability. Extensive experiments on nine benchmark datasets validate PROOF\nachieves state-of-the-art performance. Code is available at\nhttps://github.com/zhoudw-zdw/PROOF\n","authors":["Da-Wei Zhou","Yuanhan Zhang","Yan Wang","Jingyi Ning","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2305.19270v2.pdf","comment":"Accepted to TPAMI. Code is available at\n  https://github.com/zhoudw-zdw/PROOF"},{"id":"http://arxiv.org/abs/2502.08266v1","updated":"2025-02-12T10:19:50Z","published":"2025-02-12T10:19:50Z","title":"Dealing with Annotator Disagreement in Hate Speech Classification","summary":"  Hate speech detection is a crucial task, especially on social media, where\nharmful content can spread quickly. Implementing machine learning models to\nautomatically identify and address hate speech is essential for mitigating its\nimpact and preventing its proliferation. The first step in developing an\neffective hate speech detection model is to acquire a high-quality dataset for\ntraining. Labeled data is foundational for most natural language processing\ntasks, but categorizing hate speech is difficult due to the diverse and often\nsubjective nature of hate speech, which can lead to varying interpretations and\ndisagreements among annotators. This paper examines strategies for addressing\nannotator disagreement, an issue that has been largely overlooked. In\nparticular, we evaluate different approaches to deal with annotator\ndisagreement regarding hate speech classification in Turkish tweets, based on a\nfine-tuned BERT model. Our work highlights the importance of the problem and\nprovides state-of-art benchmark results for detection and understanding of hate\nspeech in online discourse.\n","authors":["Somaiyeh Dehghan","Mehmet Umut Sen","Berrin Yanikoglu"],"pdf_url":"https://arxiv.org/pdf/2502.08266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01382v2","updated":"2025-02-12T10:18:06Z","published":"2024-08-02T16:40:58Z","title":"Explaining a probabilistic prediction on the simplex with Shapley\n  compositions","summary":"  Originating in game theory, Shapley values are widely used for explaining a\nmachine learning model's prediction by quantifying the contribution of each\nfeature's value to the prediction. This requires a scalar prediction as in\nbinary classification, whereas a multiclass probabilistic prediction is a\ndiscrete probability distribution, living on a multidimensional simplex. In\nsuch a multiclass setting the Shapley values are typically computed separately\non each class in a one-vs-rest manner, ignoring the compositional nature of the\noutput distribution. In this paper, we introduce Shapley compositions as a\nwell-founded way to properly explain a multiclass probabilistic prediction,\nusing the Aitchison geometry from compositional data analysis. We prove that\nthe Shapley composition is the unique quantity satisfying linearity, symmetry\nand efficiency on the Aitchison simplex, extending the corresponding axiomatic\nproperties of the standard Shapley value. We demonstrate this proper multiclass\ntreatment in a range of scenarios.\n","authors":["Paul-Gauthier Noé","Miquel Perelló-Nieto","Jean-François Bonastre","Peter Flach"],"pdf_url":"https://arxiv.org/pdf/2408.01382v2.pdf","comment":"Published in ECAI2024's proceedings"},{"id":"http://arxiv.org/abs/2502.08262v1","updated":"2025-02-12T10:10:04Z","published":"2025-02-12T10:10:04Z","title":"GenIAS: Generator for Instantiating Anomalies in time Series","summary":"  A recent and promising approach for building time series anomaly detection\n(TSAD) models is to inject synthetic samples of anomalies within real data\nsets. The existing injection mechanisms have significant limitations - most of\nthem rely on ad hoc, hand-crafted strategies which fail to capture the natural\ndiversity of anomalous patterns, or are restricted to univariate time series\nsettings. To address these challenges, we design a generative model for TSAD\nusing a variational autoencoder, which is referred to as a Generator for\nInstantiating Anomalies in Time Series (GenIAS). GenIAS is designed to produce\ndiverse and realistic synthetic anomalies for TSAD tasks. By employing a novel\nlearned perturbation mechanism in the latent space and injecting the perturbed\npatterns in different segments of time series, GenIAS can generate anomalies\nwith greater diversity and varying scales. Further, guided by a new triplet\nloss function, which uses a min-max margin and a new variance-scaling approach\nto further enforce the learning of compact normal patterns, GenIAS ensures that\nanomalies are distinct from normal samples while remaining realistic. The\napproach is effective for both univariate and multivariate time series. We\ndemonstrate the diversity and realism of the generated anomalies. Our extensive\nexperiments demonstrate that GenIAS - when integrated into a TSAD task -\nconsistently outperforms seventeen traditional and deep anomaly detection\nmodels, thereby highlighting the potential of generative models for time series\nanomaly generation.\n","authors":["Zahra Zamanzadeh Darban","Qizhou Wang","Geoffrey I. Webb","Shirui Pan","Charu C. Aggarwal","Mahsa Salehi"],"pdf_url":"https://arxiv.org/pdf/2502.08262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08259v1","updated":"2025-02-12T10:05:25Z","published":"2025-02-12T10:05:25Z","title":"Balancing optimism and pessimism in offline-to-online learning","summary":"  We consider what we call the offline-to-online learning setting, focusing on\nstochastic finite-armed bandit problems. In offline-to-online learning, a\nlearner starts with offline data collected from interactions with an unknown\nenvironment in a way that is not under the learner's control. Given this data,\nthe learner begins interacting with the environment, gradually improving its\ninitial strategy as it collects more data to maximize its total reward. The\nlearner in this setting faces a fundamental dilemma: if the policy is deployed\nfor only a short period, a suitable strategy (in a number of senses) is the\nLower Confidence Bound (LCB) algorithm, which is based on pessimism. LCB can\neffectively compete with any policy that is sufficiently \"covered\" by the\noffline data. However, for longer time horizons, a preferred strategy is the\nUpper Confidence Bound (UCB) algorithm, which is based on optimism. Over time,\nUCB converges to the performance of the optimal policy at a rate that is nearly\nthe best possible among all online algorithms. In offline-to-online learning,\nhowever, UCB initially explores excessively, leading to worse short-term\nperformance compared to LCB. This suggests that a learner not in control of how\nlong its policy will be in use should start with LCB for short horizons and\ngradually transition to a UCB-like strategy as more rounds are played. This\narticle explores how and why this transition should occur. Our main result\nshows that our new algorithm performs nearly as well as the better of LCB and\nUCB at any point in time. The core idea behind our algorithm is broadly\napplicable, and we anticipate that our results will extend beyond the\nmulti-armed bandit setting.\n","authors":["Sentenac Flore","Lee Albin","Szepesvari Csaba"],"pdf_url":"https://arxiv.org/pdf/2502.08259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08578v2","updated":"2025-02-12T09:50:05Z","published":"2024-10-11T07:07:12Z","title":"Logarithmic Regret for Unconstrained Submodular Maximization Stochastic\n  Bandit","summary":"  We address the online unconstrained submodular maximization problem (Online\nUSM), in a setting with stochastic bandit feedback. In this framework, a\ndecision-maker receives noisy rewards from a non monotone submodular function\ntaking values in a known bounded interval. This paper proposes Double-Greedy -\nExplore-then-Commit (DG-ETC), adapting the Double-Greedy approach from the\noffline and online full-information settings. DG-ETC satisfies a $O(d\\log(dT))$\nproblem-dependent upper bound for the $1/2$-approximate pseudo-regret, as well\nas a $O(dT^{2/3}\\log(dT)^{1/3})$ problem-free one at the same time,\noutperforming existing approaches. In particular, we introduce a\nproblem-dependent notion of hardness characterizing the transition between\nlogarithmic and polynomial regime for the upper bounds.\n","authors":["Julien Zhou","Pierre Gaillard","Thibaud Rahier","Julyan Arbel"],"pdf_url":"https://arxiv.org/pdf/2410.08578v2.pdf","comment":"Camera-ready version for ALT 2025"},{"id":"http://arxiv.org/abs/2502.08253v1","updated":"2025-02-12T09:49:25Z","published":"2025-02-12T09:49:25Z","title":"Multi-View Oriented GPLVM: Expressiveness and Efficiency","summary":"  The multi-view Gaussian process latent variable model (MV-GPLVM) aims to\nlearn a unified representation from multi-view data but is hindered by\nchallenges such as limited kernel expressiveness and low computational\nefficiency. To overcome these issues, we first introduce a new duality between\nthe spectral density and the kernel function. By modeling the spectral density\nwith a bivariate Gaussian mixture, we then derive a generic and expressive\nkernel termed Next-Gen Spectral Mixture (NG-SM) for MV-GPLVMs. To address the\ninherent computational inefficiency of the NG-SM kernel, we propose a random\nFourier feature approximation. Combined with a tailored reparameterization\ntrick, this approximation enables scalable variational inference for both the\nmodel and the unified latent representations. Numerical evaluations across a\ndiverse range of multi-view datasets demonstrate that our proposed method\nconsistently outperforms state-of-the-art models in learning meaningful latent\nrepresentations.\n","authors":["Zi Yang","Ying Li","Zhidi Lin","Michael Minyi Zhang","Pablo M. Olmos"],"pdf_url":"https://arxiv.org/pdf/2502.08253v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2412.16205v2","updated":"2025-02-12T09:48:29Z","published":"2024-12-17T10:53:12Z","title":"Machine Learning-Based Estimation Of Wave Direction For Unmanned Surface\n  Vehicles","summary":"  Unmanned Surface Vehicles (USVs) have become critical tools for marine\nexploration, environmental monitoring, and autonomous navigation. Accurate\nestimation of wave direction is essential for improving USV navigation and\nensuring operational safety, but traditional methods often suffer from high\ncosts and limited spatial resolution. This paper proposes a machine\nlearning-based approach leveraging LSTM (Long Short-Term Memory) networks to\npredict wave direction using sensor data collected from USVs. Experimental\nresults show the capability of the LSTM model to learn temporal dependencies\nand provide accurate predictions, outperforming simpler baselines.\n","authors":["Manele Ait Habouche","Mickaël Kerboeuf","Goulven Guillou","Jean-Philippe Babau"],"pdf_url":"https://arxiv.org/pdf/2412.16205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12676v3","updated":"2025-02-12T09:47:58Z","published":"2023-12-20T00:31:43Z","title":"Bayesian Analysis of Combinatorial Gaussian Process Bandits","summary":"  We consider the combinatorial volatile Gaussian process (GP) semi-bandit\nproblem. Each round, an agent is provided a set of available base arms and must\nselect a subset of them to maximize the long-term cumulative reward. We study\nthe Bayesian setting and provide novel Bayesian cumulative regret bounds for\nthree GP-based algorithms: GP-UCB, GP-BayesUCB and GP-TS. Our bounds extend\nprevious results for GP-UCB and GP-TS to the infinite, volatile and\ncombinatorial setting, and to the best of our knowledge, we provide the first\nregret bound for GP-BayesUCB. Volatile arms encompass other widely considered\nbandit problems such as contextual bandits. Furthermore, we employ our\nframework to address the challenging real-world problem of online\nenergy-efficient navigation, where we demonstrate its effectiveness compared to\nthe alternatives.\n","authors":["Jack Sandberg","Niklas Åkerblom","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2312.12676v3.pdf","comment":"34 pages, 9 figures. Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2412.09119v2","updated":"2025-02-12T09:38:31Z","published":"2024-12-12T09:54:38Z","title":"The Utility and Complexity of in- and out-of-Distribution Machine\n  Unlearning","summary":"  Machine unlearning, the process of selectively removing data from trained\nmodels, is increasingly crucial for addressing privacy concerns and knowledge\ngaps post-deployment. Despite this importance, existing approaches are often\nheuristic and lack formal guarantees. In this paper, we analyze the fundamental\nutility, time, and space complexity trade-offs of approximate unlearning,\nproviding rigorous certification analogous to differential privacy. For\nin-distribution forget data -- data similar to the retain set -- we show that a\nsurprisingly simple and general procedure, empirical risk minimization with\noutput perturbation, achieves tight unlearning-utility-complexity trade-offs,\naddressing a previous theoretical gap on the separation from unlearning \"for\nfree\" via differential privacy, which inherently facilitates the removal of\nsuch data. However, such techniques fail with out-of-distribution forget data\n-- data significantly different from the retain set -- where unlearning time\ncomplexity can exceed that of retraining, even for a single sample. To address\nthis, we propose a new robust and noisy gradient descent variant that provably\namortizes unlearning time complexity without compromising utility.\n","authors":["Youssef Allouah","Joshua Kazdan","Rachid Guerraoui","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2412.09119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.06735v3","updated":"2025-02-12T09:32:17Z","published":"2022-09-14T15:52:19Z","title":"Falsification of Cyber-Physical Systems using Bayesian Optimization","summary":"  Cyber-physical systems (CPSs) are often complex and safety-critical, making\nit both challenging and crucial to ensure that the system's specifications are\nmet. Simulation-based falsification is a practical testing technique for\nincreasing confidence in a CPS's correctness, as it only requires that the\nsystem be simulated. Reducing the number of computationally intensive\nsimulations needed for falsification is a key concern. In this study, we\ninvestigate Bayesian optimization (BO), a sample-efficient approach that learns\na surrogate model to capture the relationship between input signal\nparameterization and specification evaluation. We propose two enhancements to\nthe basic BO for improving falsification: (1) leveraging local surrogate\nmodels, and (2) utilizing the user's prior knowledge. Additionally, we address\nthe formulation of acquisition functions for falsification by proposing and\nevaluating various alternatives. Our benchmark evaluation demonstrates\nsignificant improvements when using local surrogate models in BO for falsifying\nchallenging benchmark examples. Incorporating prior knowledge is found to be\nespecially beneficial when the simulation budget is constrained. For some\nbenchmark problems, the choice of acquisition function noticeably impacts the\nnumber of simulations required for successful falsification.\n","authors":["Zahra Ramezani","Kenan Šehić","Luigi Nardi","Knut Åkesson"],"pdf_url":"https://arxiv.org/pdf/2209.06735v3.pdf","comment":"Accepted in ACM Transactions on Embedded Computing Systems"},{"id":"http://arxiv.org/abs/2402.09780v3","updated":"2025-02-12T09:25:25Z","published":"2024-02-15T08:09:17Z","title":"TinyCL: An Efficient Hardware Architecture for Continual Learning on\n  Autonomous Systems","summary":"  The Continuous Learning (CL) paradigm consists of continuously evolving the\nparameters of the Deep Neural Network (DNN) model to progressively learn to\nperform new tasks without reducing the performance on previous tasks, i.e.,\navoiding the so-called catastrophic forgetting. However, the DNN parameter\nupdate in CL-based autonomous systems is extremely resource-hungry. The\nexisting DNN accelerators cannot be directly employed in CL because they only\nsupport the execution of the forward propagation. Only a few prior\narchitectures execute the backpropagation and weight update, but they lack the\ncontrol and management for CL. Towards this, we design a hardware architecture,\nTinyCL, to perform CL on resource-constrained autonomous systems. It consists\nof a processing unit that executes both forward and backward propagation, and a\ncontrol unit that manages memory-based CL workload. To minimize the memory\naccesses, the sliding window of the convolutional layer moves in a snake-like\nfashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at\nruntime to execute different operations. As per our knowledge, our proposed\nTinyCL represents the first hardware accelerator that executes CL on autonomous\nsystems. We synthesize the complete TinyCL architecture in a 65 nm CMOS\ntechnology node with the conventional ASIC design flow. It executes 1 epoch of\ntraining on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while\n1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,\nthus achieving a 58x speedup, consuming 86 mW in a 4.74 mm2 die.\n","authors":["Eugenio Ressa","Alberto Marchisio","Maurizio Martina","Guido Masera","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2402.09780v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08231v1","updated":"2025-02-12T09:20:08Z","published":"2025-02-12T09:20:08Z","title":"Keep your distance: learning dispersed embeddings on $\\mathbb{S}_d$","summary":"  Learning well-separated features in high-dimensional spaces, such as text or\nimage embeddings, is crucial for many machine learning applications. Achieving\nsuch separation can be effectively accomplished through the dispersion of\nembeddings, where unrelated vectors are pushed apart as much as possible. By\nconstraining features to be on a hypersphere, we can connect dispersion to\nwell-studied problems in mathematics and physics, where optimal solutions are\nknown for limited low-dimensional cases. However, in representation learning we\ntypically deal with a large number of features in high-dimensional space, and\nmoreover, dispersion is usually traded off with some other task-oriented\ntraining objective, making existing theoretical and numerical solutions\ninapplicable. Therefore, it is common to rely on gradient-based methods to\nencourage dispersion, usually by minimizing some function of the pairwise\ndistances. In this work, we first give an overview of existing methods from\ndisconnected literature, making new connections and highlighting similarities.\nNext, we introduce some new angles. We propose to reinterpret pairwise\ndispersion using a maximum mean discrepancy (MMD) motivation. We then propose\nan online variant of the celebrated Lloyd's algorithm, of K-Means fame, as an\neffective alternative regularizer for dispersion on generic domains. Finally,\nwe derive a novel dispersion method that directly exploits properties of the\nhypersphere. Our experiments show the importance of dispersion in image\nclassification and natural language processing tasks, and how algorithms\nexhibit different trade-offs in different regimes.\n","authors":["Evgeniia Tokarchuk","Hua Chang Bakker","Vlad Niculae"],"pdf_url":"https://arxiv.org/pdf/2502.08231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08227v1","updated":"2025-02-12T09:12:45Z","published":"2025-02-12T09:12:45Z","title":"Enhancing Sample Selection by Cutting Mislabeled Easy Examples","summary":"  Sample selection is a prevalent approach in learning with noisy labels,\naiming to identify confident samples for training. Although existing sample\nselection methods have achieved decent results by reducing the noise rate of\nthe selected subset, they often overlook that not all mislabeled examples harm\nthe model's performance equally. In this paper, we demonstrate that mislabeled\nexamples correctly predicted by the model early in the training process are\nparticularly harmful to model performance. We refer to these examples as\nMislabeled Easy Examples (MEEs). To address this, we propose Early Cutting,\nwhich introduces a recalibration step that employs the model's later training\nstate to re-select the confident subset identified early in training, thereby\navoiding misleading confidence from early learning and effectively filtering\nout MEEs. Experiments on the CIFAR, WebVision, and full ImageNet-1k datasets\ndemonstrate that our method effectively improves sample selection and model\nperformance by reducing MEEs.\n","authors":["Suqin Yuan","Lei Feng","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.08227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08226v1","updated":"2025-02-12T09:12:30Z","published":"2025-02-12T09:12:30Z","title":"TRISHUL: Towards Region Identification and Screen Hierarchy\n  Understanding for Large VLM based GUI Agents","summary":"  Recent advancements in Large Vision Language Models (LVLMs) have enabled the\ndevelopment of LVLM-based Graphical User Interface (GUI) agents under various\nparadigms. Training-based approaches, such as CogAgent and SeeClick, struggle\nwith cross-dataset and cross-platform generalization due to their reliance on\ndataset-specific training. Generalist LVLMs, such as GPT-4V, employ\nSet-of-Marks (SoM) for action grounding, but obtaining SoM labels requires\nmetadata like HTML source, which is not consistently available across\nplatforms. Moreover, existing methods often specialize in singular GUI tasks\nrather than achieving comprehensive GUI understanding. To address these\nlimitations, we introduce TRISHUL, a novel, training-free agentic framework\nthat enhances generalist LVLMs for holistic GUI comprehension. Unlike prior\nworks that focus on either action grounding (mapping instructions to GUI\nelements) or GUI referring (describing GUI elements given a location), TRISHUL\nseamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen\nParsing (HSP) and the Spatially Enhanced Element Description (SEED) module,\nwhich work synergistically to provide multi-granular, spatially, and\nsemantically enriched representations of GUI elements. Our results demonstrate\nTRISHUL's superior performance in action grounding across the ScreenSpot,\nVisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring,\nTRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new\nstandard for robust and adaptable GUI comprehension.\n","authors":["Kunal Singh","Shreyas Singh","Mukund Khanna"],"pdf_url":"https://arxiv.org/pdf/2502.08226v1.pdf","comment":"Under review at ICML 2025, 8 pages 5 figures"},{"id":"http://arxiv.org/abs/2409.01428v2","updated":"2025-02-12T09:10:44Z","published":"2024-09-02T19:13:26Z","title":"Self-Directed Learning of Convex Labelings on Graphs","summary":"  We study the problem of classifying the nodes of a given graph in the\nself-directed learning setup. This learning setting is a variant of online\nlearning, where rather than an adversary determining the sequence in which\nnodes are presented, the learner autonomously and adaptively selects them.\nWhile self-directed learning of Euclidean halfspaces, linear functions, and\ngeneral multiclass hypothesis classes was recently considered, no results\npreviously existed specifically for self-directed node classification on\ngraphs. In this paper, we address this problem developing efficient algorithms\nfor it. More specifically, we focus on the case of (geodesically) convex\nclusters, i.e., for every two nodes sharing the same label, all nodes on every\nshortest path between them also share the same label. In particular, we devise\nan algorithm with runtime polynomial in $n$ that makes only $3(h(G)+1)^4 \\ln n$\nmistakes on graphs with two convex clusters, where $n$ is the total number of\nnodes and $h(G)$ is the Hadwiger number, i.e., the size of the largest clique\nminor of the graph $G$. We also show that our algorithm is robust to the case\nthat clusters are slightly non-convex, still achieving a mistake bound\nlogarithmic in $n$. Finally, we devise a simple and efficient algorithm for\nhomophilic clusters, where strongly connected nodes tend to belong to the same\nclass.\n","authors":["Georgy Sokolov","Maximilian Thiessen","Margarita Akhmejanova","Fabio Vitale","Francesco Orabona"],"pdf_url":"https://arxiv.org/pdf/2409.01428v2.pdf","comment":"ALT 2025"},{"id":"http://arxiv.org/abs/2502.05416v2","updated":"2025-02-12T08:58:06Z","published":"2025-02-08T02:53:32Z","title":"Deep Generative Models with Hard Linear Equality Constraints","summary":"  While deep generative models~(DGMs) have demonstrated remarkable success in\ncapturing complex data distributions, they consistently fail to learn\nconstraints that encode domain knowledge and thus require constraint\nintegration. Existing solutions to this challenge have primarily relied on\nheuristic methods and often ignore the underlying data distribution, harming\nthe generative performance. In this work, we propose a probabilistically sound\napproach for enforcing the hard constraints into DGMs to generate\nconstraint-compliant and realistic data. This is achieved by our proposed\ngradient estimators that allow the constrained distribution, the data\ndistribution conditioned on constraints, to be differentiably learned. We carry\nout extensive experiments with various DGM model architectures over five image\ndatasets and three scientific applications in which domain knowledge is\ngoverned by linear equality constraints. We validate that the standard DGMs\nalmost surely generate data violating the constraints. Among all the constraint\nintegration strategies, ours not only guarantees the satisfaction of\nconstraints in generation but also archives superior generative performance\nthan the other methods across every benchmark.\n","authors":["Ruoyan Li","Dipti Ranjan Sahu","Guy Van den Broeck","Zhe Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.05416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08213v1","updated":"2025-02-12T08:48:55Z","published":"2025-02-12T08:48:55Z","title":"LLM Modules: Knowledge Transfer from a Large to a Small Model using\n  Enhanced Cross-Attention","summary":"  In this work, we propose an architecture of LLM Modules that enables the\ntransfer of knowledge from a large pre-trained model to a smaller model using\nan Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B\nmodel is frozen and its representations are passed through specially designed\nattention layers to the GPT-Neo-125M model, which is trained on limited\ncomputational resources. Experimental results on the Bespoke-Stratos-17k\ndataset demonstrate that after 15 epochs of training, the combined model\ngenerates responses comparable in quality to those obtained by distillation. We\ndiscuss the advantages of the modular approach, provide examples of input\nqueries and comparative analysis, and outline prospects for further extension\nof the method.\n","authors":["Konstantin Kolomeitsev"],"pdf_url":"https://arxiv.org/pdf/2502.08213v1.pdf","comment":"Code and pre-trained weights available at\n  https://huggingface.co/kkolomeitsev/llm-modules"},{"id":"http://arxiv.org/abs/2502.08211v1","updated":"2025-02-12T08:40:57Z","published":"2025-02-12T08:40:57Z","title":"Quality over Quantity: Boosting Data Efficiency Through Ensembled\n  Multimodal Data Curation","summary":"  In an era overwhelmed by vast amounts of data, the effective curation of\nweb-crawl datasets is essential for optimizing model performance. This paper\ntackles the challenges associated with the unstructured and heterogeneous\nnature of such datasets. Traditional heuristic curation methods often\ninadequately capture complex features, resulting in biases and the exclusion of\nrelevant data. We introduce an advanced, learning-driven approach, Ensemble\nCuration Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel\nquality-guided deduplication method to ensure balanced feature distributions.\nEcoDatum strategically integrates various unimodal and multimodal data curation\noperators within a weak supervision ensemble framework, utilizing automated\noptimization to score each data point effectively. EcoDatum, which\nsignificantly improves the data curation quality and efficiency, outperforms\nexisting state-of-the-art (SOTA) techniques, ranked 1st on the DataComp\nleaderboard, with an average performance score of 0.182 across 38 diverse\nevaluation datasets. This represents a 28% improvement over the DataComp\nbaseline method, demonstrating its effectiveness in improving dataset curation\nand model training efficiency.\n","authors":["Jinda Xu","Yuhao Song","Daming Wang","Weiwei Zhao","Minghua Chen","Kangliang Chen","Qinya Li"],"pdf_url":"https://arxiv.org/pdf/2502.08211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08209v1","updated":"2025-02-12T08:39:26Z","published":"2025-02-12T08:39:26Z","title":"Equivariant Masked Position Prediction for Efficient Molecular\n  Representation","summary":"  Graph neural networks (GNNs) have shown considerable promise in computational\nchemistry. However, the limited availability of molecular data raises concerns\nregarding GNNs' ability to effectively capture the fundamental principles of\nphysics and chemistry, which constrains their generalization capabilities. To\naddress this challenge, we introduce a novel self-supervised approach termed\nEquivariant Masked Position Prediction (EMPP), grounded in intramolecular\npotential and force theory. Unlike conventional attribute masking techniques,\nEMPP formulates a nuanced position prediction task that is more well-defined\nand enhances the learning of quantum mechanical features. EMPP also bypasses\nthe approximation of the Gaussian mixture distribution commonly used in\ndenoising methods, allowing for more accurate acquisition of physical\nproperties. Experimental results indicate that EMPP significantly enhances\nperformance of advanced molecular architectures, surpassing state-of-the-art\nself-supervised approaches. Our code is released in\nhttps://github.com/ajy112/EMPP.\n","authors":["Junyi An","Chao Qu","Yun-Fei Shi","XinHao Liu","Qianwei Tang","Fenglei Cao","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2502.08209v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.08208v1","updated":"2025-02-12T08:38:37Z","published":"2025-02-12T08:38:37Z","title":"Exploring Exploration in Bayesian Optimization","summary":"  A well-balanced exploration-exploitation trade-off is crucial for successful\nacquisition functions in Bayesian optimization. However, there is a lack of\nquantitative measures for exploration, making it difficult to analyze and\ncompare different acquisition functions. This work introduces two novel\napproaches - observation traveling salesman distance and observation entropy -\nto quantify the exploration characteristics of acquisition functions based on\ntheir selected observations. Using these measures, we examine the explorative\nnature of several well-known acquisition functions across a diverse set of\nblack-box problems, uncover links between exploration and empirical\nperformance, and reveal new relationships among existing acquisition functions.\nBeyond enabling a deeper understanding of acquisition functions, these measures\nalso provide a foundation for guiding their design in a more principled and\nsystematic manner.\n","authors":["Leonard Papenmeier","Nuojin Cheng","Stephen Becker","Luigi Nardi"],"pdf_url":"https://arxiv.org/pdf/2502.08208v1.pdf","comment":"28 pages, 34 figures"},{"id":"http://arxiv.org/abs/2502.08206v1","updated":"2025-02-12T08:38:13Z","published":"2025-02-12T08:38:13Z","title":"Optimizing Asynchronous Federated Learning: A Delicate Trade-Off Between\n  Model-Parameter Staleness and Update Frequency","summary":"  Synchronous federated learning (FL) scales poorly with the number of clients\ndue to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync\naddress this limitation by enabling asynchronous communication between clients\nand the central server. In this work, we rely on stochastic modeling to better\nunderstand the impact of design choices in asynchronous FL algorithms, such as\nthe concurrency level and routing probabilities, and we leverage this knowledge\nto optimize loss. We characterize in particular a fundamental trade-off for\noptimizing asynchronous FL: minimizing gradient estimation errors by avoiding\nmodel parameter staleness, while also speeding up the system by increasing the\nthroughput of model updates. Our two main contributions can be summarized as\nfollows. First, we prove a discrete variant of Little's law to derive a\nclosed-form expression for relative delay, a metric that quantifies staleness.\nThis allows us to efficiently minimize the average loss per model update, which\nhas been the gold standard in literature to date. Second, we observe that\nnaively optimizing this metric leads us to slow down the system drastically by\noveremphazing staleness at the detriment of throughput. This motivates us to\nintroduce an alternative metric that also takes system speed into account, for\nwhich we derive a tractable upper-bound that can be minimized numerically.\nExtensive numerical results show that these optimizations enhance accuracy by\n10% to 30%.\n","authors":["Abdelkrim Alahyane","Céline Comte","Matthieu Jonckheere","Éric Moulines"],"pdf_url":"https://arxiv.org/pdf/2502.08206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08205v1","updated":"2025-02-12T08:35:10Z","published":"2025-02-12T08:35:10Z","title":"Wisdom of the Crowds in Forecasting: Forecast Summarization for\n  Supporting Future Event Prediction","summary":"  Future Event Prediction (FEP) is an essential activity whose demand and\napplication range across multiple domains. While traditional methods like\nsimulations, predictive and time-series forecasting have demonstrated promising\noutcomes, their application in forecasting complex events is not entirely\nreliable due to the inability of numerical data to accurately capture the\nsemantic information related to events. One forecasting way is to gather and\naggregate collective opinions on the future to make predictions as cumulative\nperspectives carry the potential to help estimating the likelihood of upcoming\nevents. In this work, we organize the existing research and frameworks that aim\nto support future event prediction based on crowd wisdom through aggregating\nindividual forecasts. We discuss the challenges involved, available datasets,\nas well as the scope of improvement and future research directions for this\ntask. We also introduce a novel data model to represent individual forecast\nstatements.\n","authors":["Anisha Saha","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08202v1","updated":"2025-02-12T08:32:10Z","published":"2025-02-12T08:32:10Z","title":"Privacy amplification by random allocation","summary":"  We consider the privacy guarantees of an algorithm in which a user's data is\nused in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$\ndifferentially private steps. We demonstrate that the privacy guarantees of\nthis sampling scheme can be upper bound by the privacy guarantees of the\nwell-studied independent (or Poisson) subsampling in which each step uses the\nuser's data with probability $(1+ o(1))k/t $. Further, we provide two\nadditional analysis techniques that lead to numerical improvements in some\nparameter regimes. The case of $k=1$ has been previously studied in the context\nof DP-SGD in Balle et al. (2020) and very recently in Chua et al. (2024).\nPrivacy analysis of Balle et al. (2020) relies on privacy amplification by\nshuffling which leads to overly conservative bounds. Privacy analysis of Chua\net al. (2024a) relies on Monte Carlo simulations that are computationally\nprohibitive in many practical scenarios and have additional inherent\nlimitations.\n","authors":["Vitaly Feldman","Moshe Shenfeld"],"pdf_url":"https://arxiv.org/pdf/2502.08202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11647v3","updated":"2025-02-12T08:14:17Z","published":"2023-09-20T21:23:52Z","title":"Potential and limitations of random Fourier features for dequantizing\n  quantum machine learning","summary":"  Quantum machine learning is arguably one of the most explored applications of\nnear-term quantum devices. Much focus has been put on notions of variational\nquantum machine learning where parameterized quantum circuits (PQCs) are used\nas learning models. These PQC models have a rich structure which suggests that\nthey might be amenable to efficient dequantization via random Fourier features\n(RFF). In this work, we establish necessary and sufficient conditions under\nwhich RFF does indeed provide an efficient dequantization of variational\nquantum machine learning for regression. We build on these insights to make\nconcrete suggestions for PQC architecture design, and to identify structures\nwhich are necessary for a regression problem to admit a potential quantum\nadvantage via PQC based optimization.\n","authors":["Ryan Sweke","Erik Recio-Armengol","Sofiene Jerbi","Elies Gil-Fuster","Bryce Fuller","Jens Eisert","Johannes Jakob Meyer"],"pdf_url":"https://arxiv.org/pdf/2309.11647v3.pdf","comment":"44 pages (33+11). 6 Figures, with many clarifying figures added to\n  this version from original version. Comments and feedback welcome. Now\n  accepted in Quantum - this is the final version"},{"id":"http://arxiv.org/abs/2407.18170v3","updated":"2025-02-12T08:02:01Z","published":"2024-07-25T16:33:35Z","title":"RIDA: A Robust Attack Framework on Incomplete Graphs","summary":"  Graph Neural Networks (GNNs) are vital in data science but are increasingly\nsusceptible to adversarial attacks. To help researchers develop more robust GNN\nmodels, it's essential to focus on designing strong attack models as\nfoundational benchmarks and guiding references. Among adversarial attacks,\ngray-box poisoning attacks are noteworthy due to their effectiveness and fewer\nconstraints. These attacks exploit GNNs' need for retraining on updated data,\nthereby impacting their performance by perturbing these datasets. However,\ncurrent research overlooks the real-world scenario of incomplete graphs. To\naddress this gap, we introduce the Robust Incomplete Deep Attack Framework\n(RIDA). It is the first algorithm for robust gray-box poisoning attacks on\nincomplete graphs. The approach innovatively aggregates distant vertex\ninformation and ensures powerful data utilization. Extensive tests against 9\nSOTA baselines on 3 real-world datasets demonstrate that RIDA's superiority in\nhandling incompleteness and high attack performance on the incomplete graph.\n","authors":["Jianke Yu","Hanchen Wang","Chen Chen","Xiaoyang Wang","Lu Qin","Wenjie Zhang","Ying Zhang","Xijuan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.18170v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07005v2","updated":"2025-02-12T08:01:18Z","published":"2025-02-10T20:10:25Z","title":"Geometry-aware RL for Manipulation of Varying Shapes and Deformable\n  Objects","summary":"  Manipulating objects with varying geometries and deformable objects is a\nmajor challenge in robotics. Tasks such as insertion with different objects or\ncloth hanging require precise control and effective modelling of complex\ndynamics. In this work, we frame this problem through the lens of a\nheterogeneous graph that comprises smaller sub-graphs, such as actuators and\nobjects, accompanied by different edge types describing their interactions.\nThis graph representation serves as a unified structure for both rigid and\ndeformable objects tasks, and can be extended further to tasks comprising\nmultiple actuators. To evaluate this setup, we present a novel and challenging\nreinforcement learning benchmark, including rigid insertion of diverse objects,\nas well as rope and cloth manipulation with multiple end-effectors. These tasks\npresent a large search space, as both the initial and target configurations are\nuniformly sampled in 3D space. To address this issue, we propose a novel\ngraph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi),\nutilizing $SE(3)$\n  equivariant message passing networks as the main backbone to exploit the\ngeometric symmetry. In addition, by modeling explicit heterogeneity, HEPi can\noutperform Transformer-based and non-heterogeneous equivariant policies in\nterms of average returns, sample efficiency, and generalization to unseen\nobjects.\n","authors":["Tai Hoang","Huy Le","Philipp Becker","Vien Anh Ngo","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2502.07005v2.pdf","comment":"Accept at ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.05434v2","updated":"2025-02-12T07:46:40Z","published":"2025-02-08T03:47:00Z","title":"Sample-Efficient Reinforcement Learning from Human Feedback via\n  Information-Directed Sampling","summary":"  We study the problem of reinforcement learning from human feedback (RLHF), a\ncritical problem in training large language models, from a theoretical\nperspective. Our main contribution is the design of novel sample-efficient RLHF\nalgorithms based on information-directed sampling (IDS), an online\ndecision-making principle inspired by information theory. Our algorithms\nmaximize the sum of the value function and a mutual information term that\nencourages exploration of the unknown environment (which quantifies the\ninformation gained about the environment through observed human feedback data).\nTo tackle the challenge of large state spaces and improve sample efficiency, we\nconstruct a simplified \\emph{surrogate environment} and introduce a novel\ndistance measure (named the \\emph{$\\ell_g$-distance}), enabling our IDS-based\nalgorithm to achieve a Bayesian regret upper bound of order\n$O(H^{\\frac{3}{2}}\\sqrt{\\log(K(\\epsilon)) T})$, where $H$ is the episode\nlength, $T$ is the number of episode and $K(\\epsilon)$ is related to the\ncovering number of the environment. Specializing to the tabular settings, this\nregret bound is of order $\\tilde{O}(H^2\\sqrt{SAT})$, where $S$ and $A$ are the\nnumbers of states and actions. Finally, we propose an Approximate-IDS algorithm\nthat is computationally more efficient while maintaining nearly the same sample\nefficiency. The design principle of this approximate algorithm is not only\neffective in RLHF settings but also applicable to the standard RL framework.\nMoreover, our work showcases the value of information theory in reinforcement\nlearning and in the training of large language models.\n","authors":["Han Qi","Haochen Yang","Qiaosheng Zhang","Zhuoran Yang"],"pdf_url":"https://arxiv.org/pdf/2502.05434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08181v1","updated":"2025-02-12T07:39:44Z","published":"2025-02-12T07:39:44Z","title":"Latest Advancements Towards Catastrophic Forgetting under Data Scarcity:\n  A Comprehensive Survey on Few-Shot Class Incremental Learning","summary":"  Data scarcity significantly complicates the continual learning problem, i.e.,\nhow a deep neural network learns in dynamic environments with very few samples.\nHowever, the latest progress of few-shot class incremental learning (FSCIL)\nmethods and related studies show insightful knowledge on how to tackle the\nproblem. This paper presents a comprehensive survey on FSCIL that highlights\nseveral important aspects i.e. comprehensive and formal objectives of FSCIL\napproaches, the importance of prototype rectifications, the new learning\nparadigms based on pre-trained model and language-guided mechanism, the deeper\nanalysis of FSCIL performance metrics and evaluation, and the practical\ncontexts of FSCIL in various areas. Our extensive discussion presents the open\nchallenges, potential solutions, and future directions of FSCIL.\n","authors":["M. Anwar Ma'sum","Mahardhika Pratama","Igor Skrjanc"],"pdf_url":"https://arxiv.org/pdf/2502.08181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05806v2","updated":"2025-02-12T07:38:08Z","published":"2024-10-08T08:39:15Z","title":"A Parameter Update Balancing Algorithm for Multi-task Ranking Models in\n  Recommendation Systems","summary":"  Multi-task ranking models have become essential for modern real-world\nrecommendation systems. While most recommendation researches focus on designing\nsophisticated models for specific scenarios, achieving performance improvement\nfor multi-task ranking models across various scenarios still remains a\nsignificant challenge. Training all tasks naively can result in inconsistent\nlearning, highlighting the need for the development of multi-task optimization\n(MTO) methods to tackle this challenge. Conventional methods assume that the\noptimal joint gradient on shared parameters leads to optimal parameter updates.\nHowever, the actual update on model parameters may deviates significantly from\ngradients when using momentum based optimizers such as Adam, and we design and\nexecute statistical experiments to support the observation. In this paper, we\npropose a novel Parameter Update Balancing algorithm for multi-task\noptimization, denoted as PUB. In contrast to traditional MTO method which are\nbased on gradient level tasks fusion or loss level tasks fusion, PUB is the\nfirst work to optimize multiple tasks through parameter update balancing.\nComprehensive experiments on benchmark multi-task ranking datasets demonstrate\nthat PUB consistently improves several multi-task backbones and achieves\nstate-of-the-art performance. Additionally, experiments on benchmark computer\nvision datasets show the great potential of PUB in various multi-task learning\nscenarios. Furthermore, we deployed our method for an industrial evaluation on\nthe real-world commercial platform, HUAWEI AppGallery, where PUB significantly\nenhances the online multi-task ranking model, efficiently managing the primary\ntraffic of a crucial channel.\n","authors":["Jun Yuan","Guohao Cai","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2410.05806v2.pdf","comment":"Accepted by ICDM'24"},{"id":"http://arxiv.org/abs/2502.07531v2","updated":"2025-02-12T07:35:56Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03333v4","updated":"2025-02-12T07:19:15Z","published":"2024-03-05T21:34:23Z","title":"Federated Learning over Connected Modes","summary":"  Statistical heterogeneity in federated learning poses two major challenges:\nslow global training due to conflicting gradient signals, and the need of\npersonalization for local distributions. In this work, we tackle both\nchallenges by leveraging recent advances in \\emph{linear mode connectivity} --\nidentifying a linearly connected low-loss region in the parameter space of\nneural networks, which we call solution simplex. We propose federated learning\nover connected modes (\\textsc{Floco}), where clients are assigned local\nsubregions in this simplex based on their gradient signals, and together learn\nthe shared global solution simplex. This allows personalization of the client\nmodels to fit their local distributions within the degrees of freedom in the\nsolution simplex and homogenizes the update signals for the global simplex\ntraining. Our experiments show that \\textsc{Floco} accelerates the global\ntraining process, and significantly improves the local accuracy with minimal\ncomputational overhead in cross-silo federated learning settings.\n","authors":["Dennis Grinwald","Philipp Wiesner","Shinichi Nakajima"],"pdf_url":"https://arxiv.org/pdf/2403.03333v4.pdf","comment":"10 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2502.08167v1","updated":"2025-02-12T07:14:54Z","published":"2025-02-12T07:14:54Z","title":"DNNs May Determine Major Properties of Their Outputs Early, with Timing\n  Possibly Driven by Bias","summary":"  This paper argues that deep neural networks (DNNs) mostly determine their\noutputs during the early stages of inference, where biases inherent in the\nmodel play a crucial role in shaping this process. We draw a parallel between\nthis phenomenon and human decision-making, which often relies on fast,\nintuitive heuristics. Using diffusion models (DMs) as a case study, we\ndemonstrate that DNNs often make early-stage decision-making influenced by the\ntype and extent of bias in their design and training. Our findings offer a new\nperspective on bias mitigation, efficient inference, and the interpretation of\nmachine learning systems. By identifying the temporal dynamics of\ndecision-making in DNNs, this paper aims to inspire further discussion and\nresearch within the machine learning community.\n","authors":["Song Park","Sanghyuk Chun","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2502.08167v1.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2502.08166v1","updated":"2025-02-12T07:11:33Z","published":"2025-02-12T07:11:33Z","title":"From Individual Experience to Collective Evidence: A Reporting-Based\n  Framework for Identifying Systemic Harms","summary":"  When an individual reports a negative interaction with some system, how can\ntheir personal experience be contextualized within broader patterns of system\nbehavior? We study the incident database problem, where individual reports of\nadverse events arrive sequentially, and are aggregated over time. In this work,\nour goal is to identify whether there are subgroups--defined by any combination\nof relevant features--that are disproportionately likely to experience harmful\ninteractions with the system. We formalize this problem as a sequential\nhypothesis test, and identify conditions on reporting behavior that are\nsufficient for making inferences about disparities in true rates of harm across\nsubgroups. We show that algorithms for sequential hypothesis tests can be\napplied to this problem with a standard multiple testing correction. We then\ndemonstrate our method on real-world datasets, including mortgage decisions and\nvaccine side effects; on each, our method (re-)identifies subgroups known to\nexperience disproportionate harm using only a fraction of the data that was\ninitially used to discover them.\n","authors":["Jessica Dai","Paula Gradu","Inioluwa Deborah Raji","Benjamin Recht"],"pdf_url":"https://arxiv.org/pdf/2502.08166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08160v1","updated":"2025-02-12T07:03:32Z","published":"2025-02-12T07:03:32Z","title":"Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly","summary":"  Vertical Federated Learning (VFL) is a privacy-preserving collaborative\nlearning paradigm that enables multiple parties with distinct feature sets to\njointly train machine learning models without sharing their raw data. Despite\nits potential to facilitate cross-organizational collaborations, the deployment\nof VFL systems in real-world applications remains limited. To investigate the\ngap between existing VFL research and practical deployment, this survey\nanalyzes the real-world data distributions in potential VFL applications and\nidentifies four key findings that highlight this gap. We propose a novel\ndata-oriented taxonomy of VFL algorithms based on real VFL data distributions.\nOur comprehensive review of existing VFL algorithms reveals that some common\npractical VFL scenarios have few or no viable solutions. Based on these\nobservations, we outline key research directions aimed at bridging the gap\nbetween current VFL research and real-world applications.\n","authors":["Zhaomin Wu","Zhen Qin","Junyi Hou","Haodong Zhao","Qinbin Li","Bingsheng He","Lixin Fan"],"pdf_url":"https://arxiv.org/pdf/2502.08160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14054v2","updated":"2025-02-12T07:00:51Z","published":"2023-09-25T11:36:20Z","title":"Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning\n  in Generative Adversarial Networks","summary":"  Owing to the growing concerns about privacy and regulatory compliance, it is\ndesirable to regulate the output of generative models. To that end, the\nobjective of this work is to prevent the generation of outputs containing\nundesired features from a pre-trained Generative Adversarial Network (GAN)\nwhere the underlying training data set is inaccessible. Our approach is\ninspired by the observation that the parameter space of GANs exhibits\nmeaningful directions that can be leveraged to suppress specific undesired\nfeatures. However, such directions usually result in the degradation of the\nquality of generated samples. Our proposed two-stage method, known as\n'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also\nmaintaining the quality of generated samples. In the initial stage, we adapt a\npre-trained GAN on a set of negative samples (containing undesired features)\nprovided by the user. Subsequently, we train the original pre-trained GAN using\npositive samples, along with a repulsion regularizer. This regularizer\nencourages the learned model parameters to move away from the parameters of the\nadapted model (first stage) while not degrading the generation quality. We\nprovide theoretical insights into the proposed method. To the best of our\nknowledge, our approach stands as the first method addressing unlearning within\nthe realm of high-fidelity GANs (such as StyleGAN). We validate the\neffectiveness of our method through comprehensive experiments, encompassing\nboth class-level unlearning on the MNIST and AFHQ dataset and feature-level\nunlearning tasks on the CelebA-HQ dataset. Our code and implementation is\navailable at: https://github.com/atriguha/Adapt_Unlearn.\n","authors":["Piyush Tiwary","Atri Guha","Subhodip Panda","Prathosh A. P"],"pdf_url":"https://arxiv.org/pdf/2309.14054v2.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2411.16821v2","updated":"2025-02-12T06:54:53Z","published":"2024-11-25T17:15:41Z","title":"KL-geodesics flow matching with a novel sampling scheme","summary":"  Non-autoregressive language models generate all tokens simultaneously,\noffering potential speed advantages over traditional autoregressive models, but\nthey face challenges in modeling the complex dependencies inherent in text\ndata. In this work, we investigate a conditional flow matching approach for\ntext generation. We represent tokens as one-hot vectors in a \\(V\\)-dimensional\nsimplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which\ncorrespond to linear interpolation in logit space. We provide a theoretical\njustification that maximizing the conditional likelihood \\(P_{\\theta}(x_1 \\mid\nx_t, t)\\) yields the exact flow matching velocity under logit interpolation. To\naddress the suboptimal performance of basic inference, we propose a novel\nempirical sampling scheme that iteratively samples from the conditional\ndistribution and introduces additional noise, significantly improving results\ndespite lacking full theoretical underpinnings. Furthermore, we propose a\nhybrid inference method that combines the basic approach with the sampling\nscheme. This method demonstrates superior performance on both conditional and\nunconditional text generation experiments compared to previous SOTA method for\ndiscrete flow matching.\n","authors":["Egor Sevriugov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2411.16821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05722v2","updated":"2025-02-12T06:48:01Z","published":"2025-02-08T23:35:37Z","title":"Explainable and Class-Revealing Signal Feature Extraction via Scattering\n  Transform and Constrained Zeroth-Order Optimization","summary":"  We propose a new method to extract discriminant and explainable features from\na particular machine learning model, i.e., a combination of the scattering\ntransform and the multiclass logistic regression. Although this model is\nwell-known for its ability to learn various signal classes with high\nclassification rate, it remains elusive to understand why it can generate such\nsuccessful classification, mainly due to the nonlinearity of the scattering\ntransform. In order to uncover the meaning of the scattering transform\ncoefficients selected by the multiclass logistic regression (with the Lasso\npenalty), we adopt zeroth-order optimization algorithms to search an input\npattern that maximizes the class probability of a class of interest given the\nlearned model. In order to do so, it turns out that imposing sparsity and\nsmoothness of input patterns is important. We demonstrate the effectiveness of\nour proposed method using a couple of synthetic time-series classification\nproblems.\n","authors":["Naoki Saito","David Weber"],"pdf_url":"https://arxiv.org/pdf/2502.05722v2.pdf","comment":"5 pages; 6 figures; submitted to 2025 IEEE Statistical Signal\n  Processing Workshop"},{"id":"http://arxiv.org/abs/2405.18253v2","updated":"2025-02-12T06:47:33Z","published":"2024-05-28T15:04:17Z","title":"Proper Dataset Valuation by Pointwise Mutual Information","summary":"  Data plays a central role in the development of modern artificial\nintelligence, with high-quality data emerging as a key driver of model\nperformance. This has prompted the development of various data curation methods\nin recent years. However, measuring the effectiveness of these data curation\ntechniques remains a major challenge. Traditional evaluation methods, which\nassess a trained model's performance on specific benchmarks, risk promoting\npractices that merely make the data more similar to the test data. This issue\nexemplifies Goodhart's law: when a measure becomes a target, it ceases to be a\ngood measure. To address this, we propose an information-theoretic framework\nfor evaluating data curation methods, where dataset quality is measured by its\ninformativeness about the true model parameters using the Blackwell ordering.\nWe compare informativeness by the Shannon mutual information of the evaluated\ndata and the test data, and we propose a novel method for estimating the mutual\ninformation of datasets by training Bayesian models on embedded data and\ncomputing the mutual information from the model's parameter posteriors.\nExperiments on real-world data demonstrate that our mutual information-based\nevaluation assigns appropriately lower scores to data curation strategies that\nreduce dataset informativeness, while traditional test score-based evaluation\nmethods may favor data curation strategies that overfit to the test set but\ncompromise the training data's informativeness.\n","authors":["Shuran Zheng","Xuan Qi","Rui Ray Chen","Yongchan Kwon","James Zou"],"pdf_url":"https://arxiv.org/pdf/2405.18253v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08155v1","updated":"2025-02-12T06:47:25Z","published":"2025-02-12T06:47:25Z","title":"DGSense: A Domain Generalization Framework for Wireless Sensing","summary":"  Wireless sensing is of great benefits to our daily lives. However, wireless\nsignals are sensitive to the surroundings. Various factors, e.g. environments,\nlocations, and individuals, may induce extra impact on wireless propagation.\nSuch a change can be regarded as a domain, in which the data distribution\nshifts. A vast majority of the sensing schemes are learning-based. They are\ndependent on the training domains, resulting in performance degradation in\nunseen domains. Researchers have proposed various solutions to address this\nissue. But these solutions leverage either semi-supervised or unsupervised\ndomain adaptation techniques. They still require some data in the target\ndomains and do not perform well in unseen domains. In this paper, we propose a\ndomain generalization framework DGSense, to eliminate the domain dependence\nproblem in wireless sensing. The framework is a general solution working across\ndiverse sensing tasks and wireless technologies. Once the sensing model is\nbuilt, it can generalize to unseen domains without any data from the target\ndomain. To achieve the goal, we first increase the diversity of the training\nset by a virtual data generator, and then extract the domain independent\nfeatures via episodic training between the main feature extractor and the\ndomain feature extractors. The feature extractors employ a pre-trained Residual\nNetwork (ResNet) with an attention mechanism for spatial features, and a 1D\nConvolutional Neural Network (1DCNN) for temporal features. To demonstrate the\neffectiveness and generality of DGSense, we evaluated on WiFi gesture\nrecognition, Millimeter Wave (mmWave) activity recognition, and acoustic fall\ndetection. All the systems exhibited high generalization capability to unseen\ndomains, including new users, locations, and environments, free of new data and\nretraining.\n","authors":["Rui Zhou","Yu Cheng","Songlin Li","Hongwang Zhang","Chenxu Liu"],"pdf_url":"https://arxiv.org/pdf/2502.08155v1.pdf","comment":"15 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.04328v2","updated":"2025-02-12T18:40:46Z","published":"2025-02-06T18:59:55Z","title":"Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment","summary":"  Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.\n","authors":["Zuyan Liu","Yuhao Dong","Jiahui Wang","Ziwei Liu","Winston Hu","Jiwen Lu","Yongming Rao"],"pdf_url":"https://arxiv.org/pdf/2502.04328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19702v2","updated":"2025-02-12T16:47:30Z","published":"2024-10-25T17:19:55Z","title":"TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.\n","authors":["Xiangyu Zeng","Kunchang Li","Chenting Wang","Xinhao Li","Tianxiang Jiang","Ziang Yan","Songze Li","Yansong Shi","Zhengrong Yue","Yi Wang","Yali Wang","Yu Qiao","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19702v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.08556v1","updated":"2025-02-12T16:38:40Z","published":"2025-02-12T16:38:40Z","title":"Human-Centric Foundation Models: Perception, Generation and Agentic\n  Modeling","summary":"  Human understanding and generation are critical for modeling digital humans\nand humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)\ninspired by the success of generalist models, such as large language and vision\nmodels, have emerged to unify diverse human-centric tasks into a single\nframework, surpassing traditional task-specific approaches. In this survey, we\npresent a comprehensive overview of HcFMs by proposing a taxonomy that\ncategorizes current approaches into four groups: (1) Human-centric Perception\nFoundation Models that capture fine-grained features for multi-modal 2D and 3D\nunderstanding. (2) Human-centric AIGC Foundation Models that generate\nhigh-fidelity, diverse human-related content. (3) Unified Perception and\nGeneration Models that integrate these capabilities to enhance both human\nunderstanding and synthesis. (4) Human-centric Agentic Foundation Models that\nextend beyond perception and generation to learn human-like intelligence and\ninteractive behaviors for humanoid embodied tasks. We review state-of-the-art\ntechniques, discuss emerging challenges and future research directions. This\nsurvey aims to serve as a roadmap for researchers and practitioners working\ntowards more robust, versatile, and intelligent digital human and embodiments\nmodeling.\n","authors":["Shixiang Tang","Yizhou Wang","Lu Chen","Yuan Wang","Sida Peng","Dan Xu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2502.08556v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.08513v1","updated":"2025-02-12T15:46:47Z","published":"2025-02-12T15:46:47Z","title":"\"You'll Be Alice Adventuring in Wonderland!\" Processes, Challenges, and\n  Opportunities of Creating Animated Virtual Reality Stories","summary":"  Animated virtual reality (VR) stories, combining the presence of VR and the\nartistry of computer animation, offer a compelling way to deliver messages and\nevoke emotions. Motivated by the growing demand for immersive narrative\nexperiences, more creators are creating animated VR stories. However, a\nholistic understanding of their creation processes and challenges involved in\ncrafting these stories is still limited. Based on semi-structured interviews\nwith 21 animated VR story creators, we identify ten common stages in their\nend-to-end creation processes, ranging from idea generation to evaluation,\nwhich form diverse workflows that are story-driven or visual-driven.\nAdditionally, we highlight nine unique issues that arise during the creation\nprocess, such as a lack of reference material for multi-element plots, the\nabsence of specific functionalities for story integration, and inadequate\nsupport for audience evaluation. We compare the creation of animated VR stories\nto general XR applications and distill several future research opportunities.\n","authors":["Lin-Ping Yuan","Feilin Han","Liwenhan Xie","Junjie Zhang","Jian Zhao","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2502.08513v1.pdf","comment":"Conditionally accepted to the ACM Conference on Human Factors in\n  Computing Systems (CHI'25)"},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2407.14093v3","updated":"2025-02-12T08:49:34Z","published":"2024-07-19T07:57:48Z","title":"Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large\n  Language Models","summary":"  Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster.\n","authors":["Qiong Wu","Zhaoxi Ke","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.14093v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07531v2","updated":"2025-02-12T07:35:56Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07328v2","updated":"2025-02-12T04:00:14Z","published":"2025-02-11T07:46:29Z","title":"Music for All: Exploring Multicultural Representations in Music\n  Generation Models","summary":"  The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.\n","authors":["Atharva Mehta","Shivam Chauhan","Amirbek Djanibekov","Atharva Kulkarni","Gus Xia","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07328v2.pdf","comment":"17 pages, 5 figures, accepted to NAACL'25"},{"id":"http://arxiv.org/abs/2410.21144v4","updated":"2025-02-12T19:20:49Z","published":"2024-10-28T15:44:35Z","title":"Enhancing Learned Image Compression via Cross Window-based Attention","summary":"  In recent years, learned image compression methods have demonstrated superior\nrate-distortion performance compared to traditional image compression methods.\nRecent methods utilize convolutional neural networks (CNN), variational\nautoencoders (VAE), invertible neural networks (INN), and transformers. Despite\ntheir significant contributions, a main drawback of these models is their poor\nperformance in capturing local redundancy. Therefore, to leverage global\nfeatures along with local redundancy, we propose a CNN-based solution\nintegrated with a feature encoding module. The feature encoding module encodes\nimportant features before feeding them to the CNN and then utilizes cross-scale\nwindow-based attention, which further captures local redundancy. Cross-scale\nwindow-based attention is inspired by the attention mechanism in transformers\nand effectively enlarges the receptive field. Both the feature encoding module\nand the cross-scale window-based attention module in our architecture are\nflexible and can be incorporated into any other network architecture. We\nevaluate our method on the Kodak and CLIC datasets and demonstrate that our\napproach is effective and on par with state-of-the-art methods. Our code is\npublicly available at https://github.com/prmudgal/CWAM_IC_ISVC. .\n","authors":["Priyanka Mudgal","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21144v4.pdf","comment":"Paper accepted and presented in ISVC'24. Copyrights stay with ISVC\n  Our code is available at: https://github.com/prmudgal/CWAM_IC_ISVC"},{"id":"http://arxiv.org/abs/2502.08674v1","updated":"2025-02-12T03:32:28Z","published":"2025-02-12T03:32:28Z","title":"COutfitGAN: Learning to Synthesize Compatible Outfits Supervised by\n  Silhouette Masks and Fashion Styles","summary":"  How to recommend outfits has gained considerable attention in both academia\nand industry in recent years. Many studies have been carried out regarding\nfashion compatibility learning, to determine whether the fashion items in an\noutfit are compatible or not. These methods mainly focus on evaluating the\ncompatibility of existing outfits and rarely consider applying such knowledge\nto 'design' new fashion items. We propose the new task of generating\ncomplementary and compatible fashion items based on an arbitrary number of\ngiven fashion items. In particular, given some fashion items that can make up\nan outfit, the aim of this paper is to synthesize photo-realistic images of\nother, complementary, fashion items that are compatible with the given ones. To\nachieve this, we propose an outfit generation framework, referred to as\nCOutfitGAN, which includes a pyramid style extractor, an outfit generator, a\nUNet-based real/fake discriminator, and a collocation discriminator. To train\nand evaluate this framework, we collected a large-scale fashion outfit dataset\nwith over 200K outfits and 800K fashion items from the Internet. Extensive\nexperiments show that COutfitGAN outperforms other baselines in terms of\nsimilarity, authenticity, and compatibility measurements.\n","authors":["Dongliang Zhou","Haijun Zhang","Qun Li","Jianghong Ma","Xiaofei Xu"],"pdf_url":"https://arxiv.org/pdf/2502.08674v1.pdf","comment":"This paper was accepted by IEEE TMM"}]},"2025-02-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.09622v1","updated":"2025-02-13T18:59:47Z","published":"2025-02-13T18:59:47Z","title":"Theoretical Benefit and Limitation of Diffusion Language Model","summary":"  Diffusion language models have emerged as a promising approach for text\ngeneration. One would naturally expect this method to be an efficient\nreplacement for autoregressive models since multiple tokens can be sampled in\nparallel during each diffusion step. However, its efficiency-accuracy trade-off\nis not yet well understood. In this paper, we present a rigorous theoretical\nanalysis of a widely used type of diffusion language model, the Masked\nDiffusion Model (MDM), and find that its effectiveness heavily depends on the\ntarget evaluation metric. Under mild conditions, we prove that when using\nperplexity as the metric, MDMs can achieve near-optimal perplexity in sampling\nsteps regardless of sequence length, demonstrating that efficiency can be\nachieved without sacrificing performance. However, when using the sequence\nerror rate--which is important for understanding the \"correctness\" of a\nsequence, such as a reasoning chain--we show that the required sampling steps\nmust scale linearly with sequence length to obtain \"correct\" sequences, thereby\neliminating MDM's efficiency advantage over autoregressive models. Our analysis\nestablishes the first theoretical foundation for understanding the benefits and\nlimitations of MDMs. All theoretical findings are supported by empirical\nstudies.\n","authors":["Guhao Feng","Yihan Geng","Jian Guan","Wei Wu","Liwei Wang","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.09622v1.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.09621v1","updated":"2025-02-13T18:59:46Z","published":"2025-02-13T18:59:46Z","title":"MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency","summary":"  Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/\n","authors":["Dongzhi Jiang","Renrui Zhang","Ziyu Guo","Yanwei Li","Yu Qi","Xinyan Chen","Liuhui Wang","Jianhan Jin","Claire Guo","Shen Yan","Bo Zhang","Chaoyou Fu","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2502.09621v1.pdf","comment":"Project Page: https://mmecot.github.io/"},{"id":"http://arxiv.org/abs/2502.09620v1","updated":"2025-02-13T18:59:45Z","published":"2025-02-13T18:59:45Z","title":"Exploring the Potential of Encoder-free Architectures in 3D LMMs","summary":"  Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL\n","authors":["Yiwen Tang","Zoey Guo","Zhuhao Wang","Ray Zhang","Qizhi Chen","Junli Liu","Delin Qu","Zhigang Wang","Dong Wang","Xuelong Li","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.09620v1.pdf","comment":"The code is released at https://github.com/Ivan-Tang-3D/ENEL"},{"id":"http://arxiv.org/abs/2403.06925v2","updated":"2025-02-13T18:58:58Z","published":"2024-03-11T17:12:09Z","title":"Transformers Learn Low Sensitivity Functions: Investigations and\n  Implications","summary":"  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of their inductive biases and how those biases\ndiffer from other neural network architectures remains elusive. In this work,\nwe identify the sensitivity of the model to token-wise random perturbations in\nthe input as a unified metric which explains the inductive bias of transformers\nacross different data modalities and distinguishes them from other\narchitectures. We show that transformers have lower sensitivity than MLPs,\nCNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show\nthat this low-sensitivity bias has important implications: i) lower sensitivity\ncorrelates with improved robustness; it can also be used as an efficient\nintervention to further improve the robustness of transformers; ii) it\ncorresponds to flatter minima in the loss landscape; and iii) it can serve as a\nprogress measure for grokking. We support these findings with theoretical\nresults showing (weak) spectral bias of transformers in the NTK regime, and\nimproved robustness due to the lower sensitivity. The code is available at\nhttps://github.com/estija/sensitivity.\n","authors":["Bhavya Vasudeva","Deqing Fu","Tianyi Zhou","Elliott Kau","Youqi Huang","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2403.06925v2.pdf","comment":"ICLR 2025. 24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09606v1","updated":"2025-02-13T18:55:56Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09604v1","updated":"2025-02-13T18:55:13Z","published":"2025-02-13T18:55:13Z","title":"SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models","summary":"  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.\n","authors":["Yung-Sung Chuang","Benjamin Cohen-Wang","Shannon Zejiang Shen","Zhaofeng Wu","Hu Xu","Xi Victoria Lin","James Glass","Shang-Wen Li","Wen-tau Yih"],"pdf_url":"https://arxiv.org/pdf/2502.09604v1.pdf","comment":"Implementation available at https://github.com/voidism/SelfCite"},{"id":"http://arxiv.org/abs/2502.09601v1","updated":"2025-02-13T18:52:36Z","published":"2025-02-13T18:52:36Z","title":"CoT-Valve: Length-Compressible Chain-of-Thought Tuning","summary":"  Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer.\n","authors":["Xinyin Ma","Guangnian Wan","Runpeng Yu","Gongfan Fang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09601v1.pdf","comment":"Work in progress. Code will be released at\n  https://github.com/horseee/CoT-Valve"},{"id":"http://arxiv.org/abs/2502.09597v1","updated":"2025-02-13T18:52:03Z","published":"2025-02-13T18:52:03Z","title":"Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs","summary":"  Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/.\n","authors":["Siyan Zhao","Mingyi Hong","Yang Liu","Devamanyu Hazarika","Kaixiang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.09597v1.pdf","comment":"Accepted at ICLR 2025 as oral presentation. Code and data at:\n  https://prefeval.github.io/"},{"id":"http://arxiv.org/abs/2502.09589v1","updated":"2025-02-13T18:46:44Z","published":"2025-02-13T18:46:44Z","title":"Logical forms complement probability in understanding language model\n  (and human) performance","summary":"  With the increasing interest in using large language models (LLMs) for\nplanning in natural language, understanding their behaviors becomes an\nimportant research question. This work conducts a systematic investigation of\nLLMs' ability to perform logical reasoning in natural language. We introduce a\ncontrolled dataset of hypothetical and disjunctive syllogisms in propositional\nand modal logic and use it as the testbed for understanding LLM performance.\nOur results lead to novel insights in predicting LLM behaviors: in addition to\nthe probability of input (Gonen et al., 2023; McCoy et al., 2024), logical\nforms should be considered as orthogonal factors. In addition, we show\nsimilarities and differences between the logical reasoning performances of\nhumans and LLMs by comparing LLM and human behavioral results.\n","authors":["Yixuan Wang","Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09589v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09573v1","updated":"2025-02-13T18:31:17Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09567v1","updated":"2025-02-13T18:22:31Z","published":"2025-02-13T18:22:31Z","title":"MorphNLI: A Stepwise Approach to Natural Language Inference Using Text\n  Morphing","summary":"  We introduce MorphNLI, a modular step-by-step approach to natural language\ninference (NLI). When classifying the premise-hypothesis pairs into\n{entailment, contradiction, neutral}, we use a language model to generate the\nnecessary edits to incrementally transform (i.e., morph) the premise into the\nhypothesis. Then, using an off-the-shelf NLI model we track how the entailment\nprogresses with these atomic changes, aggregating these intermediate labels\ninto a final output. We demonstrate the advantages of our proposed method\nparticularly in realistic cross-domain settings, where our method always\noutperforms strong baselines with improvements up to 12.6% (relative). Further,\nour proposed approach is explainable as the atomic edits can be used to\nunderstand the overall NLI label.\n","authors":["Vlad Andrei Negru","Robert Vacareanu","Camelia Lemnaru","Mihai Surdeanu","Rodica Potolea"],"pdf_url":"https://arxiv.org/pdf/2502.09567v1.pdf","comment":"16 pages, 11 figures, 8 tables. Accepted for NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.09566v1","updated":"2025-02-13T18:21:15Z","published":"2025-02-13T18:21:15Z","title":"Zero-shot generation of synthetic neurosurgical data with large language\n  models","summary":"  Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.\n","authors":["Austin A. Barr","Eddie Guo","Emre Sezgin"],"pdf_url":"https://arxiv.org/pdf/2502.09566v1.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.09560v1","updated":"2025-02-13T18:11:34Z","published":"2025-02-13T18:11:34Z","title":"EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents","summary":"  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 13 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.\n","authors":["Rui Yang","Hanyang Chen","Junyu Zhang","Mark Zhao","Cheng Qian","Kangrui Wang","Qineng Wang","Teja Venkat Koripella","Marziyeh Movahedi","Manling Li","Heng Ji","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09560v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2406.05925v2","updated":"2025-02-13T18:02:34Z","published":"2024-06-09T21:58:32Z","title":"Hello Again! LLM-powered Personalized Agent for Long-term Dialogue","summary":"  Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.\n","authors":["Hao Li","Chenghao Yang","An Zhang","Yang Deng","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2406.05925v2.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2406.06773v2","updated":"2025-02-13T17:50:39Z","published":"2024-06-10T20:19:55Z","title":"Evaluating Zero-Shot Long-Context LLM Compression","summary":"  This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K.\n","authors":["Chenyu Wang","Yihan Wang","Kai Li"],"pdf_url":"https://arxiv.org/pdf/2406.06773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09532v1","updated":"2025-02-13T17:49:30Z","published":"2025-02-13T17:49:30Z","title":"Mind the Gap! Choice Independence in Using Multilingual LLMs for\n  Persuasive Co-Writing Tasks in Different Languages","summary":"  Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks.\n","authors":["Shreyan Biswas","Alexander Erlei","Ujwal Gadiraju"],"pdf_url":"https://arxiv.org/pdf/2502.09532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08489v2","updated":"2025-02-13T17:33:24Z","published":"2025-02-12T15:26:08Z","title":"Salamandra Technical Report","summary":"  This work introduces Salamandra, a suite of open-source decoder-only large\nlanguage models available in three different sizes: 2, 7, and 40 billion\nparameters. The models were trained from scratch on highly multilingual data\nthat comprises text in 35 European languages and code. Our carefully curated\ncorpus is made exclusively from open-access data compiled from a wide variety\nof sources. Along with the base models, supplementary checkpoints that were\nfine-tuned on public-domain instruction data are also released for chat\napplications. Additionally, we also share our preliminary experiments on\nmultimodality, which serve as proof-of-concept to showcase potential\napplications for the Salamandra family. Our extensive evaluations on\nmultilingual benchmarks reveal that Salamandra has strong capabilities,\nachieving competitive performance when compared to similarly sized open-source\nmodels. We provide comprehensive evaluation results both on standard downstream\ntasks as well as key aspects related to bias and safety.With this technical\nreport, we intend to promote open science by sharing all the details behind our\ndesign choices, data curation strategy and evaluation methodology. In addition\nto that, we deviate from the usual practice by making our training and\nevaluation scripts publicly accessible. We release all models under a\npermissive Apache 2.0 license in order to foster future research and facilitate\ncommercial use, thereby contributing to the open-source ecosystem of large\nlanguage models.\n","authors":["Aitor Gonzalez-Agirre","Marc Pàmies","Joan Llop","Irene Baucells","Severino Da Dalt","Daniel Tamayo","José Javier Saiz","Ferran Espuña","Jaume Prats","Javier Aula-Blasco","Mario Mina","Iñigo Pikabea","Adrián Rubio","Alexander Shvets","Anna Sallés","Iñaki Lacunza","Jorge Palomar","Júlia Falcão","Lucía Tormo","Luis Vasquez-Reina","Montserrat Marimon","Oriol Pareras","Valle Ruiz-Fernández","Marta Villegas"],"pdf_url":"https://arxiv.org/pdf/2502.08489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05331v2","updated":"2025-02-13T17:27:15Z","published":"2025-02-07T21:13:27Z","title":"Fine-Tuned LLMs are \"Time Capsules\" for Tracking Societal Bias Through\n  Books","summary":"  Books, while often rich in cultural insights, can also mirror societal biases\nof their eras - biases that Large Language Models (LLMs) may learn and\nperpetuate during training. We introduce a novel method to trace and quantify\nthese biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising\n593 fictional books across seven decades (1950-2019), to track bias evolution.\nBy fine-tuning LLMs on books from each decade and using targeted prompts, we\nexamine shifts in biases related to gender, sexual orientation, race, and\nreligion. Our findings indicate that LLMs trained on decade-specific books\nmanifest biases reflective of their times, with both gradual trends and notable\nshifts. For example, model responses showed a progressive increase in the\nportrayal of women in leadership roles (from 8% to 22%) from the 1950s to\n2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly\naligning with third-wave feminism. Same-sex relationship references increased\nmarkedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+\nvisibility. Concerningly, negative portrayals of Islam rose sharply in the\n2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we\ndemonstrate that these biases stem mainly from the books' content and not the\nmodels' architecture or initial training. Our study offers a new perspective on\nsocietal bias trends by bridging AI, literary studies, and social science\nresearch.\n","authors":["Sangmitra Madhusudan","Robert Morabito","Skye Reid","Nikta Gohari Sadr","Ali Emami"],"pdf_url":"https://arxiv.org/pdf/2502.05331v2.pdf","comment":"9 pages (excluding references), accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2408.14792v2","updated":"2025-02-13T17:22:36Z","published":"2024-08-27T05:56:04Z","title":"Measuring Human Contribution in AI-Assisted Content Generation","summary":"  With the growing prevalence of generative artificial intelligence (AI), an\nincreasing amount of content is no longer exclusively generated by humans but\nby generative AI models with human guidance. This shift presents notable\nchallenges for the delineation of originality due to the varying degrees of\nhuman contribution in AI-assisted works. This study raises the research\nquestion of measuring human contribution in AI-assisted content generation and\nintroduces a framework to address this question that is grounded in information\ntheory. By calculating mutual information between human input and AI-assisted\noutput relative to self-information of AI-assisted output, we quantify the\nproportional information contribution of humans in content generation. Our\nexperimental results demonstrate that the proposed measure effectively\ndiscriminates between varying degrees of human contribution across multiple\ncreative domains. We hope that this work lays a foundation for measuring human\ncontributions in AI-assisted content generation in the era of generative AI.\n","authors":["Yueqi Xie","Tao Qi","Jingwei Yi","Xiyuan Yang","Ryan Whalen","Junming Huang","Qian Ding","Yu Xie","Xing Xie","Fangzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2408.14792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06759v2","updated":"2025-02-13T17:12:34Z","published":"2025-02-10T18:38:57Z","title":"Rationalization Models for Text-to-SQL","summary":"  We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.\n","authors":["Gaetano Rossiello","Nhan Pham","Michael Glass","Junkyu Lee","Dharmashankar Subramanian"],"pdf_url":"https://arxiv.org/pdf/2502.06759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08168v2","updated":"2025-02-13T17:11:41Z","published":"2025-02-12T07:19:36Z","title":"SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image\n  Interpretation","summary":"  As a powerful all-weather Earth observation tool, synthetic aperture radar\n(SAR) remote sensing enables critical military reconnaissance, maritime\nsurveillance, and infrastructure monitoring. Although Vision language models\n(VLMs) have made remarkable progress in natural language processing and image\nunderstanding, their applications remain limited in professional domains due to\ninsufficient domain expertise. This paper innovatively proposes the first\nlarge-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which\ncontains approximately 2 million high-quality image-text pairs, encompasses\ndiverse scenarios with detailed target annotations. This dataset not only\nsupports several key tasks such as visual understanding and object detection\ntasks, but also has unique innovative aspects: this study develop a\nvisual-language dataset and benchmark for the SAR domain, enabling and\nevaluating VLMs' capabilities in SAR image interpretation, which provides a\nparadigmatic framework for constructing multimodal datasets across various\nremote sensing vertical domains. Through experiments on 16 mainstream VLMs, the\neffectiveness of the dataset has been fully verified. The project will be\nreleased at https://github.com/JimmyMa99/SARChat.\n","authors":["Zhiming Ma","Xiayang Xiao","Sihao Dong","Peidong Wang","HaiPeng Wang","Qingyun Pan"],"pdf_url":"https://arxiv.org/pdf/2502.08168v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09497v1","updated":"2025-02-13T17:09:52Z","published":"2025-02-13T17:09:52Z","title":"Improve LLM-based Automatic Essay Scoring with Linguistic Features","summary":"  Automatic Essay Scoring (AES) assigns scores to student essays, reducing the\ngrading workload for instructors. Developing a scoring system capable of\nhandling essays across diverse prompts is challenging due to the flexibility\nand diverse nature of the writing task. Existing methods typically fall into\ntwo categories: supervised feature-based approaches and large language model\n(LLM)-based methods. Supervised feature-based approaches often achieve higher\nperformance but require resource-intensive training. In contrast, LLM-based\nmethods are computationally efficient during inference but tend to suffer from\nlower performance. This paper combines these approaches by incorporating\nlinguistic features into LLM-based scoring. Experimental results show that this\nhybrid method outperforms baseline models for both in-domain and out-of-domain\nwriting prompts.\n","authors":["Zhaoyi Joey Hou","Alejandro Ciuba","Xiang Lorraine Li"],"pdf_url":"https://arxiv.org/pdf/2502.09497v1.pdf","comment":"To be published in the workshop Innovation and Responsibility in\n  AI-Supported Education (iRaise) at the 2025 Conference on Artificial\n  Intelligence (AAAI)"},{"id":"http://arxiv.org/abs/2312.00326v9","updated":"2025-02-13T17:06:52Z","published":"2023-12-01T03:44:54Z","title":"Agent-OM: Leveraging LLM Agents for Ontology Matching","summary":"  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.\n","authors":["Zhangcheng Qiang","Weiqing Wang","Kerry Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.00326v9.pdf","comment":"19 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09487v1","updated":"2025-02-13T16:52:06Z","published":"2025-02-13T16:52:06Z","title":"Objective quantification of mood states using large language models","summary":"  Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.\n","authors":["Jakub Onysk","Quentin Huys"],"pdf_url":"https://arxiv.org/pdf/2502.09487v1.pdf","comment":"main text - 9 pages, 5 figures;"},{"id":"http://arxiv.org/abs/2502.09457v1","updated":"2025-02-13T16:25:16Z","published":"2025-02-13T16:25:16Z","title":"The Multilingual Mind : A Survey of Multilingual Reasoning in Language\n  Models","summary":"  While reasoning and multilingual capabilities in Language Models (LMs) have\nachieved remarkable progress in recent years, their integration into a unified\nparadigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning\nrequires language models to handle logical reasoning across languages while\naddressing misalignment, biases, and challenges in low-resource settings. This\nsurvey provides the first in-depth review of multilingual reasoning in LMs. In\nthis survey, we provide a systematic overview of existing methods that leverage\nLMs for multilingual reasoning, specifically outlining the challenges,\nmotivations, and foundational aspects of applying language models to reason\nacross diverse languages. We provide an overview of the standard data resources\nused for training multilingual reasoning in LMs and the evaluation benchmarks\nemployed to assess their multilingual capabilities. Next, we analyze various\nstate-of-the-art methods and their performance on these benchmarks. Finally, we\nexplore future research opportunities to improve multilingual reasoning in LMs,\nfocusing on enhancing their ability to handle diverse languages and complex\nreasoning tasks.\n","authors":["Akash Ghosh","Debayan Datta","Sriparna Saha","Chirag Agarwal"],"pdf_url":"https://arxiv.org/pdf/2502.09457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09447v1","updated":"2025-02-13T16:16:54Z","published":"2025-02-13T16:16:54Z","title":"Pixel-Level Reasoning Segmentation via Multi-turn Conversations","summary":"  Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST.\n","authors":["Dexian Cai","Xiaocui Yang","Yongkang Liu","Daling Wang","Shi Feng","Yifei Zhang","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2502.09447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09419v1","updated":"2025-02-13T15:42:44Z","published":"2025-02-13T15:42:44Z","title":"On multi-token prediction for efficient LLM inference","summary":"  We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction.\n","authors":["Somesh Mehra","Javier Alonso Garcia","Lukas Mauch"],"pdf_url":"https://arxiv.org/pdf/2502.09419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09416v1","updated":"2025-02-13T15:39:07Z","published":"2025-02-13T15:39:07Z","title":"Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use\n  a Different Evaluation Process than Human?","summary":"  One of the goals of automatic evaluation metrics in grammatical error\ncorrection (GEC) is to rank GEC systems such that it matches human preferences.\nHowever, current automatic evaluations are based on procedures that diverge\nfrom human evaluation. Specifically, human evaluation derives rankings by\naggregating sentence-level relative evaluation results, e.g., pairwise\ncomparisons, using a rating algorithm, whereas automatic evaluation averages\nsentence-level absolute scores to obtain corpus-level scores, which are then\nsorted to determine rankings. In this study, we propose an aggregation method\nfor existing automatic evaluation metrics which aligns with human evaluation\nmethods to bridge this gap. We conducted experiments using various metrics,\nincluding edit-based metrics, $n$-gram based metrics, and sentence-level\nmetrics, and show that resolving the gap improves results for the most of\nmetrics on the SEEDA benchmark. We also found that even BERT-based metrics\nsometimes outperform the metrics of GPT-4. We publish our unified\nimplementation of the metrics and meta-evaluations.\n","authors":["Takumi Goto","Yusuke Sakai","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2502.09416v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.08441v2","updated":"2025-02-13T15:36:14Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v2.pdf","comment":"17 pages, 8 figures; figures corrected"},{"id":"http://arxiv.org/abs/2310.19347v4","updated":"2025-02-13T15:25:02Z","published":"2023-10-30T08:40:16Z","title":"Improving Factual Consistency of News Summarization by Contrastive\n  Preference Optimization","summary":"  Despite the recent progress in news summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose\nContrastive Preference Optimization (CPO) to disentangle the LLMs' propensities\nto generate faithful and fake content. Furthermore, we adopt a probing-based\nspecific training method to improve their capacity of distinguishing two types\nof propensities. In this way, LLMs can execute the instructions more accurately\nand have enhanced perception of hallucinations. Experimental results show that\nCPO significantly improves the reliability of summarization based on LLMs.\n","authors":["Huawen Feng","Yan Fan","Xiong Liu","Ting-En Lin","Zekun Yao","Yuchuan Wu","Fei Huang","Yongbin Li","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2310.19347v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02280v2","updated":"2025-02-13T15:21:43Z","published":"2024-11-04T17:09:10Z","title":"The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units","summary":"  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n","authors":["Badr AlKhamissi","Greta Tuckute","Antoine Bosselut","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2411.02280v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2412.17395v2","updated":"2025-02-13T15:11:24Z","published":"2024-12-23T08:47:42Z","title":"WarriorCoder: Learning from Expert Battles to Augment Code Large\n  Language Models","summary":"  Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to collect complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\na limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nrestricts the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder, a novel paradigm learns from\nexpert battles to address these limitations. Specifically, we create an arena\nwhere leading expert code LLMs challenge each other, with evaluations conducted\nby impartial judges. This competitive framework generates novel training data\nfrom scratch, leveraging the strengths of all participants. Experimental\nresults show that WarriorCoder achieves state-of-the-art performance compared\nto previous models of the same size, even without relying on proprietary LLMs.\n","authors":["Huawen Feng","Pu Zhao","Qingfeng Sun","Can Xu","Fangkai Yang","Lu Wang","Qianli Ma","Qingwei Lin","Saravan Rajmohan","Dongmei Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.17395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09390v1","updated":"2025-02-13T15:07:20Z","published":"2025-02-13T15:07:20Z","title":"SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models","summary":"  In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.\n","authors":["Daniel Fleischer","Moshe Berchansky","Gad Markovits","Moshe Wasserblat"],"pdf_url":"https://arxiv.org/pdf/2502.09390v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.09387v1","updated":"2025-02-13T15:04:53Z","published":"2025-02-13T15:04:53Z","title":"Truth Knows No Language: Evaluating Truthfulness Beyond English","summary":"  We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.\n","authors":["Blanca Calvo Figueras","Eneko Sagarzazu","Julen Etxaniz","Jeremy Barnes","Pablo Gamallo","Iria De Dios Flores","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2502.09387v1.pdf","comment":"13 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2411.15927v2","updated":"2025-02-13T14:55:26Z","published":"2024-11-24T17:32:20Z","title":"Generative Prompt Internalization","summary":"  Prompts used in recent large language model based applications are often\nfixed and lengthy, leading to significant computational overhead. To address\nthis challenge, we propose Generative Prompt Internalization (GenPI), a\nlightweight method that employs a joint training approach. GenPI not only\nreplicates the behavior of models with prompt inputs but also generates the\ncontent of the prompt along with reasons for why the model's behavior should\nchange accordingly. We demonstrate that our approach effectively internalizes\ncomplex prompts across various agent-based application scenarios. For effective\ntraining without interactions with the dedicated environments, we introduce a\ndata synthesis technique that autonomously collects conversational datasets by\nswapping the roles of the agent and environment. This method is especially\nuseful in scenarios where only a predefined prompt is available without a\ncorresponding training dataset. By internalizing complex prompts, Generative\nPrompt Internalization enables high performance and efficient inference without\nthe need for explicit prompts.\n","authors":["Haebin Shin","Lei Ji","Yeyun Gong","Sungdong Kim","Eunbi Choi","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2411.15927v2.pdf","comment":"NAACL 2025 (Main Conference)"},{"id":"http://arxiv.org/abs/2502.09369v1","updated":"2025-02-13T14:35:40Z","published":"2025-02-13T14:35:40Z","title":"Language Agents as Digital Representatives in Collective Decision-Making","summary":"  Consider the process of collective decision-making, in which a group of\nindividuals interactively select a preferred outcome from among a universe of\nalternatives. In this context, \"representation\" is the activity of making an\nindividual's preferences present in the process via participation by a proxy\nagent -- i.e. their \"representative\". To this end, learned models of human\nbehavior have the potential to fill this role, with practical implications for\nmulti-agent scenario studies and mechanism design. In this work, we investigate\nthe possibility of training \\textit{language agents} to behave in the capacity\nof representatives of human agents, appropriately expressing the preferences of\nthose individuals whom they stand for. First, we formalize the setting of\n\\textit{collective decision-making} -- as the episodic process of interaction\nbetween a group of agents and a decision mechanism. On this basis, we then\nformalize the problem of \\textit{digital representation} -- as the simulation\nof an agent's behavior to yield equivalent outcomes from the mechanism.\nFinally, we conduct an empirical case study in the setting of\n\\textit{consensus-finding} among diverse humans, and demonstrate the\nfeasibility of fine-tuning large language models to act as digital\nrepresentatives.\n","authors":["Daniel Jarrett","Miruna Pîslar","Michiel A. Bakker","Michael Henry Tessler","Raphael Köster","Jan Balaguer","Romuald Elie","Christopher Summerfield","Andrea Tacchetti"],"pdf_url":"https://arxiv.org/pdf/2502.09369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08514v2","updated":"2025-02-13T14:34:29Z","published":"2025-02-12T15:46:50Z","title":"Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation","summary":"  Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries.\n","authors":["Mahnaz Koupaee","Jake W. Vincent","Saab Mansour","Igor Shalyminov","Han He","Hwanjun Song","Raphael Shu","Jianfeng He","Yi Nian","Amy Wing-mei Wong","Kyu J. Han","Hang Su"],"pdf_url":"https://arxiv.org/pdf/2502.08514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10853v3","updated":"2025-02-13T14:13:41Z","published":"2024-07-15T16:04:44Z","title":"An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases","summary":"  Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments.\n","authors":["Dylan Bouchard"],"pdf_url":"https://arxiv.org/pdf/2407.10853v3.pdf","comment":"LangFair repository: https://github.com/cvs-health/langfair"},{"id":"http://arxiv.org/abs/2411.05031v2","updated":"2025-02-13T14:02:53Z","published":"2024-11-06T09:52:29Z","title":"On-Device Emoji Classifier Trained with GPT-based Data Augmentation for\n  a Mobile Keyboard","summary":"  Emojis improve communication quality among smart-phone users that use mobile\nkeyboards to exchange text. To predict emojis for users based on input text, we\nshould consider the on-device low memory and time constraints, ensure that the\non-device emoji classifier covers a wide range of emoji classes even though the\nemoji dataset is typically imbalanced, and adapt the emoji classifier output to\nuser favorites. This paper proposes an on-device emoji classifier based on\nMobileBert with reasonable memory and latency requirements for SwiftKey. To\naccount for the data imbalance, we utilize the widely used GPT to generate one\nor more tags for each emoji class. For each emoji and corresponding tags, we\nmerge the original set with GPT-generated sentences and label them with this\nemoji without human intervention to alleviate the data imbalance. At inference\ntime, we interpolate the emoji output with the user history for emojis for\nbetter emoji classifications. Results show that the proposed on-device emoji\nclassifier deployed for SwiftKey increases the accuracy performance of emoji\nprediction particularly on rare emojis and emoji engagement.\n","authors":["Hossam Amer","Joe Osborne","Michael Zaki","Mohamed Afify"],"pdf_url":"https://arxiv.org/pdf/2411.05031v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.09331v1","updated":"2025-02-13T13:49:30Z","published":"2025-02-13T13:49:30Z","title":"Beyond English: The Impact of Prompt Translation Strategies across\n  Languages and Tasks in Multilingual LLMs","summary":"  Despite advances in the multilingual capabilities of Large Language Models\n(LLMs) across diverse tasks, English remains the dominant language for LLM\nresearch and development. So, when working with a different language, this has\nled to the widespread practice of pre-translation, i.e., translating the task\nprompt into English before inference. Selective pre-translation, a more\nsurgical approach, focuses on translating specific prompt components. However,\nits current use is sporagic and lacks a systematic research foundation.\nConsequently, the optimal pre-translation strategy for various multilingual\nsettings and tasks remains unclear. In this work, we aim to uncover the optimal\nsetup for pre-translation by systematically assessing its use. Specifically, we\nview the prompt as a modular entity, composed of four functional parts:\ninstruction, context, examples, and output, either of which could be translated\nor not. We evaluate pre-translation strategies across 35 languages covering\nboth low and high-resource languages, on various tasks including Question\nAnswering (QA), Natural Language Inference (NLI), Named Entity Recognition\n(NER), and Abstractive Summarization. Our experiments show the impact of\nfactors as similarity to English, translation quality and the size of\npre-trained data, on the model performance with pre-translation. We suggest\npractical guidelines for choosing optimal strategies in various multilingual\nsettings.\n","authors":["Itai Mondshine","Tzuf Paz-Argaman","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2502.09331v1.pdf","comment":"Accepted for NAACL findings 2025"},{"id":"http://arxiv.org/abs/2502.09316v1","updated":"2025-02-13T13:30:54Z","published":"2025-02-13T13:30:54Z","title":"A Judge-free LLM Open-ended Generation Benchmark Based on the\n  Distributional Hypothesis","summary":"  Evaluating the open-ended text generation of large language models (LLMs) is\nchallenging because of the lack of a clear ground truth and the high cost of\nhuman or LLM-based assessments. We propose a novel benchmark that evaluates\nLLMs using n-gram statistics and rules, without relying on human judgement or\nLLM-as-a-judge approaches. Using 50 question and reference answer sets, we\nintroduce three new metrics based on n-grams and rules: Fluency, Truthfulness,\nand Helpfulness. Our benchmark strongly correlates with GPT-4o-based\nevaluations while requiring significantly fewer computational resources,\ndemonstrating its effectiveness as a scalable alternative for assessing LLMs'\nopen-ended generation capabilities.\n","authors":["Kentaro Imajo","Masanori Hirano","Shuji Suzuki","Hiroaki Mikami"],"pdf_url":"https://arxiv.org/pdf/2502.09316v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2502.05497v2","updated":"2025-02-13T13:22:40Z","published":"2025-02-08T09:04:16Z","title":"DeepThink: Aligning Language Models with Domain-Specific User Intents","summary":"  Supervised fine-tuning with synthesized instructions has been a common\npractice for adapting LLMs to domain-specific QA tasks. However, the\nsynthesized instructions deviate from real user questions and expected answers.\nThis study proposes a novel framework called DeepThink to generate high-quality\ninstructions. DeepThink first generates a few seed questions to mimic actual\nuser questions, simulates conversations to uncover the hidden user needs, and\nrefines the answer by conversational contexts and the retrieved documents for\nmore comprehensive answers. Experiments demonstrate that DeepThink achieves an\naverage performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based\nassistant on the real user test set in the advertising domain across dimensions\nsuch as relevance, completeness, clarity, accuracy, and actionability.\n","authors":["Yang Li","Mingxuan Luo","Yeyun Gong","Chen Lin","Jian Jiao","Yi Liu","Kaili Huang"],"pdf_url":"https://arxiv.org/pdf/2502.05497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09307v1","updated":"2025-02-13T13:19:33Z","published":"2025-02-13T13:19:33Z","title":"When the LM misunderstood the human chuckled: Analyzing garden path\n  effects in humans and language models","summary":"  Modern Large Language Models (LLMs) have shown human-like abilities in many\nlanguage tasks, sparking interest in comparing LLMs' and humans' language\nprocessing. In this paper, we conduct a detailed comparison of the two on a\nsentence comprehension task using garden-path constructions, which are\nnotoriously challenging for humans. Based on psycholinguistic research, we\nformulate hypotheses on why garden-path sentences are hard, and test these\nhypotheses on human participants and a large suite of LLMs using comprehension\nquestions. Our findings reveal that both LLMs and humans struggle with specific\nsyntactic complexities, with some models showing high correlation with human\ncomprehension. To complement our findings, we test LLM comprehension of\ngarden-path constructions with paraphrasing and text-to-image generation tasks,\nand find that the results mirror the sentence comprehension question results,\nfurther validating our findings on LLM understanding of these constructions.\n","authors":["Samuel Joseph Amouyal","Aya Meltzer-Asscher","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2502.09307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15330v2","updated":"2025-02-13T13:06:00Z","published":"2024-06-21T17:42:52Z","title":"Enhancing Large Language Model Performance with Gradient-Based Parameter\n  Selection","summary":"  Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.\n","authors":["Haoling Li","Xin Zhang","Xiao Liu","Yeyun Gong","Yifan Wang","Qi Chen","Peng Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.15330v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09284v1","updated":"2025-02-13T12:57:15Z","published":"2025-02-13T12:57:15Z","title":"SparQLe: Speech Queries to Text Translation Through LLMs","summary":"  With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that leverages self-supervised speech\nrepresentations in combination with instruction-tuned LLMs for speech-to-text\ntranslation. The proposed approach leverages a modality adapter to align\nextracted speech features with instruction-tuned LLMs using English-language\ndata. Our experiments demonstrate that this method effectively preserves the\nsemantic content of the input speech and serves as an effective bridge between\nself-supervised speech models and instruction-tuned LLMs, offering a promising\nsolution for various speech understanding applications.\n","authors":["Amirbek Djanibekov","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2502.09284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12851v3","updated":"2025-02-13T12:43:59Z","published":"2025-01-22T12:59:08Z","title":"ACEBench: Who Wins the Match Point in Tool Usage?","summary":"  Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes.\n","authors":["Chen Chen","Xinlong Hao","Weiwen Liu","Xu Huang","Xingshan Zeng","Shuai Yu","Dexun Li","Shuai Wang","Weinan Gan","Yuefeng Huang","Wulong Liu","Xinzhi Wang","Defu Lian","Baoqun Yin","Yasheng Wang","Wu Liu"],"pdf_url":"https://arxiv.org/pdf/2501.12851v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17301v2","updated":"2025-02-13T12:25:54Z","published":"2024-11-26T10:48:55Z","title":"ReFINE: A Reward-Based Framework for Interpretable and Nuanced\n  Evaluation of Radiology Report Generation","summary":"  Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ReFINE, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ReFINE. Our experiments demonstrate ReFINE's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems.\n","authors":["Yunyi Liu","Yingshu Li","Zhanyu Wang","Xinyu Liang","Lingqiao Liu","Lei Wang","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.17301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00008v5","updated":"2025-02-13T12:10:33Z","published":"2023-03-27T18:00:01Z","title":"On the Creativity of Large Language Models","summary":"  Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article, we first analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion on the dimensions of value,\nnovelty, and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press, and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered, the challenges arising from\nthem, and the potential associated risks, from both legal and ethical points of\nview.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2304.00008v5.pdf","comment":"Published in AI & SOCIETY at\n  https://link.springer.com/article/10.1007/s00146-024-02127-3"},{"id":"http://arxiv.org/abs/2502.09247v1","updated":"2025-02-13T12:03:36Z","published":"2025-02-13T12:03:36Z","title":"The Joint Entity-Relation Extraction Model Based on Span and Interactive\n  Fusion Representation for Chinese Medical Texts with Complex Semantics","summary":"  Joint entity-relation extraction is a critical task in transforming\nunstructured or semi-structured text into triplets, facilitating the\nconstruction of large-scale knowledge graphs, and supporting various downstream\napplications. Despite its importance, research on Chinese text, particularly\nwith complex semantics in specialized domains like medicine, remains limited.\nTo address this gap, we introduce the CH-DDI, a Chinese drug-drug interactions\ndataset designed to capture the intricacies of medical text. Leveraging the\nstrengths of attention mechanisms in capturing long-range dependencies, we\npropose the SEA module, which enhances the extraction of complex contextual\nsemantic information, thereby improving entity recognition and relation\nextraction. Additionally, to address the inefficiencies of existing methods in\nfacilitating information exchange between entity recognition and relation\nextraction, we present an interactive fusion representation module. This module\nemploys Cross Attention for bidirectional information exchange between the\ntasks and further refines feature extraction through BiLSTM. Experimental\nresults on both our CH-DDI dataset and public CoNLL04 dataset demonstrate that\nour model exhibits strong generalization capabilities. On the CH-DDI dataset,\nour model achieves an F1-score of 96.73% for entity recognition and 78.43% for\nrelation extraction. On the CoNLL04 dataset, it attains an entity recognition\nprecision of 89.54% and a relation extraction accuracy of 71.64%.\n","authors":["Danni Feng","Runzhi Li","Jing Wang","Siyu Yan","Lihong Ma","Yunli Xing"],"pdf_url":"https://arxiv.org/pdf/2502.09247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09245v1","updated":"2025-02-13T12:00:50Z","published":"2025-02-13T12:00:50Z","title":"You Do Not Fully Utilize Transformer's Representation Capacity","summary":"  In contrast to RNNs, which compress previous tokens into a single hidden\nstate, Transformers can attend to all previous tokens directly. However,\nstandard Transformers only use representations from the immediately preceding\nlayer. In this paper, we show that this design choice causes representation\ncollapse and leads to suboptimal performance. To address this issue, we\nintroduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that\npreserves the model's overall memory footprint while expanding its\nrepresentational capacity by allowing access to hidden states from earlier\nlayers. Through extensive experiments across various architectures and\ndifferent lookup mechanisms, we demonstrate consistent performance improvements\non a wide range of tasks. Moreover, our analysis of the learned representation\ndynamics and our exploration of depthwise circuits reveal how LIMe integrates\ninformation across layers, pointing to promising directions for future\nresearch.\n","authors":["Gleb Gerasimov","Yaroslav Aksenov","Nikita Balagansky","Viacheslav Sinii","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2502.09245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09237v1","updated":"2025-02-13T11:54:28Z","published":"2025-02-13T11:54:28Z","title":"Reliable Conversational Agents under ASP Control that Understand Natural\n  Language","summary":"  Efforts have been made to make machines converse like humans in the past few\ndecades. The recent techniques of Large Language Models (LLMs) make it possible\nto have human-like conversations with machines, but LLM's flaws of lacking\nunderstanding and reliability are well documented. We believe that the best way\nto eliminate this problem is to use LLMs only as parsers to translate text to\nknowledge and vice versa and carry out the conversation by reasoning over this\nknowledge using the answer set programming. I have been developing a framework\nbased on LLMs and ASP to realize reliable chatbots that \"understand\" human\nconversation. This framework has been used to develop task-specific chatbots as\nwell as socialbots. My future research is focused on making these chatbots\nscalable and trainable.\n","authors":["Yankai Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.09237v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09231v1","updated":"2025-02-13T11:52:55Z","published":"2025-02-13T11:52:55Z","title":"Answer Set Counting and its Applications","summary":"  We have focused on Answer Set Programming (ASP), more specifically, answer\nset counting, exploring both exact and approximate methodologies. We developed\nan exact ASP counter, sharpASP, which utilizes a compact encoding for\npropositional formulas, significantly enhancing efficiency compared to existing\nmethods that often struggle with inefficient encodings. Our evaluations\nindicate that sharpASP outperforms current ASP counters on several benchmarks.\nIn addition, we proposed an approximate ASP counter, named ApproxASP, a\nhashing-based counter integrating Gauss-Jordan elimination within the ASP\nsolver, clingo. As a practical application, we employed ApproxASP for network\nreliability estimation, demonstrating superior performance over both\ntraditional reliability estimators and #SAT-based methods.\n","authors":["Mohimenul Kabir"],"pdf_url":"https://arxiv.org/pdf/2502.09231v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09216v1","updated":"2025-02-13T11:49:17Z","published":"2025-02-13T11:49:17Z","title":"Mind the Gaps: Logical English, Prolog, and Multi-agent Systems for\n  Autonomous Vehicles","summary":"  In this paper, we present a modular system for representing and reasoning\nwith legal aspects of traffic rules for autonomous vehicles. We focus on a\nsubset of the United Kingdom's Highway Code (HC) related to junctions. As human\ndrivers and automated vehicles (AVs) will interact on the roads, especially in\nurban environments, we claim that an accessible, unitary, high-level\ncomputational model should exist and be applicable to both users. Autonomous\nvehicles introduce a shift in liability that should not bring disadvantages or\nincreased burden on human drivers. We develop a system \"in silico\" of the\nmodel. The proposed system is built of three main components: a natural\nlanguage interface, using Logical English, which encodes the rules; an internal\nrepresentation of the rules in Prolog; and an multi-agent-based simulation\nenvironment, built in NetLogo. The three components interact: Logical English\nis translated into and out of Prolog (along with some support code); Prolog and\nNetLogo interface via predicates. Such a modular approach enables the different\ncomponents to carry different \"burdens\" in the overall system; it also allows\nswapping of modules. Given NetLogo, we can visualize the effect of the modeled\nrules as well as validate the system with a simple dynamic running scenario.\nDesignated agents monitor the behaviour of the vehicles for compliance and\nrecord potential violations where they occur. The information on potential\nviolations is then utilized by Validators, to determine whether the violation\nis punishable, differentiating between exceptions and cases.\n","authors":["Galileo Sartor","Adam Wyner","Giuseppe Contissa"],"pdf_url":"https://arxiv.org/pdf/2502.09216v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09213v1","updated":"2025-02-13T11:48:46Z","published":"2025-02-13T11:48:46Z","title":"Neuro-Symbolic Contrastive Learning for Cross-domain Inference","summary":"  Pre-trained language models (PLMs) have made significant advances in natural\nlanguage inference (NLI) tasks, however their sensitivity to textual\nperturbations and dependence on large datasets indicate an over-reliance on\nshallow heuristics. In contrast, inductive logic programming (ILP) excels at\ninferring logical relationships across diverse, sparse and limited datasets,\nbut its discrete nature requires the inputs to be precisely specified, which\nlimits their application. This paper proposes a bridge between the two\napproaches: neuro-symbolic contrastive learning. This allows for smooth and\ndifferentiable optimisation that improves logical accuracy across an otherwise\ndiscrete, noisy, and sparse topological space of logical functions. We show\nthat abstract logical relationships can be effectively embedded within a\nneuro-symbolic paradigm, by representing data as logic programs and sets of\nlogic rules. The embedding space captures highly varied textual information\nwith similar semantic logical relations, but can also separate similar textual\nrelations that have dissimilar logical relations. Experimental results\ndemonstrate that our approach significantly improves the inference capabilities\nof the models in terms of generalisation and reasoning.\n","authors":["Mingyue Liu","Ryo Ueda","Zhen Wan","Katsumi Inoue","Chris G. Willcocks"],"pdf_url":"https://arxiv.org/pdf/2502.09213v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09212v1","updated":"2025-02-13T11:48:31Z","published":"2025-02-13T11:48:31Z","title":"LP-LM: No Hallucinations in Question Answering with Logic Programming","summary":"  Large language models (LLMs) are able to generate human-like responses to\nuser queries. However, LLMs exhibit inherent limitations, especially because\nthey hallucinate. This paper introduces LP-LM, a system that grounds answers to\nquestions in known facts contained in a knowledge base (KB), facilitated\nthrough semantic parsing in Prolog, and always produces answers that are\nreliable.\n  LP-LM generates a most probable constituency parse tree along with a\ncorresponding Prolog term for an input question via Prolog definite clause\ngrammar (DCG) parsing. The term is then executed against a KB of natural\nlanguage sentences also represented as Prolog terms for question answering. By\nleveraging DCG and tabling, LP-LM runs in linear time in the size of input\nsentences for sufficiently many grammar rules. Performing experiments comparing\nLP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate\non even simple questions, unlike LP-LM.\n","authors":["Katherine Wu","Yanhong A. Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09212v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2411.16495v3","updated":"2025-02-13T11:46:25Z","published":"2024-11-25T15:35:51Z","title":"AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning","summary":"  Despite the outstanding capabilities of large language models (LLMs),\nknowledge-intensive reasoning still remains a challenging task due to LLMs'\nlimitations in compositional reasoning and the hallucination problem. A\nprevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented\ngeneration (RAG), which first formulates a reasoning plan by decomposing\ncomplex questions into simpler sub-questions, and then applies iterative RAG at\neach sub-question. However, prior works exhibit two crucial problems:\ninadequate reasoning planning and poor incorporation of heterogeneous\nknowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct\naccurate heterogeneous knowledge reasoning at the atomic level. Inspired by how\nknowledge graph query languages model compositional reasoning through combining\npredefined operations, we propose three atomic knowledge operators, a unified\nset of operators for LLMs to retrieve and manipulate knowledge from\nheterogeneous sources. First, in the reasoning planning stage, AtomR decomposes\na complex question into a reasoning tree where each leaf node corresponds to an\natomic knowledge operator, achieving question decomposition that is highly\nfine-grained and orthogonal. Subsequently, in the reasoning execution stage,\nAtomR executes each atomic knowledge operator, which flexibly selects,\nretrieves, and operates atomic level knowledge from heterogeneous sources. We\nalso introduce BlendQA, a challenging benchmark specially tailored for\nheterogeneous knowledge reasoning. Experiments on three single-source and two\nmulti-source datasets show that AtomR outperforms state-of-the-art baselines by\na large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on\nBlendQA. We release our code and datasets.\n","authors":["Amy Xin","Jinxin Liu","Zijun Yao","Zhicheng Lee","Shulin Cao","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2411.16495v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10053v2","updated":"2025-02-13T11:43:39Z","published":"2024-08-19T14:48:04Z","title":"Privacy Checklist: Privacy Violation Detection Grounding on Contextual\n  Integrity Theory","summary":"  Privacy research has attracted wide attention as individuals worry that their\nprivate data can be easily leaked during interactions with smart devices,\nsocial platforms, and AI applications. Computer science researchers, on the\nother hand, commonly study privacy issues through privacy attacks and defenses\non segmented fields. Privacy research is conducted on various sub-fields,\nincluding Computer Vision (CV), Natural Language Processing (NLP), and Computer\nNetworks. Within each field, privacy has its own formulation. Though pioneering\nworks on attacks and defenses reveal sensitive privacy issues, they are\nnarrowly trapped and cannot fully cover people's actual privacy concerns.\nConsequently, the research on general and human-centric privacy research\nremains rather unexplored. In this paper, we formulate the privacy issue as a\nreasoning problem rather than simple pattern matching. We ground on the\nContextual Integrity (CI) theory which posits that people's perceptions of\nprivacy are highly correlated with the corresponding social context. Based on\nsuch an assumption, we develop the first comprehensive checklist that covers\nsocial identities, private attributes, and existing privacy regulations. Unlike\nprior works on CI that either cover limited expert annotated norms or model\nincomplete social context, our proposed privacy checklist uses the whole Health\nInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, to\nshow that we can resort to large language models (LLMs) to completely cover the\nHIPAA's regulations. Additionally, our checklist also gathers expert\nannotations across multiple ontologies to determine private information\nincluding but not limited to personally identifiable information (PII). We use\nour preliminary results on the HIPAA to shed light on future context-centric\nprivacy research to cover more privacy regulations, social norms and standards.\n","authors":["Haoran Li","Wei Fan","Yulin Chen","Jiayang Cheng","Tianshu Chu","Xuebing Zhou","Peizhao Hu","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2408.10053v2.pdf","comment":"To appear at NAACL 25"},{"id":"http://arxiv.org/abs/2412.15151v3","updated":"2025-02-13T11:37:45Z","published":"2024-12-19T18:28:41Z","title":"Language Models as Continuous Self-Evolving Data Engineers","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting a ceiling on the performance of LLMs.\nTo address this issue, we propose a novel paradigm named LANCE (LANguage models\nas Continuous self-Evolving data engineers) that enables LLMs to train\nthemselves by autonomously generating, cleaning, reviewing, and annotating data\nwith preference information. Our approach demonstrates that LLMs can serve as\ncontinuous self-evolving data engineers, significantly reducing the time and\ncost of the post-training data construction. Through iterative fine-tuning on\nQwen2 series models, we validate the effectiveness of LANCE across various\ntasks, showing that it can maintain high-quality data generation and\ncontinuously improve model performance. Across multiple benchmark dimensions,\nLANCE results in an average score enhancement of 3.64 for Qwen2-7B and 1.75 for\nQwen2-7B-Instruct. This training paradigm with autonomous data construction not\nonly reduces the reliance on human experts or external models but also ensures\nthat the data aligns with human preferences, paving the way for the development\nof future superintelligent systems that can exceed human capabilities. Codes\nare available at: https://github.com/Control-derek/LANCE.\n","authors":["Peidong Wang","Ming Wang","Zhiming Ma","Xiaocui Yang","Shi Feng","Daling Wang","Yifei Zhang","Kaisong Song"],"pdf_url":"https://arxiv.org/pdf/2412.15151v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09192v1","updated":"2025-02-13T11:32:09Z","published":"2025-02-13T11:32:09Z","title":"Thinking beyond the anthropomorphic paradigm benefits LLM research","summary":"  Anthropomorphism, or the attribution of human traits to technology, is an\nautomatic and unconscious response that occurs even in those with advanced\ntechnical expertise. In this position paper, we analyze hundreds of thousands\nof computer science research articles from the past decade and present\nempirical evidence of the prevalence and growth of anthropomorphic terminology\nin research on large language models (LLMs). This terminology reflects deeper\nanthropomorphic conceptualizations which shape how we think about and conduct\nLLM research. We argue these conceptualizations may be limiting, and that\nchallenging them opens up new pathways for understanding and improving LLMs\nbeyond human analogies. To illustrate this, we identify and analyze five core\nanthropomorphic assumptions shaping prominent methodologies across the LLM\ndevelopment lifecycle, from the assumption that models must use natural\nlanguage for reasoning tasks to the assumption that model capabilities should\nbe evaluated through human-centric benchmarks. For each assumption, we\ndemonstrate how non-anthropomorphic alternatives can open new directions for\nresearch and development.\n","authors":["Lujain Ibrahim","Myra Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.09192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08441v2","updated":"2025-02-13T11:30:41Z","published":"2024-07-11T12:30:19Z","title":"Are Large Language Models Really Bias-Free? Jailbreak Prompts for\n  Assessing Adversarial Robustness to Bias Elicitation","summary":"  Large Language Models (LLMs) have revolutionized artificial intelligence,\ndemonstrating remarkable computational power and linguistic capabilities.\nHowever, these models are inherently prone to various biases stemming from\ntheir training data. These include selection, linguistic, and confirmation\nbiases, along with common stereotypes related to gender, ethnicity, sexual\norientation, religion, socioeconomic status, disability, and age. This study\nexplores the presence of these biases within the responses given by the most\nrecent LLMs, analyzing the impact on their fairness and reliability. We also\ninvestigate how known prompt engineering techniques can be exploited to\neffectively reveal hidden biases of LLMs, testing their adversarial robustness\nagainst jailbreak prompts specially crafted for bias elicitation. Extensive\nexperiments are conducted using the most widespread LLMs at different scales,\nconfirming that LLMs can still be manipulated to produce biased or\ninappropriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhancing\nmitigation techniques to address these safety issues, toward a more sustainable\nand inclusive artificial intelligence.\n","authors":["Riccardo Cantini","Giada Cosenza","Alessio Orsino","Domenico Talia"],"pdf_url":"https://arxiv.org/pdf/2407.08441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09188v1","updated":"2025-02-13T11:22:19Z","published":"2025-02-13T11:22:19Z","title":"Matina: A Large-Scale 73B Token Persian Text Corpus","summary":"  Text corpora are essential for training models used in tasks like\nsummarization, translation, and large language models (LLMs). While various\nefforts have been made to collect monolingual and multilingual datasets in many\nlanguages, Persian has often been underrepresented due to limited resources for\ndata collection and preprocessing. Existing Persian datasets are typically\nsmall and lack content diversity, consisting mainly of weblogs and news\narticles. This shortage of high-quality, varied data has slowed the development\nof NLP models and open-source LLMs for Persian. Since model performance depends\nheavily on the quality of training data, we address this gap by introducing the\nMatina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed\nand deduplicated to ensure high data quality. We further assess its\neffectiveness by training and evaluating transformer-based models on key NLP\ntasks. Both the dataset and preprocessing codes are publicly available,\nenabling researchers to build on and improve this resource for future Persian\nNLP advancements.\n","authors":["Sara Bourbour Hosseinbeigi","Fatemeh Taherinezhad","Heshaam Faili","Hamed Baghbani","Fatemeh Nadi","Mostafa Amiri"],"pdf_url":"https://arxiv.org/pdf/2502.09188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09183v1","updated":"2025-02-13T11:17:53Z","published":"2025-02-13T11:17:53Z","title":"RefineCoder: Iterative Improving of Large Language Models via Adaptive\n  Critique Refinement for Code Generation","summary":"  Code generation has attracted increasing attention with the rise of Large\nLanguage Models (LLMs). Many studies have developed powerful code LLMs by\nsynthesizing code-related instruction data and applying supervised fine-tuning.\nHowever, these methods are limited by teacher model distillation and ignore the\npotential of iterative refinement by self-generated code. In this paper, we\npropose Adaptive Critique Refinement (ACR), which enables the model to refine\nitself by self-generated code and external critique, rather than directly\nimitating the code responses of the teacher model. Concretely, ACR includes a\ncomposite scoring system with LLM-as-a-Judge to evaluate the quality of code\nresponses and a selective critique strategy with LLM-as-a-Critic to critique\nself-generated low-quality code responses. We develop the RefineCoder series by\niteratively applying ACR, achieving continuous performance improvement on\nmultiple code generation benchmarks. Compared to the baselines of the same\nsize, our proposed RefineCoder series can achieve comparable or even superior\nperformance using less data.\n","authors":["Changzhi Zhou","Xinyu Zhang","Dandan Song","Xiancai Chen","Wanli Gu","Huipeng Ma","Yuhang Tian","Mengdi Zhang","Linmei Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09183v1.pdf","comment":"work in process"},{"id":"http://arxiv.org/abs/2502.09175v1","updated":"2025-02-13T11:05:55Z","published":"2025-02-13T11:05:55Z","title":"FLAME: Flexible LLM-Assisted Moderation Engine","summary":"  The rapid advancement of Large Language Models (LLMs) has introduced\nsignificant challenges in moderating user-model interactions. While LLMs\ndemonstrate remarkable capabilities, they remain vulnerable to adversarial\nattacks, particularly ``jailbreaking'' techniques that bypass content safety\nmeasures. Current content moderation systems, which primarily rely on input\nprompt filtering, have proven insufficient, with techniques like Best-of-N\n(BoN) jailbreaking achieving success rates of 80% or more against popular LLMs.\nIn this paper, we introduce Flexible LLM-Assisted Moderation Engine (FLAME): a\nnew approach that shifts the focus from input filtering to output moderation.\nUnlike traditional circuit-breaking methods that analyze user queries, FLAME\nevaluates model responses, offering several key advantages: (1) computational\nefficiency in both training and inference, (2) enhanced resistance to BoN\njailbreaking attacks, and (3) flexibility in defining and updating safety\ncriteria through customizable topic filtering. Our experiments demonstrate that\nFLAME significantly outperforms current moderation systems. For example, FLAME\nreduces attack success rate in GPT-4o-mini and DeepSeek-v3 by a factor of ~9,\nwhile maintaining low computational overhead. We provide comprehensive\nevaluation on various LLMs and analyze the engine's efficiency against the\nstate-of-the-art jailbreaking. This work contributes to the development of more\nrobust and adaptable content moderation systems for LLMs.\n","authors":["Ivan Bakulin","Ilia Kopanichuk","Iaroslav Bespalov","Nikita Radchenko","Vladimir Shaposhnikov","Dmitry Dylov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2502.09175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09168v1","updated":"2025-02-13T10:51:40Z","published":"2025-02-13T10:51:40Z","title":"Musical Heritage Historical Entity Linking","summary":"  Linking named entities occurring in text to their corresponding entity in a\nKnowledge Base (KB) is challenging, especially when dealing with historical\ntexts. In this work, we introduce Musical Heritage named Entities Recognition,\nClassification and Linking (MHERCL), a novel benchmark consisting of manually\nannotated sentences extrapolated from historical periodicals of the music\ndomain. MHERCL contains named entities under-represented or absent in the most\nfamous KBs. We experiment with several State-of-the-Art models on the Entity\nLinking (EL) task and show that MHERCL is a challenging dataset for all of\nthem. We propose a novel unsupervised EL model and a method to extend\nsupervised entity linkers by using Knowledge Graphs (KGs) to tackle the main\ndifficulties posed by historical documents. Our experiments reveal that relying\non unsupervised techniques and improving models with logical constraints based\non KGs and heuristics to predict NIL entities (entities not represented in the\nKB of reference) results in better EL performance on historical documents.\n","authors":["Arianna Graciotti","Nicolas Lazzari","Valentina Presutti","Rocco Tripodi"],"pdf_url":"https://arxiv.org/pdf/2502.09168v1.pdf","comment":"To appear in Artificial Intelligence Review Journal"},{"id":"http://arxiv.org/abs/2308.13916v5","updated":"2025-02-13T10:45:15Z","published":"2023-08-26T16:51:17Z","title":"Exploring Large Language Models for Knowledge Graph Completion","summary":"  Knowledge graphs play a vital role in numerous artificial intelligence tasks,\nyet they frequently face the issue of incompleteness. In this study, we explore\nutilizing Large Language Models (LLM) for knowledge graph completion. We\nconsider triples in knowledge graphs as text sequences and introduce an\ninnovative framework called Knowledge Graph LLM (KG-LLM) to model these\ntriples. Our technique employs entity and relation descriptions of a triple as\nprompts and utilizes the response for predictions. Experiments on various\nbenchmark knowledge graphs demonstrate that our method attains state-of-the-art\nperformance in tasks such as triple classification and relation prediction. We\nalso find that fine-tuning relatively smaller models (e.g., LLaMA-7B,\nChatGLM-6B) outperforms recent ChatGPT and GPT-4.\n","authors":["Liang Yao","Jiazhen Peng","Chengsheng Mao","Yuan Luo"],"pdf_url":"https://arxiv.org/pdf/2308.13916v5.pdf","comment":"Accepted by the 2025 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2502.09156v1","updated":"2025-02-13T10:36:18Z","published":"2025-02-13T10:36:18Z","title":"Improving TCM Question Answering through Tree-Organized Self-Reflective\n  Retrieval with LLMs","summary":"  Objectives: Large language models (LLMs) can harness medical knowledge for\nintelligent question answering (Q&A), promising support for auxiliary diagnosis\nand medical talent cultivation. However, there is a deficiency of highly\nefficient retrieval-augmented generation (RAG) frameworks within the domain of\nTraditional Chinese Medicine (TCM). Our purpose is to observe the effect of the\nTree-Organized Self-Reflective Retrieval (TOSRR) framework on LLMs in TCM Q&A\ntasks.\n  Materials and Methods: We introduce the novel approach of knowledge\norganization, constructing a tree structure knowledge base with hierarchy. At\ninference time, our self-reflection framework retrieves from this knowledge\nbase, integrating information across chapters. Questions from the TCM Medical\nLicensing Examination (MLE) and the college Classics Course Exam (CCE) were\nrandomly selected as benchmark datasets.\n  Results: By coupling with GPT-4, the framework can improve the best\nperformance on the TCM MLE benchmark by 19.85% in absolute accuracy, and\nimprove recall accuracy from 27% to 38% on CCE datasets. In manual evaluation,\nthe framework improves a total of 18.52 points across dimensions of safety,\nconsistency, explainability, compliance, and coherence.\n  Conclusion: The TOSRR framework can effectively improve LLM's capability in\nQ&A tasks of TCM.\n","authors":["Chang Liu","Ying Chang","Jianmin Li","Yiqian Qu","Yu Li","Lingyong Cao","Shuyuan Lin"],"pdf_url":"https://arxiv.org/pdf/2502.09156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09128v1","updated":"2025-02-13T10:05:44Z","published":"2025-02-13T10:05:44Z","title":"A Novel Dialect-Aware Framework for the Classification of Arabic\n  Dialects and Emotions","summary":"  Arabic is one of the oldest languages still in use today. As a result,\nseveral Arabic-speaking regions have developed dialects that are unique to\nthem. Dialect and emotion recognition have various uses in Arabic text\nanalysis, such as determining an online customer's origin based on their\ncomments. Furthermore, intelligent chatbots that are aware of a user's emotions\ncan respond appropriately to the user. Current research in emotion detection in\nthe Arabic language lacks awareness of how emotions are exhibited in different\ndialects, which motivates the work found in this study. This research addresses\nthe problems of dialect and emotion classification in Arabic. Specifically,\nthis is achieved by building a novel framework that can identify and predict\nArabic dialects and emotions from a given text. The framework consists of three\nmodules: A text-preprocessing module, a classification module, and a clustering\nmodule with the novel capability of building new dialect-aware emotion\nlexicons. The proposed framework generated a new emotional lexicon for\ndifferent dialects. It achieved an accuracy of 88.9% in classifying Arabic\ndialects, which outperforms the state-of-the-art results by 6.45 percentage\npoints. Furthermore, the framework achieved 89.1-79% accuracy in detecting\nemotions in the Egyptian and Gulf dialects, respectively.\n","authors":["Nasser A Alsadhan"],"pdf_url":"https://arxiv.org/pdf/2502.09128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09120v1","updated":"2025-02-13T09:55:48Z","published":"2025-02-13T09:55:48Z","title":"The influence of visual and linguistic cues on ignorance inference in\n  Vision-Language Models (VLMs)","summary":"  This study explored how Vision-Language Models (VLMs) process ignorance\nimplicatures with visual and linguistic cues. Particularly, we focused on the\neffects of contexts (precise and approximate contexts) and modifier types (bare\nnumerals, superlative, and comparative modifiers), which were considered\npragmatic and semantic factors respectively. Methodologically, we conducted a\ntruth-value judgment task in visually grounded settings using GPT-4o and Gemini\n1.5 Pro. The results indicate that while both models exhibited sensitivity to\nlinguistic cues (modifier), they failed to process ignorance implicatures with\nvisual cues (context) as humans do. Specifically, the influence of context was\nweaker and inconsistent across models, indicating challenges in pragmatic\nreasoning for VLMs. On the other hand, superlative modifiers were more strongly\nassociated with ignorance implicatures as compared to comparative modifiers,\nsupporting the semantic view. These findings highlight the need for further\nadvancements in VLMs to process language-vision information in a\ncontext-dependent way to achieve human-like pragmatic inference.\n","authors":["Ye-eun Cho","Yunho Maeng"],"pdf_url":"https://arxiv.org/pdf/2502.09120v1.pdf","comment":"13 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09100v1","updated":"2025-02-13T09:19:14Z","published":"2025-02-13T09:19:14Z","title":"Logical Reasoning in Large Language Models: A Survey","summary":"  With the emergence of advanced reasoning models like OpenAI o3 and\nDeepSeek-R1, large language models (LLMs) have demonstrated remarkable\nreasoning capabilities. However, their ability to perform rigorous logical\nreasoning remains an open question. This survey synthesizes recent advancements\nin logical reasoning within LLMs, a critical area of AI research. It outlines\nthe scope of logical reasoning in LLMs, its theoretical foundations, and the\nbenchmarks used to evaluate reasoning proficiency. We analyze existing\ncapabilities across different reasoning paradigms - deductive, inductive,\nabductive, and analogical - and assess strategies to enhance reasoning\nperformance, including data-centric tuning, reinforcement learning, decoding\nstrategies, and neuro-symbolic approaches. The review concludes with future\ndirections, emphasizing the need for further exploration to strengthen logical\nreasoning in AI systems.\n","authors":["Hanmeng Liu","Zhizhang Fu","Mengru Ding","Ruoxi Ning","Chaoli Zhang","Xiaozhang Liu","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09097v1","updated":"2025-02-13T09:13:23Z","published":"2025-02-13T09:13:23Z","title":"A Hybrid Transformer Model for Fake News Detection: Leveraging Bayesian\n  Optimization and Bidirectional Recurrent Unit","summary":"  In this paper, we propose an optimized Transformer model that integrates\nBayesian algorithms with a Bidirectional Gated Recurrent Unit (BiGRU), and\napply it to fake news classification for the first time. First, we employ the\nTF-IDF method to extract features from news texts and transform them into\nnumeric representations to facilitate subsequent machine learning tasks. Two\nsets of experiments are then conducted for fake news detection and\nclassification: one using a Transformer model optimized only with BiGRU, and\nthe other incorporating Bayesian algorithms into the BiGRU-based Transformer.\nExperimental results show that the BiGRU-optimized Transformer achieves 100%\naccuracy on the training set and 99.67% on the test set, while the addition of\nthe Bayesian algorithm maintains 100% accuracy on the training set and slightly\nimproves test-set accuracy to 99.73%. This indicates that the Bayesian\nalgorithm boosts model accuracy by 0.06%, further enhancing the detection\ncapability for fake news. Moreover, the proposed algorithm converges rapidly at\naround the 10th training epoch with accuracy nearing 100%, demonstrating both\nits effectiveness and its fast classification ability. Overall, the optimized\nTransformer model, enhanced by the Bayesian algorithm and BiGRU, exhibits\nexcellent continuous learning and detection performance, offering a robust\ntechnical means to combat the spread of fake news in the current era of\ninformation overload.\n","authors":["Tianyi Huang","Zeqiu Xu","Peiyang Yu","Jingyuan Yi","Xiaochuan Xu"],"pdf_url":"https://arxiv.org/pdf/2502.09097v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13166v4","updated":"2025-02-13T09:08:42Z","published":"2024-10-17T02:47:10Z","title":"An Evolved Universal Transformer Memory","summary":"  Prior methods propose to offset the escalating costs of modern foundation\nmodels by dropping specific parts of their contexts with hand-designed rules,\nwhile attempting to preserve their original performance. We overcome this\ntrade-off with Neural Attention Memory Models (NAMMs), introducing a learned\nnetwork for memory management that improves both the performance and efficiency\nof transformers. We evolve NAMMs atop pre-trained transformers to provide\ndifferent latent contexts focusing on the most relevant information for\nindividual layers and attention heads. NAMMs are universally applicable to any\nmodel using self-attention as they condition exclusively on the values in the\nproduced attention matrices. Learning NAMMs on a small set of problems, we\nachieve substantial performance improvements across multiple long-context\nbenchmarks while cutting the model's input contexts up to a fraction of the\noriginal sizes. We show the generality of our conditioning enables zero-shot\ntransfer of NAMMs trained only on language to entirely new transformer\narchitectures even across input modalities, with their benefits carrying over\nto vision and reinforcement learning.\n","authors":["Edoardo Cetin","Qi Sun","Tianyu Zhao","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.13166v4.pdf","comment":"Published at ICLR 2025. Source code available at\n  https://github.com/SakanaAI/evo-memory"},{"id":"http://arxiv.org/abs/2405.14445v2","updated":"2025-02-13T09:07:10Z","published":"2024-05-23T11:24:23Z","title":"Exploring the use of a Large Language Model for data extraction in\n  systematic reviews: a rapid feasibility study","summary":"  This paper describes a rapid feasibility study of using GPT-4, a large\nlanguage model (LLM), to (semi)automate data extraction in systematic reviews.\nDespite the recent surge of interest in LLMs there is still a lack of\nunderstanding of how to design LLM-based automation tools and how to robustly\nevaluate their performance. During the 2023 Evidence Synthesis Hackathon we\nconducted two feasibility studies. Firstly, to automatically extract study\ncharacteristics from human clinical, animal, and social science domain studies.\nWe used two studies from each category for prompt-development; and ten for\nevaluation. Secondly, we used the LLM to predict Participants, Interventions,\nControls and Outcomes (PICOs) labelled within 100 abstracts in the EBM-NLP\ndataset. Overall, results indicated an accuracy of around 80%, with some\nvariability between domains (82% for human clinical, 80% for animal, and 72%\nfor studies of human social sciences). Causal inference methods and study\ndesign were the data extraction items with the most errors. In the PICO study,\nparticipants and intervention/control showed high accuracy (>80%), outcomes\nwere more challenging. Evaluation was done manually; scoring methods such as\nBLEU and ROUGE showed limited value. We observed variability in the LLMs\npredictions and changes in response quality. This paper presents a template for\nfuture evaluations of LLMs in the context of data extraction for systematic\nreview automation. Our results show that there might be value in using LLMs,\nfor example as second or third reviewers. However, caution is advised when\nintegrating models such as GPT-4 into tools. Further research on stability and\nreliability in practical settings is warranted for each type of data that is\nprocessed by the LLM.\n","authors":["Lena Schmidt","Kaitlyn Hair","Sergio Graziosi","Fiona Campbell","Claudia Kapp","Alireza Khanteymoori","Dawn Craig","Mark Engelbert","James Thomas"],"pdf_url":"https://arxiv.org/pdf/2405.14445v2.pdf","comment":"Conference proceedings, peer-reviewed and presented at the 3rd\n  Workshop on Augmented Intelligence for Technology-Assisted Reviews Systems,\n  Glasgow, 2024"},{"id":"http://arxiv.org/abs/2502.09086v1","updated":"2025-02-13T09:00:32Z","published":"2025-02-13T09:00:32Z","title":"A Hybrid Model for Few-Shot Text Classification Using Transfer and\n  Meta-Learning","summary":"  With the continuous development of natural language processing (NLP)\ntechnology, text classification tasks have been widely used in multiple\napplication fields. However, obtaining labeled data is often expensive and\ndifficult, especially in few-shot learning scenarios. To solve this problem,\nthis paper proposes a few-shot text classification model based on transfer\nlearning and meta-learning. The model uses the knowledge of the pre-trained\nmodel for transfer and optimizes the model's rapid adaptability in few-sample\ntasks through a meta-learning mechanism. Through a series of comparative\nexperiments and ablation experiments, we verified the effectiveness of the\nproposed method. The experimental results show that under the conditions of few\nsamples and medium samples, the model based on transfer learning and\nmeta-learning significantly outperforms traditional machine learning and deep\nlearning methods. In addition, ablation experiments further analyzed the\ncontribution of each component to the model performance and confirmed the key\nrole of transfer learning and meta-learning in improving model accuracy.\nFinally, this paper discusses future research directions and looks forward to\nthe potential of this method in practical applications.\n","authors":["Jia Gao","Shuangquan Lyu","Guiran Liu","Binrong Zhu","Hongye Zheng","Xiaoxuan Liao"],"pdf_url":"https://arxiv.org/pdf/2502.09086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09083v1","updated":"2025-02-13T08:56:25Z","published":"2025-02-13T08:56:25Z","title":"Show Me the Work: Fact-Checkers' Requirements for Explainable Automated\n  Fact-Checking","summary":"  The pervasiveness of large language models and generative AI in online media\nhas amplified the need for effective automated fact-checking to assist\nfact-checkers in tackling the increasing volume and sophistication of\nmisinformation. The complex nature of fact-checking demands that automated\nfact-checking systems provide explanations that enable fact-checkers to\nscrutinise their outputs. However, it is unclear how these explanations should\nalign with the decision-making and reasoning processes of fact-checkers to be\neffectively integrated into their workflows. Through semi-structured interviews\nwith fact-checking professionals, we bridge this gap by: (i) providing an\naccount of how fact-checkers assess evidence, make decisions, and explain their\nprocesses; (ii) examining how fact-checkers use automated tools in practice;\nand (iii) identifying fact-checker explanation requirements for automated\nfact-checking tools. The findings show unmet explanation needs and identify\nimportant criteria for replicable fact-checking explanations that trace the\nmodel's reasoning path, reference specific evidence, and highlight uncertainty\nand information gaps.\n","authors":["Greta Warren","Irina Shklovski","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2502.09083v1.pdf","comment":"Conditionally accepted to CHI'25"},{"id":"http://arxiv.org/abs/2502.09082v1","updated":"2025-02-13T08:55:24Z","published":"2025-02-13T08:55:24Z","title":"CoSER: Coordinating LLM-Based Persona Simulation of Established Roles","summary":"  Role-playing language agents (RPLAs) have emerged as promising applications\nof large language models (LLMs). However, simulating established characters\npresents a challenging task for RPLAs, due to the lack of authentic character\ndatasets and nuanced evaluation methods using such data. In this paper, we\npresent CoSER, a collection of a high-quality dataset, open models, and an\nevaluation protocol towards effective RPLAs of established characters. The\nCoSER dataset covers 17,966 characters from 771 renowned books. It provides\nauthentic dialogues with real-world intricacies, as well as diverse data types\nsuch as conversation setups, character experiences and internal thoughts.\nDrawing from acting methodology, we introduce given-circumstance acting for\ntraining and evaluating role-playing LLMs, where LLMs sequentially portray\nmultiple characters in book scenes. Using our dataset, we develop CoSER 8B and\nCoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.\nExtensive experiments demonstrate the value of the CoSER dataset for RPLA\ntraining, evaluation and retrieval. Moreover, CoSER 70B exhibits\nstate-of-the-art performance surpassing or matching GPT-4o on our evaluation\nand three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on\nthe InCharacter and LifeChoice benchmarks respectively.\n","authors":["Xintao Wang","Heng Wang","Yifei Zhang","Xinfeng Yuan","Rui Xu","Jen-tse Huang","Siyu Yuan","Haoran Guo","Jiangjie Chen","Wei Wang","Yanghua Xiao","Shuchang Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.09082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09073v1","updated":"2025-02-13T08:42:29Z","published":"2025-02-13T08:42:29Z","title":"Enhancing RAG with Active Learning on Conversation Records: Reject\n  Incapables and Answer Capables","summary":"  Retrieval-augmented generation (RAG) is a key technique for leveraging\nexternal knowledge and reducing hallucinations in large language models (LLMs).\nHowever, RAG still struggles to fully prevent hallucinated responses. To\naddress this, it is essential to identify samples prone to hallucination or\nguide LLMs toward correct responses, which experts then annotate to develop\nhigh-quality datasets for refining LLMs. However, the growing scarcity of such\ndatasets makes their creation challenging. This paper proposes using the vast\namount of conversations from widespread LLM usage to build these datasets,\ntraining LLMs to avoid hallucination-prone questions while accurately\nresponding to manageable ones. Given the impracticality of expert-annotating\nall conversation records, the paper introduces AL4RAG, which uses active\nlearning to select the most suitable conversation samples for annotation,\noptimizing performance within an annotation budget. Additionally, recognizing\nthat traditional active learning methods are not fully compatible with RAG due\nto unsuitable distance metrics, we develop a novel sample distance measurement\nfor RAG active learning. Extensive experiments show that our method\nconsistently outperforms baselines across multiple metrics.\n","authors":["Xuzhao Geng","Haozhao Wang","Jun Wang","Wei Liu","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2502.09073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13835v2","updated":"2025-02-13T08:13:52Z","published":"2024-01-24T22:21:04Z","title":"What Large Language Models Know and What People Think They Know","summary":"  As artificial intelligence (AI) systems, particularly large language models\n(LLMs), become increasingly integrated into decision-making processes, the\nability to trust their outputs is crucial. To earn human trust, LLMs must be\nwell calibrated such that they can accurately assess and communicate the\nlikelihood of their predictions being correct. Whereas recent work has focused\non LLMs' internal confidence, less is understood about how effectively they\nconvey uncertainty to users. Here we explore the calibration gap, which refers\nto the difference between human confidence in LLM-generated answers and the\nmodels' actual confidence, and the discrimination gap, which reflects how well\nhumans and models can distinguish between correct and incorrect answers. Our\nexperiments with multiple-choice and short-answer questions reveal that users\ntend to overestimate the accuracy of LLM responses when provided with default\nexplanations. Moreover, longer explanations increased user confidence, even\nwhen the extra length did not improve answer accuracy. By adjusting LLM\nexplanations to better reflect the models' internal confidence, both the\ncalibration gap and the discrimination gap narrowed, significantly improving\nuser perception of LLM accuracy. These findings underscore the importance of\naccurate uncertainty communication and highlight the effect of explanation\nlength in influencing user trust in AI-assisted decision-making environments.\nCode and Data can be found at https://osf.io/y7pr6/ . Journal publication can\nbe found on Nature Machine Intelligence at\nhttps://www.nature.com/articles/s42256-024-00976-7 .\n","authors":["Mark Steyvers","Heliodoro Tejeda","Aakriti Kumar","Catarina Belem","Sheer Karny","Xinyue Hu","Lukas Mayer","Padhraic Smyth"],"pdf_url":"https://arxiv.org/pdf/2401.13835v2.pdf","comment":"27 pages, 10 figures For the journal publication on Nature Machine\n  Intelligence see https://www.nature.com/articles/s42256-024-00976-7 For the\n  data and code see https://osf.io/y7pr6/"},{"id":"http://arxiv.org/abs/2401.11817v2","updated":"2025-02-13T08:11:25Z","published":"2024-01-22T10:26:14Z","title":"Hallucination is Inevitable: An Innate Limitation of Large Language\n  Models","summary":"  Hallucination has been widely recognized to be a significant drawback for\nlarge language models (LLMs). There have been many works that attempt to reduce\nthe extent of hallucination. These efforts have mostly been empirical so far,\nwhich cannot answer the fundamental question whether it can be completely\neliminated. In this paper, we formalize the problem and show that it is\nimpossible to eliminate hallucination in LLMs. Specifically, we define a formal\nworld where hallucination is defined as inconsistencies between a computable\nLLM and a computable ground truth function. By employing results from learning\ntheory, we show that LLMs cannot learn all the computable functions and will\ntherefore inevitably hallucinate if used as general problem solvers. Since the\nformal world is a part of the real world which is much more complicated,\nhallucinations are also inevitable for real world LLMs. Furthermore, for real\nworld LLMs constrained by provable time complexity, we describe the\nhallucination-prone tasks and empirically validate our claims. Finally, using\nthe formal world framework, we discuss the possible mechanisms and efficacies\nof existing hallucination mitigators as well as the practical implications on\nthe safe deployment of LLMs.\n","authors":["Ziwei Xu","Sanjay Jain","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2401.11817v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09056v1","updated":"2025-02-13T08:10:45Z","published":"2025-02-13T08:10:45Z","title":"An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in\n  One Day via Model Merging","summary":"  This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.\n","authors":["Kunat Pipatanakul","Pittawat Taveekitworachai","Potsawee Manakul","Kasima Tharnpipitchai"],"pdf_url":"https://arxiv.org/pdf/2502.09056v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2405.19778v4","updated":"2025-02-13T08:03:52Z","published":"2024-05-30T07:44:16Z","title":"CharacterGPT: A Persona Reconstruction Framework for Role-Playing Agents","summary":"  With the recent introduction of Assistants API, it is expected that\ndocument-based language models will be actively used in various domains,\nespecially Role-playing. However, a key challenge lies in utilizing\nprotagonist's persona: Assistants API often fails to achieve with its search\nbecause the information extraction part is different each time and it often\nomits important information such as protagonist's backstory or relationships.\nIt is hard to maintain a consistent persona simply by using the persona\ndocument as input to the Assistants API. To address the challenge of achieving\nstable persona consistency, we propose CharacterGPT, a novel persona\nreconstruction framework to alleviate the shortcomings of the Assistants API.\nOur method involves Character Persona Training (CPT), an effective persona\nrebuilding process that updates the character persona by extracting the\ncharacter's traits from given summary of the novel for each character as if the\nstory in a novel progresses. In our experiments, we ask each character to take\nthe Big Five Inventory personality test in various settings and analyze the\nresults. To assess whether it can think outside the box, we let each character\ngenerate short novels. Extensive experiments and human evaluation demonstrate\nthat CharacterGPT presents new possibilities for role-playing agent research.\nCode and results are available at: https://github.com/Jeiyoon/charactergpt\n","authors":["Jeiyoon Park","Chanjun Park","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2405.19778v4.pdf","comment":"NAACL 2025 Industry Track (Oral)"},{"id":"http://arxiv.org/abs/2502.09042v1","updated":"2025-02-13T07:55:54Z","published":"2025-02-13T07:55:54Z","title":"Typhoon T1: An Open Thai Reasoning Model","summary":"  This paper introduces Typhoon T1, an open effort to develop an open Thai\nreasoning model. A reasoning model is a relatively new type of generative model\nbuilt on top of large language models (LLMs). A reasoning model generates a\nlong chain of thought before arriving at a final answer, an approach found to\nimprove performance on complex tasks. However, details on developing such a\nmodel are limited, especially for reasoning models that can generate traces in\na low-resource language. Typhoon T1 presents an open effort that dives into the\ndetails of developing a reasoning model in a more cost-effective way by\nleveraging supervised fine-tuning using open datasets, instead of reinforcement\nlearning. This paper shares the details about synthetic data generation and\ntraining, as well as our dataset and model weights. Additionally, we provide\ninsights gained from developing a reasoning model that generalizes across\ndomains and is capable of generating reasoning traces in a low-resource\nlanguage, using Thai as an example. We hope this open effort provides a\nfoundation for further research in this field.\n","authors":["Pittawat Taveekitworachai","Potsawee Manakul","Kasima Tharnpipitchai","Kunat Pipatanakul"],"pdf_url":"https://arxiv.org/pdf/2502.09042v1.pdf","comment":"25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.00511v2","updated":"2025-02-13T07:35:08Z","published":"2025-02-01T18:09:49Z","title":"Bridging Internal Probability and Self-Consistency for Effective and\n  Efficient LLM Reasoning","summary":"  Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities. However, single-shot inference often yields\nunreliable results for complex reasoning tasks, leading researchers to explore\nmultiple reasoning paths through methods such as perplexity and\nself-consistency. In this paper, we present the first theoretical error\ndecomposition analysis of these techniques, breaking down their error into\nestimation error and model error. Our analysis reveals a fundamental trade-off:\nperplexity methods suffer from substantial model error due to the absence of a\nproper consistency function, while self-consistency exhibits high estimation\nerror due to a slow error convergence rate. To overcome these limitations, we\npropose Reasoning-Pruning Perplexity Consistency (RPC). This approach combines\nPerplexity Consistency, which seamlessly integrates LLM perplexity with\nself-consistency, and Reasoning Pruning, which eliminates low-probability\nreasoning paths to effectively prevent the degeneration of estimation error\nreduction. Theoretical analysis demonstrates that RPC not only accelerates the\nconvergence rate of estimation error to an exponential level but also holds\nstrong potential for further reducing model error. Extensive empirical\nevaluations on seven benchmark datasets confirm that RPC can significantly\nimprove reasoning performance, sample efficiency, and confidence reliability.\n","authors":["Zhi Zhou","Tan Yuhao","Zenan Li","Yuan Yao","Lan-Zhe Guo","Xiaoxing Ma","Yu-Feng Li"],"pdf_url":"https://arxiv.org/pdf/2502.00511v2.pdf","comment":"Preliminary work"},{"id":"http://arxiv.org/abs/2502.06635v2","updated":"2025-02-13T07:31:55Z","published":"2025-02-10T16:31:37Z","title":"Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM","summary":"  Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM.\n","authors":["Qingshui Gu","Shu Li","Tianyu Zheng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06635v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07367v2","updated":"2025-02-13T07:31:13Z","published":"2024-12-10T10:06:46Z","title":"My Words Imply Your Opinion: Reader Agent-Based Propagation Enhancement\n  for Personalized Implicit Emotion Analysis","summary":"  The subtlety of emotional expressions makes implicit emotion analysis (IEA)\nparticularly sensitive to user-specific characteristics. Current studies\npersonalize emotion analysis by focusing on the author but neglect the impact\nof the intended reader on implicit emotional feedback. In this paper, we\nintroduce Personalized IEA (PIEA) and present the RAPPIE model, which addresses\nsubjective variability by incorporating reader feedback. In particular, (1) we\ncreate reader agents based on large language models to simulate reader\nfeedback, overcoming the issue of ``spiral of silence effect'' and data\nincompleteness of real reader reaction. (2) We develop a role-aware multi-view\ngraph learning to model the emotion interactive propagation process in\nscenarios with sparse reader information. (3) We construct two new PIEA\ndatasets covering English and Chinese social media with detailed user metadata,\naddressing the text-centric limitation of existing datasets. Extensive\nexperiments show that RAPPIE significantly outperforms state-of-the-art\nbaselines, demonstrating the value of incorporating reader feedback in PIEA.\n","authors":["Jian Liao","Yu Feng","Yujin Zheng","Jun Zhao","Suge Wang","Jianxing Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.07367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02364v2","updated":"2025-02-13T07:29:46Z","published":"2024-02-04T06:23:05Z","title":"Loss Landscape Degeneracy Drives Stagewise Development in Transformers","summary":"  Deep learning involves navigating a high-dimensional loss landscape over the\nneural network parameter space. Over the course of training, complex\ncomputational structures form and re-form inside the neural network, leading to\nshifts in input/output behavior. It is a priority for the science of deep\nlearning to uncover principles governing the development of neural network\nstructure and behavior. Drawing on the framework of singular learning theory,\nwe propose that model development is deeply linked to degeneracy in the local\ngeometry of the loss landscape. We investigate this link by monitoring loss\nlandscape degeneracy throughout training, as quantified by the local learning\ncoefficient, for a transformer language model and an in-context linear\nregression transformer. We show that training can be divided into distinct\nperiods of change in loss landscape degeneracy, and that these changes in\ndegeneracy coincide with significant changes in the internal computational\nstructure and the input/output behavior of the transformers. This finding\nunderscores the potential of a degeneracy-based perspective for understanding\nmodern deep learning.\n","authors":["Jesse Hoogland","George Wang","Matthew Farrugia-Roberts","Liam Carroll","Susan Wei","Daniel Murfet"],"pdf_url":"https://arxiv.org/pdf/2402.02364v2.pdf","comment":"Material on essential dynamics from v1 of this preprint has been\n  removed from v2 and developed in arXiv:2501.17745"},{"id":"http://arxiv.org/abs/2502.06572v2","updated":"2025-02-13T07:24:46Z","published":"2025-02-10T15:40:35Z","title":"LawGPT: Knowledge-Guided Data Generation and Its Application to Legal\n  LLM","summary":"  Large language models (LLMs), both proprietary and open-source, have\ndemonstrated remarkable capabilities across various natural language processing\ntasks. However, they face significant limitations in legal reasoning tasks.\nProprietary models introduce data privacy risks and high inference costs, while\nopen-source models underperform due to insufficient legal domain training data.\nTo address these limitations, we study data generation for legal reasoning to\nimprove the legal reasoning performance of open-source LLMs with the help of\nproprietary LLMs. This is challenging due to the lack of legal knowledge in\nproprietary LLMs and the difficulty in verifying the generated data. We propose\nKgDG, a knowledge-guided data generation framework for legal reasoning. Our\nframework enables leveraging legal knowledge to enhance generation diversity\nand introduces a refinement and verification process to ensure the quality of\ngenerated data. Moreover, we expand the generated dataset to further enhance\nthe LLM reasoning capabilities. Using KgDG, we create a synthetic legal\nreasoning dataset containing 50K high-quality examples. Our trained model\nLawGPT outperforms existing legal-specific LLMs and achieves performance\ncomparable to proprietary LLMs, demonstrating the effectiveness of KgDG and\nLawGPT. Our code and resources is publicly available at\nhttps://github.com/LAMDASZ-ML/Knowledge-Guide-Data-Generation .\n","authors":["Zhi Zhou","Kun-Yang Yu","Shi-Yu Tian","Xiao-Wen Yang","Jiang-Xin Shi","Pengxiao Song","Yi-Xuan Jin","Lan-Zhe Guo","Yu-Feng Li"],"pdf_url":"https://arxiv.org/pdf/2502.06572v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09017v1","updated":"2025-02-13T07:11:01Z","published":"2025-02-13T07:11:01Z","title":"Diversity Enhances an LLM's Performance in RAG and Long-context Task","summary":"  The rapid advancements in large language models (LLMs) have highlighted the\nchallenge of context window limitations, primarily due to the quadratic time\ncomplexity of the self-attention mechanism (\\(O(N^2)\\), where \\(N\\) denotes the\ncontext window length). This constraint impacts tasks such as\nretrieval-augmented generation (RAG) in question answering (Q\\&A) and long\ncontext summarization. A common approach involves selecting content with the\nhighest similarity to the query; however, this often leads to redundancy and\nthe exclusion of diverse yet relevant information. Building on principles from\nMaximal Marginal Relevance (MMR) and Farthest Point Sampling (FPS), we\nintegrate diversity into the content selection process. Our findings reveal\nthat incorporating diversity substantially increases the recall of selecting\nrelevant sentences or chunks before LLM-based Q\\&A and summarization. These\nresults highlight the importance of maintaining diversity in future LLM\napplications to further improve summarization and Q\\&A outcomes.\n","authors":["Zhchao Wang","Bin Bi","Yanqi Luo","Sitaram Asur","Claire Na Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.09017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09004v1","updated":"2025-02-13T06:49:14Z","published":"2025-02-13T06:49:14Z","title":"Hope vs. Hate: Understanding User Interactions with LGBTQ+ News Content\n  in Mainstream US News Media through the Lens of Hope Speech","summary":"  This paper makes three contributions. First, via a substantial corpus of\n1,419,047 comments posted on 3,161 YouTube news videos of major US cable news\noutlets, we analyze how users engage with LGBTQ+ news content. Our analyses\nfocus both on positive and negative content. In particular, we construct a\nfine-grained hope speech classifier that detects positive (hope speech),\nnegative, neutral, and irrelevant content. Second, in consultation with a\npublic health expert specializing on LGBTQ+ health, we conduct an annotation\nstudy with a balanced and diverse political representation and release a\ndataset of 3,750 instances with fine-grained labels and detailed annotator\ndemographic information. Finally, beyond providing a vital resource for the\nLGBTQ+ community, our annotation study and subsequent in-the-wild assessments\nreveal (1) strong association between rater political beliefs and how they rate\ncontent relevant to a marginalized community; (2) models trained on individual\npolitical beliefs exhibit considerable in-the-wild disagreement; and (3)\nzero-shot large language models (LLMs) align more with liberal raters.\n","authors":["Jonathan Pofcher","Christopher M. Homan","Randall Sell","Ashiqur R. KhudaBukhsh"],"pdf_url":"https://arxiv.org/pdf/2502.09004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13258v2","updated":"2025-02-13T06:34:43Z","published":"2024-10-17T06:30:55Z","title":"How Does Knowledge Selection Help Retrieval Augmented Generation?","summary":"  Retrieval-augmented generation (RAG) is a powerful method for enhancing\nnatural language generation by integrating external knowledge into a model's\noutput. While prior work has demonstrated the importance of improving knowledge\nretrieval for boosting generation quality, the role of knowledge selection\nremains less clear. In this paper, we perform a comprehensive analysis of how\nknowledge selection influences downstream generation performance in RAG\nsystems. By simulating different retrieval and selection conditions through a\ncontrolled mixture of gold and distractor knowledge, we assess the impact of\nthese factors on generation outcomes. Our findings indicate that the downstream\ngenerator model's capability, as well as the complexity of the task and\ndataset, significantly influence the impact of knowledge selection on the\noverall RAG system performance. In typical scenarios, improving the knowledge\nrecall score is key to enhancing generation outcomes, with the knowledge\nselector providing a limited additional benefit when a strong generator model\nis used on clear, well-defined tasks. For weaker generator models or more\nambiguous tasks and datasets, the knowledge F1 score becomes a critical factor,\nand the knowledge selector plays a more prominent role in improving overall\nperformance.\n","authors":["Xiangci Li","Jessica Ouyang"],"pdf_url":"https://arxiv.org/pdf/2410.13258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06876v2","updated":"2025-02-13T06:28:33Z","published":"2025-02-08T11:56:58Z","title":"Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and\n  Harmlessness of Large Language Model via Model Merging","summary":"  Achieving balanced alignment of large language models (LLMs) in terms of\nHelpfulness, Honesty, and Harmlessness (3H optimization) constitutes a\ncornerstone of responsible AI, with existing methods like data mixture\nstrategies facing limitations including reliance on expert knowledge and\nconflicting optimization signals. While model merging offers a promising\nalternative by integrating specialized models, its potential for 3H\noptimization remains underexplored. This paper establishes the first\ncomprehensive benchmark for model merging in 3H-aligned LLMs, systematically\nevaluating 15 methods (12 training-free merging and 3 data mixture techniques)\nacross 10 datasets associated with 5 annotation dimensions, 2 LLM families, and\n2 training paradigms. Our analysis reveals three pivotal insights: (i)\npreviously overlooked collaborative/conflicting relationships among 3H\ndimensions, (ii) the consistent superiority of model merging over data mixture\napproaches in balancing alignment trade-offs, and (iii) the critical role of\nparameter-level conflict resolution through redundant component pruning and\noutlier mitigation. Building on these findings, we propose R-TSVM, a\nReweighting-enhanced Task Singular Vector Merging method that incorporates\noutlier-aware parameter weighting and sparsity-adaptive rank selection\nstrategies adapted to the heavy-tailed parameter distribution and sparsity for\nLLMs, further improving LLM alignment across multiple evaluations. We release\nour trained models for further exploration.\n","authors":["Jinluan Yang","Dingnan Jin","Anke Tang","Li Shen","Didi Zhu","Zhengyu Chen","Daixin Wang","Qing Cui","Zhiqiang Zhang","Jun Zhou","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2502.06876v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06147v2","updated":"2025-02-13T05:37:45Z","published":"2025-02-10T04:25:05Z","title":"LegalViz: Legal Text Visualization by Text To Diagram Generation","summary":"  Legal documents including judgments and court orders require highly\nsophisticated legal knowledge for understanding. To disclose expert knowledge\nfor non-experts, we explore the problem of visualizing legal texts with\neasy-to-understand diagrams and propose a novel dataset of LegalViz with 23\nlanguages and 7,010 cases of legal document and visualization pairs, using the\nDOT graph description language of Graphviz. LegalViz provides a simple diagram\nfrom a complicated legal corpus identifying legal entities, transactions, legal\nsources, and statements at a glance, that are essential in each judgment. In\naddition, we provide new evaluation metrics for the legal diagram visualization\nby considering graph structures, textual similarities, and legal contents. We\nconducted empirical studies on few-shot and finetuning large language models\nfor generating legal diagrams and evaluated them with these metrics, including\nlegal content-based evaluation within 23 languages. Models trained with\nLegalViz outperform existing models including GPTs, confirming the\neffectiveness of our dataset.\n","authors":["Eri Onami","Taiki Miyanishi","Koki Maeda","Shuhei Kurita"],"pdf_url":"https://arxiv.org/pdf/2502.06147v2.pdf","comment":"NAACL2025"},{"id":"http://arxiv.org/abs/2501.19093v2","updated":"2025-02-13T05:32:21Z","published":"2025-01-31T12:39:28Z","title":"Improving Low-Resource Sequence Labeling with Knowledge Fusion and\n  Contextual Label Explanations","summary":"  Sequence labeling remains a significant challenge in low-resource,\ndomain-specific scenarios, particularly for character-dense languages like\nChinese. Existing methods primarily focus on enhancing model comprehension and\nimproving data diversity to boost performance. However, these approaches still\nstruggle with inadequate model applicability and semantic distribution biases\nin domain-specific contexts. To overcome these limitations, we propose a novel\nframework that combines an LLM-based knowledge enhancement workflow with a\nspan-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model.\nOur workflow employs explanation prompts to generate precise contextual\ninterpretations of target entities, effectively mitigating semantic biases and\nenriching the model's contextual understanding. The KnowFREE model further\nintegrates extension label features, enabling efficient nested entity\nextraction without relying on external knowledge during inference. Experiments\non multiple Chinese domain-specific sequence labeling datasets demonstrate that\nour approach achieves state-of-the-art performance, effectively addressing the\nchallenges posed by low-resource settings.\n","authors":["Peichao Lai","Jiaxin Gan","Feiyang Ye","Yilei Wang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2501.19093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08972v1","updated":"2025-02-13T05:20:21Z","published":"2025-02-13T05:20:21Z","title":"Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context\n  Learning","summary":"  Language models are aligned to the collective voice of many, resulting in\ngeneric outputs that do not align with specific users' styles. In this work, we\npresent Trial-Error-Explain In-Context Learning (TICL), a tuning-free method\nthat personalizes language models for text generation tasks with fewer than 10\nexamples per user. TICL iteratively expands an in-context learning prompt via a\ntrial-error-explain process, adding model-generated negative samples and\nexplanations that provide fine-grained guidance towards a specific user's\nstyle. TICL achieves favorable win rates on pairwise comparisons with\nLLM-as-a-judge up to 91.5% against the previous state-of-the-art and\noutperforms competitive tuning-free baselines for personalized alignment tasks\nof writing emails, essays and news articles. Both lexical and qualitative\nanalyses show that the negative samples and explanations enable language models\nto learn stylistic context more effectively and overcome the bias towards\nstructural and formal phrases observed in their zero-shot outputs. By\nfront-loading inference compute to create a user-specific in-context learning\nprompt that does not require extra generation steps at test time, TICL presents\na novel yet simple approach for personalized alignment.\n","authors":["Hyundong Cho","Karishma Sharma","Nicolaas Jedema","Leonardo F. R. Ribeiro","Alessandro Moschitti","Ravi Krishnan","Jonathan May"],"pdf_url":"https://arxiv.org/pdf/2502.08972v1.pdf","comment":"NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.02494v2","updated":"2025-02-13T05:14:49Z","published":"2025-02-04T17:09:44Z","title":"Analyzing Similarity Metrics for Data Selection for Language Model\n  Pretraining","summary":"  Similarity between training examples is used to curate pretraining datasets\nfor language models by many methods -- for diversification and to select\nexamples similar to high-quality data. However, similarity is typically\nmeasured with off-the-shelf embedding models that are generic or trained for\ntasks such as retrieval. This paper introduces a framework to analyze the\nsuitability of embedding models specifically for data curation in the language\nmodel pretraining setting. We quantify the correlation between similarity in\nthe embedding space to similarity in pretraining loss between different\ntraining examples, and how diversifying in the embedding space affects\npretraining quality. We analyze a variety of embedding models in our framework,\nwith experiments using the Pile dataset for pretraining a 1.7B parameter\ndecoder-only language model. We find that the embedding models we consider are\nall useful for pretraining data curation. Moreover, a simple approach of\naveraging per-token embeddings proves to be surprisingly competitive with more\nsophisticated embedding models -- likely because the latter are not designed\nspecifically for pretraining data curation. Indeed, we believe our analysis and\nevaluation framework can serve as a foundation for the design of embedding\nmodels that specifically reason about similarity in pretraining datasets.\n","authors":["Dylan Sam","Ayan Chakrabarti","Afshin Rostamizadeh","Srikumar Ramalingam","Gui Citovsky","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.02494v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.07058v2","updated":"2025-02-13T04:55:27Z","published":"2025-02-10T21:49:35Z","title":"Using Contextually Aligned Online Reviews to Measure LLMs' Performance\n  Disparities Across Language Varieties","summary":"  A language can have different varieties. These varieties can affect the\nperformance of natural language processing (NLP) models, including large\nlanguage models (LLMs), which are often trained on data from widely spoken\nvarieties. This paper introduces a novel and cost-effective approach to\nbenchmark model performance across language varieties. We argue that\ninternational online review platforms, such as Booking.com, can serve as\neffective data sources for constructing datasets that capture comments in\ndifferent language varieties from similar real-world scenarios, like reviews\nfor the same hotel with the same rating using the same language (e.g., Mandarin\nChinese) but different language varieties (e.g., Taiwan Mandarin, Mainland\nMandarin). To prove this concept, we constructed a contextually aligned dataset\ncomprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs\nin a sentiment analysis task. Our results show that LLMs consistently\nunderperform in Taiwan Mandarin.\n","authors":["Zixin Tang","Chieh-Yang Huang","Tsung-Chi Li","Ho Yin Sam Ng","Hen-Hsen Huang","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07058v2.pdf","comment":"Accepted by 2025 Annual Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics (NAACL), theme track"},{"id":"http://arxiv.org/abs/2502.08954v1","updated":"2025-02-13T04:35:55Z","published":"2025-02-13T04:35:55Z","title":"Medicine on the Edge: Comparative Performance Analysis of On-Device LLMs\n  for Clinical Reasoning","summary":"  The deployment of Large Language Models (LLM) on mobile devices offers\nsignificant potential for medical applications, enhancing privacy, security,\nand cost-efficiency by eliminating reliance on cloud-based services and keeping\nsensitive health data local. However, the performance and accuracy of on-device\nLLMs in real-world medical contexts remain underexplored. In this study, we\nbenchmark publicly available on-device LLMs using the AMEGA dataset, evaluating\naccuracy, computational efficiency, and thermal limitation across various\nmobile devices. Our results indicate that compact general-purpose models like\nPhi-3 Mini achieve a strong balance between speed and accuracy, while medically\nfine-tuned models such as Med42 and Aloe attain the highest accuracy. Notably,\ndeploying LLMs on older devices remains feasible, with memory constraints\nposing a greater challenge than raw processing power. Our study underscores the\npotential of on-device LLMs for healthcare while emphasizing the need for more\nefficient inference and models tailored to real-world clinical reasoning.\n","authors":["Leon Nissen","Philipp Zagar","Vishnu Ravi","Aydin Zahedivash","Lara Marie Reimer","Stephan Jonas","Oliver Aalami","Paul Schmiedmayer"],"pdf_url":"https://arxiv.org/pdf/2502.08954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08947v1","updated":"2025-02-13T04:01:54Z","published":"2025-02-13T04:01:54Z","title":"Structured Convergence in Large Language Model Representations via\n  Hierarchical Latent Space Folding","summary":"  Token representations in high-dimensional latent spaces often exhibit\nredundancy, limiting computational efficiency and reducing structural coherence\nacross model layers. Hierarchical latent space folding introduces a structured\ntransformation mechanism that enforces a multi-scale organization within\nlearned embeddings, refining representational compactness while preserving\nessential contextual distinctions. The proposed approach incorporates dynamic\nfolding operations that iteratively adjust token embeddings through structured\ntransformations, influencing both short-range and long-range dependencies in\nsequential processing tasks. Empirical evaluation demonstrates a reduction in\nrepresentational variance across layers, contributing to more stable perplexity\ndistributions and enhancing predictive confidence in text generation. The\nstructured redistribution of attention head utilization leads to more efficient\nallocation of computational resources, particularly in deeper layers, where\nhierarchical refinements improve contextual abstraction. Comparative analysis\nof activation sparsity patterns suggests that hierarchical adjustments\nselectively reinforce critical pathways while reducing computational overhead\nin non-essential regions of the model. Statistical assessments of token\nreordering frequencies reveal that hierarchical modifications introduce subtle\nshifts in sequential dependencies, improving contextual alignment while\nmaintaining syntactic correctness. Computational trade-offs associated with\nhierarchical folding introduce marginal increases in training time per epoch,\nyet empirical findings indicate that inference efficiency benefits from the\nstructured representation adjustments. The results highlight the impact of\nhierarchical latent space folding on optimizing model performance through\nimproved representation structuring and computational efficiency.\n","authors":["Fenella Harcourt","Naderdel Piero","Gilbert Sutherland","Daphne Holloway","Harriet Bracknell","Julian Ormsby"],"pdf_url":"https://arxiv.org/pdf/2502.08947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08946v1","updated":"2025-02-13T04:00:03Z","published":"2025-02-13T04:00:03Z","title":"The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of\n  Physical Concept Understanding","summary":"  In a systematic way, we investigate a widely asked question: Do LLMs really\nunderstand what they say?, which relates to the more familiar term Stochastic\nParrot. To this end, we propose a summative assessment over a carefully\ndesigned physical concept understanding task, PhysiCo. Our task alleviates the\nmemorization issue via the usage of grid-format inputs that abstractly describe\nphysical phenomena. The grids represents varying levels of understanding, from\nthe core phenomenon, application examples to analogies to other abstract\npatterns in the grid world. A comprehensive study on our task demonstrates: (1)\nstate-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag\nbehind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,\nas they fail on our grid task but can describe and recognize the same concepts\nwell in natural language; (3) our task challenges the LLMs due to intrinsic\ndifficulties rather than the unfamiliar grid format, as in-context learning and\nfine-tuning on same formatted data added little to their performance.\n","authors":["Mo Yu","Lemao Liu","Junjie Wu","Tsz Ting Chung","Shunchi Zhang","Jiangnan Li","Dit-Yan Yeung","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08946v1.pdf","comment":"NAACL 2025 Main Conference. First 5 authors contributed equally.\n  Project page: https://physico-benchmark.github.io/"},{"id":"http://arxiv.org/abs/2502.08943v1","updated":"2025-02-13T03:43:33Z","published":"2025-02-13T03:43:33Z","title":"Beyond the Singular: The Essential Role of Multiple Generations in\n  Effective Benchmark Evaluation and Analysis","summary":"  Large language models (LLMs) have demonstrated significant utilities in\nreal-world applications, exhibiting impressive capabilities in natural language\nprocessing and understanding. Benchmark evaluations are crucial for assessing\nthe capabilities of LLMs as they can provide a comprehensive assessment of\ntheir strengths and weaknesses. However, current evaluation methods often\noverlook the inherent randomness of LLMs by employing deterministic generation\nstrategies or relying on a single random sample, resulting in unaccounted\nsampling variance and unreliable benchmark score estimates. In this paper, we\npropose a hierarchical statistical model that provides a more comprehensive\nrepresentation of the benchmarking process by incorporating both benchmark\ncharacteristics and LLM randomness. We show that leveraging multiple\ngenerations improves the accuracy of estimating the benchmark score and reduces\nvariance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a\nprompt-level difficulty score based on correct ratios, providing fine-grained\ninsights into individual prompts. Additionally, we create a data map that\nvisualizes difficulty and semantic prompts, enabling error detection and\nquality control in benchmark construction.\n","authors":["Wenbo Zhang","Hengrui Cai","Wenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08943v1.pdf","comment":"10 pages, 1 table, 4 Figures"},{"id":"http://arxiv.org/abs/2502.08924v1","updated":"2025-02-13T03:20:37Z","published":"2025-02-13T03:20:37Z","title":"Escaping Collapse: The Strength of Weak Data for Large Language Model\n  Training","summary":"  Synthetically-generated data plays an increasingly larger role in training\nlarge language models. However, while synthetic data has been found to be\nuseful, studies have also shown that without proper curation it can cause LLM\nperformance to plateau, or even \"collapse\", after many training iterations. In\nthis paper, we formalize this question and develop a theoretical framework to\ninvestigate how much curation is needed in order to ensure that LLM performance\ncontinually improves. We find that the requirements are nearly minimal. We\ndescribe a training procedure that converges to an optimal LLM even if almost\nall of the non-synthetic training data is of poor quality. Our analysis is\ninspired by boosting, a classic machine learning technique that leverages a\nvery weak learning algorithm to produce an arbitrarily good classifier. Our\ntraining procedure subsumes many recently proposed methods for training LLMs on\nsynthetic data, and thus our analysis sheds light on why they are successful,\nand also suggests opportunities for future improvement. We present experiments\nthat validate our theory, and show that dynamically focusing labeling resources\non the most challenging examples -- in much the same way that boosting focuses\nthe efforts of the weak learner -- leads to improved performance.\n","authors":["Kareem Amin","Sara Babakniya","Alex Bie","Weiwei Kong","Umar Syed","Sergei Vassilvitskii"],"pdf_url":"https://arxiv.org/pdf/2502.08924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08923v1","updated":"2025-02-13T03:19:50Z","published":"2025-02-13T03:19:50Z","title":"CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without\n  Compromising Quality","summary":"  We introduce CopySpec, an innovative technique designed to tackle the\ninefficiencies LLMs face when generating responses that closely resemble\nprevious outputs. CopySpec identifies repeated sequences in the model's chat\nhistory and speculates that the same tokens will follow, enabling seamless\ncopying without compromising output quality or requiring additional GPU memory.\nTo evaluate the effectiveness of our approach, we conducted experiments using\nfive LLMs and five datasets: MT-Bench, CNN/DM, GSM-8K, HumanEval, and our newly\ncreated dataset, MT-Redundant. MT-Redundant, introduced in this paper,\ntransforms the second turn of MT-Bench into a request for variations of the\nfirst turn's answer, simulating real-world scenarios where users request\nmodifications to prior responses. Our results demonstrate significant\nspeed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select\nMT-Redundant categories, and 2.66x on the third turn of GSM-8K's\nself-correction tasks. Moreover, we show that CopySpec integrates seamlessly\nwith speculative decoding, yielding an average 49% additional speed-up over\nspeculative decoding for the second turn of MT-Redundant across all eight\ncategories. While LLMs, even with speculative decoding, suffer from slower\ninference as context sizes grow, CopySpec leverages the expanded context to\naccelerate inference, making it faster as the context size increases. Our code\nand dataset are publicly available at https://github.com/RazvanDu/CopySpec.\n","authors":["Razvan-Gabriel Dumitru","Minglai Yang","Vikas Yadav","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2502.08923v1.pdf","comment":"33 pages, 18 figures, 19 tables"},{"id":"http://arxiv.org/abs/2502.08916v1","updated":"2025-02-13T03:08:02Z","published":"2025-02-13T03:08:02Z","title":"PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic\n  Decision-Making Applied to Histopathology","summary":"  Diagnosing diseases through histopathology whole slide images (WSIs) is\nfundamental in modern pathology but is challenged by the gigapixel scale and\ncomplexity of WSIs. Trained histopathologists overcome this challenge by\nnavigating the WSI, looking for relevant patches, taking notes, and compiling\nthem to produce a final holistic diagnostic. Traditional AI approaches, such as\nmultiple instance learning and transformer-based models, fail short of such a\nholistic, iterative, multi-scale diagnostic procedure, limiting their adoption\nin the real-world. We introduce PathFinder, a multi-modal, multi-agent\nframework that emulates the decision-making process of expert pathologists.\nPathFinder integrates four AI agents, the Triage Agent, Navigation Agent,\nDescription Agent, and Diagnosis Agent, that collaboratively navigate WSIs,\ngather evidence, and provide comprehensive diagnoses with natural language\nexplanations. The Triage Agent classifies the WSI as benign or risky; if risky,\nthe Navigation and Description Agents iteratively focus on significant regions,\ngenerating importance maps and descriptive insights of sampled patches.\nFinally, the Diagnosis Agent synthesizes the findings to determine the\npatient's diagnostic classification. Our Experiments show that PathFinder\noutperforms state-of-the-art methods in skin melanoma diagnosis by 8% while\noffering inherent explainability through natural language descriptions of\ndiagnostically relevant patches. Qualitative analysis by pathologists shows\nthat the Description Agent's outputs are of high quality and comparable to\nGPT-4o. PathFinder is also the first AI-based system to surpass the average\nperformance of pathologists in this challenging melanoma classification task by\n9%, setting a new record for efficient, accurate, and interpretable AI-assisted\ndiagnostics in pathology. Data, code and models available at\nhttps://pathfinder-dx.github.io/\n","authors":["Fatemeh Ghezloo","Mehmet Saygin Seyfioglu","Rustin Soraki","Wisdom O. Ikezogwo","Beibin Li","Tejoram Vivekanandan","Joann G. Elmore","Ranjay Krishna","Linda Shapiro"],"pdf_url":"https://arxiv.org/pdf/2502.08916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02075v2","updated":"2025-02-13T02:57:47Z","published":"2023-07-05T07:32:34Z","title":"Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for\n  Entity Alignment","summary":"  Entity alignment (EA) aims at identifying equivalent entity pairs across\ndifferent knowledge graphs (KGs) that refer to the same real-world identity. To\nsystematically combat confirmation bias for pseudo-labeling-based entity\nalignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment\n(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the\naccuracy of entity alignment. UPL-EA consists of two complementary components:\n(1) The Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling\nas an effective means to enable more accurate determination of entity\ncorrespondences across two KGs and to mitigate the adverse impact of erroneous\nmatches. A simple but highly effective criterion is further devised to derive\npseudo-labeled entity pairs that satisfy one-to-one correspondences at each\niteration. (2) The cross-iteration pseudo-label calibration operates across\nmultiple consecutive iterations to further improve the pseudo-labeling\nprecision rate by reducing the local pseudo-label selection variability with a\ntheoretical guarantee. The two components are respectively designed to\neliminate Type I and Type II pseudo-labeling errors identified through our\nanalyse. The calibrated pseudo-labels are thereafter used to augment prior\nalignment seeds to reinforce subsequent model training for alignment inference.\nThe effectiveness of UPL-EA in eliminating pseudo-labeling errors is both\ntheoretically supported and experimentally validated. The experimental results\nshow that our approach achieves competitive performance with limited prior\nalignment seeds.\n","authors":["Qijie Ding","Jie Yin","Daokun Zhang","Junbin Gao"],"pdf_url":"https://arxiv.org/pdf/2307.02075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08910v1","updated":"2025-02-13T02:52:01Z","published":"2025-02-13T02:52:01Z","title":"InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU","summary":"  In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.\n","authors":["Heejun Lee","Geon Park","Jaduk Suh","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2502.08910v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2502.08909v1","updated":"2025-02-13T02:51:17Z","published":"2025-02-13T02:51:17Z","title":"Towards Automated Fact-Checking of Real-World Claims: Exploring Task\n  Formulation and Assessment with LLMs","summary":"  Fact-checking is necessary to address the increasing volume of\nmisinformation. Traditional fact-checking relies on manual analysis to verify\nclaims, but it is slow and resource-intensive. This study establishes baseline\ncomparisons for Automated Fact-Checking (AFC) using Large Language Models\n(LLMs) across multiple labeling schemes (binary, three-class, five-class) and\nextends traditional claim verification by incorporating analysis, verdict\nclassification, and explanation in a structured setup to provide comprehensive\njustifications for real-world claims. We evaluate Llama-3 models of varying\nsizes (3B, 8B, 70B) on 17,856 claims collected from PolitiFact (2007-2024)\nusing evidence retrieved via restricted web searches. We utilize TIGERScore as\na reference-free evaluation metric to score the justifications. Our results\nshow that larger LLMs consistently outperform smaller LLMs in classification\naccuracy and justification quality without fine-tuning. We find that smaller\nLLMs in a one-shot scenario provide comparable task performance to fine-tuned\nSmall Language Models (SLMs) with large context sizes, while larger LLMs\nconsistently surpass them. Evidence integration improves performance across all\nmodels, with larger LLMs benefiting most. Distinguishing between nuanced labels\nremains challenging, emphasizing the need for further exploration of labeling\nschemes and alignment with evidences. Our findings demonstrate the potential of\nretrieval-augmented AFC with LLMs.\n","authors":["Premtim Sahitaj","Iffat Maab","Junichi Yamagishi","Jawan Kolanowski","Sebastian Möller","Vera Schmitt"],"pdf_url":"https://arxiv.org/pdf/2502.08909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09213v2","updated":"2025-02-13T02:44:07Z","published":"2025-01-16T00:19:19Z","title":"FineMedLM-o1: Enhancing the Medical Reasoning Ability of LLM from\n  Supervised Fine-Tuning to Test-Time Training","summary":"  Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the advanced reasoning required for\ncomplex clinical scenarios, such as differential diagnosis or personalized\ntreatment suggestions. We proposed FineMedLM-o1, which leverages high-quality\nsynthetic medical data and long-form reasoning data for Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and\ndeep reasoning capabilities. Additionally, we introduced Test-Time Training\n(TTT) in the medical domain for the first time, facilitating domain adaptation\nand ensuring reliable, accurate reasoning. Experimental results demonstrate\nthat FineMedLM-o1 achieves a 23% average performance improvement over prior\nmodels on key medical benchmarks. Furthermore, the introduction of TTT provides\nan additional 14% performance boost, highlighting its effectiveness in\nenhancing medical reasoning capabilities. To support this process, we also\nproposed a novel method for synthesizing medical dialogue. Compared to other\nopen-source datasets, our dataset stands out as superior in both quality and\ncomplexity. The project and data will be released on GitHub.\n","authors":["Hongzhou Yu","Tianhao Cheng","Ying Cheng","Rui Feng"],"pdf_url":"https://arxiv.org/pdf/2501.09213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18409v4","updated":"2025-02-13T02:37:06Z","published":"2024-02-28T15:28:36Z","title":"A Cognitive Evaluation Benchmark of Image Reasoning and Description for\n  Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the Cookie Theft task in human cognitive tests, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive abilities of LVLMs\nusing images with rich semantics. The benchmark consists of 251 images along\nwith comprehensive annotations. It defines eight reasoning capabilities and\ncomprises an image description task and a visual question answering task. Our\nevaluation of well-known LVLMs shows that there is still a significant gap in\ncognitive abilities between LVLMs and humans.\n","authors":["Xiujie Song","Mengyue Wu","Kenny Q. Zhu","Chunhao Zhang","Yanyi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.18409v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02315v2","updated":"2025-02-13T02:33:32Z","published":"2025-02-04T13:36:54Z","title":"VaiBot: Shuttle Between the Instructions and Parameters of Large\n  Language Models","summary":"  How to interact with LLMs through \\emph{instructions} has been widely studied\nby researchers. However, previous studies have treated the emergence of\ninstructions and the training of LLMs on task data as separate processes,\noverlooking the inherent unity between the two. This paper proposes a neural\nnetwork framework, VaiBot, that integrates VAE and VIB, designed to uniformly\nmodel, learn, and infer both deduction and induction tasks under LLMs. Through\nexperiments, we demonstrate that VaiBot performs on par with existing baseline\nmethods in terms of deductive capabilities while significantly surpassing them\nin inductive capabilities. We also find that VaiBot can scale up using general\ninstruction-following data and exhibits excellent one-shot induction abilities.\nWe finally synergistically integrate the deductive and inductive processes of\nVaiBot. Through T-SNE dimensionality reduction, we observe that its\ninductive-deductive process significantly improves the distribution of training\nparameters, enabling it to outperform baseline methods in inductive reasoning\ntasks. The code and data for this paper can be found at\nhttps://anonymous.4open.science/r/VaiBot-021F.\n","authors":["Wangtao Sun","Haotian Xu","Huanxuan Liao","Xuanqing Yu","Zhongtao Jiang","Shizhu He","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.02315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08900v1","updated":"2025-02-13T02:27:30Z","published":"2025-02-13T02:27:30Z","title":"Can Uniform Meaning Representation Help GPT-4 Translate from Indigenous\n  Languages?","summary":"  While ChatGPT and GPT-based models are able to effectively perform many tasks\nwithout additional fine-tuning, they struggle with related to extremely\nlow-resource languages and indigenous languages. Uniform Meaning Representation\n(UMR), a semantic representation designed to capture the meaning of texts in\nmany languages, is well-poised to be leveraged in the development of\nlow-resource language technologies. In this work, we explore the downstream\ntechnical utility of UMR for low-resource languages by incorporating it into\nGPT-4 prompts. Specifically, we examine the ability of GPT-4 to perform\ntranslation from three indigenous languages (Navajo, Ar\\'apaho, and Kukama),\nwith and without demonstrations, as well as with and without UMR annotations.\nUltimately we find that in the majority of our test cases, integrating UMR into\nthe prompt results in a statistically significant increase in performance,\nwhich is a promising indication of future applications of the UMR formalism.\n","authors":["Shira Wein"],"pdf_url":"https://arxiv.org/pdf/2502.08900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08896v1","updated":"2025-02-13T02:22:48Z","published":"2025-02-13T02:22:48Z","title":"Communication is All You Need: Persuasion Dataset Construction via\n  Multi-LLM Communication","summary":"  Large Language Models (LLMs) have shown proficiency in generating persuasive\ndialogue, yet concerns about the fluency and sophistication of their outputs\npersist. This paper presents a multi-LLM communication framework designed to\nenhance the generation of persuasive data automatically. This framework\nfacilitates the efficient production of high-quality, diverse linguistic\ncontent with minimal human oversight. Through extensive evaluations, we\ndemonstrate that the generated data excels in naturalness, linguistic\ndiversity, and the strategic use of persuasion, even in complex scenarios\ninvolving social taboos. The framework also proves adept at generalizing across\nnovel contexts. Our results highlight the framework's potential to\nsignificantly advance research in both computational and social science domains\nconcerning persuasive communication.\n","authors":["Weicheng Ma","Hefan Zhang","Ivory Yang","Shiyu Ji","Joice Chen","Farnoosh Hashemi","Shubham Mohole","Ethan Gearey","Michael Macy","Saeed Hassanpour","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2502.08896v1.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.08888v1","updated":"2025-02-13T02:03:30Z","published":"2025-02-13T02:03:30Z","title":"LLM-Enhanced Multiple Instance Learning for Joint Rumor and Stance\n  Detection with Social Context Information","summary":"  The proliferation of misinformation, such as rumors on social media, has\ndrawn significant attention, prompting various expressions of stance among\nusers. Although rumor detection and stance detection are distinct tasks, they\ncan complement each other. Rumors can be identified by cross-referencing\nstances in related posts, and stances are influenced by the nature of the\nrumor. However, existing stance detection methods often require post-level\nstance annotations, which are costly to obtain. We propose a novel LLM-enhanced\nMIL approach to jointly predict post stance and claim class labels, supervised\nsolely by claim labels, using an undirected microblog propagation model. Our\nweakly supervised approach relies only on bag-level labels of claim veracity,\naligning with multi-instance learning (MIL) principles. To achieve this, we\ntransform the multi-class problem into multiple MIL-based binary classification\nproblems. We then employ a discriminative attention layer to aggregate the\noutputs from these classifiers into finer-grained classes. Experiments\nconducted on three rumor datasets and two stance datasets demonstrate the\neffectiveness of our approach, highlighting strong connections between rumor\nveracity and expressed stances in responding posts. Our method shows promising\nperformance in joint rumor and stance detection compared to the\nstate-of-the-art methods.\n","authors":["Ruichao Yang","Jing Ma","Wei Gao","Hongzhan Lin"],"pdf_url":"https://arxiv.org/pdf/2502.08888v1.pdf","comment":"Accepted by ACM TIST"},{"id":"http://arxiv.org/abs/2501.02844v2","updated":"2025-02-13T01:58:12Z","published":"2025-01-06T08:43:31Z","title":"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification","summary":"  Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.\n","authors":["Yubo Wang","Haoyang Li","Fei Teng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.02844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07972v2","updated":"2025-02-13T01:23:29Z","published":"2025-02-11T21:36:31Z","title":"Training Sparse Mixture Of Experts Text Embedding Models","summary":"  Transformer-based text embedding models have improved their performance on\nbenchmarks like MIRACL and BEIR by increasing their parameter counts. However,\nthis scaling approach introduces significant deployment challenges, including\nincreased inference latency and memory usage. These challenges are particularly\nsevere in retrieval-augmented generation (RAG) applications, where large\nmodels' increased memory requirements constrain dataset ingestion capacity, and\ntheir higher latency directly impacts query-time performance. While causal\nlanguage models have addressed similar efficiency challenges using Mixture of\nExperts (MoE) architectures, this approach hasn't been successfully adapted to\nthe general text embedding setting. In this paper, we introduce Nomic Embed v2,\nthe first general purpose MoE text embedding model. Our model outperforms\nmodels in the same parameter class on both monolingual and multilingual\nbenchmarks while also maintaining competitive performance with models twice its\nsize. We open-source all code, models, and evaluation data to ensure full\nreproducibility of our training pipeline at\n\\href{https://github.com/nomic-ai/contrastors}{https://github.com/nomic-ai/contrastors}.\n","authors":["Zach Nussbaum","Brandon Duderstadt"],"pdf_url":"https://arxiv.org/pdf/2502.07972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08866v1","updated":"2025-02-13T00:37:27Z","published":"2025-02-13T00:37:27Z","title":"BrainWavLM: Fine-tuning Speech Representations with Brain Responses to\n  Language","summary":"  Speech encoding models use auditory representations to predict how the human\nbrain responds to spoken language stimuli. Most performant encoding models\nlinearly map the hidden states of artificial neural networks to brain data, but\nthis linear restriction may limit their effectiveness. In this work, we use\nlow-rank adaptation (LoRA) to fine-tune a WavLM-based encoding model end-to-end\non a brain encoding objective, producing a model we name BrainWavLM. We show\nthat fine-tuning across all of cortex improves average encoding performance\nwith greater stability than without LoRA. This improvement comes at the expense\nof low-level regions like auditory cortex (AC), but selectively fine-tuning on\nthese areas improves performance in AC, while largely retaining gains made in\nthe rest of cortex. Fine-tuned models generalized across subjects, indicating\nthat they learned robust brain-like representations of the speech stimuli.\nFinally, by training linear probes, we showed that the brain data strengthened\nsemantic representations in the speech model without any explicit annotations.\nOur results demonstrate that brain fine-tuning produces best-in-class speech\nencoding models, and that non-linear methods have the potential to bridge the\ngap between artificial and biological representations of semantics.\n","authors":["Nishitha Vattikonda","Aditya R. Vaidya","Richard J. Antonello","Alexander G. Huth"],"pdf_url":"https://arxiv.org/pdf/2502.08866v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.08859v1","updated":"2025-02-13T00:18:34Z","published":"2025-02-13T00:18:34Z","title":"EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges","summary":"  As language models master existing reasoning benchmarks, we need new\nchallenges to evaluate their cognitive frontiers. Puzzle-solving events are\nrich repositories of challenging multimodal problems that test a wide range of\nadvanced reasoning and knowledge capabilities, making them a unique testbed for\nevaluating frontier language models. We introduce EnigmaEval, a dataset of\nproblems and solutions derived from puzzle competitions and events that probes\nmodels' ability to perform implicit knowledge synthesis and multi-step\ndeductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle\nsolving challenges models to discover hidden connections between seemingly\nunrelated pieces of information to uncover solution paths. The benchmark\ncomprises 1184 puzzles of varying complexity -- each typically requiring teams\nof skilled solvers hours to days to complete -- with unambiguous, verifiable\nsolutions that enable efficient evaluation. State-of-the-art language models\nachieve extremely low accuracy on these puzzles, even lower than other\ndifficult benchmarks such as Humanity's Last Exam, unveiling models'\nshortcomings when challenged with problems requiring unstructured and lateral\nreasoning.\n","authors":["Clinton J. Wang","Dean Lee","Cristina Menghini","Johannes Mols","Jack Doughty","Adam Khoja","Jayson Lynch","Sean Hendryx","Summer Yue","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08859v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.09623v1","updated":"2025-02-13T18:59:50Z","published":"2025-02-13T18:59:50Z","title":"Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF\n  Architectures","summary":"  Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for\nrepresenting 3D objects and scenes by encoding shape and appearance information\ninto the weights of a neural network. Recent works have shown how such weights\ncan be used as input to frameworks processing them to solve deep learning\ntasks. Yet, these frameworks can only process NeRFs with a specific, predefined\narchitecture. In this paper, we present the first framework that can ingest\nNeRFs with multiple architectures and perform inference on architectures unseen\nat training time. We achieve this goal by training a Graph Meta-Network in a\nrepresentation learning framework. Moreover, we show how a contrastive\nobjective is conducive to obtaining an architecture-agnostic latent space. In\nexperiments on both MLP-based and tri-planar NeRFs, our approach demonstrates\nrobust performance in classification and retrieval tasks that either matches or\nexceeds that of existing frameworks constrained to single architectures, thus\nproviding the first architecture-agnostic method to perform tasks on NeRFs by\nprocessing their weights.\n","authors":["Francesco Ballerini","Pierluigi Zama Ramirez","Samuele Salti","Luigi Di Stefano"],"pdf_url":"https://arxiv.org/pdf/2502.09623v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.09621v1","updated":"2025-02-13T18:59:46Z","published":"2025-02-13T18:59:46Z","title":"MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency","summary":"  Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/\n","authors":["Dongzhi Jiang","Renrui Zhang","Ziyu Guo","Yanwei Li","Yu Qi","Xinyan Chen","Liuhui Wang","Jianhan Jin","Claire Guo","Shen Yan","Bo Zhang","Chaoyou Fu","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2502.09621v1.pdf","comment":"Project Page: https://mmecot.github.io/"},{"id":"http://arxiv.org/abs/2502.09620v1","updated":"2025-02-13T18:59:45Z","published":"2025-02-13T18:59:45Z","title":"Exploring the Potential of Encoder-free Architectures in 3D LMMs","summary":"  Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL\n","authors":["Yiwen Tang","Zoey Guo","Zhuhao Wang","Ray Zhang","Qizhi Chen","Junli Liu","Delin Qu","Zhigang Wang","Dong Wang","Xuelong Li","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.09620v1.pdf","comment":"The code is released at https://github.com/Ivan-Tang-3D/ENEL"},{"id":"http://arxiv.org/abs/2502.09619v1","updated":"2025-02-13T18:59:44Z","published":"2025-02-13T18:59:44Z","title":"Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights","summary":"  With the increasing numbers of publicly available models, there are probably\npretrained, online models for most tasks users require. However, current model\nsearch methods are rudimentary, essentially a text-based search in the\ndocumentation, thus users cannot find the relevant models. This paper presents\nProbeLog, a method for retrieving classification models that can recognize a\ntarget concept, such as \"Dog\", without access to model metadata or training\ndata. Differently from previous probing methods, ProbeLog computes a descriptor\nfor each output dimension (logit) of each model, by observing its responses on\na fixed set of inputs (probes). Our method supports both logit-based retrieval\n(\"find more logits like this\") and zero-shot, text-based retrieval (\"find all\nlogits corresponding to dogs\"). As probing-based representations require\nmultiple costly feedforward passes through the model, we develop a method,\nbased on collaborative filtering, that reduces the cost of encoding\nrepositories by 3x. We demonstrate that ProbeLog achieves high retrieval\naccuracy, both in real-world and fine-grained search tasks and is scalable to\nfull-size repositories.\n","authors":["Jonathan Kahana","Or Nathan","Eliahu Horwitz","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2502.09619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09617v1","updated":"2025-02-13T18:59:19Z","published":"2025-02-13T18:59:19Z","title":"LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback\n  Over Multi-Resolution Gaussians-on-Mesh","summary":"  Generalizable rendering of an animatable human avatar from sparse inputs\nrelies on data priors and inductive biases extracted from training on large\ndata to avoid scene-specific optimization and to enable fast reconstruction.\nThis raises two main challenges: First, unlike iterative gradient-based\nadjustment in scene-specific optimization, generalizable methods must\nreconstruct the human shape representation in a single pass at inference time.\nSecond, rendering is preferably computationally efficient yet of high\nresolution. To address both challenges we augment the recently proposed dual\nshape representation, which combines the benefits of a mesh and Gaussian\npoints, in two ways. To improve reconstruction, we propose an iterative\nfeedback update framework, which successively improves the canonical human\nshape representation during reconstruction. To achieve computationally\nefficient yet high-resolution rendering, we study a coupled-multi-resolution\nGaussians-on-Mesh representation. We evaluate the proposed approach on the\nchallenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an\nanimatable representation from sparse inputs in less than 1s, renders views\nwith 95.1FPS at $1024 \\times 1024$, and achieves PSNR/LPIPS*/FID of\n24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in\nrendering quality.\n","authors":["Jing Wen","Alexander G. Schwing","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09617v1.pdf","comment":"ICLR 2025; Project page: https://wenj.github.io/LIFe-GoM/"},{"id":"http://arxiv.org/abs/2502.09616v1","updated":"2025-02-13T18:59:15Z","published":"2025-02-13T18:59:15Z","title":"Variational Rectified Flow Matching","summary":"  We study Variational Rectified Flow Matching, a framework that enhances\nclassic rectified flow matching by modeling multi-modal velocity vector-fields.\nAt inference time, classic rectified flow matching 'moves' samples from a\nsource distribution to the target distribution by solving an ordinary\ndifferential equation via integration along a velocity vector-field. At\ntraining time, the velocity vector-field is learnt by linearly interpolating\nbetween coupled samples one drawn from the source and one drawn from the target\ndistribution randomly. This leads to ''ground-truth'' velocity vector-fields\nthat point in different directions at the same location, i.e., the velocity\nvector-fields are multi-modal/ambiguous. However, since training uses a\nstandard mean-squared-error loss, the learnt velocity vector-field averages\n''ground-truth'' directions and isn't multi-modal. In contrast, variational\nrectified flow matching learns and samples from multi-modal flow directions. We\nshow on synthetic data, MNIST, CIFAR-10, and ImageNet that variational\nrectified flow matching leads to compelling results.\n","authors":["Pengsheng Guo","Alexander G. Schwing"],"pdf_url":"https://arxiv.org/pdf/2502.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09614v1","updated":"2025-02-13T18:59:13Z","published":"2025-02-13T18:59:13Z","title":"DexTrack: Towards Generalizable Neural Tracking Control for Dexterous\n  Manipulation from Human References","summary":"  We address the challenge of developing a generalizable neural tracking\ncontroller for dexterous manipulation from human references. This controller\naims to manage a dexterous robot hand to manipulate diverse objects for various\npurposes defined by kinematic human-object interactions. Developing such a\ncontroller is complicated by the intricate contact dynamics of dexterous\nmanipulation and the need for adaptivity, generalizability, and robustness.\nCurrent reinforcement learning and trajectory optimization methods often fall\nshort due to their dependence on task-specific rewards or precise system\nmodels. We introduce an approach that curates large-scale successful robot\ntracking demonstrations, comprising pairs of human references and robot\nactions, to train a neural controller. Utilizing a data flywheel, we\niteratively enhance the controller's performance, as well as the number and\nquality of successful tracking demonstrations. We exploit available tracking\ndemonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually\noptimize per-trajectory tracking by leveraging the learned tracking controller\nin a homotopy optimization method. The homotopy optimization, mimicking\nchain-of-thought, aids in solving challenging trajectory tracking problems to\nincrease demonstration diversity. We showcase our success by training a\ngeneralizable neural controller and evaluating it in both simulation and real\nworld. Our method achieves over a 10% improvement in success rates compared to\nleading baselines. The project website with animated results is available at\nhttps://meowuu7.github.io/DexTrack/.\n","authors":["Xueyi Liu","Jianibieke Adalibieke","Qianwei Han","Yuzhe Qin","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2502.09614v1.pdf","comment":"Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/\n  Code: https://github.com/Meowuu7/DexTrack/ Video:\n  https://youtu.be/zru1Z-DaiWE"},{"id":"http://arxiv.org/abs/2502.09615v1","updated":"2025-02-13T18:59:13Z","published":"2025-02-13T18:59:13Z","title":"RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets","summary":"  We present RigAnything, a novel autoregressive transformer-based model, which\nmakes 3D assets rig-ready by probabilistically generating joints, skeleton\ntopologies, and assigning skinning weights in a template-free manner. Unlike\nmost existing auto-rigging methods, which rely on predefined skeleton template\nand are limited to specific categories like humanoid, RigAnything approaches\nthe rigging problem in an autoregressive manner, iteratively predicting the\nnext joint based on the global input shape and the previous prediction. While\nautoregressive models are typically used to generate sequential data,\nRigAnything extends their application to effectively learn and represent\nskeletons, which are inherently tree structures. To achieve this, we organize\nthe joints in a breadth-first search (BFS) order, enabling the skeleton to be\ndefined as a sequence of 3D locations and the parent index. Furthermore, our\nmodel improves the accuracy of position prediction by leveraging diffusion\nmodeling, ensuring precise and consistent placement of joints within the\nhierarchy. This formulation allows the autoregressive model to efficiently\ncapture both spatial and hierarchical relationships within the skeleton.\nTrained end-to-end on both RigNet and Objaverse datasets, RigAnything\ndemonstrates state-of-the-art performance across diverse object types,\nincluding humanoids, quadrupeds, marine creatures, insects, and many more,\nsurpassing prior methods in quality, robustness, generalizability, and\nefficiency. Please check our website for more details:\nhttps://www.liuisabella.com/RigAnything.\n","authors":["Isabella Liu","Zhan Xu","Wang Yifan","Hao Tan","Zexiang Xu","Xiaolong Wang","Hao Su","Zifan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09615v1.pdf","comment":"Project page: https://www.liuisabella.com/RigAnything"},{"id":"http://arxiv.org/abs/2402.17767v2","updated":"2025-02-13T18:59:11Z","published":"2024-02-27T18:58:54Z","title":"Opening Articulated Objects in the Real World","summary":"  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated objects as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-objects/\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v2.pdf","comment":"Project webpage:\n  https://arjung128.github.io/opening-articulated-objects/"},{"id":"http://arxiv.org/abs/2502.09613v1","updated":"2025-02-13T18:59:09Z","published":"2025-02-13T18:59:09Z","title":"Latent Radiance Fields with 3D-aware 2D Representations","summary":"  Latent 3D reconstruction has shown great promise in empowering 3D semantic\nunderstanding and 3D generation by distilling 2D features into the 3D space.\nHowever, existing approaches struggle with the domain gap between 2D feature\nspace and 3D representations, resulting in degraded rendering performance. To\naddress this challenge, we propose a novel framework that integrates 3D\nawareness into the 2D latent space. The framework consists of three stages: (1)\na correspondence-aware autoencoding method that enhances the 3D consistency of\n2D latent representations, (2) a latent radiance field (LRF) that lifts these\n3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field\n(VAE-RF) alignment strategy that improves image decoding from the rendered 2D\nrepresentations. Extensive experiments demonstrate that our method outperforms\nthe state-of-the-art latent 3D reconstruction approaches in terms of synthesis\nperformance and cross-dataset generalizability across diverse indoor and\noutdoor scenes. To our knowledge, this is the first work showing the radiance\nfield representations constructed from 2D latent representations can yield\nphotorealistic 3D reconstruction performance.\n","authors":["Chaoyi Zhou","Xi Liu","Feng Luo","Siyu Huang"],"pdf_url":"https://arxiv.org/pdf/2502.09613v1.pdf","comment":"Accepted to ICLR 2025; Project page:\n  https://latent-radiance-field.github.io/LRF"},{"id":"http://arxiv.org/abs/2502.09611v1","updated":"2025-02-13T18:58:15Z","published":"2025-02-13T18:58:15Z","title":"Designing a Conditional Prior Distribution for Flow-Based Generative\n  Models","summary":"  Flow-based generative models have recently shown impressive performance for\nconditional generation tasks, such as text-to-image generation. However,\ncurrent methods transform a general unimodal noise distribution to a specific\nmode of the target data distribution. As such, every point in the initial\nsource distribution can be mapped to every point in the target distribution,\nresulting in long average paths. To this end, in this work, we tap into a\nnon-utilized property of conditional flow-based models: the ability to design a\nnon-trivial prior distribution. Given an input condition, such as a text\nprompt, we first map it to a point lying in data space, representing an\n``average\" data point with the minimal average distance to all data points of\nthe same conditional mode (e.g., class). We then utilize the flow matching\nformulation to map samples from a parametric distribution centered around this\npoint to the conditional target distribution. Experimentally, our method\nsignificantly improves training times and generation efficiency (FID, KID and\nCLIP alignment scores) compared to baselines, producing high quality samples\nusing fewer sampling steps.\n","authors":["Noam Issachar","Mohammad Salama","Raanan Fattal","Sagie Benaim"],"pdf_url":"https://arxiv.org/pdf/2502.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09608v1","updated":"2025-02-13T18:56:05Z","published":"2025-02-13T18:56:05Z","title":"Instance Segmentation of Scene Sketches Using Natural Image Priors","summary":"  Sketch segmentation involves grouping pixels within a sketch that belong to\nthe same object or instance. It serves as a valuable tool for sketch editing\ntasks, such as moving, scaling, or removing specific components. While image\nsegmentation models have demonstrated remarkable capabilities in recent years,\nsketches present unique challenges for these models due to their sparse nature\nand wide variation in styles. We introduce SketchSeg, a method for instance\nsegmentation of raster scene sketches. Our approach adapts state-of-the-art\nimage segmentation and object detection models to the sketch domain by\nemploying class-agnostic fine-tuning and refining segmentation masks using\ndepth cues. Furthermore, our method organizes sketches into sorted layers,\nwhere occluded instances are inpainted, enabling advanced sketch editing\napplications. As existing datasets in this domain lack variation in sketch\nstyles, we construct a synthetic scene sketch segmentation dataset featuring\nsketches with diverse brush strokes and varying levels of detail. We use this\ndataset to demonstrate the robustness of our approach and will release it to\npromote further research in the field.\n  Project webpage: https://sketchseg.github.io/sketch-seg/\n","authors":["Mia Tang","Yael Vinker","Chuan Yan","Lvmin Zhang","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2502.09608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09598v1","updated":"2025-02-13T18:52:14Z","published":"2025-02-13T18:52:14Z","title":"GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for\n  Remote Sensing Image Analysis","summary":"  The continuous operation of Earth-orbiting satellites generates vast and\never-growing archives of Remote Sensing (RS) images. Natural language presents\nan intuitive interface for accessing, querying, and interpreting the data from\nsuch archives. However, existing Vision-Language Models (VLMs) are\npredominantly trained on web-scraped, noisy image-text data, exhibiting limited\nexposure to the specialized domain of RS. This deficiency results in poor\nperformance on RS-specific tasks, as commonly used datasets often lack\ndetailed, scientifically accurate textual descriptions and instead emphasize\nsolely on attributes like date and location. To bridge this critical gap, we\nintroduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and\nmulti-modal RS image analysis. GAIA comprises of 205,150 meticulously curated\nRS image-text pairs, representing a diverse range of RS modalities associated\nto different spatial resolutions. Unlike existing vision-language datasets in\nRS, GAIA specifically focuses on capturing a diverse range of RS applications,\nproviding unique information about environmental changes, natural disasters,\nand various other dynamic phenomena. The dataset provides a spatially and\ntemporally balanced distribution, spanning across the globe, covering the last\n25 years with a balanced temporal distribution of observations. GAIA's\nconstruction involved a two-stage process: (1) targeted web-scraping of images\nand accompanying text from reputable RS-related sources, and (2) generation of\nfive high-quality, scientifically grounded synthetic captions for each image\nusing carefully crafted prompts that leverage the advanced vision-language\ncapabilities of GPT-4o. Our extensive experiments, including fine-tuning of\nCLIP and BLIP2 models, demonstrate that GAIA significantly improves performance\non RS image classification, cross-modal retrieval and image captioning tasks.\n","authors":["Angelos Zavras","Dimitrios Michail","Xiao Xiang Zhu","Begüm Demir","Ioannis Papoutsis"],"pdf_url":"https://arxiv.org/pdf/2502.09598v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.09573v1","updated":"2025-02-13T18:31:17Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09101v2","updated":"2025-02-13T18:20:14Z","published":"2024-11-14T00:18:04Z","title":"Heuristical Comparison of Vision Transformers Against Convolutional\n  Neural Networks for Semantic Segmentation on Remote Sensing Imagery","summary":"  Vision Transformers (ViT) have recently brought a new wave of research in the\nfield of computer vision. These models have performed particularly well in\nimage classification and segmentation. Research on semantic and instance\nsegmentation has accelerated with the introduction of the new architecture,\nwith over 80% of the top 20 benchmarks for the iSAID dataset based on either\nthe ViT architecture or the attention mechanism behind its success. This paper\nfocuses on the heuristic comparison of three key factors of using (or not\nusing) ViT for semantic segmentation of remote sensing aerial images on the\niSAID dataset. The experimental results observed during this research were\nanalyzed based on three objectives. First, we studied the use of a weighted\nfused loss function to maximize the mean Intersection over Union (mIoU) score\nand Dice score while minimizing entropy or class representation loss. Second,\nwe compared transfer learning on Meta's MaskFormer, a ViT-based semantic\nsegmentation model, against a generic UNet Convolutional Neural Network (CNN)\nbased on mIoU, Dice scores, training efficiency, and inference time. Third, we\nexamined the trade-offs between the two models in comparison to current\nstate-of-the-art segmentation models. We show that the novel combined weighted\nloss function significantly boosts the CNN model's performance compared to\ntransfer learning with ViT. The code for this implementation can be found at:\nhttps://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.\n","authors":["Ashim Dahal","Saydul Akbar Murad","Nick Rahimi"],"pdf_url":"https://arxiv.org/pdf/2411.09101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09564v1","updated":"2025-02-13T18:17:03Z","published":"2025-02-13T18:17:03Z","title":"Diffusing DeBias: a Recipe for Turning a Bug into a Feature","summary":"  Deep learning model effectiveness in classification tasks is often challenged\nby the quality and quantity of training data which, whenever containing strong\nspurious correlations between specific attributes and target labels, can result\nin unrecoverable biases in model predictions. Tackling these biases is crucial\nin improving model generalization and trust, especially in real-world\nscenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting\nas a plug-in for common methods in model debiasing while exploiting the\ninherent bias-learning tendency of diffusion models. Our approach leverages\nconditional diffusion models to generate synthetic bias-aligned images, used to\ntrain a bias amplifier model, to be further employed as an auxiliary method in\ndifferent unsupervised debiasing approaches. Our proposed method, which also\ntackles the common issue of training set memorization typical of this type of\ntech- niques, beats current state-of-the-art in multiple benchmark datasets by\nsignificant margins, demonstrating its potential as a versatile and effective\ntool for tackling dataset bias in deep learning applications.\n","authors":["Massimiliano Ciranni","Vito Paolo Pastore","Roberto Di Via","Enzo Tartaglione","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2502.09564v1.pdf","comment":"29 Pages, 12 Figures"},{"id":"http://arxiv.org/abs/2502.09563v1","updated":"2025-02-13T18:15:10Z","published":"2025-02-13T18:15:10Z","title":"Self-Calibrating Gaussian Splatting for Large Field of View\n  Reconstruction","summary":"  In this paper, we present a self-calibrating framework that jointly optimizes\ncamera parameters, lens distortion and 3D Gaussian representations, enabling\naccurate and efficient scene reconstruction. In particular, our technique\nenables high-quality scene reconstruction from Large field-of-view (FOV)\nimagery taken with wide-angle lenses, allowing the scene to be modeled from a\nsmaller number of images. Our approach introduces a novel method for modeling\ncomplex lens distortions using a hybrid network that combines invertible\nresidual networks with explicit grids. This design effectively regularizes the\noptimization process, achieving greater accuracy than conventional camera\nmodels. Additionally, we propose a cubemap-based resampling strategy to support\nlarge FOV images without sacrificing resolution or introducing distortion\nartifacts. Our method is compatible with the fast rasterization of Gaussian\nSplatting, adaptable to a wide variety of camera lens distortion, and\ndemonstrates state-of-the-art performance on both synthetic and real-world\ndatasets.\n","authors":["Youming Deng","Wenqi Xian","Guandao Yang","Leonidas Guibas","Gordon Wetzstein","Steve Marschner","Paul Debevec"],"pdf_url":"https://arxiv.org/pdf/2502.09563v1.pdf","comment":"Project Page: https://denghilbert.github.io/self-cali/"},{"id":"http://arxiv.org/abs/2501.04001v2","updated":"2025-02-13T18:14:33Z","published":"2025-01-07T18:58:54Z","title":"Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos","summary":"  This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.\n","authors":["Haobo Yuan","Xiangtai Li","Tao Zhang","Zilong Huang","Shilin Xu","Shunping Ji","Yunhai Tong","Lu Qi","Jiashi Feng","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04001v2.pdf","comment":"Project page: https://lxtgh.github.io/project/sa2va"},{"id":"http://arxiv.org/abs/2502.09560v1","updated":"2025-02-13T18:11:34Z","published":"2025-02-13T18:11:34Z","title":"EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents","summary":"  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 13 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.\n","authors":["Rui Yang","Hanyang Chen","Junyu Zhang","Mark Zhao","Cheng Qian","Kangrui Wang","Qineng Wang","Teja Venkat Koripella","Marziyeh Movahedi","Manling Li","Heng Ji","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09560v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2408.09110v2","updated":"2025-02-13T18:01:16Z","published":"2024-08-17T06:24:43Z","title":"Locate Anything on Earth: Advancing Open-Vocabulary Object Detection for\n  Remote Sensing Community","summary":"  Object detection, particularly open-vocabulary object detection, plays a\ncrucial role in Earth sciences, such as environmental monitoring, natural\ndisaster assessment, and land-use planning. However, existing open-vocabulary\ndetectors, primarily trained on natural-world images, struggle to generalize to\nremote sensing images due to a significant data domain gap. Thus, this paper\naims to advance the development of open-vocabulary object detection in remote\nsensing community. To achieve this, we first reformulate the task as Locate\nAnything on Earth (LAE) with the goal of detecting any novel concepts on Earth.\nWe then developed the LAE-Label Engine which collects, auto-annotates, and\nunifies up to 10 remote sensing datasets creating the LAE-1M - the first\nlarge-scale remote sensing object detection dataset with broad category\ncoverage. Using the LAE-1M, we further propose and train the novel LAE-DINO\nModel, the first open-vocabulary foundation object detector for the LAE task,\nfeaturing Dynamic Vocabulary Construction (DVC) and Visual-Guided Text Prompt\nLearning (VisGT) modules. DVC dynamically constructs vocabulary for each\ntraining batch, while VisGT maps visual features to semantic space, enhancing\ntext features. We comprehensively conduct experiments on established remote\nsensing benchmark DIOR, DOTAv2.0, as well as our newly introduced 80-class\nLAE-80C benchmark. Results demonstrate the advantages of the LAE-1M dataset and\nthe effectiveness of the LAE-DINO method.\n","authors":["Jiancheng Pan","Yanxing Liu","Yuqian Fu","Muyuan Ma","Jiahao Li","Danda Pani Paudel","Luc Van Gool","Xiaomeng Huang"],"pdf_url":"https://arxiv.org/pdf/2408.09110v2.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.09533v1","updated":"2025-02-13T17:50:23Z","published":"2025-02-13T17:50:23Z","title":"Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion\n  Model","summary":"  Recent advances in conditional diffusion models have shown promise for\ngenerating realistic TalkingFace videos, yet challenges persist in achieving\nconsistent head movement, synchronized facial expressions, and accurate lip\nsynchronization over extended generations. To address these, we introduce the\n\\textbf{M}otion-priors \\textbf{C}onditional \\textbf{D}iffusion \\textbf{M}odel\n(\\textbf{MCDM}), which utilizes both archived and current clip motion priors to\nenhance motion prediction and ensure temporal consistency. The model consists\nof three key elements: (1) an archived-clip motion-prior that incorporates\nhistorical frames and a reference frame to preserve identity and context; (2) a\npresent-clip motion-prior diffusion model that captures multimodal causality\nfor accurate predictions of head movements, lip sync, and expressions; and (3)\na memory-efficient temporal attention mechanism that mitigates error\naccumulation by dynamically storing and updating motion features. We also\nrelease the \\textbf{TalkingFace-Wild} dataset, a multilingual collection of\nover 200 hours of footage across 10 languages. Experimental results demonstrate\nthe effectiveness of MCDM in maintaining identity and motion continuity for\nlong-term TalkingFace generation. Code, models, and datasets will be publicly\navailable.\n","authors":["Fei Shen","Cong Wang","Junyao Gao","Qin Guo","Jisheng Dang","Jinhui Tang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.09533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09528v1","updated":"2025-02-13T17:39:28Z","published":"2025-02-13T17:39:28Z","title":"SteROI-D: System Design and Mapping for Stereo Depth Inference on\n  Regions of Interest","summary":"  Machine learning algorithms have enabled high quality stereo depth estimation\nto run on Augmented and Virtual Reality (AR/VR) devices. However, high energy\nconsumption across the full image processing stack prevents stereo depth\nalgorithms from running effectively on battery-limited devices. This paper\nintroduces SteROI-D, a full stereo depth system paired with a mapping\nmethodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity\nat the system level to save energy. SteROI-D's flexible and heterogeneous\ncompute fabric supports diverse ROIs. Importantly, we introduce a systematic\nmapping methodology to effectively handle dynamic ROIs, thereby maximizing\nenergy savings. Using these techniques, our 28nm prototype SteROI-D design\nachieves up to 4.35x reduction in total system energy compared to a baseline\nASIC.\n","authors":["Jack Erhardt","Ziang Li","Reid Pinkham","Andrew Berkovich","Zhengya Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09528v1.pdf","comment":"Accepted as a full paper by the 2025 EDGE AI FOUNDATION Austin"},{"id":"http://arxiv.org/abs/2502.09520v1","updated":"2025-02-13T17:35:57Z","published":"2025-02-13T17:35:57Z","title":"SQ-GAN: Semantic Image Communications Using Masked Vector Quantization","summary":"  This work introduces Semantically Masked VQ-GAN (SQ-GAN), a novel approach\nintegrating generative models to optimize image compression for\nsemantic/task-oriented communications. SQ-GAN employs off-the-shelf semantic\nsemantic segmentation and a new specifically developed semantic-conditioned\nadaptive mask module (SAMM) to selectively encode semantically significant\nfeatures of the images. SQ-GAN outperforms state-of-the-art image compression\nschemes such as JPEG2000 and BPG across multiple metrics, including perceptual\nquality and semantic segmentation accuracy on the post-decoding reconstructed\nimage, at extreme low compression rates expressed in bits per pixel.\n","authors":["Francesco Pezone","Sergio Barbarossa","Giuseppe Caire"],"pdf_url":"https://arxiv.org/pdf/2502.09520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09507v1","updated":"2025-02-13T17:21:37Z","published":"2025-02-13T17:21:37Z","title":"When and How Does CLIP Enable Domain and Compositional Generalization?","summary":"  The remarkable generalization performance of contrastive vision-language\nmodels like CLIP is often attributed to the diversity of their training\ndistributions. However, key questions remain unanswered: Can CLIP generalize to\nan entirely unseen domain when trained on a diverse mixture of domains (domain\ngeneralization)? Can it generalize to unseen classes within partially seen\ndomains (compositional generalization)? What factors affect such\ngeneralization? To answer these questions, we trained CLIP models on\nsystematically constructed training distributions with controlled domain\ndiversity and object class exposure. Our experiments show that domain diversity\nis essential for both domain and compositional generalization, yet\ncompositional generalization can be surprisingly weaker than domain\ngeneralization when the training distribution contains a suboptimal subset of\nthe test domain. Through data-centric and mechanistic analyses, we find that\nsuccessful generalization requires learning of shared representations already\nin intermediate layers and shared circuitry.\n","authors":["Elias Kempf","Simon Schrodi","Max Argus","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2502.09507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09501v1","updated":"2025-02-13T17:13:46Z","published":"2025-02-13T17:13:46Z","title":"Prior-Constrained Association Learning for Fine-Grained Generalized\n  Category Discovery","summary":"  This paper addresses generalized category discovery (GCD), the task of\nclustering unlabeled data from potentially known or unknown categories with the\nhelp of labeled instances from each known category. Compared to traditional\nsemi-supervised learning, GCD is more challenging because unlabeled data could\nbe from novel categories not appearing in labeled data. Current\nstate-of-the-art methods typically learn a parametric classifier assisted by\nself-distillation. While being effective, these methods do not make use of\ncross-instance similarity to discover class-specific semantics which are\nessential for representation learning and category discovery. In this paper, we\nrevisit the association-based paradigm and propose a Prior-constrained\nAssociation Learning method to capture and learn the semantic relations within\ndata. In particular, the labeled data from known categories provides a unique\nprior for the association of unlabeled data. Unlike previous methods that only\nadopts the prior as a pre or post-clustering refinement, we fully incorporate\nthe prior into the association process, and let it constrain the association\ntowards a reliable grouping outcome. The estimated semantic groups are utilized\nthrough non-parametric prototypical contrast to enhance the representation\nlearning. A further combination of both parametric and non-parametric\nclassification complements each other and leads to a model that outperforms\nexisting methods by a significant margin. On multiple GCD benchmarks, we\nperform extensive experiments and validate the effectiveness of our proposed\nmethod.\n","authors":["Menglin Wang","Zhun Zhong","Xiaojin Gong"],"pdf_url":"https://arxiv.org/pdf/2502.09501v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09482v1","updated":"2025-02-13T16:45:39Z","published":"2025-02-13T16:45:39Z","title":"Standardisation of Convex Ultrasound Data Through Geometric Analysis and\n  Augmentation","summary":"  The application of ultrasound in healthcare has seen increased diversity and\nimportance. Unlike other medical imaging modalities, ultrasound research and\ndevelopment has historically lagged, particularly in the case of applications\nwith data-driven algorithms. A significant issue with ultrasound is the extreme\nvariability of the images, due to the number of different machines available\nand the possible combination of parameter settings. One outcome of this is the\nlack of standardised and benchmarking ultrasound datasets. The method proposed\nin this article is an approach to alleviating this issue of disorganisation.\nFor this purpose, the issue of ultrasound data sparsity is examined and a novel\nperspective, approach, and solution is proposed; involving the extraction of\nthe underlying ultrasound plane within the image and representing it using\nannulus sector geometry. An application of this methodology is proposed, which\nis the extraction of scan lines and the linearisation of convex planes.\nValidation of the robustness of the proposed method is performed on both\nprivate and public data. The impact of deformation and the invertibility of\naugmentation using the estimated annulus sector parameters is also studied.\nKeywords: Ultrasound, Annulus Sector, Augmentation, Linearisation.\n","authors":["Alistair Weld","Giovanni Faoro","Luke Dixon","Sophie Camp","Arianna Menciassi","Stamatia Giannarou"],"pdf_url":"https://arxiv.org/pdf/2502.09482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09477v1","updated":"2025-02-13T16:41:44Z","published":"2025-02-13T16:41:44Z","title":"DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation\n  Networks for Quantitative Nanomaterial Analysis through Differentiable\n  Rendering and Generative Modelling","summary":"  Nanomaterials exhibit distinctive properties governed by parameters such as\nsize, shape, and surface characteristics, which critically influence their\napplications and interactions across technological, biological, and\nenvironmental contexts. Accurate quantification and understanding of these\nmaterials are essential for advancing research and innovation. In this regard,\ndeep learning segmentation networks have emerged as powerful tools that enable\nautomated insights and replace subjective methods with precise quantitative\nanalysis. However, their efficacy depends on representative annotated datasets,\nwhich are challenging to obtain due to the costly imaging of nanoparticles and\nthe labor-intensive nature of manual annotations. To overcome these\nlimitations, we introduce DiffRenderGAN, a novel generative model designed to\nproduce annotated synthetic data. By integrating a differentiable renderer into\na Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes\ntextural rendering parameters to generate realistic, annotated nanoparticle\nimages from non-annotated real microscopy images. This approach reduces the\nneed for manual intervention and enhances segmentation performance compared to\nexisting synthetic data methods by generating diverse and realistic data.\nTested on multiple ion and electron microscopy cases, including titanium\ndioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),\nDiffRenderGAN bridges the gap between synthetic and real data, advancing the\nquantification and understanding of complex nanomaterial systems.\n","authors":["Dennis Possart","Leonid Mill","Florian Vollnhals","Tor Hildebrand","Peter Suter","Mathis Hoffmann","Jonas Utz","Daniel Augsburger","Mareike Thies","Mingxuan Wu","Fabian Wagner","George Sarau","Silke Christiansen","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2502.09477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09471v1","updated":"2025-02-13T16:34:59Z","published":"2025-02-13T16:34:59Z","title":"Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for\n  Weakly-supervised Oriented Object Detection","summary":"  Accurately estimating the orientation of visual objects with compact rotated\nbounding boxes (RBoxes) has become a prominent demand, which challenges\nexisting object detection paradigms that only use horizontal bounding boxes\n(HBoxes). To equip the detectors with orientation awareness, supervised\nregression/classification modules have been introduced at the high cost of\nrotation annotation. Meanwhile, some existing datasets with oriented objects\nare already annotated with horizontal boxes or even single points. It becomes\nattractive yet remains open for effectively utilizing weaker single point and\nhorizontal annotations to train an oriented object detector (OOD). We develop\nWholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging\nvarious labeling forms (Points, HBoxes, RBoxes, and their combination) in a\nunified fashion. By only using HBox for training, our Wholly-WOOD achieves\nperformance very close to that of the RBox-trained counterpart on remote\nsensing and other areas, significantly reducing the tedious efforts on\nlabor-intensive annotation for oriented objects. The source codes are available\nat https://github.com/VisionXLab/whollywood (PyTorch-based) and\nhttps://github.com/VisionXLab/whollywood-jittor (Jittor-based).\n","authors":["Yi Yu","Xue Yang","Yansheng Li","Zhenjun Han","Feipeng Da","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2502.09471v1.pdf","comment":"18 pages, 9 figures, 9 tables, accepted by TPAMI"},{"id":"http://arxiv.org/abs/2502.07431v2","updated":"2025-02-13T16:32:33Z","published":"2025-02-11T10:19:50Z","title":"ArthroPhase: A Novel Dataset and Method for Phase Recognition in\n  Arthroscopic Video","summary":"  This study aims to advance surgical phase recognition in arthroscopic\nprocedures, specifically Anterior Cruciate Ligament (ACL) reconstruction, by\nintroducing the first arthroscopy dataset and developing a novel\ntransformer-based model. We aim to establish a benchmark for arthroscopic\nsurgical phase recognition by leveraging spatio-temporal features to address\nthe specific challenges of arthroscopic videos including limited field of view,\nocclusions, and visual distortions. We developed the ACL27 dataset, comprising\n27 videos of ACL surgeries, each labeled with surgical phases. Our model\nemploys a transformer-based architecture, utilizing temporal-aware frame-wise\nfeature extraction through a ResNet-50 and transformer layers. This approach\nintegrates spatio-temporal features and introduces a Surgical Progress Index\n(SPI) to quantify surgery progression. The model's performance was evaluated\nusing accuracy, precision, recall, and Jaccard Index on the ACL27 and Cholec80\ndatasets. The proposed model achieved an overall accuracy of 72.91% on the\nACL27 dataset. On the Cholec80 dataset, the model achieved a comparable\nperformance with the state-of-the-art methods with an accuracy of 92.4%. The\nSPI demonstrated an output error of 10.6% and 9.86% on ACL27 and Cholec80\ndatasets respectively, indicating reliable surgery progression estimation. This\nstudy introduces a significant advancement in surgical phase recognition for\narthroscopy, providing a comprehensive dataset and a robust transformer-based\nmodel. The results validate the model's effectiveness and generalizability,\nhighlighting its potential to improve surgical training, real-time assistance,\nand operational efficiency in orthopedic surgery. The publicly available\ndataset and code will facilitate future research and development in this\ncritical field.\n","authors":["Ali Bahari Malayeri","Matthias Seibold","Nicola Cavalcanti","Jonas Hein","Sascha Jecklin","Lazaros Vlachopoulos","Sandro Fucentese","Sandro Hodel","Philipp Furnstahl"],"pdf_url":"https://arxiv.org/pdf/2502.07431v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14679v3","updated":"2025-02-13T16:29:16Z","published":"2025-01-24T17:57:06Z","title":"Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation","summary":"  Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.\n","authors":["Rongzhao He","Weihao Zheng","Leilei Zhao","Ying Wang","Dalin Zhu","Dan Wu","Bin Hu"],"pdf_url":"https://arxiv.org/pdf/2501.14679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09460v1","updated":"2025-02-13T16:27:23Z","published":"2025-02-13T16:27:23Z","title":"Metamorphic Testing for Pose Estimation Systems","summary":"  Pose estimation systems are used in a variety of fields, from sports\nanalytics to livestock care. Given their potential impact, it is paramount to\nsystematically test their behaviour and potential for failure. This is a\ncomplex task due to the oracle problem and the high cost of manual labelling\nnecessary to build ground truth keypoints. This problem is exacerbated by the\nfact that different applications require systems to focus on different subjects\n(e.g., human versus animal) or landmarks (e.g., only extremities versus whole\nbody and face), which makes labelled test data rarely reusable. To combat these\nproblems we propose MET-POSE, a metamorphic testing framework for pose\nestimation systems that bypasses the need for manual annotation while assessing\nthe performance of these systems under different circumstances. MET-POSE thus\nallows users of pose estimation systems to assess the systems in conditions\nthat more closely relate to their application without having to label an ad-hoc\ntest dataset or rely only on available datasets, which may not be adapted to\ntheir application domain. While we define MET-POSE in general terms, we also\npresent a non-exhaustive list of metamorphic rules that represent common\nchallenges in computer vision applications, as well as a specific way to\nevaluate these rules. We then experimentally show the effectiveness of MET-POSE\nby applying it to Mediapipe Holistic, a state of the art human pose estimation\nsystem, with the FLIC and PHOENIX datasets. With these experiments, we outline\nnumerous ways in which the outputs of MET-POSE can uncover faults in pose\nestimation systems at a similar or higher rate than classic testing using hand\nlabelled data, and show that users can tailor the rule set they use to the\nfaults and level of accuracy relevant to their application.\n","authors":["Matias Duran","Thomas Laurent","Ellen Rushe","Anthony Ventresque"],"pdf_url":"https://arxiv.org/pdf/2502.09460v1.pdf","comment":"Accepted for publication at 2025 IEEE Conference on Software Testing,\n  Verification and Validation (ICST)"},{"id":"http://arxiv.org/abs/2410.10790v2","updated":"2025-02-13T16:20:05Z","published":"2024-10-14T17:56:19Z","title":"Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D\n  Scenes","summary":"  Recent advancements in human motion synthesis have focused on specific types\nof motions, such as human-scene interaction, locomotion or human-human\ninteraction, however, there is a lack of a unified system capable of generating\na diverse combination of motion types. In response, we introduce\nSitcom-Crafter, a comprehensive and extendable system for human motion\ngeneration in 3D space, which can be guided by extensive plot contexts to\nenhance workflow efficiency for anime and game designers. The system is\ncomprised of eight modules, three of which are dedicated to motion generation,\nwhile the remaining five are augmentation modules that ensure consistent fusion\nof motion sequences and system functionality. Central to the generation modules\nis our novel 3D scene-aware human-human interaction module, which addresses\ncollision issues by synthesizing implicit 3D Signed Distance Function (SDF)\npoints around motion spaces, thereby minimizing human-scene collisions without\nadditional data collection costs. Complementing this, our locomotion and\nhuman-scene interaction modules leverage existing methods to enrich the\nsystem's motion generation capabilities. Augmentation modules encompass plot\ncomprehension for command generation, motion synchronization for seamless\nintegration of different motion types, hand pose retrieval to enhance motion\nrealism, motion collision revision to prevent human collisions, and 3D\nretargeting to ensure visual fidelity. Experimental evaluations validate the\nsystem's ability to generate high-quality, diverse, and physically realistic\nmotions, underscoring its potential for advancing creative workflows. Project\npage: https://windvchen.github.io/Sitcom-Crafter.\n","authors":["Jianqi Chen","Panwen Hu","Xiaojun Chang","Zhenwei Shi","Michael Kampffmeyer","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2410.10790v2.pdf","comment":"Accepted by ICLR 2025. Project Page:\n  https://windvchen.github.io/Sitcom-Crafter"},{"id":"http://arxiv.org/abs/2410.10719v3","updated":"2025-02-13T16:18:59Z","published":"2024-10-14T17:00:53Z","title":"4-LEGS: 4D Language Embedded Gaussian Splatting","summary":"  The emergence of neural representations has revolutionized our means for\ndigitally viewing a wide range of 3D scenes, enabling the synthesis of\nphotorealistic images rendered from novel views. Recently, several techniques\nhave been proposed for connecting these low-level representations with the\nhigh-level semantics understanding embodied within the scene. These methods\nelevate the rich semantic understanding from 2D imagery to 3D representations,\ndistilling high-dimensional spatial features onto 3D space. In our work, we are\ninterested in connecting language with a dynamic modeling of the world. We show\nhow to lift spatio-temporal features to a 4D representation based on 3D\nGaussian Splatting. This enables an interactive interface where the user can\nspatiotemporally localize events in the video from text prompts. We demonstrate\nour system on public 3D video datasets of people and animals performing various\nactions.\n","authors":["Gal Fiebelman","Tamir Cohen","Ayellet Morgenstern","Peter Hedman","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2410.10719v3.pdf","comment":"Eurographics 2025. Project webpage:\n  https://tau-vailab.github.io/4-LEGS/"},{"id":"http://arxiv.org/abs/2502.09447v1","updated":"2025-02-13T16:16:54Z","published":"2025-02-13T16:16:54Z","title":"Pixel-Level Reasoning Segmentation via Multi-turn Conversations","summary":"  Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST.\n","authors":["Dexian Cai","Xiaocui Yang","Yongkang Liu","Daling Wang","Shi Feng","Yifei Zhang","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2502.09447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17438v2","updated":"2025-02-13T16:11:35Z","published":"2023-05-27T10:26:23Z","title":"On the Importance of Backbone to the Adversarial Robustness of Object\n  Detectors","summary":"  Object detection is a critical component of various security-sensitive\napplications, such as autonomous driving and video surveillance. However,\nexisting object detectors are vulnerable to adversarial attacks, which poses a\nsignificant challenge to their reliability and security. Through experiments,\nfirst, we found that existing works on improving the adversarial robustness of\nobject detectors give a false sense of security. Second, we found that\nadversarially pre-trained backbone networks were essential for enhancing the\nadversarial robustness of object detectors. We then proposed a simple yet\neffective recipe for fast adversarial fine-tuning on object detectors with\nadversarially pre-trained backbones. Without any modifications to the structure\nof object detectors, our recipe achieved significantly better adversarial\nrobustness than previous works. Finally, we explored the potential of different\nmodern object detector designs for improving adversarial robustness with our\nrecipe and demonstrated interesting findings, which inspired us to design\nstate-of-the-art (SOTA) robust detectors. Our empirical results set a new\nmilestone for adversarially robust object detection. Code and trained\ncheckpoints are available at https://github.com/thu-ml/oddefense.\n","authors":["Xiao Li","Hang Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2305.17438v2.pdf","comment":"Accepted by IEEE TIFS"},{"id":"http://arxiv.org/abs/2410.01404v2","updated":"2025-02-13T16:06:54Z","published":"2024-10-02T10:31:10Z","title":"Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection","summary":"  Skins wrapping around our bodies, leathers covering over the sofa, sheet\nmetal coating the car - it suggests that objects are enclosed by a series of\ncontinuous surfaces, which provides us with informative geometry prior for\nobjectness deduction. In this paper, we propose Gaussian-Det which leverages\nGaussian Splatting as surface representation for multi-view based 3D object\ndetection. Unlike existing monocular or NeRF-based methods which depict the\nobjects via discrete positional data, Gaussian-Det models the objects in a\ncontinuous manner by formulating the input Gaussians as feature descriptors on\na mass of partial surfaces. Furthermore, to address the numerous outliers\ninherently introduced by Gaussian splatting, we accordingly devise a Closure\nInferring Module (CIM) for the comprehensive surface-based objectness\ndeduction. CIM firstly estimates the probabilistic feature residuals for\npartial surfaces given the underdetermined nature of Gaussian Splatting, which\nare then coalesced into a holistic representation on the overall surface\nclosure of the object proposal. In this way, the surface information\nGaussian-Det exploits serves as the prior on the quality and reliability of\nobjectness and the information basis of proposal refinement. Experiments on\nboth synthetic and real-world datasets demonstrate that Gaussian-Det\noutperforms various existing approaches, in terms of both average precision and\nrecall.\n","authors":["Hongru Yan","Yu Zheng","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2410.01404v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09434v1","updated":"2025-02-13T15:56:44Z","published":"2025-02-13T15:56:44Z","title":"Redistribute Ensemble Training for Mitigating Memorization in Diffusion\n  Models","summary":"  Diffusion models, known for their tremendous ability to generate high-quality\nsamples, have recently raised concerns due to their data memorization behavior,\nwhich poses privacy risks. Recent methods for memory mitigation have primarily\naddressed the issue within the context of the text modality in cross-modal\ngeneration tasks, restricting their applicability to specific conditions. In\nthis paper, we propose a novel method for diffusion models from the perspective\nof visual modality, which is more generic and fundamental for mitigating\nmemorization. Directly exposing visual data to the model increases memorization\nrisk, so we design a framework where models learn through proxy model\nparameters instead. Specially, the training dataset is divided into multiple\nshards, with each shard training a proxy model, then aggregated to form the\nfinal model. Additionally, practical analysis of training losses illustrates\nthat the losses for easily memorable images tend to be obviously lower. Thus,\nwe skip the samples with abnormally low loss values from the current mini-batch\nto avoid memorizing. However, balancing the need to skip memorization-prone\nsamples while maintaining sufficient training data for high-quality image\ngeneration presents a key challenge. Thus, we propose IET-AGC+, which\nredistributes highly memorizable samples between shards, to mitigate these\nsamples from over-skipping. Furthermore, we dynamically augment samples based\non their loss values to further reduce memorization. Extensive experiments and\nanalysis on four datasets show that our method successfully reduces memory\ncapacity while maintaining performance. Moreover, we fine-tune the pre-trained\ndiffusion models, e.g., Stable Diffusion, and decrease the memorization score\nby 46.7\\%, demonstrating the effectiveness of our method. Code is available in:\nhttps://github.com/liuxiao-guan/IET_AGC.\n","authors":["Xiaoliu Guan","Yu Wu","Huayang Huang","Xiao Liu","Jiaxu Miao","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.09434v1.pdf","comment":"12 pages,9 figures. arXiv admin note: substantial text overlap with\n  arXiv:2407.15328"},{"id":"http://arxiv.org/abs/2502.09425v1","updated":"2025-02-13T15:47:45Z","published":"2025-02-13T15:47:45Z","title":"A 3D Facial Reconstruction Evaluation Methodology: Comparing Smartphone\n  Scans with Deep Learning Based Methods Using Geometry and Morphometry\n  Criteria","summary":"  Three-dimensional (3D) facial shape analysis has gained interest due to its\npotential clinical applications. However, the high cost of advanced 3D facial\nacquisition systems limits their widespread use, driving the development of\nlow-cost acquisition and reconstruction methods. This study introduces a novel\nevaluation methodology that goes beyond traditional geometry-based benchmarks\nby integrating morphometric shape analysis techniques, providing a statistical\nframework for assessing facial morphology preservation. As a case study, we\ncompare smartphone-based 3D scans with state-of-the-art deep learning\nreconstruction methods from 2D images, using high-end stereophotogrammetry\nmodels as ground truth. This methodology enables a quantitative assessment of\nglobal and local shape differences, offering a biologically meaningful\nvalidation approach for low-cost 3D facial acquisition and reconstruction\ntechniques.\n","authors":["Álvaro Heredia-Lidón","Alejandro Moñux-Bernal","Alejandro González","Luis M. Echeverry-Quiceno","Max Rubert","Aroa Casado","María Esther Esteban","Mireia Andreu-Montoriol","Susanna Gallardo","Cristina Ruffo","Neus Martínez-Abadías","Xavier Sevillano"],"pdf_url":"https://arxiv.org/pdf/2502.09425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00315v3","updated":"2025-02-13T15:46:35Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2410.15959v3","updated":"2025-02-13T15:38:06Z","published":"2024-10-21T12:43:54Z","title":"Diffusion Transformer Policy: Scaling Diffusion Transformer for\n  Generalist Vision-Language-Action Learning","summary":"  Recent large vision-language action models pretrained on diverse robot\ndatasets have demonstrated the potential for generalizing to new environments\nwith a few in-domain data. However, those approaches usually predict individual\ndiscretized or continuous action by a small action head, which limits the\nability in handling diverse action spaces. In contrast, we model the continuous\naction sequence with a large multi-modal diffusion transformer, dubbed as\nDiffusion Transformer Policy, in which we directly denoise action chunks by a\nlarge transformer model rather than a small action head for action embedding.\nBy leveraging the scaling capability of transformers, the proposed approach can\neffectively model continuous end-effector actions across large diverse robot\ndatasets, and achieve better generalization performance. Extensive experiments\ndemonstrate the effectiveness and generalization of Diffusion Transformer\nPolicy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world\nFranka arm, achieving consistent better performance on Real-to-Sim benchmark\nSimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo.\nSpecifically, without bells and whistles, the proposed approach achieves\nstate-of-the-art performance with only a single third-view camera stream in the\nCalvin task ABC->D, improving the average number of tasks completed in a row of\n5 to 3.6, and the pretraining stage significantly facilitates the success\nsequence length on the Calvin by over 1.2. Project Page:\nhttps://zhihou7.github.io/dit_policy_vla/\n","authors":["Zhi Hou","Tianyi Zhang","Yuwen Xiong","Hengjun Pu","Chengyang Zhao","Ronglei Tong","Yu Qiao","Jifeng Dai","Yuntao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15959v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09411v1","updated":"2025-02-13T15:36:12Z","published":"2025-02-13T15:36:12Z","title":"ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation","summary":"  Diffusion models enable high-quality and diverse visual content synthesis.\nHowever, they struggle to generate rare or unseen concepts. To address this\nchallenge, we explore the usage of Retrieval-Augmented Generation (RAG) with\nimage generation models. We propose ImageRAG, a method that dynamically\nretrieves relevant images based on a given text prompt, and uses them as\ncontext to guide the generation process. Prior approaches that used retrieved\nimages to improve generation, trained models specifically for retrieval-based\ngeneration. In contrast, ImageRAG leverages the capabilities of existing image\nconditioning models, and does not require RAG-specific training. Our approach\nis highly adaptable and can be applied across different model types, showing\nsignificant improvement in generating rare and fine-grained concepts using\ndifferent base models.\n  Our project page is available at: https://rotem-shalev.github.io/ImageRAG\n","authors":["Rotem Shalev-Arkushin","Rinon Gal","Amit H. Bermano","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2502.09411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07508v2","updated":"2025-02-13T15:28:13Z","published":"2025-02-11T12:22:35Z","title":"Enhance-A-Video: Better Generated Video for Free","summary":"  DiT-based video generation has achieved remarkable results, but research into\nenhancing existing models remains relatively unexplored. In this work, we\nintroduce a training-free approach to enhance the coherence and quality of\nDiT-based generated videos, named Enhance-A-Video. The core idea is enhancing\nthe cross-frame correlations based on non-diagonal temporal attention\ndistributions. Thanks to its simple design, our approach can be easily applied\nto most DiT-based video generation frameworks without any retraining or\nfine-tuning. Across various DiT-based video generation models, our approach\ndemonstrates promising improvements in both temporal consistency and visual\nquality. We hope this research can inspire future explorations in video\ngeneration enhancement.\n","authors":["Yang Luo","Xuanlei Zhao","Mengzhao Chen","Kaipeng Zhang","Wenqi Shao","Kai Wang","Zhangyang Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2502.07508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08544v2","updated":"2025-02-13T15:04:42Z","published":"2025-02-12T16:28:21Z","title":"Moment of Untruth: Dealing with Negative Queries in Video Moment\n  Retrieval","summary":"  Video Moment Retrieval is a common task to evaluate the performance of\nvisual-language models - it involves localising start and end times of moments\nin videos from query sentences. The current task formulation assumes that the\nqueried moment is present in the video, resulting in false positive moment\npredictions when irrelevant query sentences are provided. In this paper we\npropose the task of Negative-Aware Video Moment Retrieval (NA-VMR), which\nconsiders both moment retrieval accuracy and negative query rejection accuracy.\nWe make the distinction between In-Domain and Out-of-Domain negative queries\nand provide new evaluation benchmarks for two popular video moment retrieval\ndatasets: QVHighlights and Charades-STA. We analyse the ability of current SOTA\nvideo moment retrieval approaches to adapt to Negative-Aware Video Moment\nRetrieval and propose UniVTG-NA, an adaptation of UniVTG designed to tackle\nNA-VMR. UniVTG-NA achieves high negative rejection accuracy (avg. $98.4\\%$)\nscores while retaining moment retrieval scores to within $3.87\\%$ Recall@1.\nDataset splits and code are available at\nhttps://github.com/keflanagan/MomentofUntruth\n","authors":["Kevin Flanagan","Dima Damen","Michael Wray"],"pdf_url":"https://arxiv.org/pdf/2502.08544v2.pdf","comment":"16 pages, 9 figures. Accepted at WACV 2025. Paper webpage:\n  https://keflanagan.github.io/Moment-of-Untruth"},{"id":"http://arxiv.org/abs/2502.06607v2","updated":"2025-02-13T14:57:44Z","published":"2025-02-10T16:04:54Z","title":"Illegal Waste Detection in Remote Sensing Images: A Case Study","summary":"  Environmental crime currently represents the third largest criminal activity\nworldwide while threatening ecosystems as well as human health. Among the\ncrimes related to this activity, improper waste management can nowadays be\ncountered more easily thanks to the increasing availability and decreasing cost\nof Very-High-Resolution Remote Sensing images, which enable semi-automatic\nterritory scanning in search of illegal landfills. This paper proposes a\npipeline, developed in collaboration with professionals from a local\nenvironmental agency, for detecting candidate illegal dumping sites leveraging\na classifier of Remote Sensing images. To identify the best configuration for\nsuch classifier, an extensive set of experiments was conducted and the impact\nof diverse image characteristics and training settings was thoroughly analyzed.\nThe local environmental agency was then involved in an experimental exercise\nwhere outputs from the developed classifier were integrated in the experts'\neveryday work, resulting in time savings with respect to manual\nphoto-interpretation. The classifier was eventually run with valuable results\non a location outside of the training area, highlighting potential for\ncross-border applicability of the proposed pipeline.\n","authors":["Federico Gibellini","Piero Fraternali","Giacomo Boracchi","Luca Morandini","Andrea Diecidue","Simona Malegori"],"pdf_url":"https://arxiv.org/pdf/2502.06607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09356v1","updated":"2025-02-13T14:21:03Z","published":"2025-02-13T14:21:03Z","title":"Galileo: Learning Global and Local Features in Pretrained Remote Sensing\n  Models","summary":"  From crop mapping to flood detection, machine learning in remote sensing has\na wide range of societally beneficial applications. The commonalities between\nremote sensing data in these applications present an opportunity for pretrained\nmachine learning models tailored to remote sensing to reduce the labeled data\nand effort required to solve individual tasks. However, such models must be:\n(i) flexible enough to ingest input data of varying sensor modalities and\nshapes (i.e., of varying spatial and temporal dimensions), and (ii) able to\nmodel Earth surface phenomena of varying scales and types. To solve this gap,\nwe present Galileo, a family of pretrained remote sensing models designed to\nflexibly process multimodal remote sensing data. We also introduce a novel and\nhighly effective self-supervised learning approach to learn both large- and\nsmall-scale features, a challenge not addressed by previous models. Our Galileo\nmodels obtain state-of-the-art results across diverse remote sensing tasks.\n","authors":["Gabriel Tseng","Anthony Fuller","Marlena Reil","Henry Herzog","Patrick Beukema","Favyen Bastani","James R. Green","Evan Shelhamer","Hannah Kerner","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2502.09356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06181v2","updated":"2025-02-13T14:18:50Z","published":"2025-02-10T06:21:16Z","title":"CANeRV: Content Adaptive Neural Representation for Video Compression","summary":"  Recent advances in video compression introduce implicit neural representation\n(INR) based methods, which effectively capture global dependencies and\ncharacteristics of entire video sequences. Unlike traditional and deep learning\nbased approaches, INR-based methods optimize network parameters from a global\nperspective, resulting in superior compression potential. However, most current\nINR methods utilize a fixed and uniform network architecture across all frames,\nlimiting their adaptability to dynamic variations within and between video\nsequences. This often leads to suboptimal compression outcomes as these methods\nstruggle to capture the distinct nuances and transitions in video content. To\novercome these challenges, we propose Content Adaptive Neural Representation\nfor Video Compression (CANeRV), an innovative INR-based video compression\nnetwork that adaptively conducts structure optimisation based on the specific\ncontent of each video sequence. To better capture dynamic information across\nvideo sequences, we propose a dynamic sequence-level adjustment (DSA).\nFurthermore, to enhance the capture of dynamics between frames within a\nsequence, we implement a dynamic frame-level adjustment (DFA). {Finally, to\neffectively capture spatial structural information within video frames, thereby\nenhancing the detail restoration capabilities of CANeRV, we devise a structure\nlevel hierarchical structural adaptation (HSA).} Experimental results\ndemonstrate that CANeRV can outperform both H.266/VVC and state-of-the-art\nINR-based video compression techniques across diverse video datasets.\n","authors":["Lv Tang","Jun Zhu","Xinfeng Zhang","Li Zhang","Siwei Ma","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2502.06181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09352v1","updated":"2025-02-13T14:18:41Z","published":"2025-02-13T14:18:41Z","title":"Wasserstein distributional adversarial training for deep neural networks","summary":"  Design of adversarial attacks for deep neural networks, as well as methods of\nadversarial training against them, are subject of intense research. In this\npaper, we propose methods to train against distributional attack threats,\nextending the TRADES method used for pointwise attacks. Our approach leverages\nrecent contributions and relies on sensitivity analysis for Wasserstein\ndistributionally robust optimization problems. We introduce an efficient\nfine-tuning method which can be deployed on a previously trained model. We test\nour methods on a range of pre-trained models on RobustBench. These experimental\nresults demonstrate the additional training enhances Wasserstein distributional\nrobustness, while maintaining original levels of pointwise robustness, even for\nalready very successful networks. The improvements are less marked for models\npre-trained using huge synthetic datasets of 20-100M images. However,\nremarkably, sometimes our methods are still able to improve their performance\neven when trained using only the original training dataset (50k images).\n","authors":["Xingjian Bai","Guangyi He","Yifan Jiang","Jan Obloj"],"pdf_url":"https://arxiv.org/pdf/2502.09352v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.11934v3","updated":"2025-02-13T14:16:44Z","published":"2024-03-18T16:33:29Z","title":"Image and Point-cloud Classification for Jet Analysis in High-Energy\n  Physics: A survey","summary":"  Nowadays, there has been a growing trend in the field of high-energy physics\n(HEP), in both its experimental and phenomenological studies, to incorporate\nmachine learning (ML) and its specialized branch, deep learning (DL). This\nreview paper provides a thorough illustration of these applications using\ndifferent ML and DL approaches. The first part of the paper examines the basics\nof various particle physics types and establishes guidelines for assessing\nparticle physics alongside the available learning models. Next, a detailed\nclassification is provided for representing Jets that are reconstructed in\nhigh-energy collisions, mainly in proton-proton collisions at well-defined beam\nenergies. This section covers various datasets, preprocessing techniques, and\nfeature extraction and selection methods. The presented techniques can be\napplied to future hadron-hadron colliders (HHC), such as the high-luminosity\nLHC (HL-LHC) and the future circular collider - hadron-hadron (FCChh). The\nauthors then explore several AI techniques analyses designed specifically for\nboth image and point-cloud (PC) data in HEP. Additionally, a closer look is\ntaken at the classification associated with Jet tagging in hadron collisions.\nIn this review, various state-of-the-art (SOTA) techniques in ML and DL are\nexamined, with a focus on their implications for HEP demands. More precisely,\nthis discussion addresses various applications in extensive detail, such as Jet\ntagging, Jet tracking, particle classification, and more. The review concludes\nwith an analysis of the current state of HEP using DL methodologies. It\nhighlights the challenges and potential areas for future research, which are\nillustrated for each application.\n","authors":["Hamza Kheddar","Yassine Himeur","Abbes Amira","Rachik Soualah"],"pdf_url":"https://arxiv.org/pdf/2403.11934v3.pdf","comment":"Accepted paper in Frontier of Physics"},{"id":"http://arxiv.org/abs/2502.09325v1","updated":"2025-02-13T13:38:17Z","published":"2025-02-13T13:38:17Z","title":"A Benchmark for Crime Surveillance Video Analysis with Large Models","summary":"  Anomaly analysis in surveillance videos is a crucial topic in computer\nvision. In recent years, multimodal large language models (MLLMs) have\noutperformed task-specific models in various domains. Although MLLMs are\nparticularly versatile, their abilities to understand anomalous concepts and\ndetails are insufficiently studied because of the outdated benchmarks of this\nfield not providing MLLM-style QAs and efficient algorithms to assess the\nmodel's open-ended text responses. To fill this gap, we propose a benchmark for\ncrime surveillance video analysis with large models denoted as UCVL, including\n1,829 videos and reorganized annotations from the UCF-Crime and UCF-Crime\nAnnotation datasets. We design six types of questions and generate diverse QA\npairs. Then we develop detailed instructions and use OpenAI's GPT-4o for\naccurate assessment. We benchmark eight prevailing MLLMs ranging from 0.5B to\n40B parameters, and the results demonstrate the reliability of this bench.\nMoreover, we finetune LLaVA-OneVision on UCVL's training set. The improvement\nvalidates our data's high quality for video anomaly analysis.\n","authors":["Haoran Chen","Dong Yi","Moyan Cao","Chensen Huang","Guibo Zhu","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09311v1","updated":"2025-02-13T13:25:13Z","published":"2025-02-13T13:25:13Z","title":"Mitigating the Impact of Prominent Position Shift in Drone-based RGBT\n  Object Detection","summary":"  Drone-based RGBT object detection plays a crucial role in many\naround-the-clock applications. However, real-world drone-viewed RGBT data\nsuffers from the prominent position shift problem, i.e., the position of a tiny\nobject differs greatly in different modalities. For instance, a slight\ndeviation of a tiny object in the thermal modality will induce it to drift from\nthe main body of itself in the RGB modality. Considering RGBT data are usually\nlabeled on one modality (reference), this will cause the unlabeled modality\n(sensed) to lack accurate supervision signals and prevent the detector from\nlearning a good representation. Moreover, the mismatch of the corresponding\nfeature point between the modalities will make the fused features confusing for\nthe detection head. In this paper, we propose to cast the cross-modality box\nshift issue as the label noise problem and address it on the fly via a novel\nMean Teacher-based Cross-modality Box Correction head ensemble (CBC). In this\nway, the network can learn more informative representations for both\nmodalities. Furthermore, to alleviate the feature map mismatch problem in RGBT\nfusion, we devise a Shifted Window-Based Cascaded Alignment (SWCA) module. SWCA\nmines long-range dependencies between the spatially unaligned features inside\nshifted windows and cascaded aligns the sensed features with the reference\nones. Extensive experiments on two drone-based RGBT object detection datasets\ndemonstrate that the correction results are both visually and quantitatively\nfavorable, thereby improving the detection performance. In particular, our CBC\nmodule boosts the precision of the sensed modality ground truth by 25.52 aSim\npoints. Overall, the proposed detector achieves an mAP_50 of 43.55 points on\nRGBTDronePerson and surpasses a state-of-the-art method by 8.6 mAP50 on a shift\nsubset of DroneVehicle dataset. The code and data will be made publicly\navailable.\n","authors":["Yan Zhang","Wen Yang","Chang Xu","Qian Hu","Fang Xu","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2502.09311v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.01401v2","updated":"2025-02-13T13:15:27Z","published":"2025-02-03T14:32:36Z","title":"Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection","summary":"  3D visual grounding (3DVG) is challenging because of the requirement of\nunderstanding on visual information, language and spatial relationships. While\nsupervised approaches have achieved superior performance, they are constrained\nby the scarcity and high cost of 3D vision-language datasets. On the other\nhand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for\ntraining data. However, these methods incur prohibitive time and token costs\nduring inference. To address the challenges, we introduce a novel training-free\nsymbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual\nGrounder, that offers significantly reduced inference costs compared to\nprevious agent-based methods while maintaining comparable performance. EaSe\nuses LLM generated codes to compute on spatial relationships. EaSe also\nimplements an automatic pipeline to evaluate and optimize the quality of these\ncodes and integrate VLMs to assist in the grounding process. Experimental\nresults demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2%\nAcc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover,\nit substantially reduces the inference time and cost, offering a balanced\ntrade-off between performance and efficiency. Codes are available at\nhttps://github.com/OpenRobotLab/EaSe.\n","authors":["Boyu Mi","Hanqing Wang","Tai Wang","Yilun Chen","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2502.01401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09296v1","updated":"2025-02-13T13:09:55Z","published":"2025-02-13T13:09:55Z","title":"A Physics-Informed Deep Learning Model for MRI Brain Motion Correction","summary":"  Background: MRI is crucial for brain imaging but is highly susceptible to\nmotion artifacts due to long acquisition times. This study introduces\nPI-MoCoNet, a physics-informed motion correction network that integrates\nspatial and k-space information to remove motion artifacts without explicit\nmotion parameter estimation, enhancing image fidelity and diagnostic\nreliability. Materials and Methods: PI-MoCoNet consists of a motion detection\nnetwork (U-net with spatial averaging) to identify corrupted k-space lines and\na motion correction network (U-net with Swin Transformer blocks) to reconstruct\nmotion-free images. The correction is guided by three loss functions:\nreconstruction (L1), perceptual (LPIPS), and data consistency (Ldc). Motion\nartifacts were simulated via rigid phase encoding perturbations and evaluated\non IXI and MR-ART datasets against Pix2Pix, CycleGAN, and U-net using PSNR,\nSSIM, and NMSE. Results: PI-MoCoNet significantly improved image quality. On\nIXI, for minor artifacts, PSNR increased from 34.15 dB to 45.95 dB, SSIM from\n0.87 to 1.00, and NMSE reduced from 0.55% to 0.04%. For moderate artifacts,\nPSNR improved from 30.23 dB to 42.16 dB, SSIM from 0.80 to 0.99, and NMSE from\n1.32% to 0.09%. For heavy artifacts, PSNR rose from 27.99 dB to 36.01 dB, SSIM\nfrom 0.75 to 0.97, and NMSE decreased from 2.21% to 0.36%. On MR-ART,\nPI-MoCoNet achieved PSNR gains of ~10 dB and SSIM improvements of up to 0.20,\nwith NMSE reductions of ~6%. Ablation studies confirmed the importance of data\nconsistency and perceptual losses, yielding a 1 dB PSNR gain and 0.17% NMSE\nreduction. Conclusions: PI-MoCoNet effectively mitigates motion artifacts in\nbrain MRI, outperforming existing methods. Its ability to integrate spatial and\nk-space information makes it a promising tool for clinical use in motion-prone\nsettings. Code: https://github.com/mosaf/PI-MoCoNet.git.\n","authors":["Mojtaba Safari","Shansong Wang","Zach Eidex","Richard Qiu","Chih-Wei Chang","David S. Yu","Xiaofeng Yang"],"pdf_url":"https://arxiv.org/pdf/2502.09296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03633v3","updated":"2025-02-13T13:02:25Z","published":"2024-12-04T18:55:45Z","title":"NBM: an Open Dataset for the Acoustic Monitoring of Nocturnal Migratory\n  Birds in Europe","summary":"  The persisting threats on migratory bird populations highlight the urgent\nneed for effective monitoring techniques that could assist in their\nconservation. Among these, passive acoustic monitoring is an essential tool,\nparticularly for nocturnal migratory species that are difficult to track\notherwise. This work presents the Nocturnal Bird Migration (NBM) dataset, a\ncollection of 13,359 annotated vocalizations from 117 species of the Western\nPalearctic. The dataset includes precise time and frequency annotations,\ngathered by dozens of bird enthusiasts across France, enabling novel downstream\nacoustic analysis. In particular, we prove the utility of this database by\ntraining an original two-stage deep object detection model tailored for the\nprocessing of audio data. While allowing the precise localization of bird calls\nin spectrograms, this model shows competitive accuracy on the 45 main species\nof the dataset with state-of-the-art systems trained on much larger audio\ncollections. These results highlight the interest of fostering similar\nopen-science initiatives to acquire costly but valuable fine-grained\nannotations of audio files. All data and code are made openly available.\n","authors":["Louis Airale","Adrien Pajot","Juliette Linossier"],"pdf_url":"https://arxiv.org/pdf/2412.03633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09285v1","updated":"2025-02-13T13:00:33Z","published":"2025-02-13T13:00:33Z","title":"EmoAssist: Emotional Assistant for Visual Impairment Community","summary":"  The rapid advancement of large multi-modality models (LMMs) has significantly\npropelled the integration of artificial intelligence into practical\napplications. Visual Question Answering (VQA) systems, which can process\nmulti-modal data including vision, text, and audio, hold great potential for\nassisting the Visual Impairment (VI) community in navigating complex and\ndynamic real-world environments. However, existing VI assistive LMMs overlook\nthe emotional needs of VI individuals, and current benchmarks lack emotional\nevaluation of these LMMs. To address these gaps, this paper introduces the\nEmoAssist Benchmark, a comprehensive benchmark designed to evaluate the\nassistive performance of LMMs for the VI community. To the best of our\nknowledge, this is the first benchmark that incorporates emotional intelligence\nas a key consideration. Furthermore, we propose the EmoAssist Model, an\nEmotion-Assistive LMM specifically designed for the VI community. The EmoAssist\nModel utilizes Direct Preference Optimization (DPO) to align outputs with human\nemotional preferences. Experiment results demonstrate that the EmoAssist Model\nsignificantly enhances the recognition of implicit emotions and intentions of\nVI users, delivers empathetic responses, and provides actionable guidance.\nSpecifically, it shows respective improvements of 147.8% and 89.7% in the\nEmpathy and Suggestion metrics on the EmoAssist Benchmark, compared to the\npre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o.\n","authors":["Xingyu Qi","He Li","Linjie Li","Zhenyu Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18581v2","updated":"2025-02-13T12:59:30Z","published":"2024-06-05T16:27:34Z","title":"Dream-in-Style: Text-to-3D Generation Using Stylized Score Distillation","summary":"  We present a method to generate 3D objects in styles. Our method takes a text\nprompt and a style reference image as input and reconstructs a neural radiance\nfield to synthesize a 3D model with the content aligning with the text prompt\nand the style following the reference image. To simultaneously generate the 3D\nobject and perform style transfer in one go, we propose a stylized score\ndistillation loss to guide a text-to-3D optimization process to output visually\nplausible geometry and appearance. Our stylized score distillation is based on\na combination of an original pretrained text-to-image model and its modified\nsibling with the key and value features of self-attention layers manipulated to\ninject styles from the reference image. Comparisons with state-of-the-art\nmethods demonstrated the strong visual performance of our method, further\nsupported by the quantitative results from our user study.\n","authors":["Hubert Kompanowski","Binh-Son Hua"],"pdf_url":"https://arxiv.org/pdf/2406.18581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09282v1","updated":"2025-02-13T12:54:13Z","published":"2025-02-13T12:54:13Z","title":"FE-LWS: Refined Image-Text Representations via Decoder Stacking and\n  Fused Encodings for Remote Sensing Image Captioning","summary":"  Remote sensing image captioning aims to generate descriptive text from remote\nsensing images, typically employing an encoder-decoder framework. In this\nsetup, a convolutional neural network (CNN) extracts feature representations\nfrom the input image, which then guide the decoder in a sequence-to-sequence\ncaption generation process. Although much research has focused on refining the\ndecoder, the quality of image representations from the encoder remains crucial\nfor accurate captioning. This paper introduces a novel approach that integrates\nfeatures from two distinct CNN based encoders, capturing complementary\ninformation to enhance caption generation. Additionally, we propose a weighted\naveraging technique to combine the outputs of all GRUs in the stacked decoder.\nFurthermore, a comparison-based beam search strategy is incorporated to refine\ncaption selection. The results demonstrate that our fusion-based approach,\nalong with the enhanced stacked decoder, significantly outperforms both the\ntransformer-based state-of-the-art model and other LSTM-based baselines.\n","authors":["Swadhin Das","Raksha Sharma"],"pdf_url":"https://arxiv.org/pdf/2502.09282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09278v1","updated":"2025-02-13T12:49:25Z","published":"2025-02-13T12:49:25Z","title":"ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View\n  Gaussian Optimization","summary":"  Recent advances in diffusion models have significantly improved 3D\ngeneration, enabling the use of assets generated from an image for embodied AI\nsimulations. However, the one-to-many nature of the image-to-3D problem limits\ntheir use due to inconsistent content and quality across views. Previous models\noptimize a 3D model by sampling views from a view-conditioned diffusion prior,\nbut diffusion models cannot guarantee view consistency. Instead, we present\nConsistentDreamer, where we first generate a set of fixed multi-view prior\nimages and sample random views between them with another diffusion model\nthrough a score distillation sampling (SDS) loss. Thereby, we limit the\ndiscrepancies between the views guided by the SDS loss and ensure a consistent\nrough shape. In each iteration, we also use our generated multi-view prior\nimages for fine-detail reconstruction. To balance between the rough shape and\nthe fine-detail optimizations, we introduce dynamic task-dependent weights\nbased on homoscedastic uncertainty, updated automatically in each iteration.\nAdditionally, we employ opacity, depth distortion, and normal alignment losses\nto refine the surface for mesh extraction. Our method ensures better view\nconsistency and visual quality compared to the state-of-the-art.\n","authors":["Onat Şahin","Mohammad Altillawi","George Eskandar","Carlos Carbone","Ziyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09278v1.pdf","comment":"Manuscript accepted by Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2502.09274v1","updated":"2025-02-13T12:39:26Z","published":"2025-02-13T12:39:26Z","title":"FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation","summary":"  3D scene understanding is a critical yet challenging task in autonomous\ndriving, primarily due to the irregularity and sparsity of LiDAR data, as well\nas the computational demands of processing large-scale point clouds. Recent\nmethods leverage the range-view representation to improve processing\nefficiency. To mitigate the performance drop caused by information loss\ninherent to the \"many-to-one\" problem, where multiple nearby 3D points are\nmapped to the same 2D grids and only the closest is retained, prior works tend\nto choose a higher azimuth resolution for range-view projection. However, this\ncan bring the drawback of reducing the proportion of pixels that carry\ninformation and heavier computation within the network. We argue that it is not\nthe optimal solution and show that, in contrast, decreasing the resolution is\nmore advantageous in both efficiency and accuracy. In this work, we present a\ncomprehensive re-design of the workflow for range-view-based LiDAR semantic\nsegmentation. Our approach addresses data representation, augmentation, and\npost-processing methods for improvements. Through extensive experiments on two\npublic datasets, we demonstrate that our pipeline significantly enhances the\nperformance of various network architectures over their baselines, paving the\nway for more effective LiDAR-based perception in autonomous systems.\n","authors":["Bin Yang","Alexandru Paul Condurache"],"pdf_url":"https://arxiv.org/pdf/2502.09274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09269v1","updated":"2025-02-13T12:31:09Z","published":"2025-02-13T12:31:09Z","title":"Memory-based Ensemble Learning in CMR Semantic Segmentation","summary":"  Existing models typically segment either the entire 3D frame or 2D slices\nindependently to derive clinical functional metrics from ventricular\nsegmentation in cardiac cine sequences. While performing well overall, they\nstruggle at the end slices. To address this, we leverage spatial continuity to\nextract global uncertainty from segmentation variance and use it as memory in\nour ensemble learning method, Streaming, for classifier weighting, balancing\noverall and end-slice performance. Additionally, we introduce the End\nCoefficient (EC) to quantify end-slice accuracy. Experiments on ACDC and M\\&Ms\ndatasets show that our framework achieves near-state-of-the-art Dice Similarity\nCoefficient (DSC) and outperforms all models on end-slice performance,\nimproving patient-specific segmentation accuracy.\n","authors":["Yiwei Liu","Ziyi Wu","Liang Zhong","Linyi Wen","Yuankai Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16290v3","updated":"2025-02-13T12:16:47Z","published":"2024-10-05T20:03:57Z","title":"A Unified Model for Compressed Sensing MRI Across Undersampling Patterns","summary":"  Compressed Sensing MRI reconstructs images of the body's internal anatomy\nfrom undersampled measurements, thereby reducing the scan time - the time\nsubjects need to remain still. Recently, deep neural networks have shown great\npotential for reconstructing high-fidelity images from highly undersampled\nmeasurements in the frequency space. However, one needs to train multiple\nmodels for different undersampling patterns and desired output image\nresolutions, since most networks operate on a fixed discretization. Such\napproaches are highly impractical in clinical settings, where undersampling\npatterns and image resolutions are frequently changed to accommodate different\nreal-time imaging and diagnostic requirements.\n  We propose a unified model robust to different measurement undersampling\npatterns and image resolutions in compressed sensing MRI. Our model is based on\nneural operators, a discretization-agnostic architecture. Neural operators are\nemployed in both image and measurement space, which capture local and global\nimage features for MRI reconstruction. Empirically, we achieve consistent\nperformance across different undersampling rates and patterns, with an average\n11 percent SSIM and 4dB PSNR improvement over a state-of-the-art CNN,\nEnd-to-End VarNet. For efficiency, our inference speed is also 1,400x faster\nthan diffusion methods. The resolution-agnostic design also enhances zero-shot\nsuper-resolution and extended field of view in reconstructed images. Our\nunified model offers a versatile solution for MRI, adapting seamlessly to\nvarious measurement undersampling and imaging resolutions, making it highly\neffective for flexible and reliable clinical imaging. Our code is available at\nhttps://armeet.ca/nomri.\n","authors":["Armeet Singh Jatyani","Jiayun Wang","Aditi Chandrashekar","Zihui Wu","Miguel Liu-Schiaffini","Bahareh Tolooshams","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2410.16290v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09256v1","updated":"2025-02-13T12:11:58Z","published":"2025-02-13T12:11:58Z","title":"DynSegNet:Dynamic Architecture Adjustment for Adversarial Learning in\n  Segmenting Hemorrhagic Lesions from Fundus Images","summary":"  The hemorrhagic lesion segmentation plays a critical role in ophthalmic\ndiagnosis, directly influencing early disease detection, treatment planning,\nand therapeutic efficacy evaluation. However, the task faces significant\nchallenges due to lesion morphological variability, indistinct boundaries, and\nlow contrast with background tissues. To improve diagnostic accuracy and\ntreatment outcomes, developing advanced segmentation techniques remains\nimperative. This paper proposes an adversarial learning-based dynamic\narchitecture adjustment approach that integrates hierarchical U-shaped\nencoder-decoder, residual blocks, attention mechanisms, and ASPP modules. By\ndynamically optimizing feature fusion, our method enhances segmentation\nperformance. Experimental results demonstrate a Dice coefficient of 0.6802, IoU\nof 0.5602, Recall of 0.766, Precision of 0.6525, and Accuracy of 0.9955,\neffectively addressing the challenges in fundus image hemorrhage\nsegmentation.[* Corresponding author.]\n","authors":["Zesheng Li","Minwen Liao","Haoran Chen","Yan Su","Chengchang Pan","Honggang Qi"],"pdf_url":"https://arxiv.org/pdf/2502.09256v1.pdf","comment":"12 pages,4 figures"},{"id":"http://arxiv.org/abs/2410.08646v2","updated":"2025-02-13T12:10:11Z","published":"2024-10-11T09:16:30Z","title":"Fully Unsupervised Dynamic MRI Reconstruction via Diffeo-Temporal\n  Equivariance","summary":"  Reconstructing dynamic MRI image sequences from undersampled accelerated\nmeasurements is crucial for faster and higher spatiotemporal resolution\nreal-time imaging of cardiac motion, free breathing motion and many other\napplications. Classical paradigms, such as gated cine MRI, assume periodicity,\ndisallowing imaging of true motion. Supervised deep learning methods are\nfundamentally flawed as, in dynamic imaging, ground truth fully-sampled videos\nare impossible to truly obtain. We propose an unsupervised framework to learn\nto reconstruct dynamic MRI sequences from undersampled measurements alone by\nleveraging natural geometric spatiotemporal equivariances of MRI. Dynamic\nDiffeomorphic Equivariant Imaging (DDEI) significantly outperforms\nstate-of-the-art unsupervised methods such as SSDU on highly accelerated\ndynamic cardiac imaging. Our method is agnostic to the underlying neural\nnetwork architecture and can be used to adapt the latest models and\npost-processing approaches. Our code and video demos are at\nhttps://github.com/Andrewwango/ddei.\n","authors":["Andrew Wang","Mike Davies"],"pdf_url":"https://arxiv.org/pdf/2410.08646v2.pdf","comment":"Conference paper at ISBI 2025"},{"id":"http://arxiv.org/abs/2502.09211v1","updated":"2025-02-13T11:47:59Z","published":"2025-02-13T11:47:59Z","title":"Visual Graph Question Answering with ASP and LLMs for Language Parsing","summary":"  Visual Question Answering (VQA) is a challenging problem that requires to\nprocess multimodal input. Answer-Set Programming (ASP) has shown great\npotential in this regard to add interpretability and explainability to modular\nVQA architectures. In this work, we address the problem of how to integrate ASP\nwith modules for vision and natural language processing to solve a new and\ndemanding VQA variant that is concerned with images of graphs (not graphs in\nsymbolic form). Images containing graph-based structures are an ubiquitous and\npopular form of visualisation. Here, we deal with the particular problem of\ngraphs inspired by transit networks, and we introduce a novel dataset that\namends an existing one by adding images of graphs that resemble metro lines.\nOur modular neuro-symbolic approach combines optical graph recognition for\ngraph parsing, a pretrained optical character recognition neural network for\nparsing labels, Large Language Models (LLMs) for language processing, and ASP\nfor reasoning. This method serves as a first baseline and achieves an overall\naverage accuracy of 73% on the dataset. Our evaluation provides further\nevidence of the potential of modular neuro-symbolic systems, in particular with\npretrained models that do not involve any further training and logic\nprogramming for reasoning, to solve complex VQA tasks.\n","authors":["Jakob Johannes Bauer","Thomas Eiter","Nelson Higuera Ruiz","Johannes Oetsch"],"pdf_url":"https://arxiv.org/pdf/2502.09211v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453. This work was partially\n  funded from the Bosch Center for AI"},{"id":"http://arxiv.org/abs/2502.09202v1","updated":"2025-02-13T11:40:46Z","published":"2025-02-13T11:40:46Z","title":"Faster than real-time detection of shot boundaries, sampling structure\n  and dynamic keyframes in video","summary":"  The detection of shot boundaries (hardcuts and short dissolves), sampling\nstructure (progressive / interlaced / pulldown) and dynamic keyframes in a\nvideo are fundamental video analysis tasks which have to be done before any\nfurther high-level analysis tasks. We present a novel algorithm which does all\nthese analysis tasks in an unified way, by utilizing a combination of\ninter-frame and intra-frame measures derived from the motion field and\nnormalized cross correlation. The algorithm runs four times faster than\nreal-time due to sparse and selective calculation of these measures. An initial\nevaluation furthermore shows that the proposed algorithm is extremely robust\neven for challenging content showing large camera or object motion,\nflashlights, flicker or low contrast / noise.\n","authors":["Hannes Fassold"],"pdf_url":"https://arxiv.org/pdf/2502.09202v1.pdf","comment":"Accepted for ICISPC 2024"},{"id":"http://arxiv.org/abs/2502.07838v2","updated":"2025-02-13T11:13:14Z","published":"2025-02-11T02:31:45Z","title":"NanoVLMs: How small can we go and still make coherent Vision Language\n  Models?","summary":"  Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have\ngarnered significant research attention for their ability to leverage Large\nLanguage Models (LLMs) in multimodal tasks. However, their potential is\nconstrained by inherent challenges, including proprietary restrictions,\nsubstantial computational demands, and limited accessibility. Smaller models,\nsuch as GIT and BLIP, exhibit marked limitations, often failing to generate\ncoherent and consistent text beyond a few tokens, even with extensive training.\nThis underscores a pivotal inquiry: how small can a VLM be and still produce\nfluent and consistent text? Drawing inspiration from the exceptional learning\nprocess of 3-4 year old children, who rely heavily on visual cues for\nunderstanding and communication, we introduce two novel datasets: ShortDesc\n(featuring concise image descriptions) and LongDesc (containing more detailed\nimage descriptions). These datasets consist of image-text pairs where the text\nis restricted to the simple vocabulary and syntax typically used by young\nchildren, generated with a scaled- down model, GPT-4o. Using these datasets, we\ndemonstrate that it is possible to train VLMs that are significantly smaller,\nup to 10 times smaller than state of the art(SOTA) small VLMs while maintaining\narchitectural simplicity. To evaluate the outputs, we leverage GPT-4o to grade\nthe text, as if stories written by students, on creativity, meaningfulness, and\nconsistency, assigning scores out of 10. This method addresses limitations of\nstandard benchmarks by accommodating unstructured outputs and providing a\nmultidimensional evaluation of the model capabilities. Our findings contribute\nto the development of lightweight, accessible multimodal models for resource\nconstrained environments.\n","authors":["Mukund Agarwalla","Himanshu Kumar","Raj Dandekar","Rajat Dandekar","Sreedath Panat"],"pdf_url":"https://arxiv.org/pdf/2502.07838v2.pdf","comment":"11 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09164v1","updated":"2025-02-13T10:48:11Z","published":"2025-02-13T10:48:11Z","title":"E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot\n  Object Customization","summary":"  We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked\n$\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions\nand $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient\nframework for zero-shot object image customization. Unlike prior works reliant\non resource-intensive Unet architectures, our approach employs lightweight\nmasked diffusion transformers operating on latent patches, offering\nsignificantly improved computational efficiency. The framework integrates three\ncore components: (1) an efficient masked diffusion transformer for processing\nautoencoder latents, (2) a disentangled condition design that ensures\ncompactness while preserving background alignment and fine details, and (3) a\nlearnable Conditions Collector that consolidates multiple inputs into a compact\nrepresentation for efficient denoising and learning. E-MD3C outperforms the\nexisting approach on the VITON-HD dataset across metrics such as PSNR, FID,\nSSIM, and LPIPS, demonstrating clear advantages in parameters, memory\nefficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our\nTransformer-based 468M model delivers $2.5\\times$ faster inference and uses\n$\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent\ndiffusion model.\n","authors":["Trung X. Pham","Zhang Kang","Ji Woo Hong","Xuran Zheng","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2502.09164v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.02548v3","updated":"2025-02-13T10:46:38Z","published":"2024-06-04T17:59:31Z","title":"Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance\n  Segmentation","summary":"  Recent works on open-vocabulary 3D instance segmentation show strong promise,\nbut at the cost of slow inference speed and high computation requirements. This\nhigh computation cost is typically due to their heavy reliance on 3D clip\nfeatures, which require computationally expensive 2D foundation models like\nSegment Anything (SAM) and CLIP for multi-view aggregation into 3D. As a\nconsequence, this hampers their applicability in many real-world applications\nthat require both fast and accurate predictions. To this end, we propose a fast\nyet accurate open-vocabulary 3D instance segmentation approach, named Open-YOLO\n3D, that effectively leverages only 2D object detection from multi-view RGB\nimages for open-vocabulary 3D instance segmentation. We address this task by\ngenerating class-agnostic 3D masks for objects in the scene and associating\nthem with text prompts. We observe that the projection of class-agnostic 3D\npoint cloud instances already holds instance information; thus, using SAM might\nonly result in redundancy that unnecessarily increases the inference time. We\nempirically find that a better performance of matching text prompts to 3D masks\ncan be achieved in a faster fashion with a 2D object detector. We validate our\nOpen-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios:\n(i) with ground truth masks, where labels are required for given object\nproposals, and (ii) with class-agnostic 3D proposals generated from a 3D\nproposal network. Our Open-YOLO 3D achieves state-of-the-art performance on\nboth datasets while obtaining up to $\\sim$16$\\times$ speedup compared to the\nbest existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D\nachieves mean average precision (mAP) of 24.7\\% while operating at 22 seconds\nper scene. Code and model are available at github.com/aminebdj/OpenYOLO3D.\n","authors":["Mohamed El Amine Boudjoghra","Angela Dai","Jean Lahoud","Hisham Cholakkal","Rao Muhammad Anwer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2406.02548v3.pdf","comment":"ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.09150v1","updated":"2025-02-13T10:25:52Z","published":"2025-02-13T10:25:52Z","title":"Shortcut Learning Susceptibility in Vision Classifiers","summary":"  Shortcut learning, where machine learning models exploit spurious\ncorrelations in data instead of capturing meaningful features, poses a\nsignificant challenge to building robust and generalizable models. This\nphenomenon is prevalent across various machine learning applications, including\nvision, natural language processing, and speech recognition, where models may\nfind unintended cues that minimize training loss but fail to capture the\nunderlying structure of the data. Vision classifiers such as Convolutional\nNeural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers\n(ViTs) leverage distinct architectural principles to process spatial and\nstructural information, making them differently susceptible to shortcut\nlearning. In this study, we systematically evaluate these architectures by\nintroducing deliberate shortcuts into the dataset that are positionally\ncorrelated with class labels, creating a controlled setup to assess whether\nmodels rely on these artificial cues or learn actual distinguishing features.\nWe perform both quantitative evaluation by training on the shortcut-modified\ndataset and testing them on two different test sets -- one containing the same\nshortcuts and another without them -- to determine the extent of reliance on\nshortcuts. Additionally, qualitative evaluation is performed by using network\ninversion-based reconstruction techniques to analyze what the models\ninternalize in their weights, aiming to reconstruct the training data as\nperceived by the classifiers. We evaluate shortcut learning behavior across\nmultiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and\nCIFAR-10, to compare the susceptibility of different vision classifier\narchitectures to shortcut reliance and assess their varying degrees of\nsensitivity to spurious correlations.\n","authors":["Pirzada Suhail","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2502.09150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09148v1","updated":"2025-02-13T10:23:45Z","published":"2025-02-13T10:23:45Z","title":"Multimodal HIE Lesion Segmentation in Neonates: A Comparative Study of\n  Loss Functions","summary":"  Segmentation of Hypoxic-Ischemic Encephalopathy (HIE) lesions in neonatal MRI\nis a crucial but challenging task due to diffuse multifocal lesions with\nvarying volumes and the limited availability of annotated HIE lesion datasets.\nUsing the BONBID-HIE dataset, we implemented a 3D U-Net with optimized\npreprocessing, augmentation, and training strategies to overcome data\nconstraints. The goal of this study is to identify the optimal loss function\nspecifically for the HIE lesion segmentation task. To this end, we evaluated\nvarious loss functions, including Dice, Dice-Focal, Tversky, Hausdorff Distance\n(HausdorffDT) Loss, and two proposed compound losses -- Dice-Focal-HausdorffDT\nand Tversky-HausdorffDT -- to enhance segmentation performance. The results\nshow that different loss functions predict distinct segmentation masks, with\ncompound losses outperforming standalone losses. Tversky-HausdorffDT Loss\nachieves the highest Dice and Normalized Surface Dice scores, while\nDice-Focal-HausdorffDT Loss minimizes Mean Surface Distance. This work\nunderscores the significance of task-specific loss function optimization,\ndemonstrating that combining region-based and boundary-aware losses leads to\nmore accurate HIE lesion segmentation, even with limited training data.\n","authors":["Annayah Usman","Abdul Haseeb","Tahir Syed"],"pdf_url":"https://arxiv.org/pdf/2502.09148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09143v1","updated":"2025-02-13T10:18:44Z","published":"2025-02-13T10:18:44Z","title":"Feature-based Graph Attention Networks Improve Online Continual Learning","summary":"  Online continual learning for image classification is crucial for models to\nadapt to new data while retaining knowledge of previously learned tasks. This\ncapability is essential to address real-world challenges involving dynamic\nenvironments and evolving data distributions. Traditional approaches\npredominantly employ Convolutional Neural Networks, which are limited to\nprocessing images as grids and primarily capture local patterns rather than\nrelational information. Although the emergence of transformer architectures has\nimproved the ability to capture relationships, these models often require\nsignificantly larger resources. In this paper, we present a novel online\ncontinual learning framework based on Graph Attention Networks (GATs), which\neffectively capture contextual relationships and dynamically update the\ntask-specific representation via learned attention weights. Our approach\nutilizes a pre-trained feature extractor to convert images into graphs using\nhierarchical feature maps, representing information at varying levels of\ngranularity. These graphs are then processed by a GAT and incorporate an\nenhanced global pooling strategy to improve classification performance for\ncontinual learning. In addition, we propose the rehearsal memory duplication\ntechnique that improves the representation of the previous tasks while\nmaintaining the memory budget. Comprehensive evaluations on benchmark datasets,\nincluding SVHN, CIFAR10, CIFAR100, and MiniImageNet, demonstrate the\nsuperiority of our method compared to the state-of-the-art methods.\n","authors":["Adjovi Sim","Zhengkui Wang","Aik Beng Ng","Shalini De Mello","Simon See","Wonmin Byeon"],"pdf_url":"https://arxiv.org/pdf/2502.09143v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2407.03635v2","updated":"2025-02-13T10:18:34Z","published":"2024-07-04T04:55:14Z","title":"SSP-IR: Semantic and Structure Priors for Diffusion-based Realistic\n  Image Restoration","summary":"  Realistic image restoration is a crucial task in computer vision, and\ndiffusion-based models for image restoration have garnered significant\nattention due to their ability to produce realistic results. Restoration can be\nseen as a controllable generation conditioning on priors. However, due to the\nseverity of image degradation, existing diffusion-based restoration methods\ncannot fully exploit priors from low-quality images and still have many\nchallenges in perceptual quality, semantic fidelity, and structure accuracy.\nBased on the challenges, we introduce a novel image restoration method, SSP-IR.\nOur approach aims to fully exploit semantic and structure priors from\nlow-quality images to guide the diffusion model in generating semantically\nfaithful and structurally accurate natural restoration results. Specifically,\nwe integrate the visual comprehension capabilities of Multimodal Large Language\nModels (explicit) and the visual representations of the original image\n(implicit) to acquire accurate semantic prior. To extract\ndegradation-independent structure prior, we introduce a Processor with RGB and\nFFT constraints to extract structure prior from the low-quality images, guiding\nthe diffusion model and preventing the generation of unreasonable artifacts.\nLastly, we employ a multi-level attention mechanism to integrate the acquired\nsemantic and structure priors. The qualitative and quantitative results\ndemonstrate that our method outperforms other state-of-the-art methods overall\non both synthetic and real-world datasets. Our project page is\nhttps://zyhrainbow.github.io/projects/SSP-IR.\n","authors":["Yuhong Zhang","Hengsheng Zhang","Zhengxue Cheng","Rong Xie","Li Song","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.03635v2.pdf","comment":"To be published in IEEE TCSVT"},{"id":"http://arxiv.org/abs/2502.09140v1","updated":"2025-02-13T10:15:16Z","published":"2025-02-13T10:15:16Z","title":"Replay-free Online Continual Learning with Self-Supervised MultiPatches","summary":"  Online Continual Learning (OCL) methods train a model on a non-stationary\ndata stream where only a few examples are available at a time, often leveraging\nreplay strategies. However, usage of replay is sometimes forbidden, especially\nin applications with strict privacy regulations. Therefore, we propose\nContinual MultiPatches (CMP), an effective plug-in for existing OCL\nself-supervised learning strategies that avoids the use of replay samples. CMP\ngenerates multiple patches from a single example and projects them into a\nshared feature space, where patches coming from the same example are pushed\ntogether without collapsing into a single point. CMP surpasses replay and other\nSSL-based strategies on OCL streams, challenging the role of replay as a go-to\nsolution for self-supervised OCL.\n","authors":["Giacomo Cignoni","Andrea Cossu","Alex Gomez-Villa","Joost van de Weijer","Antonio Carta"],"pdf_url":"https://arxiv.org/pdf/2502.09140v1.pdf","comment":"Accepted at ESANN 2025"},{"id":"http://arxiv.org/abs/2407.02371v3","updated":"2025-02-13T10:13:24Z","published":"2024-07-02T15:40:29Z","title":"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video\n  Generation","summary":"  Text-to-video (T2V) generation has recently garnered significant attention\nthanks to the large multi-modality model Sora. However, T2V generation still\nfaces two important challenges: 1) Lacking a precise open sourced high-quality\ndataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M,\nare either with low quality or too large for most research institutions.\nTherefore, it is challenging but crucial to collect a precise high-quality\ntext-video pairs for T2V generation. 2) Ignoring to fully utilize textual\ninformation. Recent T2V methods have focused on vision transformers, using a\nsimple cross attention module for video generation, which falls short of\nthoroughly extracting semantic information from text prompt. To address these\nissues, we introduce OpenVid-1M, a precise high-quality dataset with expressive\ncaptions. This open-scenario dataset contains over 1 million text-video pairs,\nfacilitating research on T2V generation. Furthermore, we curate 433K 1080p\nvideos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition\nvideo generation. Additionally, we propose a novel Multi-modal Video Diffusion\nTransformer (MVDiT) capable of mining both structure information from visual\ntokens and semantic information from text tokens. Extensive experiments and\nablation studies verify the superiority of OpenVid-1M over previous datasets\nand the effectiveness of our MVDiT.\n","authors":["Kepan Nan","Rui Xie","Penghao Zhou","Tiehan Fan","Zhenheng Yang","Zhijie Chen","Xiang Li","Jian Yang","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2407.02371v3.pdf","comment":"20 pages, 15 figures, Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09125v1","updated":"2025-02-13T10:03:29Z","published":"2025-02-13T10:03:29Z","title":"Automatic Pruning via Structured Lasso with Class-wise Information","summary":"  Most pruning methods concentrate on unimportant filters of neural networks.\nHowever, they face the loss of statistical information due to a lack of\nconsideration for class-wise data. In this paper, from the perspective of\nleveraging precise class-wise information for model pruning, we utilize\nstructured lasso with guidance from Information Bottleneck theory. Our approach\nensures that statistical information is retained during the pruning process.\nWith these techniques, we introduce two innovative adaptive network pruning\nschemes: sparse graph-structured lasso pruning with Information Bottleneck\n(\\textbf{sGLP-IB}) and sparse tree-guided lasso pruning with Information\nBottleneck (\\textbf{sTLP-IB}). The key aspect is pruning model filters using\nsGLP-IB and sTLP-IB to better capture class-wise relatedness. Compared to\nmultiple state-of-the-art methods, our approaches demonstrate superior\nperformance across three datasets and six model architectures in extensive\nexperiments. For instance, using the VGG16 model on the CIFAR-10 dataset, we\nachieve a parameter reduction of 85%, a decrease in FLOPs by 61%, and maintain\nan accuracy of 94.10% (0.14% higher than the original model); we reduce the\nparameters by 55% with the accuracy at 76.12% using the ResNet architecture on\nImageNet (only drops 0.03%). In summary, we successfully reduce model size and\ncomputational resource usage while maintaining accuracy. Our codes are at\nhttps://anonymous.4open.science/r/IJCAI-8104.\n","authors":["Xiang Liu","Mingchen Li","Xia Li","Leigang Qu","Zifan Peng","Yijun Song","Zemin Liu","Linshan Jiang","Jialin Li"],"pdf_url":"https://arxiv.org/pdf/2502.09125v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.17869v2","updated":"2025-02-13T10:02:14Z","published":"2023-10-27T03:07:05Z","title":"Grid Jigsaw Representation with CLIP: A New Perspective on Image\n  Clustering","summary":"  Unsupervised representation learning for image clustering is essential in\ncomputer vision. Although the advancement of visual models has improved image\nclustering with efficient visual representations, challenges still remain.\nFirstly, existing features often lack the ability to represent the internal\nstructure of images, hindering the accurate clustering of visually similar\nimages. Secondly, finer-grained semantic labels are often missing, limiting the\nability to capture nuanced differences and similarities between images. In this\npaper, we propose a new perspective on image clustering, the pretrain-based\nGrid Jigsaw Representation (pGJR). Inspired by human jigsaw puzzle processing,\nwe modify the traditional jigsaw learning to gain a more sequential and\nincremental understanding of image structure. We also leverage the pretrained\nCLIP to extract the prior features which can benefit from the enhanced\ncross-modal representation for richer and more nuanced semantic information and\nlabel level differentiation. Our experiments demonstrate that using the\npretrained model as a feature extractor can accelerate the convergence of\nclustering. We append the GJR module to pGJR and observe significant\nimprovements on common-use benchmark datasets. The experimental results\nhighlight the effectiveness of our approach in the clustering task, as\nevidenced by improvements in the ACC, NMI, and ARI metrics, as well as the\nsuper-fast convergence speed.\n","authors":["Zijie Song","Zhenzhen Hu","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2310.17869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08925v3","updated":"2025-02-13T10:00:58Z","published":"2024-10-11T15:50:31Z","title":"An Overview of Prototype Formulations for Interpretable Deep Learning","summary":"  Prototypical part networks offer interpretable alternatives to black-box deep\nlearning models. However, many of these networks rely on Euclidean prototypes,\nwhich may limit their flexibility. This work provides a comprehensive overview\nof various prototype formulations. Experiments conducted on the CUB-200-2011,\nStanford Cars, and Oxford Flowers datasets demonstrate the effectiveness and\nversatility of these different formulations.\n","authors":["Maximilian Xiling Li","Korbinian Franz Rudolf","Nils Blank","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.08925v3.pdf","comment":"Equal Contribution of M.X.Li and K.F.Rudolf"},{"id":"http://arxiv.org/abs/2502.09122v1","updated":"2025-02-13T09:57:25Z","published":"2025-02-13T09:57:25Z","title":"Improving Deep Regression with Tightness","summary":"  For deep regression, preserving the ordinality of the targets with respect to\nthe feature representation improves performance across various tasks. However,\na theoretical explanation for the benefits of ordinality is still lacking. This\nwork reveals that preserving ordinality reduces the conditional entropy\n$H(Z|Y)$ of representation $Z$ conditional on the target $Y$. However, our\nfindings reveal that typical regression losses do little to reduce $H(Z|Y)$,\neven though it is vital for generalization performance. With this motivation,\nwe introduce an optimal transport-based regularizer to preserve the similarity\nrelationships of targets in the feature space to reduce $H(Z|Y)$. Additionally,\nwe introduce a simple yet efficient strategy of duplicating the regressor\ntargets, also with the aim of reducing $H(Z|Y)$. Experiments on three\nreal-world regression tasks verify the effectiveness of our strategies to\nimprove deep regression. Code:\nhttps://github.com/needylove/Regression_tightness.\n","authors":["Shihao Zhang","Yuguang Yan","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2502.09122v1.pdf","comment":"ICLR 2025, Code: https://github.com/needylove/Regression_tightness"},{"id":"http://arxiv.org/abs/2404.03713v2","updated":"2025-02-13T09:48:12Z","published":"2024-04-04T17:46:20Z","title":"Explaining Explainability: Recommendations for Effective Use of Concept\n  Activation Vectors","summary":"  Concept-based explanations translate the internal representations of deep\nlearning models into a language that humans are familiar with: concepts. One\npopular method for finding concepts is Concept Activation Vectors (CAVs), which\nare learnt using a probe dataset of concept exemplars. In this work, we\ninvestigate three properties of CAVs: (1) inconsistency across layers, (2)\nentanglement with other concepts, and (3) spatial dependency. Each property\nprovides both challenges and opportunities in interpreting models. We introduce\ntools designed to detect the presence of these properties, provide insight into\nhow each property can lead to misleading explanations, and provide\nrecommendations to mitigate their impact. To demonstrate practical\napplications, we apply our recommendations to a melanoma classification task,\nshowing how entanglement can lead to uninterpretable results and that the\nchoice of negative probe set can have a substantial impact on the meaning of a\nCAV. Further, we show that understanding these properties can be used to our\nadvantage. For example, we introduce spatially dependent CAVs to test if a\nmodel is translation invariant with respect to a specific concept and class.\nOur experiments are performed on natural images (ImageNet), skin lesions (ISIC\n2019), and a new synthetic dataset, Elements. Elements is designed to capture a\nknown ground truth relationship between concepts and classes. We release this\ndataset to facilitate further research in understanding and evaluating\ninterpretability methods.\n","authors":["Angus Nicolson","Lisa Schut","J. Alison Noble","Yarin Gal"],"pdf_url":"https://arxiv.org/pdf/2404.03713v2.pdf","comment":"Accepted by Transactions on Machine Learning Research (02/2025)"},{"id":"http://arxiv.org/abs/2502.09111v1","updated":"2025-02-13T09:41:08Z","published":"2025-02-13T09:41:08Z","title":"DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance\n  Prior","summary":"  Gaussian SLAM systems excel in real-time rendering and fine-grained\nreconstruction compared to NeRF-based systems. However, their reliance on\nextensive keyframes is impractical for deployment in real-world robotic\nsystems, which typically operate under sparse-view conditions that can result\nin substantial holes in the map. To address these challenges, we introduce\nDenseSplat, the first SLAM system that effectively combines the advantages of\nNeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for\ninitializing primitives that densely populate maps and seamlessly fill gaps. It\nalso implements geometry-aware primitive sampling and pruning strategies to\nmanage granularity and enhance rendering efficiency. Moreover, DenseSplat\nintegrates loop closure and bundle adjustment, significantly enhancing\nframe-to-frame tracking accuracy. Extensive experiments on multiple large-scale\ndatasets demonstrate that DenseSplat achieves superior performance in tracking\nand mapping compared to current state-of-the-art methods.\n","authors":["Mingrui Li","Shuhong Liu","Tianchen Deng","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09110v1","updated":"2025-02-13T09:40:26Z","published":"2025-02-13T09:40:26Z","title":"Pulling Back the Curtain: Unsupervised Adversarial Detection via\n  Contrastive Auxiliary Networks","summary":"  Deep learning models are widely employed in safety-critical applications yet\nremain susceptible to adversarial attacks -- imperceptible perturbations that\ncan significantly degrade model performance. Conventional defense mechanisms\npredominantly focus on either enhancing model robustness or detecting\nadversarial inputs independently. In this work, we propose an Unsupervised\nadversarial detection via Contrastive Auxiliary Networks (U-CAN) to uncover\nadversarial behavior within auxiliary feature representations, without the need\nfor adversarial examples. U-CAN is embedded within selected intermediate layers\nof the target model. These auxiliary networks, comprising projection layers and\nArcFace-based linear layers, refine feature representations to more effectively\ndistinguish between benign and adversarial inputs. Comprehensive experiments\nacross multiple datasets (CIFAR-10, Mammals, and a subset of ImageNet) and\narchitectures (ResNet-50, VGG-16, and ViT) demonstrate that our method\nsurpasses existing unsupervised adversarial detection techniques, achieving\nsuperior F1 scores against four distinct attack methods. The proposed framework\nprovides a scalable and effective solution for enhancing the security and\nreliability of deep learning systems.\n","authors":["Eylon Mizrahi","Raz Lapid","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2502.09110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06657v2","updated":"2025-02-13T09:33:39Z","published":"2023-12-11T18:59:31Z","title":"Learning Naturally Aggregated Appearance for Efficient 3D Editing","summary":"  Neural radiance fields, which represent a 3D scene as a color field and a\ndensity field, have demonstrated great progress in novel view synthesis yet are\nunfavorable for editing due to the implicitness. This work studies the task of\nefficient 3D editing, where we focus on editing speed and user interactivity.\nTo this end, we propose to learn the color field as an explicit 2D appearance\naggregation, also called canonical image, with which users can easily customize\ntheir 3D editing via 2D image processing. We complement the canonical image\nwith a projection field that maps 3D points onto 2D pixels for texture query.\nThis field is initialized with a pseudo canonical camera model and optimized\nwith offset regularity to ensure the naturalness of the canonical image.\nExtensive experiments on different datasets suggest that our representation,\ndubbed AGAP, well supports various ways of 3D editing (e.g., stylization,\ninstance segmentation, and interactive drawing). Our approach demonstrates\nremarkable efficiency by being at least 20 times faster per edit compared to\nexisting NeRF-based editing methods. Project page is available at\nhttps://felixcheng97.github.io/AGAP/.\n","authors":["Ka Leong Cheng","Qiuyu Wang","Zifan Shi","Kecheng Zheng","Yinghao Xu","Hao Ouyang","Qifeng Chen","Yujun Shen"],"pdf_url":"https://arxiv.org/pdf/2312.06657v2.pdf","comment":"Project page: https://felixcheng97.github.io/AGAP/; accepted to 3DV\n  2025"},{"id":"http://arxiv.org/abs/2408.07422v2","updated":"2025-02-13T09:32:44Z","published":"2024-08-14T10:00:16Z","title":"LLMI3D: MLLM-based 3D Perception from a Single 2D Image","summary":"  Recent advancements in autonomous driving, augmented reality, robotics, and\nembodied intelligence have necessitated 3D perception algorithms. However,\ncurrent 3D perception methods, especially specialized small models, exhibit\npoor generalization in open scenarios. On the other hand, multimodal large\nlanguage models (MLLMs) excel in general capacity but underperform in 3D tasks,\ndue to weak 3D local spatial object perception, poor text-based geometric\nnumerical output, and inability to handle camera focal variations. To address\nthese challenges, we propose the following solutions: Spatial-Enhanced Local\nFeature Mining for better spatial feature extraction, 3D Query Token-Derived\nInfo Decoding for precise geometric regression, and Geometry Projection-Based\n3D Reasoning for handling camera focal length variations. We employ\nparameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a\npowerful 3D perception MLLM. Additionally, we have constructed the IG3D\ndataset, which provides fine-grained descriptions and question-answer\nannotations. Extensive experiments demonstrate that our LLMI3D achieves\nstate-of-the-art performance, outperforming other methods by a large margin.\n","authors":["Fan Yang","Sicheng Zhao","Yanhao Zhang","Hui Chen","Haonan Lu","Jungong Han","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2408.07422v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08299v2","updated":"2025-02-13T09:28:41Z","published":"2025-02-12T10:59:45Z","title":"When do they StOP?: A First Step Towards Automatically Identifying Team\n  Communication in the Operating Room","summary":"  Purpose: Surgical performance depends not only on surgeons' technical skills\nbut also on team communication within and across the different professional\ngroups present during the operation. Therefore, automatically identifying team\ncommunication in the OR is crucial for patient safety and advances in the\ndevelopment of computer-assisted surgical workflow analysis and intra-operative\nsupport systems. To take the first step, we propose a new task of detecting\ncommunication briefings involving all OR team members, i.e. the team Time-out\nand the StOP?-protocol, by localizing their start and end times in video\nrecordings of surgical operations. Methods: We generate an OR dataset of real\nsurgeries, called Team-OR, with more than one hundred hours of surgical videos\ncaptured by the multi-view camera system in the OR. The dataset contains\ntemporal annotations of 33 Time-out and 22 StOP?-protocol activities in total.\nWe then propose a novel group activity detection approach, where we encode both\nscene context and action features, and use an efficient neural network model to\noutput the results. Results: The experimental results on the Team-OR dataset\nshow that our approach outperforms existing state-of-the-art temporal action\ndetection approaches. It also demonstrates the lack of research on group\nactivities in the OR, proving the significance of our dataset. Conclusion: We\ninvestigate the Team Time-Out and the StOP?-protocol in the OR, by presenting\nthe first OR dataset with temporal annotations of group activities protocols,\nand introducing a novel group activity detection approach that outperforms\nexisting approaches. Code is available at\nhttps://github.com/CAMMA-public/Team-OR.\n","authors":["Keqi Chen","Lilien Schewski","Vinkle Srivastav","Joël Lavanchy","Didier Mutter","Guido Beldi","Sandra Keller","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2502.08299v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09093v1","updated":"2025-02-13T09:04:28Z","published":"2025-02-13T09:04:28Z","title":"From Visuals to Vocabulary: Establishing Equivalence Between Image and\n  Text Token Through Autoregressive Pre-training in MLLMs","summary":"  While MLLMs perform well on perceptual tasks, they lack precise multimodal\nalignment, limiting performance. To address this challenge, we propose Vision\nDynamic Embedding-Guided Pretraining (VDEP), a hybrid autoregressive training\nparadigm for MLLMs. Utilizing dynamic embeddings from the MLP following the\nvisual encoder, this approach supervises image hidden states and integrates\nimage tokens into autoregressive training. Existing MLLMs primarily focused on\nrecovering information from textual inputs, often neglecting the effective\nprocessing of image data. In contrast, the key improvement of this work is the\nreinterpretation of multimodal alignment as a process of recovering information\nfrom input data, with particular emphasis on reconstructing detailed visual\nfeatures.The proposed method seamlessly integrates into standard models without\narchitectural changes. Experiments on 13 benchmarks show VDEP outperforms\nbaselines, surpassing existing methods.\n","authors":["Mingxiao Li","Fang Qu","Zhanpeng Chen","Na Su","Zhizhou Zhong","Ziyang Chen","Nan Du","Xiaolong Li"],"pdf_url":"https://arxiv.org/pdf/2502.09093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09088v1","updated":"2025-02-13T09:01:00Z","published":"2025-02-13T09:01:00Z","title":"Unsupervised Anomaly Detection on Implicit Shape representations for\n  Sarcopenia Detection","summary":"  Sarcopenia is an age-related progressive loss of muscle mass and strength\nthat significantly impacts daily life. A commonly studied criterion for\ncharacterizing the muscle mass has been the combination of 3D imaging and\nmanual segmentations. In this paper, we instead study the muscles' shape. We\nrely on an implicit neural representation (INR) to model normal muscle shapes.\nWe then introduce an unsupervised anomaly detection method to identify\nsarcopenic muscles based on the reconstruction error of the implicit model.\nRelying on a conditional INR with an auto-decoding strategy, we also learn a\nlatent representation of the muscles that clearly separates normal from\nabnormal muscles in an unsupervised fashion. Experimental results on a dataset\nof 103 segmented volumes indicate that our double anomaly detection strategy\neffectively discriminates sarcopenic and non-sarcopenic muscles.\n","authors":["Louise Piecuch","Jeremie Huet","Antoine Frouin","Antoine Nordez","Anne-Sophie Boureau","Diana Mateus"],"pdf_url":"https://arxiv.org/pdf/2502.09088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18880v3","updated":"2025-02-13T09:00:13Z","published":"2024-05-29T08:39:31Z","title":"EventZoom: A Progressive Approach to Event-Based Data Augmentation for\n  Enhanced Neuromorphic Vision","summary":"  Dynamic Vision Sensors (DVS) capture event data with high temporal resolution\nand low power consumption, presenting a more efficient solution for visual\nprocessing in dynamic and real-time scenarios compared to conventional video\ncapture methods. Event data augmentation serve as an essential method for\novercoming the limitation of scale and diversity in event datasets. Our\ncomparative experiments demonstrate that the two factors, spatial integrity and\ntemporal continuity, can significantly affect the capacity of event data\naugmentation, which are guarantee for maintaining the sparsity and high dynamic\nrange characteristics unique to event data. However, existing augmentation\nmethods often neglect the preservation of spatial integrity and temporal\ncontinuity. To address this, we developed a novel event data augmentation\nstrategy EventZoom, which employs a temporal progressive strategy, embedding\ntransformed samples into the original samples through progressive scaling and\nshifting. The scaling process avoids the spatial information loss associated\nwith cropping, while the progressive strategy prevents interruptions or abrupt\nchanges in temporal information. We validated EventZoom across various\nsupervised learning frameworks. The experimental results show that EventZoom\nconsistently outperforms existing event data augmentation methods with SOTA\nperformance. For the first time, we have concurrently employed Semi-supervised\nand Unsupervised learning to verify feasibility on event augmentation\nalgorithms, demonstrating the applicability and effectiveness of EventZoom as a\npowerful event-based data augmentation tool in handling real-world scenes with\nhigh dynamics and variability environments.\n","authors":["Yiting Dong","Xiang He","Guobin Shen","Dongcheng Zhao","Yang Li","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2405.18880v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2502.03966v3","updated":"2025-02-13T08:54:42Z","published":"2025-02-06T10:59:44Z","title":"MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation","summary":"  In this paper, we present synthetic data generation framework for flood\nhazard detection system. For high fidelity and quality, we characterize several\nreal-world properties into virtual world and simulate the flood situation by\ncontrolling them. For the sake of efficiency, recent generative models in\nimage-to-3D and urban city synthesis are leveraged to easily composite flood\nenvironments so that we avoid data bias due to the hand-crafted manner. Based\non our framework, we build the flood synthetic dataset with 5 levels, dubbed\nMultiFloodSynth which contains rich annotation types like normal map,\nsegmentation, 3D bounding box for a variety of downstream task. In experiments,\nour dataset demonstrate the enhanced performance of flood hazard detection with\non-par realism compared with real dataset.\n","authors":["YoonJe Kang","Yonghoon Jung","Wonseop Shin","Bumsoo Kim","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2502.03966v3.pdf","comment":"6 pages, 6 figures. Accepted as Oral Presentation to AAAI 2025\n  Workshop on Good-Data"},{"id":"http://arxiv.org/abs/2502.09080v1","updated":"2025-02-13T08:54:04Z","published":"2025-02-13T08:54:04Z","title":"BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian\n  Primitives for Weakly-Supervised Cross-View Localization","summary":"  This paper addresses the problem of weakly supervised cross-view\nlocalization, where the goal is to estimate the pose of a ground camera\nrelative to a satellite image with noisy ground truth annotations. A common\napproach to bridge the cross-view domain gap for pose estimation is Bird's-Eye\nView (BEV) synthesis. However, existing methods struggle with height ambiguity\ndue to the lack of depth information in ground images and satellite height\nmaps. Previous solutions either assume a flat ground plane or rely on complex\nmodels, such as cross-view transformers. We propose BevSplat, a novel method\nthat resolves height ambiguity by using feature-based Gaussian primitives. Each\npixel in the ground image is represented by a 3D Gaussian with semantic and\nspatial features, which are synthesized into a BEV feature map for relative\npose estimation. Additionally, to address challenges with panoramic query\nimages, we introduce an icosphere-based supervision strategy for the Gaussian\nprimitives. We validate our method on the widely used KITTI and VIGOR datasets,\nwhich include both pinhole and panoramic query images. Experimental results\nshow that BevSplat significantly improves localization accuracy over prior\napproaches.\n","authors":["Qiwei Wang","Shaoxun Wu","Yujiao Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09075v1","updated":"2025-02-13T08:45:43Z","published":"2025-02-13T08:45:43Z","title":"PTZ-Calib: Robust Pan-Tilt-Zoom Camera Calibration","summary":"  In this paper, we present PTZ-Calib, a robust two-stage PTZ camera\ncalibration method, that efficiently and accurately estimates camera parameters\nfor arbitrary viewpoints. Our method includes an offline and an online stage.\nIn the offline stage, we first uniformly select a set of reference images that\nsufficiently overlap to encompass a complete 360{\\deg} view. We then utilize\nthe novel PTZ-IBA (PTZ Incremental Bundle Adjustment) algorithm to\nautomatically calibrate the cameras within a local coordinate system.\nAdditionally, for practical application, we can further optimize camera\nparameters and align them with the geographic coordinate system using extra\nglobal reference 3D information. In the online stage, we formulate the\ncalibration of any new viewpoints as a relocalization problem. Our approach\nbalances the accuracy and computational efficiency to meet real-world demands.\nExtensive evaluations demonstrate our robustness and superior performance over\nstate-of-the-art methods on various real and synthetic datasets. Datasets and\nsource code can be accessed online at https://github.com/gjgjh/PTZ-Calib\n","authors":["Jinhui Guo","Lubin Fan","Bojian Wu","Jiaqi Gu","Shen Cao","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2502.09075v1.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2203.13310v5","updated":"2025-02-13T08:33:30Z","published":"2022-03-24T19:28:54Z","title":"MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection","summary":"  Monocular 3D object detection has long been a challenging task in autonomous\ndriving. Most existing methods follow conventional 2D detectors to first\nlocalize object centers, and then predict 3D attributes by neighboring\nfeatures. However, only using local visual features is insufficient to\nunderstand the scene-level 3D spatial structures and ignores the long-range\ninter-object depth relations. In this paper, we introduce the first DETR\nframework for Monocular DEtection with a depth-guided TRansformer, named\nMonoDETR. We modify the vanilla transformer to be depth-aware and guide the\nwhole detection process by contextual depth cues. Specifically, concurrent to\nthe visual encoder that captures object appearances, we introduce to predict a\nforeground depth map, and specialize a depth encoder to extract non-local depth\nembeddings. Then, we formulate 3D object candidates as learnable queries and\npropose a depth-guided decoder to conduct object-scene depth interactions. In\nthis way, each object query estimates its 3D attributes adaptively from the\ndepth-guided regions on the image and is no longer constrained to local visual\nfeatures. On KITTI benchmark with monocular images as input, MonoDETR achieves\nstate-of-the-art performance and requires no extra dense depth annotations.\nBesides, our depth-guided modules can also be plug-and-play to enhance\nmulti-view 3D object detectors on nuScenes dataset, demonstrating our superior\ngeneralization capacity. Code is available at\nhttps://github.com/ZrrSkywalker/MonoDETR.\n","authors":["Renrui Zhang","Han Qiu","Tai Wang","Ziyu Guo","Yiwen Tang","Xuanzhuo Xu","Ziteng Cui","Yu Qiao","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2203.13310v5.pdf","comment":"Accepted by ICCV 2023. Code is available at\n  https://github.com/ZrrSkywalker/MonoDETR"},{"id":"http://arxiv.org/abs/2502.09064v1","updated":"2025-02-13T08:26:54Z","published":"2025-02-13T08:26:54Z","title":"StyleBlend: Enhancing Style-Specific Content Creation in Text-to-Image\n  Diffusion Models","summary":"  Synthesizing visually impressive images that seamlessly align both text\nprompts and specific artistic styles remains a significant challenge in\nText-to-Image (T2I) diffusion models. This paper introduces StyleBlend, a\nmethod designed to learn and apply style representations from a limited set of\nreference images, enabling content synthesis of both text-aligned and\nstylistically coherent. Our approach uniquely decomposes style into two\ncomponents, composition and texture, each learned through different strategies.\nWe then leverage two synthesis branches, each focusing on a corresponding style\ncomponent, to facilitate effective style blending through shared features\nwithout affecting content generation. StyleBlend addresses the common issues of\ntext misalignment and weak style representation that previous methods have\nstruggled with. Extensive qualitative and quantitative comparisons demonstrate\nthe superiority of our approach.\n","authors":["Zichong Chen","Shijin Wang","Yang Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.09064v1.pdf","comment":"Accepted to Eurographics 2025. Project page:\n  https://zichongc.github.io/StyleBlend/"},{"id":"http://arxiv.org/abs/2410.02130v2","updated":"2025-02-13T08:24:37Z","published":"2024-10-03T01:23:44Z","title":"MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers\n  for Open-Domain Sound Generation","summary":"  We introduce MDSGen, a novel framework for vision-guided open-domain sound\ngeneration optimized for model parameter size, memory consumption, and\ninference speed. This framework incorporates two key innovations: (1) a\nredundant video feature removal module that filters out unnecessary visual\ninformation, and (2) a temporal-aware masking strategy that leverages temporal\ncontext for enhanced audio generation accuracy. In contrast to existing\nresource-heavy Unet-based models, \\texttt{MDSGen} employs denoising masked\ndiffusion transformers, facilitating efficient generation without reliance on\npre-trained diffusion models. Evaluated on the benchmark VGGSound dataset, our\nsmallest model (5M parameters) achieves $97.9$% alignment accuracy, using\n$172\\times$ fewer parameters, $371$% less memory, and offering $36\\times$\nfaster inference than the current 860M-parameter state-of-the-art model\n($93.9$% accuracy). The larger model (131M parameters) reaches nearly $99$%\naccuracy while requiring $6.5\\times$ fewer parameters. These results highlight\nthe scalability and effectiveness of our approach. The code is available at\nhttps://bit.ly/mdsgen.\n","authors":["Trung X. Pham","Tri Ton","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2410.02130v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09057v1","updated":"2025-02-13T08:11:10Z","published":"2025-02-13T08:11:10Z","title":"Vision-Language In-Context Learning Driven Few-Shot Visual Inspection\n  Model","summary":"  We propose general visual inspection model using Vision-Language Model~(VLM)\nwith few-shot images of non-defective or defective products, along with\nexplanatory texts that serve as inspection criteria. Although existing VLM\nexhibit high performance across various tasks, they are not trained on specific\ntasks such as visual inspection. Thus, we construct a dataset consisting of\ndiverse images of non-defective and defective products collected from the web,\nalong with unified formatted output text, and fine-tune VLM. For new products,\nour method employs In-Context Learning, which allows the model to perform\ninspections with an example of non-defective or defective image and the\ncorresponding explanatory texts with visual prompts. This approach eliminates\nthe need to collect a large number of training samples and re-train the model\nfor each product. The experimental results show that our method achieves high\nperformance, with MCC of 0.804 and F1-score of 0.950 on MVTec AD in a one-shot\nmanner. Our code is available\nat~https://github.com/ia-gu/Vision-Language-In-Context-Learning-Driven-Few-Shot-Visual-Inspection-Model.\n","authors":["Shiryu Ueno","Yoshikazu Hayashi","Shunsuke Nakatsuka","Yusei Yamada","Hiroaki Aizawa","Kunihito Kato"],"pdf_url":"https://arxiv.org/pdf/2502.09057v1.pdf","comment":"VISAPP 2025"},{"id":"http://arxiv.org/abs/2212.08650v3","updated":"2025-02-13T08:06:42Z","published":"2022-12-16T18:51:41Z","title":"ColorSense: A Study on Color Vision in Machine Visual Recognition","summary":"  Color vision is essential for human visual perception, but its impact on\nmachine perception is still underexplored. There has been an intensified demand\nfor understanding its role in machine perception for safety-critical tasks such\nas assistive driving and surgery but lacking suitable datasets. To fill this\ngap, we curate multipurpose datasets ColorSense, by collecting 110,000\nnon-trivial human annotations of foreground and background color labels from\npopular visual recognition benchmarks. To investigate the impact of color\nvision on machine perception, we assign each image a color discrimination level\nbased on its dominant foreground and background colors and use it to study the\nimpact of color vision on machine perception. We validate the use of our\ndatasets by demonstrating that the level of color discrimination has a\ndominating effect on the performance of mainstream machine perception models.\nSpecifically, we examine the perception ability of machine vision by\nconsidering key factors such as model architecture, training objective, model\nsize, training data, and task complexity. Furthermore, to investigate how color\nand environmental factors affect the robustness of visual recognition in\nmachine perception, we integrate our ColorSense datasets with image corruptions\nand perform a more comprehensive visual perception evaluation. Our findings\nsuggest that object recognition tasks such as classification and localization\nare susceptible to color vision bias, especially for high-stakes cases such as\nvehicle classes, and advanced mitigation techniques such as data augmentation\nand so on only give marginal improvement. Our analyses highlight the need for\nnew approaches toward the performance evaluation of machine perception models\nin real-world applications. Lastly, we present various potential applications\nof ColorSense such as studying spurious correlations.\n","authors":["Ming-Chang Chiu","Yingfei Wang","Derrick Eui Gyu Kim","Pin-Yu Chen","Xuezhe Ma"],"pdf_url":"https://arxiv.org/pdf/2212.08650v3.pdf","comment":"12 pages, 11 figures, Accepted at Secure and Trustworthy Machine\n  Learning"},{"id":"http://arxiv.org/abs/2502.09051v1","updated":"2025-02-13T08:05:44Z","published":"2025-02-13T08:05:44Z","title":"AIDE: Agentically Improve Visual Language Model with Domain Experts","summary":"  The enhancement of Visual Language Models (VLMs) has traditionally relied on\nknowledge distillation from larger, more capable models. This dependence\ncreates a fundamental bottleneck for improving state-of-the-art systems,\nparticularly when no superior models exist. We introduce AIDE (Agentic\nImprovement through Domain Experts), a novel framework that enables VLMs to\nautonomously enhance their capabilities by leveraging specialized domain expert\nmodels. AIDE operates through a four-stage process: (1) identifying instances\nfor refinement, (2) engaging domain experts for targeted analysis, (3)\nsynthesizing expert outputs with existing data, and (4) integrating enhanced\ninstances into the training pipeline. Experiments on multiple benchmarks,\nincluding MMMU, MME, MMBench, etc., demonstrate AIDE's ability to achieve\nnotable performance gains without relying on larger VLMs nor human supervision.\nOur framework provides a scalable, resource-efficient approach to continuous\nVLM improvement, addressing critical limitations in current methodologies,\nparticularly valuable when larger models are unavailable to access.\n","authors":["Ming-Chang Chiu","Fuxiao Liu","Karan Sapra","Andrew Tao","Yaser Jacoob","Xuezhe Ma","Zhiding Yu","Guilin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09051v1.pdf","comment":"6 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2502.09045v1","updated":"2025-02-13T08:01:29Z","published":"2025-02-13T08:01:29Z","title":"Evolution of Data-driven Single- and Multi-Hazard Susceptibility Mapping\n  and Emergence of Deep Learning Methods","summary":"  Data-driven susceptibility mapping of natural hazards has harnessed the\nadvances in classification methods used on heterogeneous sources represented as\nraster images. Susceptibility mapping is an important step towards risk\nassessment for any natural hazard. Increasingly, multiple hazards co-occur\nspatially, temporally, or both, which calls for an in-depth study on\nmulti-hazard susceptibility mapping. In recent years, single-hazard\nsusceptibility mapping algorithms have become well-established and have been\nextended to multi-hazard susceptibility mapping. Deep learning is also emerging\nas a promising method for single-hazard susceptibility mapping. Here, we\ndiscuss the evolution of methods for a single hazard, their extensions to\nmulti-hazard maps as a late fusion of decisions, and the use of deep learning\nmethods in susceptibility mapping. We finally propose a vision for adapting\ndata fusion strategies in multimodal deep learning to multi-hazard\nsusceptibility mapping. From the background study of susceptibility methods, we\ndemonstrate that deep learning models are promising, untapped methods for\nmulti-hazard susceptibility mapping. Data fusion strategies provide a larger\nspace of deep learning models applicable to multi-hazard susceptibility\nmapping.\n","authors":["Jaya Sreevalsan-Nair","Aswathi Mundayatt"],"pdf_url":"https://arxiv.org/pdf/2502.09045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09039v1","updated":"2025-02-13T07:48:56Z","published":"2025-02-13T07:48:56Z","title":"Large Images are Gaussians: High-Quality Large Image Representation with\n  Levels of 2D Gaussian Splatting","summary":"  While Implicit Neural Representations (INRs) have demonstrated significant\nsuccess in image representation, they are often hindered by large training\nmemory and slow decoding speed. Recently, Gaussian Splatting (GS) has emerged\nas a promising solution in 3D reconstruction due to its high-quality novel view\nsynthesis and rapid rendering capabilities, positioning it as a valuable tool\nfor a broad spectrum of applications. In particular, a GS-based representation,\n2DGS, has shown potential for image fitting. In our work, we present\n\\textbf{L}arge \\textbf{I}mages are \\textbf{G}aussians (\\textbf{LIG}), which\ndelves deeper into the application of 2DGS for image representations,\naddressing the challenge of fitting large images with 2DGS in the situation of\nnumerous Gaussian points, through two distinct modifications: 1) we adopt a\nvariant of representation and optimization strategy, facilitating the fitting\nof a large number of Gaussian points; 2) we propose a Level-of-Gaussian\napproach for reconstructing both coarse low-frequency initialization and fine\nhigh-frequency details. Consequently, we successfully represent large images as\nGaussian points and achieve high-quality large image representation,\ndemonstrating its efficacy across various types of large images. Code is\navailable at\n{\\href{https://github.com/HKU-MedAI/LIG}{https://github.com/HKU-MedAI/LIG}}.\n","authors":["Lingting Zhu","Guying Lin","Jinnan Chen","Xinjie Zhang","Zhenchao Jin","Zhao Wang","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2502.09039v1.pdf","comment":"Accepted by 39th Annual AAAI Conference on Artificial Intelligence\n  (AAAI 2025). 10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.01694v2","updated":"2025-02-13T07:42:33Z","published":"2024-12-02T16:37:50Z","title":"Enhancing Video-LLM Reasoning via Agent-of-Thoughts Distillation","summary":"  This paper tackles the problem of video question answering (VideoQA), a task\nthat often requires multi-step reasoning and a profound understanding of\nspatial-temporal dynamics. While large video-language models perform well on\nbenchmarks, they often lack explainability and spatial-temporal grounding. In\nthis paper, we propose Agent-of-Thoughts Distillation (AoTD), a method that\nenhances models by incorporating automatically generated Chain-of-Thoughts\n(CoTs) into the instruction-tuning process. Specifically, we leverage an\nagent-based system to decompose complex questions into sub-tasks, and address\nthem with specialized vision models, the intermediate results are then treated\nas reasoning chains. We also introduce a verification mechanism using a large\nlanguage model (LLM) to ensure the reliability of generated CoTs. Extensive\nexperiments demonstrate that AoTD improves the performance on multiple-choice\nand open-ended benchmarks.\n","authors":["Yudi Shi","Shangzhe Di","Qirui Chen","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2412.01694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11876v3","updated":"2025-02-13T07:31:32Z","published":"2023-09-21T08:22:44Z","title":"Multi-level Asymmetric Contrastive Learning for Volumetric Medical Image\n  Segmentation Pre-training","summary":"  Medical image segmentation is a fundamental yet challenging task due to the\narduous process of acquiring large volumes of high-quality labeled data from\nexperts. Contrastive learning offers a promising but still problematic solution\nto this dilemma. Firstly existing medical contrastive learning strategies focus\non extracting image-level representation, which ignores abundant multi-level\nrepresentations. Furthermore they underutilize the decoder either by random\ninitialization or separate pre-training from the encoder, thereby neglecting\nthe potential collaboration between the encoder and decoder. To address these\nissues, we propose a novel multi-level asymmetric contrastive learning\nframework named MACL for volumetric medical image segmentation pre-training.\nSpecifically, we design an asymmetric contrastive learning structure to\npre-train encoder and decoder simultaneously to provide better initialization\nfor segmentation models. Moreover, we develop a multi-level contrastive\nlearning strategy that integrates correspondences across feature-level,\nimage-level, and pixel-level representations to ensure the encoder and decoder\ncapture comprehensive details from representations of varying scales and\ngranularities during the pre-training phase. Finally, experiments on 8 medical\nimage datasets indicate our MACL framework outperforms existing 11 contrastive\nlearning strategies. i.e. Our MACL achieves a superior performance with more\nprecise predictions from visualization figures and 1.72%, 7.87%, 2.49% and\n1.48% Dice higher than previous best results on ACDC, MMWHS, HVSMR and CHAOS\nwith 10% labeled data, respectively. And our MACL also has a strong\ngeneralization ability among 5 variant U-Net backbones. Our code will be\nreleased at https://github.com/stevezs315/MACL.\n","authors":["Shuang Zeng","Lei Zhu","Xinliang Zhang","Micky C Nnamdi","Wenqi Shi","J Ben Tamo","Qian Chen","Hangzhou He","Lujia Jin","Zifeng Tian","Qiushi Ren","Zhaoheng Xie","Yanye Lu"],"pdf_url":"https://arxiv.org/pdf/2309.11876v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09026v1","updated":"2025-02-13T07:31:03Z","published":"2025-02-13T07:31:03Z","title":"Billet Number Recognition Based on Test-Time Adaptation","summary":"  During the steel billet production process, it is essential to recognize\nmachine-printed or manually written billet numbers on moving billets in\nreal-time. To address the issue of low recognition accuracy for existing scene\ntext recognition methods, caused by factors such as image distortions and\ndistribution differences between training and test data, we propose a billet\nnumber recognition method that integrates test-time adaptation with prior\nknowledge. First, we introduce a test-time adaptation method into a model that\nuses the DB network for text detection and the SVTR network for text\nrecognition. By minimizing the model's entropy during the testing phase, the\nmodel can adapt to the distribution of test data without the need for\nsupervised fine-tuning. Second, we leverage the billet number encoding rules as\nprior knowledge to assess the validity of each recognition result. Invalid\nresults, which do not comply with the encoding rules, are replaced. Finally, we\nintroduce a validation mechanism into the CTC algorithm using prior knowledge\nto address its limitations in recognizing damaged characters. Experimental\nresults on real datasets, including both machine-printed billet numbers and\nhandwritten billet numbers, show significant improvements in evaluation\nmetrics, validating the effectiveness of the proposed method.\n","authors":["Yuan Wei","Xiuzhuang Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.09026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09020v1","updated":"2025-02-13T07:16:16Z","published":"2025-02-13T07:16:16Z","title":"EventSTR: A Benchmark Dataset and Baselines for Event Stream based Scene\n  Text Recognition","summary":"  Mainstream Scene Text Recognition (STR) algorithms are developed based on RGB\ncameras which are sensitive to challenging factors such as low illumination,\nmotion blur, and cluttered backgrounds. In this paper, we propose to recognize\nthe scene text using bio-inspired event cameras by collecting and annotating a\nlarge-scale benchmark dataset, termed EventSTR. It contains 9,928\nhigh-definition (1280 * 720) event samples and involves both Chinese and\nEnglish characters. We also benchmark multiple STR algorithms as the baselines\nfor future works to compare. In addition, we propose a new event-based scene\ntext recognition framework, termed SimC-ESTR. It first extracts the event\nfeatures using a visual encoder and projects them into tokens using a Q-former\nmodule. More importantly, we propose to augment the vision tokens based on a\nmemory mechanism before feeding into the large language models. A\nsimilarity-based error correction mechanism is embedded within the large\nlanguage model to correct potential minor errors fundamentally based on\ncontextual information. Extensive experiments on the newly proposed EventSTR\ndataset and two simulation STR datasets fully demonstrate the effectiveness of\nour proposed model. We believe that the dataset and algorithmic model can\ninnovatively propose an event-based STR task and are expected to accelerate the\napplication of event cameras in various industries. The source code and\npre-trained models will be released on https://github.com/Event-AHU/EventSTR\n","authors":["Xiao Wang","Jingtao Jiang","Dong Li","Futian Wang","Lin Zhu","Yaowei Wang","Yongyong Tian","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2502.09020v1.pdf","comment":"In Peer Review"},{"id":"http://arxiv.org/abs/2412.13655v2","updated":"2025-02-13T07:13:57Z","published":"2024-12-18T09:34:32Z","title":"VIIS: Visible and Infrared Information Synthesis for Severe Low-light\n  Image Enhancement","summary":"  Images captured in severe low-light circumstances often suffer from\nsignificant information absence. Existing singular modality image enhancement\nmethods struggle to restore image regions lacking valid information. By\nleveraging light-impervious infrared images, visible and infrared image fusion\nmethods have the potential to reveal information hidden in darkness. However,\nthey primarily emphasize inter-modal complementation but neglect intra-modal\nenhancement, limiting the perceptual quality of output images. To address these\nlimitations, we propose a novel task, dubbed visible and infrared information\nsynthesis (VIIS), which aims to achieve both information enhancement and fusion\nof the two modalities. Given the difficulty in obtaining ground truth in the\nVIIS task, we design an information synthesis pretext task (ISPT) based on\nimage augmentation. We employ a diffusion model as the framework and design a\nsparse attention-based dual-modalities residual (SADMR) conditioning mechanism\nto enhance information interaction between the two modalities. This mechanism\nenables features with prior knowledge from both modalities to adaptively and\niteratively attend to each modality's information during the denoising process.\nOur extensive experiments demonstrate that our model qualitatively and\nquantitatively outperforms not only the state-of-the-art methods in relevant\nfields but also the newly designed baselines capable of both information\nenhancement and fusion. The code is available at\nhttps://github.com/Chenz418/VIIS.\n","authors":["Chen Zhao","Mengyuan Yu","Fan Yang","Peiguang Jing"],"pdf_url":"https://arxiv.org/pdf/2412.13655v2.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2502.09018v1","updated":"2025-02-13T07:11:07Z","published":"2025-02-13T07:11:07Z","title":"Zero-shot Concept Bottleneck Models","summary":"  Concept bottleneck models (CBMs) are inherently interpretable and\nintervenable neural network models, which explain their final label prediction\nby the intermediate prediction of high-level semantic concepts. However, they\nrequire target task training to learn input-to-concept and concept-to-label\nmappings, incurring target dataset collections and training resources. In this\npaper, we present \\textit{zero-shot concept bottleneck models} (Z-CBMs), which\npredict concepts and labels in a fully zero-shot manner without training neural\nnetworks. Z-CBMs utilize a large-scale concept bank, which is composed of\nmillions of vocabulary extracted from the web, to describe arbitrary input in\nvarious domains. For the input-to-concept mapping, we introduce concept\nretrieval, which dynamically finds input-related concepts by the cross-modal\nsearch on the concept bank. In the concept-to-label inference, we apply concept\nregression to select essential concepts from the retrieved concepts by sparse\nlinear regression. Through extensive experiments, we confirm that our Z-CBMs\nprovide interpretable and intervenable concepts without any additional\ntraining. Code will be available at https://github.com/yshinya6/zcbm.\n","authors":["Shin'ya Yamaguchi","Kosuke Nishida","Daiki Chijiwa","Yasutoshi Ida"],"pdf_url":"https://arxiv.org/pdf/2502.09018v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.03628v2","updated":"2025-02-13T07:05:53Z","published":"2023-12-06T17:19:00Z","title":"Boosting Segment Anything Model Towards Open-Vocabulary Learning","summary":"  The recent Segment Anything Model (SAM) has emerged as a new paradigmatic\nvision foundation model, showcasing potent zero-shot generalization and\nflexible prompting. Despite SAM finding applications and adaptations in various\ndomains, its primary limitation lies in the inability to grasp object\nsemantics. In this paper, we present Sambor to seamlessly integrate SAM with\nthe open-vocabulary object detector in an end-to-end framework. While retaining\nall the remarkable capabilities inherent to SAM, we boost it to detect\narbitrary objects from human inputs like category names or reference\nexpressions. Building upon the SAM image encoder, we introduce a novel\nSideFormer module designed to acquire SAM features adept at perceiving objects\nand inject comprehensive semantic information for recognition. In addition, we\ndevise an Open-set RPN that leverages SAM proposals to assist in finding\npotential objects. Consequently, Sambor enables the open-vocabulary detector to\nequally focus on generalizing both localization and classification sub-tasks.\nOur approach demonstrates superior zero-shot performance across benchmarks,\nincluding COCO and LVIS, proving highly competitive against previous\nstate-of-the-art methods. We aspire for this work to serve as a meaningful\nendeavor in endowing SAM to recognize diverse object categories and advancing\nopen-vocabulary learning with the support of vision foundation models.\n","authors":["Xumeng Han","Longhui Wei","Xuehui Yu","Zhiyang Dou","Xin He","Kuiran Wang","Yingfei Sun","Zhenjun Han","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2312.03628v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.01061v2","updated":"2025-02-13T06:56:29Z","published":"2025-02-03T05:17:32Z","title":"OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human\n  Animation Models","summary":"  End-to-end human animation, such as audio-driven talking human generation,\nhas undergone notable advancements in the recent few years. However, existing\nmethods still struggle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper, we propose\nOmniHuman, a Diffusion Transformer-based framework that scales up data by\nmixing motion-related conditions into the training phase. To this end, we\nintroduce two training principles for these mixed conditions, along with the\ncorresponding model architecture and inference strategy. These designs enable\nOmniHuman to fully leverage data-driven motion generation, ultimately achieving\nhighly realistic human video generation. More importantly, OmniHuman supports\nvarious portrait contents (face close-up, portrait, half-body, full-body),\nsupports both talking and singing, handles human-object interactions and\nchallenging body poses, and accommodates different image styles. Compared to\nexisting end-to-end audio-driven methods, OmniHuman not only produces more\nrealistic videos, but also offers greater flexibility in inputs. It also\nsupports multiple driving modalities (audio-driven, video-driven and combined\ndriving signals). Video samples are provided on the ttfamily project page\n(https://omnihuman-lab.github.io)\n","authors":["Gaojie Lin","Jianwen Jiang","Jiaqi Yang","Zerong Zheng","Chao Liang"],"pdf_url":"https://arxiv.org/pdf/2502.01061v2.pdf","comment":"https://omnihuman-lab.github.io/"},{"id":"http://arxiv.org/abs/2502.08636v2","updated":"2025-02-13T06:42:15Z","published":"2025-02-12T18:53:20Z","title":"PulseCheck457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large\n  Multimodal Models","summary":"  Although large multimodal models (LMMs) have demonstrated remarkable\ncapabilities in visual scene interpretation and reasoning, their capacity for\ncomplex and precise 3-dimensional spatial reasoning remains uncertain. Existing\nbenchmarks focus predominantly on 2D spatial understanding and lack a framework\nto comprehensively evaluate 6D spatial reasoning across varying complexities.\nTo address this limitation, we present PulseCheck457, a scalable and unbiased\nsynthetic dataset designed with 4 key capability for spatial reasoning:\nmulti-object recognition, 2D location, 3D location, and 3D orientation. We\ndevelop a cascading evaluation structure, constructing 7 question types across\n5 difficulty levels that range from basic single object recognition to our new\nproposed complex 6D spatial reasoning tasks. We evaluated various large\nmultimodal models (LMMs) on PulseCheck457, observing a general decline in\nperformance as task complexity increases, particularly in 3D reasoning and 6D\nspatial tasks. To quantify these challenges, we introduce the Relative\nPerformance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning\ncapabilities. Leveraging the unbiased attribute design of our dataset, we also\nuncover prediction biases across different attributes, with similar patterns\nobserved in real-world image settings.\n","authors":["Xingrui Wang","Wufei Ma","Tiezheng Zhang","Celso M de Melo","Jieneng Chen","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2502.08636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09000v1","updated":"2025-02-13T06:32:19Z","published":"2025-02-13T06:32:19Z","title":"Residual Transformer Fusion Network for Salt and Pepper Image Denoising","summary":"  Convolutional Neural Network (CNN) has been widely used in unstructured\ndatasets, one of which is image denoising. Image denoising is a noisy image\nreconstruction process that aims to reduce additional noise that occurs from\nthe noisy image with various strategies. Image denoising has a problem, namely\nthat some image denoising methods require some prior knowledge of information\nabout noise. To overcome this problem, a combined architecture of Convolutional\nVision Transformer (CvT) and Residual Networks (ResNet) is used which is called\nthe Residual Transformer Fusion Network (RTF-Net). In general, the process in\nthis architecture can be divided into two parts, Noise Suppression Network\n(NSN) and Structure Enhancement Network (SEN). Residual Block is used in the\nNoise Suppression Network and is used to learn the noise map in the image,\nwhile the CvT is used in the Structure Enhancement Network and is used to learn\nthe details that need to be added to the image processed by the Noise\nSuppression Network. The model was trained using the DIV2K Training Set\ndataset, and validation using the DIV2K Validation Set. After doing the\ntraining, the model was tested using Lena, Bridge, Pepper, and BSD300 images\nwith noise levels ranging from 30%, 50%, and 70% and the PSNR results were\ncompared with the DBA, NASNLM, PARIGI, NLSF, NLSF-MLP and NLSF-CNN methods. The\ntest results show that the proposed method is superior in all cases except for\nPepper's image with a noise level of 30%, where NLSF-CNN is superior with a\nPSNR value of 32.99 dB, while the proposed method gets a PSNR value of 31.70\ndB.\n","authors":["Bintang Pradana Erlangga Putra","Heri Prasetyo","Esti Suryani"],"pdf_url":"https://arxiv.org/pdf/2502.09000v1.pdf","comment":"8 pages, 17 figures"},{"id":"http://arxiv.org/abs/2411.07742v2","updated":"2025-02-13T06:27:51Z","published":"2024-11-12T12:07:27Z","title":"Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial\n  Pruning","summary":"  This paper studies point cloud perception within outdoor environments.\nExisting methods face limitations in recognizing objects located at a distance\nor occluded, due to the sparse nature of outdoor point clouds. In this work, we\nobserve a significant mitigation of this problem by accumulating multiple\ntemporally consecutive LiDAR sweeps, resulting in a remarkable improvement in\nperception accuracy. However, the computation cost also increases, hindering\nprevious approaches from utilizing a large number of LiDAR sweeps. To tackle\nthis challenge, we find that a considerable portion of points in the\naccumulated point cloud is redundant, and discarding these points has minimal\nimpact on perception accuracy. We introduce a simple yet effective Gumbel\nSpatial Pruning (GSP) layer that dynamically prunes points based on a learned\nend-to-end sampling. The GSP layer is decoupled from other network components\nand thus can be seamlessly integrated into existing point cloud network\narchitectures. Without incurring additional computational overhead, we increase\nthe number of LiDAR sweeps from 10, a common practice, to as many as 40.\nConsequently, there is a significant enhancement in perception performance. For\ninstance, in nuScenes 3D object detection and BEV map segmentation tasks, our\npruning strategy improves the vanilla TransL baseline and other baseline\nmethods.\n","authors":["Tianyu Sun","Jianhao Li","Xueqian Zhang","Zhongdao Wang","Bailan Feng","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.07742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08997v1","updated":"2025-02-13T06:24:07Z","published":"2025-02-13T06:24:07Z","title":"Hierarchical Vision Transformer with Prototypes for Interpretable\n  Medical Image Classification","summary":"  Explainability is a highly demanded requirement for applications in high-risk\nareas such as medicine. Vision Transformers have mainly been limited to\nattention extraction to provide insight into the model's reasoning. Our\napproach combines the high performance of Vision Transformers with the\nintroduction of new explainability capabilities. We present HierViT, a Vision\nTransformer that is inherently interpretable and adapts its reasoning to that\nof humans. A hierarchical structure is used to process domain-specific features\nfor prediction. It is interpretable by design, as it derives the target output\nwith human-defined features that are visualized by exemplary images\n(prototypes). By incorporating domain knowledge about these decisive features,\nthe reasoning is semantically similar to human reasoning and therefore\nintuitive. Moreover, attention heatmaps visualize the crucial regions for\nidentifying each feature, thereby providing HierViT with a versatile tool for\nvalidating predictions. Evaluated on two medical benchmark datasets, LIDC-IDRI\nfor lung nodule assessment and derm7pt for skin lesion classification, HierViT\nachieves superior and comparable prediction accuracy, respectively, while\noffering explanations that align with human reasoning.\n","authors":["Luisa Gallée","Catharina Silvia Lisson","Meinrad Beer","Michael Götz"],"pdf_url":"https://arxiv.org/pdf/2502.08997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08163v2","updated":"2025-02-13T06:08:21Z","published":"2025-01-14T14:41:51Z","title":"DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction","summary":"  The accelerated MRI reconstruction poses a challenging ill-posed inverse\nproblem due to the significant undersampling in k-space. Deep neural networks,\nsuch as CNNs and ViT, have shown substantial performance improvements for this\ntask while encountering the dilemma between global receptive fields and\nefficient computation. To this end, this paper pioneers exploring Mamba, a new\nparadigm for long-range dependency modeling with linear complexity, for\nefficient and effective MRI reconstruction. However, directly applying Mamba to\nMRI reconstruction faces three significant issues: (1) Mamba's row-wise and\ncolumn-wise scanning disrupts k-space's unique spectrum, leaving its potential\nin k-space learning unexplored. (2) Existing Mamba methods unfold feature maps\nwith multiple lengthy scanning paths, leading to long-range forgetting and high\ncomputational burden. (3) Mamba struggles with spatially-varying contents,\nresulting in limited diversity of local representations. To address these, we\npropose a dual-domain multi-scale Mamba for MRI reconstruction from the\nfollowing perspectives: (1) We pioneer vision Mamba in k-space learning. A\ncircular scanning is customized for spectrum unfolding, benefiting the global\nmodeling of k-space. (2) We propose a multi-scale Mamba with an efficient\nscanning strategy in both image and k-space domains. It mitigates long-range\nforgetting and achieves a better trade-off between efficiency and performance.\n(3) We develop a local diversity enhancement module to improve the\nspatially-varying representation of Mamba. Extensive experiments are conducted\non three public datasets for MRI reconstruction under various undersampling\npatterns. Comprehensive results demonstrate that our method significantly\noutperforms state-of-the-art methods with lower computational cost.\nImplementation code will be available at\nhttps://github.com/XiaoMengLiLiLi/DM-Mamba.\n","authors":["Yucong Meng","Zhiwei Yang","Zhijian Song","Yonghong Shi"],"pdf_url":"https://arxiv.org/pdf/2501.08163v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08988v1","updated":"2025-02-13T05:51:41Z","published":"2025-02-13T05:51:41Z","title":"Latents of latents to delineate pixels: hybrid Matryoshka\n  autoencoder-to-U-Net pairing for segmenting large medical images in GPU-poor\n  and low-data regimes","summary":"  Medical images are often high-resolution and lose important detail if\ndownsampled, making pixel-level methods such as semantic segmentation much less\nefficient if performed on a low-dimensional image. We propose a low-rank\nMatryoshka projection and a hybrid segmenting architecture that preserves\nimportant information while retaining sufficient pixel geometry for pixel-level\ntasks. We design the Matryoshka Autoencoder (MatAE-U-Net) which combines the\nhierarchical encoding of the Matryoshka Autoencoder with the spatial\nreconstruction capabilities of a U-Net decoder, leveraging multi-scale feature\nextraction and skip connections to enhance accuracy and generalisation. We\napply it to the problem of segmenting the left ventricle (LV) in\nechocardiographic images using the Stanford EchoNet-D dataset, including 1,000\nstandardised video-mask pairs of cardiac ultrasound videos resized to 112x112\npixels. The MatAE-UNet model achieves a Mean IoU of 77.68\\%, Mean Pixel\nAccuracy of 97.46\\%, and Dice Coefficient of 86.91\\%, outperforming the\nbaseline U-Net, which attains a Mean IoU of 74.70\\%, Mean Pixel Accuracy of\n97.31\\%, and Dice Coefficient of 85.20\\%. The results highlight the potential\nof using the U-Net in the recursive Matroshka latent space for imaging problems\nwith low-contrast such as echocardiographic analysis.\n","authors":["Tahir Syed","Ariba Khan","Sawera Hanif"],"pdf_url":"https://arxiv.org/pdf/2502.08988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04207v2","updated":"2025-02-13T05:40:03Z","published":"2025-02-06T16:47:28Z","title":"Enhanced Feature-based Image Stitching for Endoscopic Videos in\n  Pediatric Eosinophilic Esophagitis","summary":"  Video endoscopy represents a major advance in the investigation of\ngastrointestinal diseases. Reviewing endoscopy videos often involves frequent\nadjustments and reorientations to piece together a complete view, which can be\nboth time-consuming and prone to errors. Image stitching techniques address\nthis issue by providing a continuous and complete visualization of the examined\narea. However, endoscopic images, particularly those of the esophagus, present\nunique challenges. The smooth surface, lack of distinct feature points, and\nnon-horizontal orientation complicate the stitching process, rendering\ntraditional feature-based methods often ineffective for these types of images.\nIn this paper, we propose a novel preprocessing pipeline designed to enhance\nendoscopic image stitching through advanced computational techniques. Our\napproach converts endoscopic video data into continuous 2D images by following\nfour key steps: (1) keyframe selection, (2) image rotation adjustment to\ncorrect distortions, (3) surface unwrapping using polar coordinate\ntransformation to generate a flat image, and (4) feature point matching\nenhanced by Adaptive Histogram Equalization for improved feature detection. We\nevaluate stitching quality through the assessment of valid feature point match\npairs. Experiments conducted on 20 pediatric endoscopy videos demonstrate that\nour method significantly improves image alignment and stitching quality\ncompared to traditional techniques, laying a robust foundation for more\neffective panoramic image creation.\n","authors":["Juming Xiong","Muyang Li","Ruining Deng","Tianyuan Yao","Shunxing Bao","Regina N Tyree","Girish Hiremath","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2502.04207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08977v1","updated":"2025-02-13T05:27:50Z","published":"2025-02-13T05:27:50Z","title":"Text-driven 3D Human Generation via Contrastive Preference Optimization","summary":"  Recent advances in Score Distillation Sampling (SDS) have improved 3D human\ngeneration from textual descriptions. However, existing methods still face\nchallenges in accurately aligning 3D models with long and complex textual\ninputs. To address this challenge, we propose a novel framework that introduces\ncontrastive preferences, where human-level preference models, guided by both\npositive and negative prompts, assist SDS for improved alignment. Specifically,\nwe design a preference optimization module that integrates multiple models to\ncomprehensively capture the full range of textual features. Furthermore, we\nintroduce a negation preference module to mitigate over-optimization of\nirrelevant details by leveraging static-dynamic negation prompts, effectively\npreventing ``reward hacking\". Extensive experiments demonstrate that our method\nachieves state-of-the-art results, significantly enhancing texture realism and\nvisual alignment with textual descriptions, particularly for long and complex\ninputs.\n","authors":["Pengfei Zhou","Xukun Shen","Yong Hu"],"pdf_url":"https://arxiv.org/pdf/2502.08977v1.pdf","comment":"8"},{"id":"http://arxiv.org/abs/2502.08974v1","updated":"2025-02-13T05:21:02Z","published":"2025-02-13T05:21:02Z","title":"Topo2Seq: Enhanced Topology Reasoning via Topology Sequence Learning","summary":"  Extracting lane topology from perspective views (PV) is crucial for planning\nand control in autonomous driving. This approach extracts potential drivable\ntrajectories for self-driving vehicles without relying on high-definition (HD)\nmaps. However, the unordered nature and weak long-range perception of the\nDETR-like framework can result in misaligned segment endpoints and limited\ntopological prediction capabilities. Inspired by the learning of contextual\nrelationships in language models, the connectivity relations in roads can be\ncharacterized as explicit topology sequences. In this paper, we introduce\nTopo2Seq, a novel approach for enhancing topology reasoning via topology\nsequences learning. The core concept of Topo2Seq is a randomized order\nprompt-to-sequence learning between lane segment decoder and topology sequence\ndecoder. The dual-decoder branches simultaneously learn the lane topology\nsequences extracted from the Directed Acyclic Graph (DAG) and the lane graph\ncontaining geometric information. Randomized order prompt-to-sequence learning\nextracts unordered key points from the lane graph predicted by the lane segment\ndecoder, which are then fed into the prompt design of the topology sequence\ndecoder to reconstruct an ordered and complete lane graph. In this way, the\nlane segment decoder learns powerful long-range perception and accurate\ntopological reasoning from the topology sequence decoder. Notably, topology\nsequence decoder is only introduced during training and does not affect the\ninference efficiency. Experimental evaluations on the OpenLane-V2 dataset\ndemonstrate the state-of-the-art performance of Topo2Seq in topology reasoning.\n","authors":["Yiming Yang","Yueru Luo","Bingkun He","Erlong Li","Zhipeng Cao","Chao Zheng","Shuqi Mei","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2502.08974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11356v2","updated":"2025-02-13T05:20:48Z","published":"2024-09-17T17:00:52Z","title":"RenderWorld: World Model with Self-Supervised 3D Label","summary":"  End-to-end autonomous driving with vision-only is not only more\ncost-effective compared to LiDAR-vision fusion but also more reliable than\ntraditional methods. To achieve a economical and robust purely visual\nautonomous driving system, we propose RenderWorld, a vision-only end-to-end\nautonomous driving framework, which generates 3D occupancy labels using a\nself-supervised gaussian-based Img2Occ Module, then encodes the labels by\nAM-VAE, and uses world model for forecasting and planning. RenderWorld employs\nGaussian Splatting to represent 3D scenes and render 2D images greatly improves\nsegmentation accuracy and reduces GPU memory consumption compared with\nNeRF-based methods. By applying AM-VAE to encode air and non-air separately,\nRenderWorld achieves more fine-grained scene element representation, leading to\nstate-of-the-art performance in both 4D occupancy forecasting and motion\nplanning from autoregressive world model.\n","authors":["Ziyang Yan","Wenzhen Dong","Yihua Shao","Yuhang Lu","Liu Haiyang","Jingwen Liu","Haozhe Wang","Zhe Wang","Yan Wang","Fabio Remondino","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2409.11356v2.pdf","comment":"Accepted in 2025 IEEE International Conference on Robotics and\n  Automation (ICRA)"},{"id":"http://arxiv.org/abs/2410.17610v3","updated":"2025-02-13T05:15:07Z","published":"2024-10-23T07:06:08Z","title":"ImDy: Human Inverse Dynamics from Imitated Observations","summary":"  Inverse dynamics (ID), which aims at reproducing the driven torques from\nhuman kinematic observations, has been a critical tool for gait analysis.\nHowever, it is hindered from wider application to general motion due to its\nlimited scalability. Conventional optimization-based ID requires expensive\nlaboratory setups, restricting its availability. To alleviate this problem, we\npropose to exploit the recently progressive human motion imitation algorithms\nto learn human inverse dynamics in a data-driven manner. The key insight is\nthat the human ID knowledge is implicitly possessed by motion imitators, though\nnot directly applicable. In light of this, we devise an efficient data\ncollection pipeline with state-of-the-art motion imitation algorithms and\nphysics simulators, resulting in a large-scale human inverse dynamics benchmark\nas Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint\ntorque and full-body ground reaction force data. With ImDy, we train a\ndata-driven human inverse dynamics solver ImDyS(olver) in a fully supervised\nmanner, which conducts ID and ground reaction force estimation simultaneously.\nExperiments on ImDy and real-world data demonstrate the impressive competency\nof ImDyS in human inverse dynamics and ground reaction force estimation.\nMoreover, the potential of ImDy(-S) as a fundamental motion analysis tool is\nexhibited with downstream applications. The project page is\nhttps://foruck.github.io/ImDy/.\n","authors":["Xinpeng Liu","Junxuan Liang","Zili Lin","Haowen Hou","Yong-Lu Li","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2410.17610v3.pdf","comment":"To appear in ICLR 2025. Yong-Lu Li and Cewu Lu are the corresponding\n  authors"},{"id":"http://arxiv.org/abs/2501.15001v2","updated":"2025-02-13T04:15:47Z","published":"2025-01-25T00:29:24Z","title":"What if Eye...? Computationally Recreating Vision Evolution","summary":"  Vision systems in nature show remarkable diversity, from simple\nlight-sensitive patches to complex camera eyes with lenses. While natural\nselection has produced these eyes through countless mutations over millions of\nyears, they represent just one set of realized evolutionary paths. Testing\nhypotheses about how environmental pressures shaped eye evolution remains\nchallenging since we cannot experimentally isolate individual factors.\nComputational evolution offers a way to systematically explore alternative\ntrajectories. Here we show how environmental demands drive three fundamental\naspects of visual evolution through an artificial evolution framework that\nco-evolves both physical eye structure and neural processing in embodied\nagents. First, we demonstrate computational evidence that task specific\nselection drives bifurcation in eye evolution - orientation tasks like\nnavigation in a maze leads to distributed compound-type eyes while an object\ndiscrimination task leads to the emergence of high-acuity camera-type eyes.\nSecond, we reveal how optical innovations like lenses naturally emerge to\nresolve fundamental tradeoffs between light collection and spatial precision.\nThird, we uncover systematic scaling laws between visual acuity and neural\nprocessing, showing how task complexity drives coordinated evolution of sensory\nand computational capabilities. Our work introduces a novel paradigm that\nilluminates evolutionary principles shaping vision by creating targeted\nsingle-player games where embodied agents must simultaneously evolve visual\nsystems and learn complex behaviors. Through our unified genetic encoding\nframework, these embodied agents serve as next-generation hypothesis testing\nmachines while providing a foundation for designing manufacturable bio-inspired\nvision systems. Website: http://eyes.mit.edu/\n","authors":["Kushagra Tiwary","Aaron Young","Zaid Tasneem","Tzofi Klinghoffer","Akshat Dave","Tomaso Poggio","Dan-Eric Nilsson","Brian Cheung","Ramesh Raskar"],"pdf_url":"https://arxiv.org/pdf/2501.15001v2.pdf","comment":"Website: http://eyes.mit.edu/"},{"id":"http://arxiv.org/abs/2502.08946v1","updated":"2025-02-13T04:00:03Z","published":"2025-02-13T04:00:03Z","title":"The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of\n  Physical Concept Understanding","summary":"  In a systematic way, we investigate a widely asked question: Do LLMs really\nunderstand what they say?, which relates to the more familiar term Stochastic\nParrot. To this end, we propose a summative assessment over a carefully\ndesigned physical concept understanding task, PhysiCo. Our task alleviates the\nmemorization issue via the usage of grid-format inputs that abstractly describe\nphysical phenomena. The grids represents varying levels of understanding, from\nthe core phenomenon, application examples to analogies to other abstract\npatterns in the grid world. A comprehensive study on our task demonstrates: (1)\nstate-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag\nbehind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,\nas they fail on our grid task but can describe and recognize the same concepts\nwell in natural language; (3) our task challenges the LLMs due to intrinsic\ndifficulties rather than the unfamiliar grid format, as in-context learning and\nfine-tuning on same formatted data added little to their performance.\n","authors":["Mo Yu","Lemao Liu","Junjie Wu","Tsz Ting Chung","Shunchi Zhang","Jiangnan Li","Dit-Yan Yeung","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08946v1.pdf","comment":"NAACL 2025 Main Conference. First 5 authors contributed equally.\n  Project page: https://physico-benchmark.github.io/"},{"id":"http://arxiv.org/abs/2502.07856v2","updated":"2025-02-13T03:55:03Z","published":"2025-02-11T14:57:33Z","title":"MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE\n  Solvers","summary":"  In applications of diffusion models, controllable generation is of practical\nsignificance, but is also challenging. Current methods for controllable\ngeneration primarily focus on modifying the score function of diffusion models,\nwhile Mean Reverting (MR) Diffusion directly modifies the structure of the\nstochastic differential equation (SDE), making the incorporation of image\nconditions simpler and more natural. However, current training-free fast\nsamplers are not directly applicable to MR Diffusion. And thus MR Diffusion\nrequires hundreds of NFEs (number of function evaluations) to obtain\nhigh-quality samples. In this paper, we propose a new algorithm named MRS (MR\nSampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time\nSDE and the probability flow ordinary differential equation (PF-ODE) associated\nwith MR Diffusion, and derive semi-analytical solutions. The solutions consist\nof an analytical function and an integral parameterized by a neural network.\nBased on this solution, we can generate high-quality samples in fewer steps.\nOur approach does not require training and supports all mainstream\nparameterizations, including noise prediction, data prediction and velocity\nprediction. Extensive experiments demonstrate that MR Sampler maintains high\nsampling quality with a speedup of 10 to 20 times across ten different image\nrestoration tasks. Our algorithm accelerates the sampling procedure of MR\nDiffusion, making it more practical in controllable generation.\n","authors":["Ao Li","Wei Fang","Hongbo Zhao","Le Lu","Ge Yang","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2502.07856v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08940v1","updated":"2025-02-13T03:41:50Z","published":"2025-02-13T03:41:50Z","title":"Towards Understanding Why Data Augmentation Improves Generalization","summary":"  Data augmentation is a cornerstone technique in deep learning, widely used to\nimprove model generalization. Traditional methods like random cropping and\ncolor jittering, as well as advanced techniques such as CutOut, Mixup, and\nCutMix, have achieved notable success across various domains. However, the\nmechanisms by which data augmentation improves generalization remain poorly\nunderstood, and existing theoretical analyses typically focus on individual\ntechniques without a unified explanation. In this work, we present a unified\ntheoretical framework that elucidates how data augmentation enhances\ngeneralization through two key effects: partial semantic feature removal and\nfeature mixing. Partial semantic feature removal reduces the model's reliance\non individual feature, promoting diverse feature learning and better\ngeneralization. Feature mixing, by scaling down original semantic features and\nintroducing noise, increases training complexity, driving the model to develop\nmore robust features. Advanced methods like CutMix integrate both effects,\nachieving complementary benefits. Our theoretical insights are further\nsupported by experimental results, validating the effectiveness of this unified\nperspective.\n","authors":["Jingyang Li","Jiachun Pan","Kim-Chuan Toh","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08932v1","updated":"2025-02-13T03:29:42Z","published":"2025-02-13T03:29:42Z","title":"On the Promise for Assurance of Differentiable Neurosymbolic Reasoning\n  Paradigms","summary":"  To create usable and deployable Artificial Intelligence (AI) systems, there\nrequires a level of assurance in performance under many different conditions.\nMany times, deployed machine learning systems will require more classic logic\nand reasoning performed through neurosymbolic programs jointly with artificial\nneural network sensing. While many prior works have examined the assurance of a\nsingle component of the system solely with either the neural network alone or\nentire enterprise systems, very few works have examined the assurance of\nintegrated neurosymbolic systems. Within this work, we assess the assurance of\nend-to-end fully differentiable neurosymbolic systems that are an emerging\nmethod to create data-efficient and more interpretable models. We perform this\ninvestigation using Scallop, an end-to-end neurosymbolic library, across\nclassification and reasoning tasks in both the image and audio domains. We\nassess assurance across adversarial robustness, calibration, user performance\nparity, and interpretability of solutions for catching misaligned solutions. We\nfind end-to-end neurosymbolic methods present unique opportunities for\nassurance beyond their data efficiency through our empirical results but not\nacross the board. We find that this class of neurosymbolic models has higher\nassurance in cases where arithmetic operations are defined and where there is\nhigh dimensionality to the input space, where fully neural counterparts\nstruggle to learn robust reasoning operations. We identify the relationship\nbetween neurosymbolic models' interpretability to catch shortcuts that later\nresult in increased adversarial vulnerability despite performance parity.\nFinally, we find that the promise of data efficiency is typically only in the\ncase of class imbalanced reasoning problems.\n","authors":["Luke E. Richards","Jessie Yaros","Jasen Babcock","Coung Ly","Robin Cosbey","Timothy Doster","Cynthia Matuszek"],"pdf_url":"https://arxiv.org/pdf/2502.08932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08927v1","updated":"2025-02-13T03:23:17Z","published":"2025-02-13T03:23:17Z","title":"Dynamic watermarks in images generated by diffusion models","summary":"  High-fidelity text-to-image diffusion models have revolutionized visual\ncontent generation, but their widespread use raises significant ethical\nconcerns, including intellectual property protection and the misuse of\nsynthetic media. To address these challenges, we propose a novel multi-stage\nwatermarking framework for diffusion models, designed to establish copyright\nand trace generated images back to their source. Our multi-stage watermarking\ntechnique involves embedding: (i) a fixed watermark that is localized in the\ndiffusion model's learned noise distribution and, (ii) a human-imperceptible,\ndynamic watermark in generates images, leveraging a fine-tuned decoder. By\nleveraging the Structural Similarity Index Measure (SSIM) and cosine\nsimilarity, we adapt the watermark's shape and color to the generated content\nwhile maintaining robustness. We demonstrate that our method enables reliable\nsource verification through watermark classification, even when the dynamic\nwatermark is adjusted for content-specific variations. Source model\nverification is enabled through watermark classification. o support further\nresearch, we generate a dataset of watermarked images and introduce a\nmethodology to evaluate the statistical impact of watermarking on generated\ncontent.Additionally, we rigorously test our framework against various attack\nscenarios, demonstrating its robustness and minimal impact on image quality.\nOur work advances the field of AI-generated content security by providing a\nscalable solution for model ownership verification and misuse prevention.\n","authors":["Yunzhuo Chen","Naveed Akhtar","Nur Al Hasan Haldar","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2502.08927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14755v2","updated":"2025-02-13T03:15:41Z","published":"2024-04-23T05:36:33Z","title":"SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework\n  with Interactive Vision-Language Models","summary":"  With the continuous advancement of vision language models (VLMs) technology,\nremarkable research achievements have emerged in the dermatology field, the\nfourth most prevalent human disease category. However, despite these\nadvancements, VLM still faces explainable problems to user in diagnosis due to\nthe inherent complexity of dermatological conditions, existing tools offer\nrelatively limited support for user comprehension. We propose SkinGEN, a\ndiagnosis-to-generation framework that leverages the stable diffusion(SD) model\nto generate reference demonstrations from diagnosis results provided by VLM,\nthereby enhancing the visual explainability for users. Through extensive\nexperiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for\nskin condition image generation. We conduct a user study with 32 participants\nevaluating both the system performance and explainability. Results demonstrate\nthat SkinGEN significantly improves users' comprehension of VLM predictions and\nfosters increased trust in the diagnostic process. This work paves the way for\nmore transparent and user-centric VLM applications in dermatology and beyond.\n","authors":["Bo Lin","Yingjing Xu","Xuanwen Bao","Zhou Zhao","Zhouyang Wang","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2404.14755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11566v3","updated":"2025-02-13T03:15:37Z","published":"2024-02-18T12:27:59Z","title":"Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data\n  Augmentation and Consistency Training","summary":"  The 2D human pose estimation (HPE) is a basic visual problem. However, its\nsupervised learning requires massive keypoint labels, which is labor-intensive\nto collect. Thus, we aim at boosting a pose estimator by excavating extra\nunlabeled data with semi-supervised learning (SSL). Most previous SSHPE methods\nare consistency-based and strive to maintain consistent outputs for differently\naugmented inputs. Under this genre, we find that SSHPE can be boosted from two\ncores: advanced data augmentations and concise consistency training ways.\nSpecifically, for the first core, we discover the synergistic effects of\nexisting augmentations, and reveal novel paradigms for conveniently producing\nnew superior HPE-oriented augmentations which can more effectively add noise on\nunlabeled samples. We can therefore establish paired easy-hard augmentations\nwith larger difficulty gaps. For the second core, we propose to repeatedly\naugment unlabeled images with diverse hard augmentations, and generate\nmulti-path predictions sequentially for optimizing multi-losses in a single\nnetwork. This simple and compact design is interpretable, and easily benefits\nfrom newly found augmentations. Comparing to state-of-the-art SSL approaches,\nour method brings substantial improvements on public datasets. And we\nextensively validate the superiority and versatility of our approach on\nconventional human body images, overhead fisheye images, and human hand images.\nThe code is released in https://github.com/hnuzhy/MultiAugs.\n","authors":["Huayi Zhou","Mukun Luo","Fei Jiang","Yue Ding","Hongtao Lu","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2402.11566v3.pdf","comment":"under review. Semi-Supervised 2D Human Pose Estimation"},{"id":"http://arxiv.org/abs/2502.08921v1","updated":"2025-02-13T03:15:18Z","published":"2025-02-13T03:15:18Z","title":"Detecting Malicious Concepts Without Image Generation in AIGC","summary":"  The task of text-to-image generation has achieved tremendous success in\npractice, with emerging concept generation models capable of producing highly\npersonalized and customized content. Fervor for concept generation is\nincreasing rapidly among users, and platforms for concept sharing have sprung\nup. The concept owners may upload malicious concepts and disguise them with\nnon-malicious text descriptions and example images to deceive users into\ndownloading and generating malicious content. The platform needs a quick method\nto determine whether a concept is malicious to prevent the spread of malicious\nconcepts. However, simply relying on concept image generation to judge whether\na concept is malicious requires time and computational resources. Especially,\nas the number of concepts uploaded and downloaded on the platform continues to\nincrease, this approach becomes impractical and poses a risk of generating\nmalicious content. In this paper, we propose Concept QuickLook, the first\nsystematic work to incorporate malicious concept detection into research, which\nperforms detection based solely on concept files without generating any images.\nWe define malicious concepts and design two work modes for detection: concept\nmatching and fuzzy detection. Extensive experiments demonstrate that the\nproposed Concept QuickLook can detect malicious concepts and demonstrate\npracticality in concept sharing platforms. We also design robustness\nexperiments to further validate the effectiveness of the solution. We hope this\nwork can initiate malicious concept detection tasks and provide some\ninspiration.\n","authors":["Kun Xu","Yushu Zhang","Shuren Qi","Tao Wang","Wenying Wen","Yuming Fang"],"pdf_url":"https://arxiv.org/pdf/2502.08921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00626v3","updated":"2025-02-13T03:11:20Z","published":"2024-02-01T14:41:20Z","title":"Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks","summary":"  Typographic attacks, adding misleading text to images, can deceive\nvision-language models (LVLMs). The susceptibility of recent large LVLMs like\nGPT4-V to such attacks is understudied, raising concerns about amplified\nmisinformation in personal assistant applications. Previous attacks use simple\nstrategies, such as random misleading words, which don't fully exploit LVLMs'\nlanguage reasoning abilities. We introduce an experimental setup for testing\ntypographic attacks on LVLMs and propose two novel self-generated attacks: (1)\nClass-based attacks, where the model identifies a similar class to deceive\nitself, and (2) Reasoned attacks, where an advanced LVLM suggests an attack\ncombining a deceiving class and description. Our experiments show these attacks\nsignificantly reduce classification performance by up to 60\\% and are effective\nacross different models, including InstructBLIP and MiniGPT4. Code:\nhttps://github.com/mqraitem/Self-Gen-Typo-Attack\n","authors":["Maan Qraitem","Nazia Tasnim","Piotr Teterwak","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2402.00626v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06855v3","updated":"2025-02-13T03:10:12Z","published":"2024-02-10T01:36:39Z","title":"For Better or For Worse? Learning Minimum Variance Features With Label\n  Augmentation","summary":"  Data augmentation has been pivotal in successfully training deep learning\nmodels on classification tasks over the past decade. An important subclass of\ndata augmentation techniques - which includes both label smoothing and Mixup -\ninvolves modifying not only the input data but also the input label during\nmodel training. In this work, we analyze the role played by the label\naugmentation aspect of such methods. We first prove that linear models on\nbinary classification data trained with label augmentation learn only the\nminimum variance features in the data, while standard training (which includes\nweight decay) can learn higher variance features. We then use our techniques to\nshow that even for nonlinear models and general data distributions, the label\nsmoothing and Mixup losses are lower bounded by a function of the model output\nvariance. Lastly, we demonstrate empirically that this aspect of label\nsmoothing and Mixup can be a positive and a negative. On the one hand, we show\nthat the strong performance of label smoothing and Mixup on image\nclassification benchmarks is correlated with learning low variance hidden\nrepresentations. On the other hand, we show that Mixup and label smoothing can\nbe more susceptible to low variance spurious correlations in the training data.\n","authors":["Muthu Chidambaram","Rong Ge"],"pdf_url":"https://arxiv.org/pdf/2402.06855v3.pdf","comment":"ICLR 2025, 25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.08916v1","updated":"2025-02-13T03:08:02Z","published":"2025-02-13T03:08:02Z","title":"PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic\n  Decision-Making Applied to Histopathology","summary":"  Diagnosing diseases through histopathology whole slide images (WSIs) is\nfundamental in modern pathology but is challenged by the gigapixel scale and\ncomplexity of WSIs. Trained histopathologists overcome this challenge by\nnavigating the WSI, looking for relevant patches, taking notes, and compiling\nthem to produce a final holistic diagnostic. Traditional AI approaches, such as\nmultiple instance learning and transformer-based models, fail short of such a\nholistic, iterative, multi-scale diagnostic procedure, limiting their adoption\nin the real-world. We introduce PathFinder, a multi-modal, multi-agent\nframework that emulates the decision-making process of expert pathologists.\nPathFinder integrates four AI agents, the Triage Agent, Navigation Agent,\nDescription Agent, and Diagnosis Agent, that collaboratively navigate WSIs,\ngather evidence, and provide comprehensive diagnoses with natural language\nexplanations. The Triage Agent classifies the WSI as benign or risky; if risky,\nthe Navigation and Description Agents iteratively focus on significant regions,\ngenerating importance maps and descriptive insights of sampled patches.\nFinally, the Diagnosis Agent synthesizes the findings to determine the\npatient's diagnostic classification. Our Experiments show that PathFinder\noutperforms state-of-the-art methods in skin melanoma diagnosis by 8% while\noffering inherent explainability through natural language descriptions of\ndiagnostically relevant patches. Qualitative analysis by pathologists shows\nthat the Description Agent's outputs are of high quality and comparable to\nGPT-4o. PathFinder is also the first AI-based system to surpass the average\nperformance of pathologists in this challenging melanoma classification task by\n9%, setting a new record for efficient, accurate, and interpretable AI-assisted\ndiagnostics in pathology. Data, code and models available at\nhttps://pathfinder-dx.github.io/\n","authors":["Fatemeh Ghezloo","Mehmet Saygin Seyfioglu","Rustin Soraki","Wisdom O. Ikezogwo","Beibin Li","Tejoram Vivekanandan","Joann G. Elmore","Ranjay Krishna","Linda Shapiro"],"pdf_url":"https://arxiv.org/pdf/2502.08916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08914v1","updated":"2025-02-13T03:05:42Z","published":"2025-02-13T03:05:42Z","title":"Diffusion Models Through a Global Lens: Are They Culturally Inclusive?","summary":"  Text-to-image diffusion models have recently enabled the creation of visually\ncompelling, detailed images from textual prompts. However, their ability to\naccurately represent various cultural nuances remains an open question. In our\nwork, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion\nmodels whether they can generate culturally specific images spanning ten\ncountries. We show that these models often fail to generate cultural artifacts\nin architecture, clothing, and food, especially for underrepresented country\nregions, by conducting a fine-grained analysis of different similarity aspects,\nrevealing significant disparities in cultural relevance, description fidelity,\nand realism compared to real-world reference images. With the collected human\nevaluations, we develop a neural-based image-image similarity metric, namely,\nCultDiff-S, to predict human judgment on real and generated images with\ncultural artifacts. Our work highlights the need for more inclusive generative\nAI systems and equitable dataset representation over a wide range of cultures.\n","authors":["Zahra Bayramli","Ayhan Suleymanzade","Na Min An","Huzama Ahmad","Eunsu Kim","Junyeong Park","James Thorne","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2502.08914v1.pdf","comment":"17 pages, 17 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.08905v1","updated":"2025-02-13T02:41:34Z","published":"2025-02-13T02:41:34Z","title":"DiffoRA: Enabling Parameter-Efficient LLM Fine-Tuning via Differential\n  Low-Rank Matrix Adaptation","summary":"  The Parameter-Efficient Fine-Tuning (PEFT) methods have been extensively\nresearched for large language models in the downstream tasks. Among all the\nexisting approaches, the Low-Rank Adaptation (LoRA) has gained popularity for\nits streamlined design by incorporating low-rank matrices into existing\npre-trained models. Though effective, LoRA allocates every module an identical\nlow-rank matrix, which ignores the varying properties and contributions across\ndifferent components. Moreover, the existing adaptive LoRA solutions rely\nhighly on intuitive importance scoring indicators to adjust the interior rank\nof the decomposition matrices. In this paper, we propose a new PEFT scheme\ncalled DiffoRA, which is theoretically grounded and enables module-wise\nadoption of LoRA. At the core of our DiffoRA lies a Differential Adaptation\nMatrix (DAM) to determine which module is the most suitable and essential for\nfine-tuning. We explain how the designed matrix impacts the convergence rate\nand generalization capability of a pre-trained model. Furthermore, we construct\nthe DAM via continuous relaxation and discretization with weight-sharing\noptimizations. We fully implement our DiffoRA and design comprehensive\nexperiments to evaluate its performance. The experimental results demonstrate\nthat our approach achieves the best model accuracy over all the\nstate-of-the-art baselines across various benchmarks.\n","authors":["Tangyu Jiang","Haodi Wang","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.08905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18409v4","updated":"2025-02-13T02:37:06Z","published":"2024-02-28T15:28:36Z","title":"A Cognitive Evaluation Benchmark of Image Reasoning and Description for\n  Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the Cookie Theft task in human cognitive tests, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive abilities of LVLMs\nusing images with rich semantics. The benchmark consists of 251 images along\nwith comprehensive annotations. It defines eight reasoning capabilities and\ncomprises an image description task and a visual question answering task. Our\nevaluation of well-known LVLMs shows that there is still a significant gap in\ncognitive abilities between LVLMs and humans.\n","authors":["Xiujie Song","Mengyue Wu","Kenny Q. Zhu","Chunhao Zhang","Yanyi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.18409v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08902v1","updated":"2025-02-13T02:36:01Z","published":"2025-02-13T02:36:01Z","title":"CoL3D: Collaborative Learning of Single-view Depth and Camera Intrinsics\n  for Metric 3D Shape Recovery","summary":"  Recovering the metric 3D shape from a single image is particularly relevant\nfor robotics and embodied intelligence applications, where accurate spatial\nunderstanding is crucial for navigation and interaction with environments.\nUsually, the mainstream approaches achieve it through monocular depth\nestimation. However, without camera intrinsics, the 3D metric shape can not be\nrecovered from depth alone. In this study, we theoretically demonstrate that\ndepth serves as a 3D prior constraint for estimating camera intrinsics and\nuncover the reciprocal relations between these two elements. Motivated by this,\nwe propose a collaborative learning framework for jointly estimating depth and\ncamera intrinsics, named CoL3D, to learn metric 3D shapes from single images.\nSpecifically, CoL3D adopts a unified network and performs collaborative\noptimization at three levels: depth, camera intrinsics, and 3D point clouds.\nFor camera intrinsics, we design a canonical incidence field mechanism as a\nprior that enables the model to learn the residual incident field for enhanced\ncalibration. Additionally, we incorporate a shape similarity measurement loss\nin the point cloud space, which improves the quality of 3D shapes essential for\nrobotic applications. As a result, when training and testing on a single\ndataset with in-domain settings, CoL3D delivers outstanding performance in both\ndepth estimation and camera calibration across several indoor and outdoor\nbenchmark datasets, which leads to remarkable 3D shape quality for the\nperception capabilities of robots.\n","authors":["Chenghao Zhang","Lubin Fan","Shen Cao","Bojian Wu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2502.08902v1.pdf","comment":"Accepted at ICRA 2025"},{"id":"http://arxiv.org/abs/2406.06230v3","updated":"2025-02-13T02:32:43Z","published":"2024-06-10T13:00:22Z","title":"UEMM-Air: Make Unmanned Aerial Vehicles Perform More Multi-modal Tasks","summary":"  The development of multi-modal learning for Unmanned Aerial Vehicles (UAVs)\ntypically relies on a large amount of pixel-aligned multi-modal image data.\nHowever, existing datasets face challenges such as limited modalities, high\nconstruction costs, and imprecise annotations. To this end, we propose a\nsynthetic multi-modal UAV-based multi-task dataset, UEMM-Air. Specifically, we\nsimulate various UAV flight scenarios and object types using the Unreal Engine\n(UE). Then we design the UAV's flight logic to automatically collect data from\ndifferent scenarios, perspectives, and altitudes. Furthermore, we propose a\nnovel heuristic automatic annotation algorithm to generate accurate object\ndetection labels. Finally, we utilize labels to generate text descriptions of\nimages to make our UEMM-Air support more cross-modality tasks. In total, our\nUEMM-Air consists of 120k pairs of images with 6 modalities and precise\nannotations. Moreover, we conduct numerous experiments and establish new\nbenchmark results on our dataset. We also found that models pre-trained on\nUEMM-Air exhibit better performance on downstream tasks compared to other\nsimilar datasets. The dataset is publicly available\n(https://github.com/1e12Leon/UEMM-Air) to support the research of multi-modal\ntasks on UAVs.\n","authors":["Liang Yao","Fan Liu","Shengxiang Xu","Chuanyi Zhang","Xing Ma","Jianyu Jiang","Zequan Wang","Shimin Di","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.06230v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08884v1","updated":"2025-02-13T01:52:02Z","published":"2025-02-13T01:52:02Z","title":"ShapeLib: designing a library of procedural 3D shape abstractions with\n  Large Language Models","summary":"  Procedural representations are desirable, versatile, and popular shape\nencodings. Authoring them, either manually or using data-driven procedures,\nremains challenging, as a well-designed procedural representation should be\ncompact, intuitive, and easy to manipulate. A long-standing problem in shape\nanalysis studies how to discover a reusable library of procedural functions,\nwith semantically aligned exposed parameters, that can explain an entire shape\nfamily. We present ShapeLib as the first method that leverages the priors of\nfrontier LLMs to design a library of 3D shape abstraction functions. Our system\naccepts two forms of design intent: text descriptions of functions to include\nin the library and a seed set of exemplar shapes. We discover procedural\nabstractions that match this design intent by proposing, and then validating,\nfunction applications and implementations. The discovered shape functions in\nthe library are not only expressive but also generalize beyond the seed set to\na full family of shapes. We train a recognition network that learns to infer\nshape programs based on our library from different visual modalities\n(primitives, voxels, point clouds). Our shape functions have parameters that\nare semantically interpretable and can be modified to produce plausible shape\nvariations. We show that this allows inferred programs to be successfully\nmanipulated by an LLM given a text prompt. We evaluate ShapeLib on different\ndatasets and show clear advantages over existing methods and alternative\nformulations.\n","authors":["R. Kenny Jones","Paul Guerrero","Niloy J. Mitra","Daniel Ritchie"],"pdf_url":"https://arxiv.org/pdf/2502.08884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08869v1","updated":"2025-02-13T00:42:11Z","published":"2025-02-13T00:42:11Z","title":"Harnessing Vision Models for Time Series Analysis: A Survey","summary":"  Time series analysis has witnessed the inspiring development from traditional\nautoregressive models, deep learning models, to recent Transformers and Large\nLanguage Models (LLMs). Efforts in leveraging vision models for time series\nanalysis have also been made along the way but are less visible to the\ncommunity due to the predominant research on sequence modeling in this domain.\nHowever, the discrepancy between continuous time series and the discrete token\nspace of LLMs, and the challenges in explicitly modeling the correlations of\nvariates in multivariate time series have shifted some research attentions to\nthe equally successful Large Vision Models (LVMs) and Vision Language Models\n(VLMs). To fill the blank in the existing literature, this survey discusses the\nadvantages of vision models over LLMs in time series analysis. It provides a\ncomprehensive and in-depth overview of the existing methods, with dual views of\ndetailed taxonomy that answer the key research questions including how to\nencode time series as images and how to model the imaged time series for\nvarious tasks. Additionally, we address the challenges in the pre- and\npost-processing steps involved in this framework and outline future directions\nto further advance time series analysis with vision models.\n","authors":["Jingchao Ni","Ziming Zhao","ChengAo Shen","Hanghang Tong","Dongjin Song","Wei Cheng","Dongsheng Luo","Haifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08869v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.15118v2","updated":"2025-02-13T17:27:18Z","published":"2025-01-25T08:09:37Z","title":"ABXI: Invariant Interest Adaptation for Task-Guided Cross-Domain\n  Sequential Recommendation","summary":"  Cross-Domain Sequential Recommendation (CDSR) has recently gained attention\nfor countering data sparsity by transferring knowledge across domains. A common\napproach merges domain-specific sequences into cross-domain sequences, serving\nas bridges to connect domains. One key challenge is to correctly extract the\nshared knowledge among these sequences and appropriately transfer it. Most\nexisting works directly transfer unfiltered cross-domain knowledge rather than\nextracting domain-invariant components and adaptively integrating them into\ndomain-specific modelings. Another challenge lies in aligning the\ndomain-specific and cross-domain sequences. Existing methods align these\nsequences based on timestamps, but this approach can cause prediction\nmismatches when the current tokens and their targets belong to different\ndomains. In such cases, the domain-specific knowledge carried by the current\ntokens may degrade performance. To address these challenges, we propose the\nA-B-Cross-to-Invariant Learning Recommender (ABXI). Specifically, leveraging\nLoRA's effectiveness for efficient adaptation, ABXI incorporates two types of\nLoRAs to facilitate knowledge adaptation. First, all sequences are processed\nthrough a shared encoder that employs a domain LoRA for each sequence, thereby\npreserving unique domain characteristics. Next, we introduce an invariant\nprojector that extracts domain-invariant interests from cross-domain\nrepresentations, utilizing an invariant LoRA to adapt these interests into\nmodeling each specific domain. Besides, to avoid prediction mismatches, all\ndomain-specific sequences are aligned to match the domains of the cross-domain\nground truths. Experimental results on three datasets demonstrate that our\napproach outperforms other CDSR counterparts by a large margin. The codes are\navailable in https://github.com/DiMarzioBian/ABXI.\n","authors":["Qingtian Bian","Marcus Vinícius de Carvalho","Tieying Li","Jiaxing Xu","Hui Fang","Yiping Ke"],"pdf_url":"https://arxiv.org/pdf/2501.15118v2.pdf","comment":"Accepted by WebConf '25 (WWW '25)"},{"id":"http://arxiv.org/abs/2312.00326v9","updated":"2025-02-13T17:06:52Z","published":"2023-12-01T03:44:54Z","title":"Agent-OM: Leveraging LLM Agents for Ontology Matching","summary":"  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.\n","authors":["Zhangcheng Qiang","Weiqing Wang","Kerry Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.00326v9.pdf","comment":"19 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09375v1","updated":"2025-02-13T14:44:15Z","published":"2025-02-13T14:44:15Z","title":"FARM: Frequency-Aware Model for Cross-Domain Live-Streaming\n  Recommendation","summary":"  Live-streaming services have attracted widespread popularity due to their\nreal-time interactivity and entertainment value. Users can engage with\nlive-streaming authors by participating in live chats, posting likes, or\nsending virtual gifts to convey their preferences and support. However, the\nlive-streaming services faces serious data-sparsity problem, which can be\nattributed to the following two points: (1) User's valuable behaviors are\nusually sparse, e.g., like, comment and gift, which are easily overlooked by\nthe model, making it difficult to describe user's personalized preference. (2)\nThe main exposure content on our platform is short-video, which is 9 times\nhigher than the exposed live-streaming, leading to the inability of\nlive-streaming content to fully model user preference. To this end, we propose\na Frequency-Aware Model for Cross-Domain Live-Streaming Recommendation, termed\nas FARM. Specifically, we first present the intra-domain frequency aware module\nto enable our model to perceive user's sparse yet valuable behaviors, i.e.,\nhigh-frequency information, supported by the Discrete Fourier Transform (DFT).\nTo transfer user preference across the short-video and live-streaming domains,\nwe propose a novel preference align before fuse strategy, which consists of two\nparts: the cross-domain preference align module to align user preference in\nboth domains with contrastive learning, and the cross-domain preference fuse\nmodule to further fuse user preference in both domains using a serious of\ntailor-designed attention mechanisms. Extensive offline experiments and online\nA/B testing on Kuaishou live-streaming services demonstrate the effectiveness\nand superiority of FARM. Our FARM has been deployed in online live-streaming\nservices and currently serves hundreds of millions of users on Kuaishou.\n","authors":["Xiaodong Li","Ruochen Yang","Shuang Wen","Shen Wang","Yueyang Liu","Guoquan Wang","Weisong Hu","Qiang Luo","Jiawei Sheng","Tingwen Liu","Jiangxia Cao","Shuang Yang","Zhaojie Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18378v3","updated":"2025-02-13T13:35:51Z","published":"2024-12-24T12:07:48Z","title":"RaSeRec: Retrieval-Augmented Sequential Recommendation","summary":"  Although prevailing supervised and self-supervised learning augmented\nsequential recommendation (SeRec) models have achieved improved performance\nwith powerful neural network architectures, we argue that they still suffer\nfrom two limitations: (1) Preference Drift, where models trained on past data\ncan hardly accommodate evolving user preference; and (2) Implicit Memory, where\nhead patterns dominate parametric learning, making it harder to recall long\ntails. In this work, we explore retrieval augmentation in SeRec, to address\nthese limitations. Specifically, we propose a Retrieval-Augmented Sequential\nRecommendation framework, named RaSeRec, the main idea of which is to maintain\na dynamic memory bank to accommodate preference drifts and retrieve relevant\nmemories to augment user modeling explicitly. It consists of two stages: (i)\ncollaborative-based pre-training, which learns to recommend and retrieve; (ii)\nretrieval-augmented fine-tuning, which learns to leverage retrieved memories.\nExtensive experiments on three datasets fully demonstrate the superiority and\neffectiveness of RaSeRec. The implementation code is available at\nhttps://github.com/HITsz-TMG/RaSeRec.\n","authors":["Xinping Zhao","Baotian Hu","Yan Zhong","Shouzheng Huang","Zihao Zheng","Meng Wang","Haofen Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.18378v3.pdf","comment":"14 pages, 8 figures, 9 tables"},{"id":"http://arxiv.org/abs/2502.09319v1","updated":"2025-02-13T13:33:45Z","published":"2025-02-13T13:33:45Z","title":"Bridging Jensen Gap for Max-Min Group Fairness Optimization in\n  Recommendation","summary":"  Group max-min fairness (MMF) is commonly used in fairness-aware recommender\nsystems (RS) as an optimization objective, as it aims to protect marginalized\nitem groups and ensures a fair competition platform. However, our theoretical\nanalysis indicates that integrating MMF constraint violates the assumption of\nsample independence during optimization, causing the loss function to deviate\nfrom linear additivity. Such nonlinearity property introduces the Jensen gap\nbetween the model's convergence point and the optimal point if mini-batch\nsampling is applied. Both theoretical and empirical studies show that as the\nmini-batch size decreases and the group size increases, the Jensen gap will\nwiden accordingly. Some methods using heuristic re-weighting or debiasing\nstrategies have the potential to bridge the Jensen gap. However, they either\nlack theoretical guarantees or suffer from heavy computational costs. To\novercome these limitations, we first theoretically demonstrate that the\nMMF-constrained objective can be essentially reformulated as a group-weighted\noptimization objective. Then we present an efficient and effective algorithm\nnamed FairDual, which utilizes a dual optimization technique to minimize the\nJensen gap. Our theoretical analysis demonstrates that FairDual can achieve a\nsub-linear convergence rate to the globally optimal solution and the Jensen gap\ncan be well bounded under a mini-batch sampling strategy with random shuffle.\nExtensive experiments conducted using six large-scale RS backbone models on\nthree publicly available datasets demonstrate that FairDual outperforms all\nbaselines in terms of both accuracy and fairness. Our data and codes are shared\nat https://github.com/XuChen0427/FairDual.\n","authors":["Chen Xu","Yuxin Li","Wenjie Wang","Liang Pang","Jun Xu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.09319v1.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09304v1","updated":"2025-02-13T13:16:16Z","published":"2025-02-13T13:16:16Z","title":"KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for\n  Graph-RAG","summary":"  Graph-RAG constructs a knowledge graph from text chunks to improve retrieval\nin Large Language Model (LLM)-based question answering. It is particularly\nuseful in domains such as biomedicine, law, and political science, where\nretrieval often requires multi-hop reasoning over proprietary documents. Some\nexisting Graph-RAG systems construct KNN graphs based on text chunk relevance,\nbut this coarse-grained approach fails to capture entity relationships within\ntexts, leading to sub-par retrieval and generation quality. To address this,\nrecent solutions leverage LLMs to extract entities and relationships from text\nchunks, constructing triplet-based knowledge graphs. However, this approach\nincurs significant indexing costs, especially for large document collections.\n  To ensure a good result accuracy while reducing the indexing cost, we propose\nKET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small\nset of key text chunks and leverages an LLM to construct a knowledge graph\nskeleton. It then builds a text-keyword bipartite graph from all text chunks,\nserving as a lightweight alternative to a full knowledge graph. During\nretrieval, KET-RAG searches both structures: it follows the local search\nstrategy of existing Graph-RAG systems on the skeleton while mimicking this\nsearch on the bipartite graph to improve retrieval quality. We evaluate eight\nsolutions on two real-world datasets, demonstrating that KET-RAG outperforms\nall competitors in indexing cost, retrieval effectiveness, and generation\nquality. Notably, it achieves comparable or superior retrieval quality to\nMicrosoft's Graph-RAG while reducing indexing costs by over an order of\nmagnitude. Additionally, it improves the generation quality by up to 32.4%\nwhile lowering indexing costs by around 20%.\n","authors":["Yiqian Huang","Shiqi Zhang","Xiaokui Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.09304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05033v2","updated":"2025-02-13T12:09:50Z","published":"2024-07-06T09:58:58Z","title":"PeaPOD: Personalized Prompt Distillation for Generative Recommendation","summary":"  Recently, researchers have investigated the capabilities of Large Language\nModels (LLMs) for generative recommender systems. Existing LLM-based\nrecommender models are trained by adding user and item IDs to a discrete prompt\ntemplate. However, the disconnect between IDs and natural language makes it\ndifficult for the LLM to learn the relationship between users. To address this\nissue, we propose a PErsonAlized PrOmpt Distillation (PeaPOD) approach, to\ndistill user preferences as personalized soft prompts. Considering the\ncomplexities of user preferences in the real world, we maintain a shared set of\nlearnable prompts that are dynamically weighted based on the user's interests\nto construct the user-personalized prompt in a compositional manner.\nExperimental results on three real-world datasets demonstrate the effectiveness\nof our PeaPOD model on sequential recommendation, top-n recommendation, and\nexplanation generation tasks.\n","authors":["Jerome Ramos","Bin Wu","Aldo Lipani"],"pdf_url":"https://arxiv.org/pdf/2407.05033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09155v1","updated":"2025-02-13T10:36:17Z","published":"2025-02-13T10:36:17Z","title":"Use of Air Quality Sensor Network Data for Real-time Pollution-Aware POI\n  Suggestion","summary":"  This demo paper presents AirSense-R, a privacy-preserving mobile application\nthat provides real-time, pollution-aware recommendations for points of interest\n(POIs) in urban environments. By combining real-time air quality monitoring\ndata with user preferences, the proposed system aims to help users make\nhealth-conscious decisions about the locations they visit. The application\nutilizes collaborative filtering for personalized suggestions, and federated\nlearning for privacy protection, and integrates air pollutant readings from\nAirSENCE sensor networks in cities such as Bari, Italy, and Cork, Ireland.\nAdditionally, the AirSENCE prediction engine can be employed to detect anomaly\nreadings and interpolate for air quality readings in areas with sparse sensor\ncoverage. This system offers a promising, health-oriented POI recommendation\nsolution that adapts dynamically to current urban air quality conditions while\nsafeguarding user privacy. The code of AirTOWN and a demonstration video is\nmade available at the following repo:\nhttps://github.com/AirtownApp/Airtown-Application.git.\n","authors":["Giuseppe Fasano","Yashar Deldjoo","Tommaso di Noia","Bianca Lau","Sina Adham-Khiabani","Eric Morris","Xia Liu","Ganga Chinna Rao Devarapu","Liam O'Faolain"],"pdf_url":"https://arxiv.org/pdf/2502.09155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09089v1","updated":"2025-02-13T09:01:34Z","published":"2025-02-13T09:01:34Z","title":"Semantic Ads Retrieval at Walmart eCommerce with Language Models\n  Progressively Trained on Multiple Knowledge Domains","summary":"  Sponsored search in e-commerce poses several unique and complex challenges.\nThese challenges stem from factors such as the asymmetric language structure\nbetween search queries and product names, the inherent ambiguity in user search\nintent, and the vast volume of sparse and imbalanced search corpus data. The\nrole of the retrieval component within a sponsored search system is pivotal,\nserving as the initial step that directly affects the subsequent ranking and\nbidding systems. In this paper, we present an end-to-end solution tailored to\noptimize the ads retrieval system on Walmart.com. Our approach is to pretrain\nthe BERT-like classification model with product category information, enhancing\nthe model's understanding of Walmart product semantics. Second, we design a\ntwo-tower Siamese Network structure for embedding structures to augment\ntraining efficiency. Third, we introduce a Human-in-the-loop Progressive Fusion\nTraining method to ensure robust model performance. Our results demonstrate the\neffectiveness of this pipeline. It enhances the search relevance metric by up\nto 16% compared to a baseline DSSM-based model. Moreover, our large-scale\nonline A/B testing demonstrates that our approach surpasses the ad revenue of\nthe existing production model.\n","authors":["Zhaodong Wang","Weizhi Du","Md Omar Faruk Rokon","Pooshpendu Adhikary","Yanbing Xue","Jiaxuan Xu","Jianghong Zhou","Kuang-chih Lee","Musen Wen"],"pdf_url":"https://arxiv.org/pdf/2502.09089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09058v1","updated":"2025-02-13T08:19:45Z","published":"2025-02-13T08:19:45Z","title":"Unleashing the Power of Large Language Model for Denoising\n  Recommendation","summary":"  Recommender systems are crucial for personalizing user experiences but often\ndepend on implicit feedback data, which can be noisy and misleading. Existing\ndenoising studies involve incorporating auxiliary information or learning\nstrategies from interaction data. However, they struggle with the inherent\nlimitations of external knowledge and interaction data, as well as the\nnon-universality of certain predefined assumptions, hindering accurate noise\nidentification. Recently, large language models (LLMs) have gained attention\nfor their extensive world knowledge and reasoning abilities, yet their\npotential in enhancing denoising in recommendations remains underexplored. In\nthis paper, we introduce LLaRD, a framework leveraging LLMs to improve\ndenoising in recommender systems, thereby boosting overall recommendation\nperformance. Specifically, LLaRD generates denoising-related knowledge by first\nenriching semantic insights from observational data via LLMs and inferring\nuser-item preference knowledge. It then employs a novel Chain-of-Thought (CoT)\ntechnique over user-item interaction graphs to reveal relation knowledge for\ndenoising. Finally, it applies the Information Bottleneck (IB) principle to\nalign LLM-generated denoising knowledge with recommendation targets, filtering\nout noise and irrelevant LLM knowledge. Empirical results demonstrate LLaRD's\neffectiveness in enhancing denoising and recommendation accuracy.\n","authors":["Shuyao Wang","Zhi Zheng","Yongduo Sui","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.09058v1.pdf","comment":"12 pages, 5 figures, 4 tables. Accecpted by WWW 2025"},{"id":"http://arxiv.org/abs/2502.09050v1","updated":"2025-02-13T08:05:14Z","published":"2025-02-13T08:05:14Z","title":"Leveraging Member-Group Relations via Multi-View Graph Filtering for\n  Effective Group Recommendation","summary":"  Group recommendation aims at providing optimized recommendations tailored to\ndiverse groups, enabling groups to enjoy appropriate items. On the other hand,\nmost existing group recommendation methods are built upon deep neural network\n(DNN) architectures designed to capture the intricate relationships between\nmember-level and group-level interactions. While these DNN-based approaches\nhave proven their effectiveness, they require complex and expensive training\nprocedures to incorporate group-level interactions in addition to member-level\ninteractions. To overcome such limitations, we introduce Group-GF, a new\napproach for extremely fast recommendations of items to each group via\nmulti-view graph filtering (GF) that offers a holistic view of complex\nmember-group dynamics, without the need for costly model training.\nSpecifically, in Group-GF, we first construct three item similarity graphs\nmanifesting different viewpoints for GF. Then, we discover a distinct\npolynomial graph filter for each similarity graph and judiciously aggregate the\nthree graph filters. Extensive experiments demonstrate the effectiveness of\nGroup-GF in terms of significantly reducing runtime and achieving\nstate-of-the-art recommendation accuracy.\n","authors":["Chae-Hyun Kim","Yoon-Ryung Choi","Jin-Duk Park","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2502.09050v1.pdf","comment":"5 pages, 3 figures, 4 tables; ACM Web Conference (WWW 2025) (to\n  appear) (Please cite our conference version.)"},{"id":"http://arxiv.org/abs/2502.09046v1","updated":"2025-02-13T08:01:38Z","published":"2025-02-13T08:01:38Z","title":"Criteria-Aware Graph Filtering: Extremely Fast Yet Accurate\n  Multi-Criteria Recommendation","summary":"  Multi-criteria (MC) recommender systems, which utilize MC rating information\nfor recommendation, are increasingly widespread in various e-commerce domains.\nHowever, the MC recommendation using training-based collaborative filtering,\nrequiring consideration of multiple ratings compared to single-criterion\ncounterparts, often poses practical challenges in achieving state-of-the-art\nperformance along with scalable model training. To solve this problem, we\npropose CA-GF, a training-free MC recommendation method, which is built upon\ncriteria-aware graph filtering for efficient yet accurate MC recommendations.\nSpecifically, first, we construct an item-item similarity graph using an MC\nuser-expansion graph. Next, we design CA-GF composed of the following key\ncomponents, including 1) criterion-specific graph filtering where the optimal\nfilter for each criterion is found using various types of polynomial low-pass\nfilters and 2) criteria preference-infused aggregation where the smoothed\nsignals from each criterion are aggregated. We demonstrate that CA-GF is (a)\nefficient: providing the computational efficiency, offering the extremely fast\nruntime of less than 0.2 seconds even on the largest benchmark dataset, (b)\naccurate: outperforming benchmark MC recommendation methods, achieving\nsubstantial accuracy gains up to 24% compared to the best competitor, and (c)\ninterpretable: providing interpretations for the contribution of each criterion\nto the model prediction based on visualizations.\n","authors":["Jin-Duk Park","Jaemin Yoo","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2502.09046v1.pdf","comment":"12 pages, 8 figures, 7 tables; ACM Web Conference (WWW 2025) (to\n  appear) (Please cite our conference version.)"},{"id":"http://arxiv.org/abs/2502.05561v2","updated":"2025-02-13T07:44:07Z","published":"2025-02-08T13:20:13Z","title":"Diffusion Model for Interest Refinement in Multi-Interest Recommendation","summary":"  Multi-interest candidate matching plays a pivotal role in personalized\nrecommender systems, as it captures diverse user interests from their\nhistorical behaviors. Most existing methods utilize attention mechanisms to\ngenerate interest representations by aggregating historical item embeddings.\nHowever, these methods only capture overall item-level relevance, leading to\ncoarse-grained interest representations that include irrelevant information. To\naddress this issue, we propose the Diffusion Multi-Interest model (DMI), a\nnovel framework for refining user interest representations at the dimension\nlevel. Specifically, DMI first introduces controllable noise into\ncoarse-grained interest representations at the dimensional level. Then, in the\niterative reconstruction process, DMI combines a cross-attention mechanism and\nan item pruning strategy to reconstruct the personalized interest vectors with\nthe guidance of tailored collaborative information. Extensive experiments\ndemonstrate the effectiveness of DMI, surpassing state-of-the-art methods on\noffline evaluations and an online A/B test. Successfully deployed in the\nreal-world recommender system, DMI effectively enhances user satisfaction and\nsystem performance at scale, serving the major traffic of hundreds of millions\nof daily active users. \\footnote{The code will be released for reproducibility\nonce the paper is accepted.}\n","authors":["Yankun Le","Haoran Li","Baoyuan Ou","Yingjie Qin","Zhixuan Yang","Ruilong Su","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.05561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09027v1","updated":"2025-02-13T07:31:34Z","published":"2025-02-13T07:31:34Z","title":"A Contextual-Aware Position Encoding for Sequential Recommendation","summary":"  Sequential recommendation (SR), which encodes user activity to predict the\nnext action, has emerged as a widely adopted strategy in developing commercial\npersonalized recommendation systems. A critical component of modern SR models\nis the attention mechanism, which synthesizes users' historical activities.\nThis mechanism is typically order-invariant and generally relies on position\nencoding (PE). Conventional SR models simply assign a learnable vector to each\nposition, resulting in only modest gains compared to traditional recommendation\nmodels. Moreover, limited research has been conducted on position encoding\ntailored for sequential recommendation, leaving a significant gap in addressing\nits unique requirements. To bridge this gap, we propose a novel\nContextual-Aware Position Encoding method for sequential recommendation,\nabbreviated as CAPE. To the best of our knowledge, CAPE is the first PE method\nspecifically designed for sequential recommendation. Comprehensive experiments\nconducted on benchmark SR datasets demonstrate that CAPE consistently enhances\nmultiple mainstream backbone models and achieves state-of-the-art performance,\nacross small and large scale model size. Furthermore, we deployed CAPE in an\nindustrial setting on a real-world commercial platform, clearly showcasing the\neffectiveness of our approach. Our source code is available at\nhttps://github.com/yjdy/CAPE.\n","authors":["Jun Yuan","Guohao Cai","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2502.09027v1.pdf","comment":"Accepted by WWW'25 Industry Track"},{"id":"http://arxiv.org/abs/2502.03375v3","updated":"2025-02-13T02:17:49Z","published":"2025-02-05T17:14:45Z","title":"Interactive Visualization Recommendation with Hier-SUCB","summary":"  Visualization recommendation aims to enable rapid visual analysis of massive\ndatasets. In real-world scenarios, it is essential to quickly gather and\ncomprehend user preferences to cover users from diverse backgrounds, including\nvarying skill levels and analytical tasks. Previous approaches to personalized\nvisualization recommendations are non-interactive and rely on initial user data\nfor new users. As a result, these models cannot effectively explore options or\nadapt to real-time feedback. To address this limitation, we propose an\ninteractive personalized visualization recommendation (PVisRec) system that\nlearns on user feedback from previous interactions. For more interactive and\naccurate recommendations, we propose Hier-SUCB, a contextual combinatorial\nsemi-bandit in the PVisRec setting. Theoretically, we show an improved overall\nregret bound with the same rank of time but an improved rank of action space.\nWe further demonstrate the effectiveness of Hier-SUCB through extensive\nexperiments where it is comparable to offline methods and outperforms other\nbandit algorithms in the setting of visualization recommendation.\n","authors":["Songwen Hu","Ryan A. Rossi","Tong Yu","Junda Wu","Handong Zhao","Sungchul Kim","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2502.03375v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02844v2","updated":"2025-02-13T01:58:12Z","published":"2025-01-06T08:43:31Z","title":"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification","summary":"  Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.\n","authors":["Yubo Wang","Haoyang Li","Fei Teng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.02844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07972v2","updated":"2025-02-13T01:23:29Z","published":"2025-02-11T21:36:31Z","title":"Training Sparse Mixture Of Experts Text Embedding Models","summary":"  Transformer-based text embedding models have improved their performance on\nbenchmarks like MIRACL and BEIR by increasing their parameter counts. However,\nthis scaling approach introduces significant deployment challenges, including\nincreased inference latency and memory usage. These challenges are particularly\nsevere in retrieval-augmented generation (RAG) applications, where large\nmodels' increased memory requirements constrain dataset ingestion capacity, and\ntheir higher latency directly impacts query-time performance. While causal\nlanguage models have addressed similar efficiency challenges using Mixture of\nExperts (MoE) architectures, this approach hasn't been successfully adapted to\nthe general text embedding setting. In this paper, we introduce Nomic Embed v2,\nthe first general purpose MoE text embedding model. Our model outperforms\nmodels in the same parameter class on both monolingual and multilingual\nbenchmarks while also maintaining competitive performance with models twice its\nsize. We open-source all code, models, and evaluation data to ensure full\nreproducibility of our training pipeline at\n\\href{https://github.com/nomic-ai/contrastors}{https://github.com/nomic-ai/contrastors}.\n","authors":["Zach Nussbaum","Brandon Duderstadt"],"pdf_url":"https://arxiv.org/pdf/2502.07972v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.09622v1","updated":"2025-02-13T18:59:47Z","published":"2025-02-13T18:59:47Z","title":"Theoretical Benefit and Limitation of Diffusion Language Model","summary":"  Diffusion language models have emerged as a promising approach for text\ngeneration. One would naturally expect this method to be an efficient\nreplacement for autoregressive models since multiple tokens can be sampled in\nparallel during each diffusion step. However, its efficiency-accuracy trade-off\nis not yet well understood. In this paper, we present a rigorous theoretical\nanalysis of a widely used type of diffusion language model, the Masked\nDiffusion Model (MDM), and find that its effectiveness heavily depends on the\ntarget evaluation metric. Under mild conditions, we prove that when using\nperplexity as the metric, MDMs can achieve near-optimal perplexity in sampling\nsteps regardless of sequence length, demonstrating that efficiency can be\nachieved without sacrificing performance. However, when using the sequence\nerror rate--which is important for understanding the \"correctness\" of a\nsequence, such as a reasoning chain--we show that the required sampling steps\nmust scale linearly with sequence length to obtain \"correct\" sequences, thereby\neliminating MDM's efficiency advantage over autoregressive models. Our analysis\nestablishes the first theoretical foundation for understanding the benefits and\nlimitations of MDMs. All theoretical findings are supported by empirical\nstudies.\n","authors":["Guhao Feng","Yihan Geng","Jian Guan","Wei Wu","Liwei Wang","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.09622v1.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.09619v1","updated":"2025-02-13T18:59:44Z","published":"2025-02-13T18:59:44Z","title":"Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights","summary":"  With the increasing numbers of publicly available models, there are probably\npretrained, online models for most tasks users require. However, current model\nsearch methods are rudimentary, essentially a text-based search in the\ndocumentation, thus users cannot find the relevant models. This paper presents\nProbeLog, a method for retrieving classification models that can recognize a\ntarget concept, such as \"Dog\", without access to model metadata or training\ndata. Differently from previous probing methods, ProbeLog computes a descriptor\nfor each output dimension (logit) of each model, by observing its responses on\na fixed set of inputs (probes). Our method supports both logit-based retrieval\n(\"find more logits like this\") and zero-shot, text-based retrieval (\"find all\nlogits corresponding to dogs\"). As probing-based representations require\nmultiple costly feedforward passes through the model, we develop a method,\nbased on collaborative filtering, that reduces the cost of encoding\nrepositories by 3x. We demonstrate that ProbeLog achieves high retrieval\naccuracy, both in real-world and fine-grained search tasks and is scalable to\nfull-size repositories.\n","authors":["Jonathan Kahana","Or Nathan","Eliahu Horwitz","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2502.09619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09616v1","updated":"2025-02-13T18:59:15Z","published":"2025-02-13T18:59:15Z","title":"Variational Rectified Flow Matching","summary":"  We study Variational Rectified Flow Matching, a framework that enhances\nclassic rectified flow matching by modeling multi-modal velocity vector-fields.\nAt inference time, classic rectified flow matching 'moves' samples from a\nsource distribution to the target distribution by solving an ordinary\ndifferential equation via integration along a velocity vector-field. At\ntraining time, the velocity vector-field is learnt by linearly interpolating\nbetween coupled samples one drawn from the source and one drawn from the target\ndistribution randomly. This leads to ''ground-truth'' velocity vector-fields\nthat point in different directions at the same location, i.e., the velocity\nvector-fields are multi-modal/ambiguous. However, since training uses a\nstandard mean-squared-error loss, the learnt velocity vector-field averages\n''ground-truth'' directions and isn't multi-modal. In contrast, variational\nrectified flow matching learns and samples from multi-modal flow directions. We\nshow on synthetic data, MNIST, CIFAR-10, and ImageNet that variational\nrectified flow matching leads to compelling results.\n","authors":["Pengsheng Guo","Alexander G. Schwing"],"pdf_url":"https://arxiv.org/pdf/2502.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09614v1","updated":"2025-02-13T18:59:13Z","published":"2025-02-13T18:59:13Z","title":"DexTrack: Towards Generalizable Neural Tracking Control for Dexterous\n  Manipulation from Human References","summary":"  We address the challenge of developing a generalizable neural tracking\ncontroller for dexterous manipulation from human references. This controller\naims to manage a dexterous robot hand to manipulate diverse objects for various\npurposes defined by kinematic human-object interactions. Developing such a\ncontroller is complicated by the intricate contact dynamics of dexterous\nmanipulation and the need for adaptivity, generalizability, and robustness.\nCurrent reinforcement learning and trajectory optimization methods often fall\nshort due to their dependence on task-specific rewards or precise system\nmodels. We introduce an approach that curates large-scale successful robot\ntracking demonstrations, comprising pairs of human references and robot\nactions, to train a neural controller. Utilizing a data flywheel, we\niteratively enhance the controller's performance, as well as the number and\nquality of successful tracking demonstrations. We exploit available tracking\ndemonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually\noptimize per-trajectory tracking by leveraging the learned tracking controller\nin a homotopy optimization method. The homotopy optimization, mimicking\nchain-of-thought, aids in solving challenging trajectory tracking problems to\nincrease demonstration diversity. We showcase our success by training a\ngeneralizable neural controller and evaluating it in both simulation and real\nworld. Our method achieves over a 10% improvement in success rates compared to\nleading baselines. The project website with animated results is available at\nhttps://meowuu7.github.io/DexTrack/.\n","authors":["Xueyi Liu","Jianibieke Adalibieke","Qianwei Han","Yuzhe Qin","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2502.09614v1.pdf","comment":"Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/\n  Code: https://github.com/Meowuu7/DexTrack/ Video:\n  https://youtu.be/zru1Z-DaiWE"},{"id":"http://arxiv.org/abs/2402.17767v2","updated":"2025-02-13T18:59:11Z","published":"2024-02-27T18:58:54Z","title":"Opening Articulated Objects in the Real World","summary":"  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated objects as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-objects/\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v2.pdf","comment":"Project webpage:\n  https://arjung128.github.io/opening-articulated-objects/"},{"id":"http://arxiv.org/abs/2403.06925v2","updated":"2025-02-13T18:58:58Z","published":"2024-03-11T17:12:09Z","title":"Transformers Learn Low Sensitivity Functions: Investigations and\n  Implications","summary":"  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of their inductive biases and how those biases\ndiffer from other neural network architectures remains elusive. In this work,\nwe identify the sensitivity of the model to token-wise random perturbations in\nthe input as a unified metric which explains the inductive bias of transformers\nacross different data modalities and distinguishes them from other\narchitectures. We show that transformers have lower sensitivity than MLPs,\nCNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show\nthat this low-sensitivity bias has important implications: i) lower sensitivity\ncorrelates with improved robustness; it can also be used as an efficient\nintervention to further improve the robustness of transformers; ii) it\ncorresponds to flatter minima in the loss landscape; and iii) it can serve as a\nprogress measure for grokking. We support these findings with theoretical\nresults showing (weak) spectral bias of transformers in the NTK regime, and\nimproved robustness due to the lower sensitivity. The code is available at\nhttps://github.com/estija/sensitivity.\n","authors":["Bhavya Vasudeva","Deqing Fu","Tianyi Zhou","Elliott Kau","Youqi Huang","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2403.06925v2.pdf","comment":"ICLR 2025. 24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09611v1","updated":"2025-02-13T18:58:15Z","published":"2025-02-13T18:58:15Z","title":"Designing a Conditional Prior Distribution for Flow-Based Generative\n  Models","summary":"  Flow-based generative models have recently shown impressive performance for\nconditional generation tasks, such as text-to-image generation. However,\ncurrent methods transform a general unimodal noise distribution to a specific\nmode of the target data distribution. As such, every point in the initial\nsource distribution can be mapped to every point in the target distribution,\nresulting in long average paths. To this end, in this work, we tap into a\nnon-utilized property of conditional flow-based models: the ability to design a\nnon-trivial prior distribution. Given an input condition, such as a text\nprompt, we first map it to a point lying in data space, representing an\n``average\" data point with the minimal average distance to all data points of\nthe same conditional mode (e.g., class). We then utilize the flow matching\nformulation to map samples from a parametric distribution centered around this\npoint to the conditional target distribution. Experimentally, our method\nsignificantly improves training times and generation efficiency (FID, KID and\nCLIP alignment scores) compared to baselines, producing high quality samples\nusing fewer sampling steps.\n","authors":["Noam Issachar","Mohammad Salama","Raanan Fattal","Sagie Benaim"],"pdf_url":"https://arxiv.org/pdf/2502.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13904v3","updated":"2025-02-13T18:58:14Z","published":"2025-01-23T18:34:09Z","title":"Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models","summary":"  Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank factorization scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.\n","authors":["Linh Tran","Wei Sun","Stacy Patterson","Ana Milanova"],"pdf_url":"https://arxiv.org/pdf/2501.13904v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09609v1","updated":"2025-02-13T18:57:20Z","published":"2025-02-13T18:57:20Z","title":"Score-of-Mixture Training: Training One-Step Generative Models Made\n  Simple","summary":"  We propose Score-of-Mixture Training (SMT), a novel framework for training\none-step generative models by minimizing a class of divergences called the\n$\\alpha$-skew Jensen-Shannon divergence. At its core, SMT estimates the score\nof mixture distributions between real and fake samples across multiple noise\nlevels. Similar to consistency models, our approach supports both training from\nscratch (SMT) and distillation using a pretrained diffusion model, which we\ncall Score-of-Mixture Distillation (SMD). It is simple to implement, requires\nminimal hyperparameter tuning, and ensures stable training. Experiments on\nCIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even\noutperform existing methods.\n","authors":["Tejas Jayashankar","J. Jon Ryu","Gregory Wornell"],"pdf_url":"https://arxiv.org/pdf/2502.09609v1.pdf","comment":"27 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09606v1","updated":"2025-02-13T18:55:56Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09604v1","updated":"2025-02-13T18:55:13Z","published":"2025-02-13T18:55:13Z","title":"SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models","summary":"  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.\n","authors":["Yung-Sung Chuang","Benjamin Cohen-Wang","Shannon Zejiang Shen","Zhaofeng Wu","Hu Xu","Xi Victoria Lin","James Glass","Shang-Wen Li","Wen-tau Yih"],"pdf_url":"https://arxiv.org/pdf/2502.09604v1.pdf","comment":"Implementation available at https://github.com/voidism/SelfCite"},{"id":"http://arxiv.org/abs/2502.09597v1","updated":"2025-02-13T18:52:03Z","published":"2025-02-13T18:52:03Z","title":"Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs","summary":"  Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/.\n","authors":["Siyan Zhao","Mingyi Hong","Yang Liu","Devamanyu Hazarika","Kaixiang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.09597v1.pdf","comment":"Accepted at ICLR 2025 as oral presentation. Code and data at:\n  https://prefeval.github.io/"},{"id":"http://arxiv.org/abs/2502.09591v1","updated":"2025-02-13T18:48:04Z","published":"2025-02-13T18:48:04Z","title":"Censor Dependent Variational Inference","summary":"  This paper provides a comprehensive analysis of variational inference in\nlatent variable models for survival analysis, emphasizing the distinctive\nchallenges associated with applying variational methods to survival data. We\nidentify a critical weakness in the existing methodology, demonstrating how a\npoorly designed variational distribution may hinder the objective of survival\nanalysis tasks--modeling time-to-event distributions. We prove that the optimal\nvariational distribution, which perfectly bounds the log-likelihood, may depend\non the censoring mechanism. To address this issue, we propose censor-dependent\nvariational inference (CDVI), tailored for latent variable models in survival\nanalysis. More practically, we introduce CD-CVAE, a V-structure Variational\nAutoencoder (VAE) designed for the scalable implementation of CDVI. Further\ndiscussion extends some existing theories and training techniques to survival\nanalysis. Extensive experiments validate our analysis and demonstrate\nsignificant improvements in the estimation of individual survival\ndistributions.\n","authors":["Chuanhui Liu","Xiao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09587v1","updated":"2025-02-13T18:45:56Z","published":"2025-02-13T18:45:56Z","title":"Rolling Ahead Diffusion for Traffic Scene Simulation","summary":"  Realistic driving simulation requires that NPCs not only mimic natural\ndriving behaviors but also react to the behavior of other simulated agents.\nRecent developments in diffusion-based scenario generation focus on creating\ndiverse and realistic traffic scenarios by jointly modelling the motion of all\nthe agents in the scene. However, these traffic scenarios do not react when the\nmotion of agents deviates from their modelled trajectories. For example, the\nego-agent can be controlled by a stand along motion planner. To produce\nreactive scenarios with joint scenario models, the model must regenerate the\nscenario at each timestep based on new observations in a Model Predictive\nControl (MPC) fashion. Although reactive, this method is time-consuming, as one\ncomplete possible future for all NPCs is generated per simulation step.\nAlternatively, one can utilize an autoregressive model (AR) to predict only the\nimmediate next-step future for all NPCs. Although faster, this method lacks the\ncapability for advanced planning. We present a rolling diffusion based traffic\nscene generation model which mixes the benefits of both methods by predicting\nthe next step future and simultaneously predicting partially noised further\nfuture steps at the same time. We show that such model is efficient compared to\ndiffusion model based AR, achieving a beneficial compromise between reactivity\nand computational efficiency.\n","authors":["Yunpeng Liu","Matthew Niedoba","William Harvey","Adam Scibior","Berend Zwartsenberg","Frank Wood"],"pdf_url":"https://arxiv.org/pdf/2502.09587v1.pdf","comment":"Accepted to Workshop on Machine Learning for Autonomous Driving at\n  AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09583v1","updated":"2025-02-13T18:41:55Z","published":"2025-02-13T18:41:55Z","title":"Learning to Coordinate with Experts","summary":"  When deployed in dynamic environments, AI agents will inevitably encounter\nchallenges that exceed their individual capabilities. Leveraging assistance\nfrom expert agents-whether human or AI-can significantly enhance safety and\nperformance in such situations. However, querying experts is often costly,\nnecessitating the development of agents that can efficiently request and\nutilize expert guidance. In this paper, we introduce a fundamental coordination\nproblem called Learning to Yield and Request Control (YRC), where the objective\nis to learn a strategy that determines when to act autonomously and when to\nseek expert assistance. We consider a challenging practical setting in which an\nagent does not interact with experts during training but must adapt to novel\nenvironmental changes and expert interventions at test time. To facilitate\nempirical research, we introduce YRC-Bench, an open-source benchmark featuring\ndiverse domains. YRC-Bench provides a standardized Gym-like API, simulated\nexperts, evaluation pipeline, and implementation of competitive baselines.\nTowards tackling the YRC problem, we propose a novel validation approach and\ninvestigate the performance of various learning methods across diverse\nenvironments, yielding insights that can guide future research.\n","authors":["Mohamad H. Danesh","Tu Trinh","Benjamin Plaut","Nguyen X. Khanh"],"pdf_url":"https://arxiv.org/pdf/2502.09583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20092v2","updated":"2025-02-13T18:38:13Z","published":"2024-10-26T06:06:08Z","title":"OGBench: Benchmarking Offline Goal-Conditioned RL","summary":"  Offline goal-conditioned reinforcement learning (GCRL) is a major problem in\nreinforcement learning (RL) because it provides a simple, unsupervised, and\ndomain-agnostic way to acquire diverse behaviors and representations from\nunlabeled data without rewards. Despite the importance of this setting, we lack\na standard benchmark that can systematically evaluate the capabilities of\noffline GCRL algorithms. In this work, we propose OGBench, a new, high-quality\nbenchmark for algorithms research in offline goal-conditioned RL. OGBench\nconsists of 8 types of environments, 85 datasets, and reference implementations\nof 6 representative offline GCRL algorithms. We have designed these challenging\nand realistic environments and datasets to directly probe different\ncapabilities of algorithms, such as stitching, long-horizon reasoning, and the\nability to handle high-dimensional inputs and stochasticity. While\nrepresentative algorithms may rank similarly on prior benchmarks, our\nexperiments reveal stark strengths and weaknesses in these different\ncapabilities, providing a strong foundation for building new algorithms.\nProject page: https://seohong.me/projects/ogbench\n","authors":["Seohong Park","Kevin Frans","Benjamin Eysenbach","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2410.20092v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09573v1","updated":"2025-02-13T18:31:17Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09571v1","updated":"2025-02-13T18:29:48Z","published":"2025-02-13T18:29:48Z","title":"DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra","summary":"  Mass spectrometry plays a fundamental role in elucidating the structures of\nunknown molecules and subsequent scientific discoveries. One formulation of the\nstructure elucidation task is the conditional $\\textit{de novo}$ generation of\nmolecular structure given a mass spectrum. Toward a more accurate and efficient\nscientific discovery pipeline for small molecules, we present DiffMS, a\nformula-restricted encoder-decoder generative network that achieves\nstate-of-the-art performance on this task. The encoder utilizes a transformer\narchitecture and models mass spectra domain knowledge such as peak formulae and\nneutral losses, and the decoder is a discrete graph diffusion model restricted\nby the heavy-atom composition of a known chemical formula. To develop a robust\ndecoder that bridges latent embeddings and molecular structures, we pretrain\nthe diffusion decoder with fingerprint-structure pairs, which are available in\nvirtually infinite quantities, compared to structure-spectrum pairs that number\nin the tens of thousands. Extensive experiments on established benchmarks show\nthat DiffMS outperforms existing models on $\\textit{de novo}$ molecule\ngeneration. We provide several ablations to demonstrate the effectiveness of\nour diffusion and pretraining approaches and show consistent performance\nscaling with increasing pretraining dataset size. DiffMS code is publicly\navailable at https://github.com/coleygroup/DiffMS.\n","authors":["Montgomery Bohde","Mrunali Manjrekar","Runzhong Wang","Shuiwang Ji","Connor W. Coley"],"pdf_url":"https://arxiv.org/pdf/2502.09571v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09570v1","updated":"2025-02-13T18:28:17Z","published":"2025-02-13T18:28:17Z","title":"Enhancing the Utility of Higher-Order Information in Relational Learning","summary":"  Higher-order information is crucial for relational learning in many domains\nwhere relationships extend beyond pairwise interactions. Hypergraphs provide a\nnatural framework for modeling such relationships, which has motivated recent\nextensions of graph neural net- work architectures to hypergraphs. However,\ncomparisons between hypergraph architectures and standard graph-level models\nremain limited. In this work, we systematically evaluate a selection of\nhypergraph-level and graph-level architectures, to determine their\neffectiveness in leveraging higher-order information in relational learning.\nOur results show that graph-level architectures applied to hypergraph\nexpansions often outperform hypergraph- level ones, even on inputs that are\nnaturally parametrized as hypergraphs. As an alternative approach for\nleveraging higher-order information, we propose hypergraph-level encodings\nbased on classical hypergraph characteristics. While these encodings do not\nsignificantly improve hypergraph architectures, they yield substantial\nperformance gains when combined with graph-level models. Our theoretical\nanalysis shows that hypergraph-level encodings provably increase the\nrepresentational power of message-passing graph neural networks beyond that of\ntheir graph-level counterparts.\n","authors":["Raphael Pellegrin","Lukas Fesser","Melanie Weber"],"pdf_url":"https://arxiv.org/pdf/2502.09570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08593v2","updated":"2025-02-13T18:24:40Z","published":"2025-02-12T17:32:23Z","title":"Toward Universal Laws of Outlier Propagation","summary":"  We argue that Algorithmic Information Theory (AIT) admits a principled way to\nquantify outliers in terms of so-called randomness deficiency. For the\nprobability distribution generated by a causal Bayesian network, we show that\nthe randomness deficiency of the joint state decomposes into randomness\ndeficiencies of each causal mechanism, subject to the Independence of\nMechanisms Principle. Accordingly, anomalous joint observations can be\nquantitatively attributed to their root causes, i.e., the mechanisms that\nbehaved anomalously. As an extension of Levin's law of randomness conservation,\nwe show that weak outliers cannot cause strong ones when Independence of\nMechanisms holds. We show how these information theoretic laws provide a better\nunderstanding of the behaviour of outliers defined with respect to existing\nscores.\n","authors":["Aram Ebtekar","Yuhao Wang","Dominik Janzing"],"pdf_url":"https://arxiv.org/pdf/2502.08593v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10238v2","updated":"2025-02-13T18:22:34Z","published":"2024-07-14T15:11:13Z","title":"Asymptotic Normality of Generalized Low-Rank Matrix Sensing via\n  Riemannian Geometry","summary":"  We prove an asymptotic normality guarantee for generalized low-rank matrix\nsensing -- i.e., matrix sensing under a general convex loss $\\bar\\ell(\\langle\nX,M\\rangle,y^*)$, where $M\\in\\mathbb{R}^{d\\times d}$ is the unknown rank-$k$\nmatrix, $X$ is a measurement matrix, and $y^*$ is the corresponding\nmeasurement. Our analysis relies on tools from Riemannian geometry to handle\ndegeneracy of the Hessian of the loss due to rotational symmetry in the\nparameter space. In particular, we parameterize the manifold of low-rank\nmatrices by $\\bar\\theta\\bar\\theta^\\top$, where\n$\\bar\\theta\\in\\mathbb{R}^{d\\times k}$. Then, assuming the minimizer of the\nempirical loss $\\bar\\theta^0\\in\\mathbb{R}^{d\\times k}$ is in a constant size\nball around the true parameters $\\bar\\theta^*$, we prove\n$\\sqrt{n}(\\phi^0-\\phi^*)\\xrightarrow{D}N(0,(H^*)^{-1})$ as $n\\to\\infty$, where\n$\\phi^0$ and $\\phi^*$ are representations of $\\bar\\theta^*$ and $\\bar\\theta^0$\nin the horizontal space of the Riemannian quotient manifold\n$\\mathbb{R}^{d\\times k}/\\text{O}(k)$, and $H^*$ is the Hessian of the true loss\nin the same representation.\n","authors":["Osbert Bastani"],"pdf_url":"https://arxiv.org/pdf/2407.10238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09566v1","updated":"2025-02-13T18:21:15Z","published":"2025-02-13T18:21:15Z","title":"Zero-shot generation of synthetic neurosurgical data with large language\n  models","summary":"  Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.\n","authors":["Austin A. Barr","Eddie Guo","Emre Sezgin"],"pdf_url":"https://arxiv.org/pdf/2502.09566v1.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.09564v1","updated":"2025-02-13T18:17:03Z","published":"2025-02-13T18:17:03Z","title":"Diffusing DeBias: a Recipe for Turning a Bug into a Feature","summary":"  Deep learning model effectiveness in classification tasks is often challenged\nby the quality and quantity of training data which, whenever containing strong\nspurious correlations between specific attributes and target labels, can result\nin unrecoverable biases in model predictions. Tackling these biases is crucial\nin improving model generalization and trust, especially in real-world\nscenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting\nas a plug-in for common methods in model debiasing while exploiting the\ninherent bias-learning tendency of diffusion models. Our approach leverages\nconditional diffusion models to generate synthetic bias-aligned images, used to\ntrain a bias amplifier model, to be further employed as an auxiliary method in\ndifferent unsupervised debiasing approaches. Our proposed method, which also\ntackles the common issue of training set memorization typical of this type of\ntech- niques, beats current state-of-the-art in multiple benchmark datasets by\nsignificant margins, demonstrating its potential as a versatile and effective\ntool for tackling dataset bias in deep learning applications.\n","authors":["Massimiliano Ciranni","Vito Paolo Pastore","Roberto Di Via","Enzo Tartaglione","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2502.09564v1.pdf","comment":"29 Pages, 12 Figures"},{"id":"http://arxiv.org/abs/2502.07864v2","updated":"2025-02-13T18:07:04Z","published":"2025-02-11T18:20:18Z","title":"TransMLA: Multi-Head Latent Attention Is All You Need","summary":"  Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.\n","authors":["Fanxu Meng","Zengwei Yao","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07864v2.pdf","comment":"https://github.com/fxmeng/TransMLA"},{"id":"http://arxiv.org/abs/2502.09553v1","updated":"2025-02-13T18:05:12Z","published":"2025-02-13T18:05:12Z","title":"SyntheticPop: Attacking Speaker Verification Systems With Synthetic\n  VoicePops","summary":"  Voice Authentication (VA), also known as Automatic Speaker Verification\n(ASV), is a widely adopted authentication method, particularly in automated\nsystems like banking services, where it serves as a secondary layer of user\nauthentication. Despite its popularity, VA systems are vulnerable to various\nattacks, including replay, impersonation, and the emerging threat of deepfake\naudio that mimics the voice of legitimate users. To mitigate these risks,\nseveral defense mechanisms have been proposed. One such solution, Voice Pops,\naims to distinguish an individual's unique phoneme pronunciations during the\nenrollment process. While promising, the effectiveness of VA+VoicePop against a\nbroader range of attacks, particularly logical or adversarial attacks, remains\ninsufficiently explored. We propose a novel attack method, which we refer to as\nSyntheticPop, designed to target the phoneme recognition capabilities of the\nVA+VoicePop system. The SyntheticPop attack involves embedding synthetic \"pop\"\nnoises into spoofed audio samples, significantly degrading the model's\nperformance. We achieve an attack success rate of over 95% while poisoning 20%\nof the training dataset. Our experiments demonstrate that VA+VoicePop achieves\n69% accuracy under normal conditions, 37% accuracy when subjected to a baseline\nlabel flipping attack, and just 14% accuracy under our proposed SyntheticPop\nattack, emphasizing the effectiveness of our method.\n","authors":["Eshaq Jamdar","Amith Kamath Belman"],"pdf_url":"https://arxiv.org/pdf/2502.09553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18970v3","updated":"2025-02-13T17:57:28Z","published":"2024-10-24T17:59:16Z","title":"WASP: A Weight-Space Approach to Detecting Learned Spuriousness","summary":"  It is of crucial importance to train machine learning models such that they\nclearly understand what defines each class in a given task. Though there is a\nsum of works dedicated to identifying the spurious correlations featured by a\ndataset that may impact the model's understanding of the classes, all current\napproaches rely solely on data or error analysis. That is, they cannot point\nout spurious correlations learned by the model that are not already pointed out\nby the counterexamples featured in the validation or training sets. We propose\na method that transcends this limitation, switching the focus from analyzing a\nmodel's predictions to analyzing the model's weights, the mechanism behind the\nmaking of the decisions, which proves to be more insightful. Our proposed\nWeight-space Approach to detecting Spuriousness (WASP) relies on analyzing the\nweights of foundation models as they drift towards capturing various (spurious)\ncorrelations while being fine-tuned on a given dataset. We demonstrate that\ndifferent from previous works, our method (i) can expose spurious correlations\nfeatured by a dataset even when they are not exposed by training or validation\ncounterexamples, (ii) it works for multiple modalities such as image and text,\nand (iii) it can uncover previously untapped spurious correlations learned by\nImageNet-1k classifiers.\n","authors":["Cristian Daniel Păduraru","Antonio Bărbălau","Radu Filipescu","Andrei Liviu Nicolicioiu","Elena Burceanu"],"pdf_url":"https://arxiv.org/pdf/2410.18970v3.pdf","comment":"8 pages, 4 figures, 6 tables, under review"},{"id":"http://arxiv.org/abs/2502.09534v1","updated":"2025-02-13T17:50:27Z","published":"2025-02-13T17:50:27Z","title":"Fast Tensor Completion via Approximate Richardson Iteration","summary":"  We study tensor completion (TC) through the lens of low-rank tensor\ndecomposition (TD). Many TD algorithms use fast alternating minimization\nmethods, which solve highly structured linear regression problems at each step\n(e.g., for CP, Tucker, and tensor-train decompositions). However, such\nalgebraic structure is lost in TC regression problems, making direct extensions\nunclear. To address this, we propose a lifting approach that approximately\nsolves TC regression problems using structured TD regression algorithms as\nblackbox subroutines, enabling sublinear-time methods. We theoretically analyze\nthe convergence rate of our approximate Richardson iteration based algorithm,\nand we demonstrate on real-world tensors that its running time can be 100x\nfaster than direct methods for CP completion.\n","authors":["Mehrdad Ghadiri","Matthew Fahrbach","Yunbum Kook","Ali Jadbabaie"],"pdf_url":"https://arxiv.org/pdf/2502.09534v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.09525v1","updated":"2025-02-13T17:37:42Z","published":"2025-02-13T17:37:42Z","title":"Robust Learning of Multi-index Models via Iterative Subspace\n  Approximation","summary":"  We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists.\n  As applications, we provide faster robust learners for the following concept\nclasses:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$.\n","authors":["Ilias Diakonikolas","Giannis Iakovidis","Daniel M. Kane","Nikos Zarifis"],"pdf_url":"https://arxiv.org/pdf/2502.09525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09511v1","updated":"2025-02-13T17:22:50Z","published":"2025-02-13T17:22:50Z","title":"Diffusion Models for Molecules: A Survey of Methods and Tasks","summary":"  Generative tasks about molecules, including but not limited to molecule\ngeneration, are crucial for drug discovery and material design, and have\nconsistently attracted significant attention. In recent years, diffusion models\nhave emerged as an impressive class of deep generative models, sparking\nextensive research and leading to numerous studies on their application to\nmolecular generative tasks. Despite the proliferation of related work, there\nremains a notable lack of up-to-date and systematic surveys in this area.\nParticularly, due to the diversity of diffusion model formulations, molecular\ndata modalities, and generative task types, the research landscape is\nchallenging to navigate, hindering understanding and limiting the area's\ngrowth. To address this, this paper conducts a comprehensive survey of\ndiffusion model-based molecular generative methods. We systematically review\nthe research from the perspectives of methodological formulations, data\nmodalities, and task types, offering a novel taxonomy. This survey aims to\nfacilitate understanding and further flourishing development in this area. The\nrelevant papers are summarized at:\nhttps://github.com/AzureLeon1/awesome-molecular-diffusion-models.\n","authors":["Liang Wang","Chao Song","Zhiyuan Liu","Yu Rong","Qiang Liu","Shu Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09509v1","updated":"2025-02-13T17:21:51Z","published":"2025-02-13T17:21:51Z","title":"EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling","summary":"  Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.\n","authors":["Theodoros Kouzelis","Ioannis Kakogeorgiou","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2502.09509v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09507v1","updated":"2025-02-13T17:21:37Z","published":"2025-02-13T17:21:37Z","title":"When and How Does CLIP Enable Domain and Compositional Generalization?","summary":"  The remarkable generalization performance of contrastive vision-language\nmodels like CLIP is often attributed to the diversity of their training\ndistributions. However, key questions remain unanswered: Can CLIP generalize to\nan entirely unseen domain when trained on a diverse mixture of domains (domain\ngeneralization)? Can it generalize to unseen classes within partially seen\ndomains (compositional generalization)? What factors affect such\ngeneralization? To answer these questions, we trained CLIP models on\nsystematically constructed training distributions with controlled domain\ndiversity and object class exposure. Our experiments show that domain diversity\nis essential for both domain and compositional generalization, yet\ncompositional generalization can be surprisingly weaker than domain\ngeneralization when the training distribution contains a suboptimal subset of\nthe test domain. Through data-centric and mechanistic analyses, we find that\nsuccessful generalization requires learning of shared representations already\nin intermediate layers and shared circuitry.\n","authors":["Elias Kempf","Simon Schrodi","Max Argus","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2502.09507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09503v1","updated":"2025-02-13T17:15:26Z","published":"2025-02-13T17:15:26Z","title":"AttentionSmithy: A Modular Framework for Rapid Transformer Development\n  and Customization","summary":"  Transformer architectures have transformed AI applications but remain complex\nto customize for domain experts lacking low-level implementation expertise. We\nintroduce AttentionSmithy, a modular software package that simplifies\ntransformer innovation by breaking down key components into reusable building\nblocks: attention modules, feed-forward networks, normalization layers, and\npositional encodings. Users can rapidly prototype and evaluate transformer\nvariants without extensive coding. Our framework supports four positional\nencoding strategies and integrates with neural architecture search for\nautomated design. We validate AttentionSmithy by replicating the original\ntransformer under resource constraints and optimizing translation performance\nby combining positional encodings. Additionally, we demonstrate its\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\ntype classification. These case studies highlight AttentionSmithy's potential\nto accelerate research across diverse fields by removing framework\nimplementation barriers.\n","authors":["Caleb Cranney","Jesse G. Meyer"],"pdf_url":"https://arxiv.org/pdf/2502.09503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09502v1","updated":"2025-02-13T17:14:18Z","published":"2025-02-13T17:14:18Z","title":"Scalable First-order Method for Certifying Optimal k-Sparse GLMs","summary":"  This paper investigates the problem of certifying optimality for sparse\ngeneralized linear models (GLMs), where sparsity is enforced through an\n$\\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can\ncertify optimality by pruning nodes using dual bounds, existing methods for\ncomputing these bounds are either computationally intensive or exhibit slow\nconvergence, limiting their scalability to large-scale problems. To address\nthis challenge, we propose a first-order proximal gradient algorithm designed\nto solve the perspective relaxation of the problem within a BnB framework.\nSpecifically, we formulate the relaxed problem as a composite optimization\nproblem and demonstrate that the proximal operator of the non-smooth component\ncan be computed exactly in log-linear time complexity, eliminating the need to\nsolve a computationally expensive second-order cone program. Furthermore, we\nintroduce a simple restart strategy that enhances convergence speed while\nmaintaining low per-iteration complexity. Extensive experiments on synthetic\nand real-world datasets show that our approach significantly accelerates dual\nbound computations and is highly effective in providing optimality certificates\nfor large-scale problems.\n","authors":["Jiachang Liu","Soroosh Shafiee","Andrea Lodi"],"pdf_url":"https://arxiv.org/pdf/2502.09502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09500v1","updated":"2025-02-13T17:10:43Z","published":"2025-02-13T17:10:43Z","title":"Eidetic Learning: an Efficient and Provable Solution to Catastrophic\n  Forgetting","summary":"  Catastrophic forgetting -- the phenomenon of a neural network learning a task\nt1 and losing the ability to perform it after being trained on some other task\nt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,\n1989]. We present a method, Eidetic Learning, that provably solves catastrophic\nforgetting. A network trained with Eidetic Learning -- here, an EideticNet --\nrequires no rehearsal or replay. We consider successive discrete tasks and show\nhow at inference time an EideticNet automatically routes new instances without\nauxiliary task information. An EideticNet bears a family resemblance to the\nsparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network\ncapacity is partitioned across tasks and the network itself performs\ndata-conditional routing. An EideticNet is easy to implement and train, is\nefficient, and has time and space complexity linear in the number of\nparameters. The guarantee of our method holds for normalization layers of\nmodern neural networks during both pre-training and fine-tuning. We show with a\nvariety of network architectures and sets of tasks that EideticNets are immune\nto forgetting. While the practical benefits of EideticNets are substantial, we\nbelieve they can be benefit practitioners and theorists alike. The code for\ntraining EideticNets is available at\n\\href{https://github.com/amazon-science/eideticnet-training}{this https URL}.\n","authors":["Nicholas Dronen","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2502.09500v1.pdf","comment":"16 pages, 6 figures; code is available at\n  https://github.com/amazon-science/eideticnet-training"},{"id":"http://arxiv.org/abs/2501.14346v2","updated":"2025-02-13T17:03:04Z","published":"2025-01-24T09:17:57Z","title":"HorNets: Learning from Discrete and Continuous Signals with Routing\n  Neural Networks","summary":"  Construction of neural network architectures suitable for learning from both\ncontinuous and discrete tabular data is a challenging research endeavor.\nContemporary high-dimensional tabular data sets are often characterized by a\nrelatively small instance count, requiring data-efficient learning. We propose\nHorNets (Horn Networks), a neural network architecture with state-of-the-art\nperformance on synthetic and real-life data sets from scarce-data tabular\ndomains. HorNets are based on a clipped polynomial-like activation function,\nextended by a custom discrete-continuous routing mechanism that decides which\npart of the neural network to optimize based on the input's cardinality. By\nexplicitly modeling parts of the feature combination space or combining whole\nspace in a linear attention-like manner, HorNets dynamically decide which mode\nof operation is the most suitable for a given piece of data with no explicit\nsupervision. This architecture is one of the few approaches that reliably\nretrieves logical clauses (including noisy XNOR) and achieves state-of-the-art\nclassification performance on 14 real-life biomedical high-dimensional data\nsets. HorNets are made freely available under a permissive license alongside a\nsynthetic generator of categorical benchmarks.\n","authors":["Boshko Koloski","Nada Lavrač","Blaž Škrlj"],"pdf_url":"https://arxiv.org/pdf/2501.14346v2.pdf","comment":"Accepted to the ACML conference journal track with the Machine\n  Learning journal. The first and the last authors share an equal contribution"},{"id":"http://arxiv.org/abs/2502.09496v1","updated":"2025-02-13T17:03:03Z","published":"2025-02-13T17:03:03Z","title":"On Agnostic PAC Learning in the Small Error Regime","summary":"  Binary classification in the classic PAC model exhibits a curious phenomenon:\nEmpirical Risk Minimization (ERM) learners are suboptimal in the realizable\ncase yet optimal in the agnostic case. Roughly speaking, this owes itself to\nthe fact that non-realizable distributions $\\mathcal{D}$ are simply more\ndifficult to learn than realizable distributions -- even when one discounts a\nlearner's error by $\\mathrm{err}(h^*_{\\mathcal{D}})$, the error of the best\nhypothesis in $\\mathcal{H}$ for $\\mathcal{D}$. Thus, optimal agnostic learners\nare permitted to incur excess error on (easier-to-learn) distributions\n$\\mathcal{D}$ for which $\\tau = \\mathrm{err}(h^*_{\\mathcal{D}})$ is small.\n  Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses this\nshortcoming by including $\\tau$ itself as a parameter in the agnostic error\nterm. In this more fine-grained model, they demonstrate tightness of the error\nlower bound $\\tau + \\Omega \\left(\\sqrt{\\frac{\\tau (d + \\log(1 / \\delta))}{m}} +\n\\frac{d + \\log(1 / \\delta)}{m} \\right)$ in a regime where $\\tau > d/m$, and\nleave open the question of whether there may be a higher lower bound when $\\tau\n\\approx d/m$, with $d$ denoting $\\mathrm{VC}(\\mathcal{H})$. In this work, we\nresolve this question by exhibiting a learner which achieves error $c \\cdot\n\\tau + O \\left(\\sqrt{\\frac{\\tau (d + \\log(1 / \\delta))}{m}} + \\frac{d + \\log(1\n/ \\delta)}{m} \\right)$ for a constant $c \\leq 2.1$, thus matching the lower\nbound when $\\tau \\approx d/m$. Further, our learner is computationally\nefficient and is based upon careful aggregations of ERM classifiers, making\nprogress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS\n`24). We leave open the interesting question of whether our approach can be\nrefined to lower the constant from 2.1 to 1, which would completely settle the\ncomplexity of agnostic learning.\n","authors":["Julian Asilis","Mikael Møller Høgsgaard","Grigoris Velegkas"],"pdf_url":"https://arxiv.org/pdf/2502.09496v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2502.09495v1","updated":"2025-02-13T17:01:45Z","published":"2025-02-13T17:01:45Z","title":"Cracking the Code: Enhancing Development finance understanding with\n  artificial intelligence","summary":"  Analyzing development projects is crucial for understanding donors aid\nstrategies, recipients priorities, and to assess development finance capacity\nto adress development issues by on-the-ground actions. In this area, the\nOrganisation for Economic Co-operation and Developments (OECD) Creditor\nReporting System (CRS) dataset is a reference data source. This dataset\nprovides a vast collection of project narratives from various sectors\n(approximately 5 million projects). While the OECD CRS provides a rich source\nof information on development strategies, it falls short in informing project\npurposes due to its reporting process based on donors self-declared main\nobjectives and pre-defined industrial sectors. This research employs a novel\napproach that combines Machine Learning (ML) techniques, specifically Natural\nLanguage Processing (NLP), an innovative Python topic modeling technique called\nBERTopic, to categorise (cluster) and label development projects based on their\nnarrative descriptions. By revealing existing yet hidden topics of development\nfinance, this application of artificial intelligence enables a better\nunderstanding of donor priorities and overall development funding and provides\nmethods to analyse public and private projects narratives.\n","authors":["Pierre Beaucoral"],"pdf_url":"https://arxiv.org/pdf/2502.09495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09494v1","updated":"2025-02-13T17:00:11Z","published":"2025-02-13T17:00:11Z","title":"Communicating Likelihoods with Normalising Flows","summary":"  We present a machine-learning-based workflow to model an unbinned likelihood\nfrom its samples. A key advancement over existing approaches is the validation\nof the learned likelihood using rigorous statistical tests of the joint\ndistribution, such as the Kolmogorov-Smirnov test of the joint distribution.\nOur method enables the reliable communication of experimental and\nphenomenological likelihoods for subsequent analyses. We demonstrate its\neffectiveness through three case studies in high-energy physics. To support\nbroader adoption, we provide an open-source reference implementation, nabu.\n","authors":["Jack Y. Araz","Anja Beck","Méril Reboud","Michael Spannowsky","Danny van Dyk"],"pdf_url":"https://arxiv.org/pdf/2502.09494v1.pdf","comment":"4 pages + references, 1 figure"},{"id":"http://arxiv.org/abs/2502.09490v1","updated":"2025-02-13T16:57:07Z","published":"2025-02-13T16:57:07Z","title":"Inverse Design with Dynamic Mode Decomposition","summary":"  We introduce a computationally efficient method for the automation of inverse\ndesign in science and engineering. Based on simple least-square regression, the\nunderlying dynamic mode decomposition algorithm can be used to construct a\nlow-rank subspace spanning multiple experiments in parameter space. The\nproposed inverse design dynamic mode composition (ID-DMD) algorithm leverages\nthe computed low-dimensional subspace to enable fast digital design and\noptimization on laptop-level computing, including the potential to prescribe\nthe dynamics themselves. Moreover, the method is robust to noise, physically\ninterpretable, and can provide uncertainty quantification metrics. The\narchitecture can also efficiently scale to large-scale design problems using\nrandomized algorithms in the ID-DMD. The simplicity of the method and its\nimplementation are highly attractive in practice, and the ID-DMD has been\ndemonstrated to be an order of magnitude more accurate than competing methods\nwhile simultaneously being 3-5 orders faster on challenging engineering design\nproblems ranging from structural vibrations to fluid dynamics. Due to its\nspeed, robustness, interpretability, and ease-of-use, ID-DMD in comparison with\nother leading machine learning methods represents a significant advancement in\ndata-driven methods for inverse design and optimization, promising a paradigm\nshift in how to approach inverse design in practice.\n","authors":["Yunpeng Zhu","Liangliang Cheng","Anping Jing","Hanyu Huo","Ziqiang Lang","Bo Zhang","J. Nathan Kutz"],"pdf_url":"https://arxiv.org/pdf/2502.09490v1.pdf","comment":"29 pages, 19 figures"},{"id":"http://arxiv.org/abs/2502.09487v1","updated":"2025-02-13T16:52:06Z","published":"2025-02-13T16:52:06Z","title":"Objective quantification of mood states using large language models","summary":"  Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.\n","authors":["Jakub Onysk","Quentin Huys"],"pdf_url":"https://arxiv.org/pdf/2502.09487v1.pdf","comment":"main text - 9 pages, 5 figures;"},{"id":"http://arxiv.org/abs/2410.13879v2","updated":"2025-02-13T16:43:47Z","published":"2024-10-03T00:48:07Z","title":"Mixed-curvature decision trees and random forests","summary":"  Decision trees (DTs) and their random forest (RF) extensions are workhorses\nof classification and regression in Euclidean spaces. However, algorithms for\nlearning in non-Euclidean spaces are still limited. We extend DT and RF\nalgorithms to product manifolds: Cartesian products of several hyperbolic,\nhyperspherical, or Euclidean components. Such manifolds handle heterogeneous\ncurvature while still factorizing neatly into simpler components, making them\ncompelling embedding spaces for complex datasets. Our novel angular\nreformulation of DTs respects the geometry of the product manifold, yielding\nsplits that are geodesically convex, maximum-margin, and composable. In the\nspecial cases of single-component manifolds, our method simplifies to its\nEuclidean or hyperbolic counterparts, or introduces hyperspherical DT\nalgorithms, depending on the curvature. We benchmark our method on various\nclassification, regression, and link prediction tasks on synthetic data, graph\nembeddings, mixed-curvature variational autoencoder latent spaces, and\nempirical data. Compared to 7 other classifiers, product RFs ranked first on 25\nout of 57 benchmarks, and placed in the top 2 for 46 out of 57. This highlights\nthe value of product RFs as straightforward yet powerful new tools for data\nanalysis in product manifolds. Code for our paper is available at\nhttps://github.com/pchlenski/manify.\n","authors":["Philippe Chlenski","Quentin Chu","Raiyan R. Khan","Kaizhu Du","Antonio Khalil Moretti","Itsik Pe'er"],"pdf_url":"https://arxiv.org/pdf/2410.13879v2.pdf","comment":"27 pages, 11 figures. Submitted to ICML 2025"},{"id":"http://arxiv.org/abs/2502.09479v1","updated":"2025-02-13T16:43:32Z","published":"2025-02-13T16:43:32Z","title":"Assessing Generative AI value in a public sector context: evidence from\n  a field experiment","summary":"  The emergence of Generative AI (Gen AI) has motivated an interest in\nunderstanding how it could be used to enhance productivity across various\ntasks. We add to research results for the performance impact of Gen AI on\ncomplex knowledge-based tasks in a public sector setting. In a pre-registered\nexperiment, after establishing a baseline level of performance, we find mixed\nevidence for two types of composite tasks related to document understanding and\ndata analysis. For the Documents task, the treatment group using Gen AI had a\n17% improvement in answer quality scores (as judged by human evaluators) and a\n34% improvement in task completion time compared to a control group. For the\nData task, we find the Gen AI treatment group experienced a 12% reduction in\nquality scores and no significant difference in mean completion time compared\nto the control group. These results suggest that the benefits of Gen AI may be\ntask and potentially respondent dependent. We also discuss field notes and\nlessons learned, as well as supplementary insights from a post-trial survey and\nfeedback workshop with participants.\n","authors":["Trevor Fitzpatrick","Seamus Kelly","Patrick Carey","David Walsh","Ruairi Nugent"],"pdf_url":"https://arxiv.org/pdf/2502.09479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09477v1","updated":"2025-02-13T16:41:44Z","published":"2025-02-13T16:41:44Z","title":"DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation\n  Networks for Quantitative Nanomaterial Analysis through Differentiable\n  Rendering and Generative Modelling","summary":"  Nanomaterials exhibit distinctive properties governed by parameters such as\nsize, shape, and surface characteristics, which critically influence their\napplications and interactions across technological, biological, and\nenvironmental contexts. Accurate quantification and understanding of these\nmaterials are essential for advancing research and innovation. In this regard,\ndeep learning segmentation networks have emerged as powerful tools that enable\nautomated insights and replace subjective methods with precise quantitative\nanalysis. However, their efficacy depends on representative annotated datasets,\nwhich are challenging to obtain due to the costly imaging of nanoparticles and\nthe labor-intensive nature of manual annotations. To overcome these\nlimitations, we introduce DiffRenderGAN, a novel generative model designed to\nproduce annotated synthetic data. By integrating a differentiable renderer into\na Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes\ntextural rendering parameters to generate realistic, annotated nanoparticle\nimages from non-annotated real microscopy images. This approach reduces the\nneed for manual intervention and enhances segmentation performance compared to\nexisting synthetic data methods by generating diverse and realistic data.\nTested on multiple ion and electron microscopy cases, including titanium\ndioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),\nDiffRenderGAN bridges the gap between synthetic and real data, advancing the\nquantification and understanding of complex nanomaterial systems.\n","authors":["Dennis Possart","Leonid Mill","Florian Vollnhals","Tor Hildebrand","Peter Suter","Mathis Hoffmann","Jonas Utz","Daniel Augsburger","Mareike Thies","Mingxuan Wu","Fabian Wagner","George Sarau","Silke Christiansen","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2502.09477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16333v2","updated":"2025-02-13T16:41:13Z","published":"2024-10-19T15:42:49Z","title":"Conformal Predictive Portfolio Selection","summary":"  This study examines portfolio selection using predictive models for portfolio\nreturns. Portfolio selection is a fundamental task in finance, and a variety of\nmethods have been developed to achieve this goal. For instance, the\nmean-variance approach constructs portfolios by balancing the trade-off between\nthe mean and variance of asset returns, while the quantile-based approach\noptimizes portfolios by considering tail risk. These methods often depend on\ndistributional information estimated from historical data using predictive\nmodels, each of which carries its own uncertainty. To address this, we propose\na framework for predictive portfolio selection via conformal prediction ,\ncalled \\emph{Conformal Predictive Portfolio Selection} (CPPS). Our approach\nforecasts future portfolio returns, computes the corresponding prediction\nintervals, and selects the portfolio of interest based on these intervals. The\nframework is flexible and can accommodate a wide range of predictive models,\nincluding autoregressive (AR) models, random forests, and neural networks. We\ndemonstrate the effectiveness of the CPPS framework by applying it to an AR\nmodel and validate its performance through empirical studies, showing that it\ndelivers superior returns compared to simpler strategies.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2410.16333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09473v1","updated":"2025-02-13T16:36:25Z","published":"2025-02-13T16:36:25Z","title":"Learning to Predict Global Atrial Fibrillation Dynamics from Sparse\n  Measurements","summary":"  Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-all\ntreatment with limited success in persistent AF. This may be due to our\ninability to map the dynamics of AF with the limited resolution and coverage\nprovided by sequential contact mapping catheters, preventing effective patient\nphenotyping for personalised, targeted ablation. Here we introduce FibMap, a\ngraph recurrent neural network model that reconstructs global AF dynamics from\nsparse measurements. Trained and validated on 51 non-contact whole atria\nrecordings, FibMap reconstructs whole atria dynamics from 10% surface coverage,\nachieving a 210% lower mean absolute error and an order of magnitude higher\nperformance in tracking phase singularities compared to baseline methods.\nClinical utility of FibMap is demonstrated on real-world contact mapping\nrecordings, achieving reconstruction fidelity comparable to non-contact\nmapping. FibMap's state-spaces and patient-specific parameters offer insights\nfor electrophenotyping AF. Integrating FibMap into clinical practice could\nenable personalised AF care and improve outcomes.\n","authors":["Alexander Jenkins","Andrea Cini","Joseph Barker","Alexander Sharp","Arunashis Sau","Varun Valentine","Srushti Valasang","Xinyang Li","Tom Wong","Timothy Betts","Danilo Mandic","Cesare Alippi","Fu Siong Ng"],"pdf_url":"https://arxiv.org/pdf/2502.09473v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.20440v2","updated":"2025-02-13T16:35:17Z","published":"2024-09-30T16:00:23Z","title":"Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits","summary":"  Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regret\nfor adversarial as well as stochastic bandit problems and allow for a\nstreamlined analysis. Nonetheless, FTRL algorithms require the solution of an\noptimization problem in every iteration and are thus computationally\nchallenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achieve\ncomputational efficiency by perturbing the estimates of the rewards of the\narms, but their regret analysis is cumbersome. We propose a new FTPL algorithm\nthat generates optimal policies for both adversarial and stochastic multi-armed\nbandits. Like FTRL, our algorithm admits a unified regret analysis, and similar\nto FTPL, it offers low computational costs. Unlike existing FTPL algorithms\nthat rely on independent additive disturbances governed by a \\textit{known}\ndistribution, we allow for disturbances governed by an \\textit{ambiguous}\ndistribution that is only known to belong to a given set and propose a\nprinciple of optimism in the face of ambiguity. Consequently, our framework\ngeneralizes existing FTPL algorithms. It also encapsulates a broad range of\nFTRL methods as special cases, including several optimal ones, which appears to\nbe impossible with current FTPL methods. Finally, we use techniques from\ndiscrete choice theory to devise an efficient bisection algorithm for computing\nthe optimistic arm sampling probabilities. This algorithm is up to $10^4$ times\nfaster than standard FTRL algorithms that solve an optimization problem in\nevery iteration. Our results not only settle existing conjectures but also\nprovide new insights into the impact of perturbations by mapping FTRL to FTPL.\n","authors":["Mengmeng Li","Daniel Kuhn","Bahar Taşkesen"],"pdf_url":"https://arxiv.org/pdf/2409.20440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17163v2","updated":"2025-02-13T16:32:55Z","published":"2024-05-27T13:36:50Z","title":"Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep\n  Graph Networks","summary":"  The dynamics of information diffusion within graphs is a critical open issue\nthat heavily influences graph representation learning, especially when\nconsidering long-range propagation. This calls for principled approaches that\ncontrol and regulate the degree of propagation and dissipation of information\nthroughout the neural flow. Motivated by this, we introduce (port-)Hamiltonian\nDeep Graph Networks, a novel framework that models neural information flow in\ngraphs by building on the laws of conservation of Hamiltonian dynamical\nsystems. We reconcile under a single theoretical and practical framework both\nnon-dissipative long-range propagation and non-conservative behaviors,\nintroducing tools from mechanical systems to gauge the equilibrium between the\ntwo components. Our approach can be applied to general message-passing\narchitectures, and it provides theoretical guarantees on information\nconservation in time. Empirical results prove the effectiveness of our\nport-Hamiltonian scheme in pushing simple graph convolutional architectures to\nstate-of-the-art performance in long-range benchmarks.\n","authors":["Simon Heilig","Alessio Gravina","Alessandro Trenta","Claudio Gallicchio","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2405.17163v2.pdf","comment":"Accepted at ICLR 2025 (https://openreview.net/forum?id=03EkqSCKuO)"},{"id":"http://arxiv.org/abs/2411.03263v2","updated":"2025-02-13T16:28:07Z","published":"2024-11-05T17:02:29Z","title":"Proxy-informed Bayesian transfer learning with unknown sources","summary":"  Generalization outside the scope of one's training data requires leveraging\nprior knowledge about the effects that transfer, and the effects that don't,\nbetween different data sources. Transfer learning is a framework for specifying\nand refining this knowledge about sets of source (training) and target\n(prediction) data. A challenging open problem is addressing the empirical\nphenomenon of negative transfer, whereby the transfer learner performs worse on\nthe target data after taking the source data into account than before. We first\nintroduce a Bayesian perspective on negative transfer, and then a method to\naddress it. The key insight from our formulation is that negative transfer can\nstem from misspecified prior information about non-transferable causes of the\nsource data. Our proposed method, proxy-informed robust method for\nprobabilistic transfer learning (PROMPT), does not require prior knowledge of\nthe source data (the data sources may be \"unknown\"). PROMPT is thus applicable\nwhen differences between tasks are unobserved, such as in the presence of\nlatent confounders. Moreover, the learner need not have access to observations\nin the target task (cannot \"fine-tune\"), and instead makes use of proxy\n(indirect) information. Our theoretical results show that the threat of\nnegative transfer does not depend on the informativeness of the proxy\ninformation, highlighting the usefulness of PROMPT in cases where only noisy\nindirect information, such as human feedback, is available.\n","authors":["Sabina J. Sloman","Julien Martinelli","Samuel Kaski"],"pdf_url":"https://arxiv.org/pdf/2411.03263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09445v1","updated":"2025-02-13T16:15:43Z","published":"2025-02-13T16:15:43Z","title":"A Differentiable Rank-Based Objective For Better Feature Learning","summary":"  In this paper, we leverage existing statistical methods to better understand\nfeature learning from data. We tackle this by modifying the model-free variable\nselection method, Feature Ordering by Conditional Independence (FOCI), which is\nintroduced in \\cite{azadkia2021simple}. While FOCI is based on a non-parametric\ncoefficient of conditional dependence, we introduce its parametric,\ndifferentiable approximation. With this approximate coefficient of correlation,\nwe present a new algorithm called difFOCI, which is applicable to a wider range\nof machine learning problems thanks to its differentiable nature and learnable\nparameters. We present difFOCI in three contexts: (1) as a variable selection\nmethod with baseline comparisons to FOCI, (2) as a trainable model parametrized\nwith a neural network, and (3) as a generic, widely applicable neural network\nregularizer, one that improves feature learning with better management of\nspurious correlations. We evaluate difFOCI on increasingly complex problems\nranging from basic variable selection in toy examples to saliency map\ncomparisons in convolutional networks. We then show how difFOCI can be\nincorporated in the context of fairness to facilitate classifications without\nrelying on sensitive data.\n","authors":["Krunoslav Lehman Pavasovic","David Lopez-Paz","Giulio Biroli","Levent Sagun"],"pdf_url":"https://arxiv.org/pdf/2502.09445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19082v2","updated":"2025-02-13T16:14:34Z","published":"2025-01-31T12:15:58Z","title":"A Bias-Correction Decentralized Stochastic Gradient Algorithm with\n  Momentum Acceleration","summary":"  Distributed stochastic optimization algorithms can simultaneously process\nlarge-scale datasets, significantly accelerating model training. However, their\neffectiveness is often hindered by the sparsity of distributed networks and\ndata heterogeneity. In this paper, we propose a momentum-accelerated\ndistributed stochastic gradient algorithm, termed Exact-Diffusion with Momentum\n(EDM), which mitigates the bias from data heterogeneity and incorporates\nmomentum techniques commonly used in deep learning to enhance convergence rate.\nOur theoretical analysis demonstrates that the EDM algorithm converges\nsub-linearly to the neighborhood of the optimal solution, the radius of which\nis irrespective of data heterogeneity, when applied to non-convex objective\nfunctions; under the Polyak-Lojasiewicz condition, which is a weaker assumption\nthan strong convexity, it converges linearly to the target region. Our analysis\ntechniques employed to handle momentum in complex distributed parameter update\nstructures yield a sufficiently tight convergence upper bound, offering a new\nperspective for the theoretical analysis of other momentum-based distributed\nalgorithms.\n","authors":["Yuchen Hu","Xi Chen","Weidong Liu","Xiaojun Mao"],"pdf_url":"https://arxiv.org/pdf/2501.19082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09443v1","updated":"2025-02-13T16:12:17Z","published":"2025-02-13T16:12:17Z","title":"Relational Conformal Prediction for Correlated Time Series","summary":"  We address the problem of uncertainty quantification in time series\nforecasting by exploiting observations at correlated sequences. Relational deep\nlearning methods leveraging graph representations are among the most effective\ntools for obtaining point estimates from spatiotemporal data and correlated\ntime series. However, the problem of exploiting relational structures to\nestimate the uncertainty of such predictions has been largely overlooked in the\nsame context. To this end, we propose a novel distribution-free approach based\non the conformal prediction framework and quantile regression. Despite the\nrecent applications of conformal prediction to sequential data, existing\nmethods operate independently on each target time series and do not account for\nrelationships among them when constructing the prediction interval. We fill\nthis void by introducing a novel conformal prediction method based on graph\ndeep learning operators. Our method, named Conformal Relational Prediction\n(CoRel), does not require the relational structure (graph) to be known as a\nprior and can be applied on top of any pre-trained time series predictor.\nAdditionally, CoRel includes an adaptive component to handle non-exchangeable\ndata and changes in the input time series. Our approach provides accurate\ncoverage and archives state-of-the-art uncertainty quantification in relevant\nbenchmarks.\n","authors":["Andrea Cini","Alexander Jenkins","Danilo Mandic","Cesare Alippi","Filippo Maria Bianchi"],"pdf_url":"https://arxiv.org/pdf/2502.09443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17438v2","updated":"2025-02-13T16:11:35Z","published":"2023-05-27T10:26:23Z","title":"On the Importance of Backbone to the Adversarial Robustness of Object\n  Detectors","summary":"  Object detection is a critical component of various security-sensitive\napplications, such as autonomous driving and video surveillance. However,\nexisting object detectors are vulnerable to adversarial attacks, which poses a\nsignificant challenge to their reliability and security. Through experiments,\nfirst, we found that existing works on improving the adversarial robustness of\nobject detectors give a false sense of security. Second, we found that\nadversarially pre-trained backbone networks were essential for enhancing the\nadversarial robustness of object detectors. We then proposed a simple yet\neffective recipe for fast adversarial fine-tuning on object detectors with\nadversarially pre-trained backbones. Without any modifications to the structure\nof object detectors, our recipe achieved significantly better adversarial\nrobustness than previous works. Finally, we explored the potential of different\nmodern object detector designs for improving adversarial robustness with our\nrecipe and demonstrated interesting findings, which inspired us to design\nstate-of-the-art (SOTA) robust detectors. Our empirical results set a new\nmilestone for adversarially robust object detection. Code and trained\ncheckpoints are available at https://github.com/thu-ml/oddefense.\n","authors":["Xiao Li","Hang Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2305.17438v2.pdf","comment":"Accepted by IEEE TIFS"},{"id":"http://arxiv.org/abs/2410.11415v2","updated":"2025-02-13T16:02:42Z","published":"2024-10-15T09:02:55Z","title":"KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI","summary":"  A popular approach to neurosymbolic AI involves mapping logic formulas to\narithmetic circuits (computation graphs consisting of sums and products) and\npassing the outputs of a neural network through these circuits. This approach\nenforces symbolic constraints onto a neural network in a principled and\nend-to-end differentiable way. Unfortunately, arithmetic circuits are\nchallenging to run on modern AI accelerators as they exhibit a high degree of\nirregular sparsity. To address this limitation, we introduce knowledge layers\n(KLay), a new data structure to represent arithmetic circuits that can be\nefficiently parallelized on GPUs. Moreover, we contribute two algorithms used\nin the translation of traditional circuit representations to KLay and a further\nalgorithm that exploits parallelization opportunities during circuit\nevaluations. We empirically show that KLay achieves speedups of multiple orders\nof magnitude over the state of the art, thereby paving the way towards scaling\nneurosymbolic AI to larger real-world applications.\n","authors":["Jaron Maene","Vincent Derkinderen","Pedro Zuidberg Dos Martires"],"pdf_url":"https://arxiv.org/pdf/2410.11415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09432v1","updated":"2025-02-13T15:55:00Z","published":"2025-02-13T15:55:00Z","title":"Dual Formulation for Non-Rectangular Lp Robust Markov Decision Processes","summary":"  We study robust Markov decision processes (RMDPs) with non-rectangular\nuncertainty sets, which capture interdependencies across states unlike\ntraditional rectangular models. While non-rectangular robust policy evaluation\nis generally NP-hard, even in approximation, we identify a powerful class of\n$L_p$-bounded uncertainty sets that avoid these complexity barriers due to\ntheir structural simplicity. We further show that this class can be decomposed\ninto infinitely many \\texttt{sa}-rectangular $L_p$-bounded sets and leverage\nits structural properties to derive a novel dual formulation for $L_p$ RMDPs.\nThis formulation provides key insights into the adversary's strategy and\nenables the development of the first robust policy evaluation algorithms for\nnon-rectangular RMDPs. Empirical results demonstrate that our approach\nsignificantly outperforms brute-force methods, establishing a promising\nfoundation for future investigation into non-rectangular robust MDPs.\n","authors":["Navdeep Kumar","Adarsh Gupta","Maxence Mohamed Elfatihi","Giorgia Ramponi","Kfir Yehuda Levy","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2502.09432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08097v3","updated":"2025-02-13T15:53:59Z","published":"2024-05-13T18:24:03Z","title":"A Galois theorem for machine learning: Functions on symmetric matrices\n  and point clouds via lightweight invariant features","summary":"  In this work, we present a mathematical formulation for machine learning of\n(1) functions on symmetric matrices that are invariant with respect to the\naction of permutations by conjugation, and (2) functions on point clouds that\nare invariant with respect to rotations, reflections, and permutations of the\npoints. To achieve this, we provide a general construction of generically\nseparating invariant features using ideas inspired by Galois theory. We\nconstruct $O(n^2)$ invariant features derived from generators for the field of\nrational functions on $n\\times n$ symmetric matrices that are invariant under\njoint permutations of rows and columns. We show that these invariant features\ncan separate all distinct orbits of symmetric matrices except for a measure\nzero set; such features can be used to universally approximate invariant\nfunctions on almost all weighted graphs. For point clouds in a fixed dimension,\nwe prove that the number of invariant features can be reduced, generically\nwithout losing expressivity, to $O(n)$, where $n$ is the number of points. We\ncombine these invariant features with DeepSets to learn functions on symmetric\nmatrices and point clouds with varying sizes. We empirically demonstrate the\nfeasibility of our approach on molecule property regression and point cloud\ndistance prediction.\n","authors":["Ben Blum-Smith","Ningyuan Huang","Marco Cuturi","Soledad Villar"],"pdf_url":"https://arxiv.org/pdf/2405.08097v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00315v3","updated":"2025-02-13T15:46:35Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2410.01706v2","updated":"2025-02-13T15:43:25Z","published":"2024-10-02T16:15:26Z","title":"Sable: a Performant, Efficient and Scalable Sequence Model for MARL","summary":"  As multi-agent reinforcement learning (MARL) progresses towards solving\nlarger and more complex problems, it becomes increasingly important that\nalgorithms exhibit the key properties of (1) strong performance, (2) memory\nefficiency and (3) scalability. In this work, we introduce Sable, a performant,\nmemory efficient and scalable sequence modeling approach to MARL. Sable works\nby adapting the retention mechanism in Retentive Networks to achieve\ncomputationally efficient processing of multi-agent observations with long\ncontext memory for temporal reasoning. Through extensive evaluations across six\ndiverse environments, we demonstrate how Sable is able to significantly\noutperform existing state-of-the-art methods in a large number of diverse tasks\n(34 out of 45 tested). Furthermore, Sable maintains performance as we scale the\nnumber of agents, handling environments with more than a thousand agents while\nexhibiting a linear increase in memory usage. Finally, we conduct ablation\nstudies to isolate the source of Sable's performance gains and confirm its\nefficient computational memory usage.\n","authors":["Omayma Mahjoub","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Simon du Toit","Jemma Daniel","Louay Ben Nessir","Louise Beyers","Claude Formanek","Liam Clark","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2410.01706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09419v1","updated":"2025-02-13T15:42:44Z","published":"2025-02-13T15:42:44Z","title":"On multi-token prediction for efficient LLM inference","summary":"  We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction.\n","authors":["Somesh Mehra","Javier Alonso Garcia","Lukas Mauch"],"pdf_url":"https://arxiv.org/pdf/2502.09419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09417v1","updated":"2025-02-13T15:40:39Z","published":"2025-02-13T15:40:39Z","title":"A Survey of Reinforcement Learning for Optimization in Automation","summary":"  Reinforcement Learning (RL) has become a critical tool for optimization\nchallenges within automation, leading to significant advancements in several\nareas. This review article examines the current landscape of RL within\nautomation, with a particular focus on its roles in manufacturing, energy\nsystems, and robotics. It discusses state-of-the-art methods, major challenges,\nand upcoming avenues of research within each sector, highlighting RL's capacity\nto solve intricate optimization challenges. The paper reviews the advantages\nand constraints of RL-driven optimization methods in automation. It points out\nprevalent challenges encountered in RL optimization, including issues related\nto sample efficiency and scalability; safety and robustness; interpretability\nand trustworthiness; transfer learning and meta-learning; and real-world\ndeployment and integration. It further explores prospective strategies and\nfuture research pathways to navigate these challenges. Additionally, the survey\nincludes a comprehensive list of relevant research papers, making it an\nindispensable guide for scholars and practitioners keen on exploring this\ndomain.\n","authors":["Ahmad Farooq","Kamran Iqbal"],"pdf_url":"https://arxiv.org/pdf/2502.09417v1.pdf","comment":"8 pages, 4 tables, and 1 figure. Accepted at IEEE 20th International\n  Conference on Automation Science and Engineering (CASE) 2024"},{"id":"http://arxiv.org/abs/2410.08751v2","updated":"2025-02-13T15:36:19Z","published":"2024-10-11T12:10:51Z","title":"Zero-Shot Offline Imitation Learning via Optimal Transport","summary":"  Zero-shot imitation learning algorithms hold the promise of reproducing\nunseen behavior from as little as a single demonstration at test time. Existing\npractical approaches view the expert demonstration as a sequence of goals,\nenabling imitation with a high-level goal selector, and a low-level\ngoal-conditioned policy. However, this framework can suffer from myopic\nbehavior: the agent's immediate actions towards achieving individual goals may\nundermine long-term objectives. We introduce a novel method that mitigates this\nissue by directly optimizing the occupancy matching objective that is intrinsic\nto imitation learning. We propose to lift a goal-conditioned value function to\na distance between occupancies, which are in turn approximated via a learned\nworld model. The resulting method can learn from offline, suboptimal data, and\nis capable of non-myopic, zero-shot imitation, as we demonstrate in complex,\ncontinuous benchmarks.\n","authors":["Thomas Rupf","Marco Bagatella","Nico Gürtler","Jonas Frey","Georg Martius"],"pdf_url":"https://arxiv.org/pdf/2410.08751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08441v2","updated":"2025-02-13T15:36:14Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v2.pdf","comment":"17 pages, 8 figures; figures corrected"},{"id":"http://arxiv.org/abs/2411.02280v2","updated":"2025-02-13T15:21:43Z","published":"2024-11-04T17:09:10Z","title":"The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units","summary":"  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n","authors":["Badr AlKhamissi","Greta Tuckute","Antoine Bosselut","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2411.02280v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.09396v1","updated":"2025-02-13T15:16:53Z","published":"2025-02-13T15:16:53Z","title":"A hierarchical approach for assessing the vulnerability of tree-based\n  classification models to membership inference attack","summary":"  Machine learning models can inadvertently expose confidential properties of\ntheir training data, making them vulnerable to membership inference attacks\n(MIA). While numerous evaluation methods exist, many require computationally\nexpensive processes, such as training multiple shadow models. This article\npresents two new complementary approaches for efficiently identifying\nvulnerable tree-based models: an ante-hoc analysis of hyperparameter choices\nand a post-hoc examination of trained model structure. While these new methods\ncannot certify whether a model is safe from MIA, they provide practitioners\nwith a means to significantly reduce the number of models that need to undergo\nexpensive MIA assessment through a hierarchical filtering approach.\n  More specifically, it is shown that the rank order of disclosure risk for\ndifferent hyperparameter combinations remains consistent across datasets,\nenabling the development of simple, human-interpretable rules for identifying\nrelatively high-risk models before training. While this ante-hoc analysis\ncannot determine absolute safety since this also depends on the specific\ndataset, it allows the elimination of unnecessarily risky configurations during\nhyperparameter tuning. Additionally, computationally inexpensive structural\nmetrics serve as indicators of MIA vulnerability, providing a second filtering\nstage to identify risky models after training but before conducting expensive\nattacks. Empirical results show that hyperparameter-based risk prediction rules\ncan achieve high accuracy in predicting the most at risk combinations of\nhyperparameters across different tree-based model types, while requiring no\nmodel training. Moreover, target model accuracy is not seen to correlate with\nprivacy risk, suggesting opportunities to optimise model configurations for\nboth performance and privacy.\n","authors":["Richard J. Preen","Jim Smith"],"pdf_url":"https://arxiv.org/pdf/2502.09396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09395v1","updated":"2025-02-13T15:16:52Z","published":"2025-02-13T15:16:52Z","title":"Robot Pouring: Identifying Causes of Spillage and Selecting Alternative\n  Action Parameters Using Probabilistic Actual Causation","summary":"  In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a\nlarge variety of objects and goals. When confronted with an unexpected or\nunwanted outcome, we take corrective actions and try again until achieving the\ndesired result. The reasoning performed to identify a cause of the observed\noutcome and to select an appropriate corrective action is a crucial aspect of\nhuman reasoning for successful task execution. Central to this reasoning is the\nassumption that a factor is responsible for producing the observed outcome. In\nthis paper, we investigate the use of probabilistic actual causation to\ndetermine whether a factor is the cause of an observed undesired outcome.\nFurthermore, we show how the actual causation probabilities can be used to find\nalternative actions to change the outcome. We apply the probabilistic actual\ncausation analysis to a robot pouring task. When spillage occurs, the analysis\nindicates whether a task parameter is the cause and how it should be changed to\navoid spillage. The analysis requires a causal graph of the task and the\ncorresponding conditional probability distributions. To fulfill these\nrequirements, we perform a complete causal modeling procedure (i.e., task\nanalysis, definition of variables, determination of the causal graph structure,\nand estimation of conditional probability distributions) using data from a\nrealistic simulation of the robot pouring task, covering a large combinatorial\nspace of task parameters. Based on the results, we discuss the implications of\nthe variables' representation and how the alternative actions suggested by the\nactual causation analysis would compare to the alternative solutions proposed\nby a human observer. The practical use of the analysis of probabilistic actual\ncausation to select alternative action parameters is demonstrated.\n","authors":["Jaime Maldonado","Jonas Krumme","Christoph Zetzsche","Vanessa Didelez","Kerstin Schill"],"pdf_url":"https://arxiv.org/pdf/2502.09395v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.09390v1","updated":"2025-02-13T15:07:20Z","published":"2025-02-13T15:07:20Z","title":"SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models","summary":"  In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.\n","authors":["Daniel Fleischer","Moshe Berchansky","Gad Markovits","Moshe Wasserblat"],"pdf_url":"https://arxiv.org/pdf/2502.09390v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.14441v2","updated":"2025-02-13T14:52:43Z","published":"2025-01-24T12:22:18Z","title":"Impact of Batch Normalization on Convolutional Network Representations","summary":"  Batch normalization (BatchNorm) is a popular layer normalization technique\nused when training deep neural networks. It has been shown to enhance the\ntraining speed and accuracy of deep learning models. However, the mechanics by\nwhich BatchNorm achieves these benefits is an active area of research, and\ndifferent perspectives have been proposed. In this paper, we investigate the\neffect of BatchNorm on the resulting hidden representations, that is, the\nvectors of activation values formed as samples are processed at each hidden\nlayer. Specifically, we consider the sparsity of these representations, as well\nas their implicit clustering -- the creation of groups of representations that\nare similar to some extent. We contrast image classification models trained\nwith and without batch normalization and highlight consistent differences\nobserved. These findings highlight that BatchNorm's effect on representational\nsparsity is not a significant factor affecting generalization, while the\nrepresentations of models trained with BatchNorm tend to show more advantageous\nclustering characteristics.\n","authors":["Hermanus L. Potgieter","Coenraad Mouton","Marelie H. Davel"],"pdf_url":"https://arxiv.org/pdf/2501.14441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09376v1","updated":"2025-02-13T14:45:11Z","published":"2025-02-13T14:45:11Z","title":"LoRA Training Provably Converges to a Low-Rank Global Minimum or It\n  Fails Loudly (But it Probably Won't Fail)","summary":"  Low-rank adaptation (LoRA) has become a standard approach for fine-tuning\nlarge foundation models. However, our theoretical understanding of LoRA remains\nlimited as prior analyses of LoRA's training dynamics either rely on\nlinearization arguments or consider highly simplified setups. In this work, we\nanalyze the LoRA loss landscape without such restrictive assumptions. We define\ntwo regimes: a ``special regime'', which includes idealized setups where\nlinearization arguments hold, and a ``generic regime'' representing more\nrealistic setups where linearization arguments do not hold. In the generic\nregime, we show that LoRA training converges to a global minimizer with low\nrank and small magnitude, or a qualitatively distinct solution with high rank\nand large magnitude. Finally, we argue that the zero-initialization and weight\ndecay in LoRA training induce an implicit bias toward the low-rank,\nsmall-magnitude region of the parameter space -- where global minima lie --\nthus shedding light on why LoRA training usually succeeds in finding global\nminima.\n","authors":["Junsu Kim","Jaeyeon Kim","Ernest K. Ryu"],"pdf_url":"https://arxiv.org/pdf/2502.09376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09374v1","updated":"2025-02-13T14:43:22Z","published":"2025-02-13T14:43:22Z","title":"Mitigating multiple single-event upsets during deep neural network\n  inference using fault-aware training","summary":"  Deep neural networks (DNNs) are increasingly used in safety-critical\napplications. Reliable fault analysis and mitigation are essential to ensure\ntheir functionality in harsh environments that contain high radiation levels.\nThis study analyses the impact of multiple single-bit single-event upsets in\nDNNs by performing fault injection at the level of a DNN model. Additionally, a\nfault aware training (FAT) methodology is proposed that improves the DNNs'\nrobustness to faults without any modification to the hardware. Experimental\nresults show that the FAT methodology improves the tolerance to faults up to a\nfactor 3.\n","authors":["Toon Vinck","Naïn Jonckers","Gert Dekkers","Jeffrey Prinzie","Peter Karsmakers"],"pdf_url":"https://arxiv.org/pdf/2502.09374v1.pdf","comment":"7 pages, 4 figures, Topical Workshop on Electronics for Particle\n  Physics"},{"id":"http://arxiv.org/abs/2502.07465v2","updated":"2025-02-13T14:38:24Z","published":"2025-02-11T11:16:59Z","title":"Crime Forecasting: A Spatio-temporal Analysis with Deep Learning Models","summary":"  This study uses deep-learning models to predict city partition crime counts\non specific days. It helps police enhance surveillance, gather intelligence,\nand proactively prevent crimes. We formulate crime count prediction as a\nspatiotemporal sequence challenge, where both input data and prediction targets\nare spatiotemporal sequences. In order to improve the accuracy of crime\nforecasting, we introduce a new model that combines Convolutional Neural\nNetworks (CNN) and Long Short-Term Memory (LSTM) networks. We conducted a\ncomparative analysis to access the effects of various data sequences, including\nraw and binned data, on the prediction errors of four deep learning forecasting\nmodels. Directly inputting raw crime data into the forecasting model causes\nhigh prediction errors, making the model unsuitable for real - world use. The\nfindings indicate that the proposed CNN-LSTM model achieves optimal performance\nwhen crime data is categorized into 10 or 5 groups. Data binning can enhance\nforecasting model performance, but poorly defined intervals may reduce map\ngranularity. Compared to dividing into 5 bins, binning into 10 intervals\nstrikes an optimal balance, preserving data characteristics and surpassing raw\ndata in predictive modelling efficacy.\n","authors":["Li Mao","Wei Du","Shuo Wen","Qi Li","Tong Zhang","Wei Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.07465v2.pdf","comment":"The paper was submitted without the consent of all co-authors. The\n  content of the paper is incomplete and requires substantial additional work\n  before it can be considered a complete and coherent submission"},{"id":"http://arxiv.org/abs/2502.09369v1","updated":"2025-02-13T14:35:40Z","published":"2025-02-13T14:35:40Z","title":"Language Agents as Digital Representatives in Collective Decision-Making","summary":"  Consider the process of collective decision-making, in which a group of\nindividuals interactively select a preferred outcome from among a universe of\nalternatives. In this context, \"representation\" is the activity of making an\nindividual's preferences present in the process via participation by a proxy\nagent -- i.e. their \"representative\". To this end, learned models of human\nbehavior have the potential to fill this role, with practical implications for\nmulti-agent scenario studies and mechanism design. In this work, we investigate\nthe possibility of training \\textit{language agents} to behave in the capacity\nof representatives of human agents, appropriately expressing the preferences of\nthose individuals whom they stand for. First, we formalize the setting of\n\\textit{collective decision-making} -- as the episodic process of interaction\nbetween a group of agents and a decision mechanism. On this basis, we then\nformalize the problem of \\textit{digital representation} -- as the simulation\nof an agent's behavior to yield equivalent outcomes from the mechanism.\nFinally, we conduct an empirical case study in the setting of\n\\textit{consensus-finding} among diverse humans, and demonstrate the\nfeasibility of fine-tuning large language models to act as digital\nrepresentatives.\n","authors":["Daniel Jarrett","Miruna Pîslar","Michiel A. Bakker","Michael Henry Tessler","Raphael Köster","Jan Balaguer","Romuald Elie","Christopher Summerfield","Andrea Tacchetti"],"pdf_url":"https://arxiv.org/pdf/2502.09369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09365v1","updated":"2025-02-13T14:33:02Z","published":"2025-02-13T14:33:02Z","title":"Simple Path Structural Encoding for Graph Transformers","summary":"  Graph transformers extend global self-attention to graph-structured data,\nachieving notable success in graph learning. Recently, random walk structural\nencoding (RWSE) has been found to further enhance their predictive power by\nencoding both structural and positional information into the edge\nrepresentation. However, RWSE cannot always distinguish between edges that\nbelong to different local graph patterns, which reduces its ability to capture\nthe full structural complexity of graphs. This work introduces Simple Path\nStructural Encoding (SPSE), a novel method that utilizes simple path counts for\nedge encoding. We show theoretically and experimentally that SPSE overcomes the\nlimitations of RWSE, providing a richer representation of graph structures,\nparticularly for capturing local cyclic patterns. To make SPSE computationally\ntractable, we propose an efficient approximate algorithm for simple path\ncounting. SPSE demonstrates significant performance improvements over RWSE on\nvarious benchmarks, including molecular and long-range graph datasets,\nachieving statistically significant gains in discriminative tasks. These\nresults pose SPSE as a powerful edge encoding alternative for enhancing the\nexpressivity of graph transformers.\n","authors":["Louis Airale","Antonio Longa","Mattia Rigon","Andrea Passerini","Roberto Passerone"],"pdf_url":"https://arxiv.org/pdf/2502.09365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09363v1","updated":"2025-02-13T14:31:49Z","published":"2025-02-13T14:31:49Z","title":"The Accuracy Cost of Weakness: A Theoretical Analysis of Fixed-Segment\n  Weak Labeling for Events in Time","summary":"  Accurate labels are critical for deriving robust machine learning models.\nLabels are used to train supervised learning models and to evaluate most\nmachine learning paradigms. In this paper, we model the accuracy and cost of a\ncommon weak labeling process where annotators assign presence or absence labels\nto fixed-length data segments for a given event class. The annotator labels a\nsegment as \"present\" if it sufficiently covers an event from that class, e.g.,\na birdsong sound event in audio data. We analyze how the segment length affects\nthe label accuracy and the required number of annotations, and compare this\nfixed-length labeling approach with an oracle method that uses the true event\nactivations to construct the segments. Furthermore, we quantify the gap between\nthese methods and verify that in most realistic scenarios the oracle method is\nbetter than the fixed-length labeling method in both accuracy and cost. Our\nfindings provide a theoretical justification for adaptive weak labeling\nstrategies that mimic the oracle process, and a foundation for optimizing weak\nlabeling processes in sequence labeling tasks.\n","authors":["John Martinsson","Olof Mogren","Tuomas Virtanen","Maria Sandsten"],"pdf_url":"https://arxiv.org/pdf/2502.09363v1.pdf","comment":"Submitted to TMLR"},{"id":"http://arxiv.org/abs/2411.17287v2","updated":"2025-02-13T14:31:24Z","published":"2024-11-26T10:19:16Z","title":"Privacy-Preserving Federated Unsupervised Domain Adaptation for\n  Regression on Small-Scale and High-Dimensional Biological Data","summary":"  Machine learning models often struggle with generalization in small,\nheterogeneous datasets due to domain shifts caused by variations in data\ncollection and population differences. This challenge is particularly\npronounced in biological data, where data is high-dimensional, small-scale, and\ndecentralized across institutions. While federated domain adaptation methods\n(FDA) aim to address these challenges, most existing approaches rely on deep\nlearning and focus on classification tasks, making them unsuitable for\nsmall-scale, high-dimensional applications. In this work, we propose freda, a\nprivacy-preserving federated method for unsupervised domain adaptation in\nregression tasks. Unlike deep learning-based FDA approaches, freda is the first\nmethod to enable the federated training of Gaussian Processes to model complex\nfeature relationships while ensuring complete data privacy through randomized\nencoding and secure aggregation. This allows for effective domain adaptation\nwithout direct access to raw data, making it well-suited for applications\ninvolving high-dimensional, heterogeneous datasets. We evaluate freda on the\nchallenging task of age prediction from DNA methylation data, demonstrating\nthat it achieves performance comparable to the centralized state-of-the-art\nmethod while preserving complete data privacy.\n","authors":["Cem Ata Baykara","Ali Burak Ünal","Nico Pfeifer","Mete Akgün"],"pdf_url":"https://arxiv.org/pdf/2411.17287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09352v1","updated":"2025-02-13T14:18:41Z","published":"2025-02-13T14:18:41Z","title":"Wasserstein distributional adversarial training for deep neural networks","summary":"  Design of adversarial attacks for deep neural networks, as well as methods of\nadversarial training against them, are subject of intense research. In this\npaper, we propose methods to train against distributional attack threats,\nextending the TRADES method used for pointwise attacks. Our approach leverages\nrecent contributions and relies on sensitivity analysis for Wasserstein\ndistributionally robust optimization problems. We introduce an efficient\nfine-tuning method which can be deployed on a previously trained model. We test\nour methods on a range of pre-trained models on RobustBench. These experimental\nresults demonstrate the additional training enhances Wasserstein distributional\nrobustness, while maintaining original levels of pointwise robustness, even for\nalready very successful networks. The improvements are less marked for models\npre-trained using huge synthetic datasets of 20-100M images. However,\nremarkably, sometimes our methods are still able to improve their performance\neven when trained using only the original training dataset (50k images).\n","authors":["Xingjian Bai","Guangyi He","Yifan Jiang","Jan Obloj"],"pdf_url":"https://arxiv.org/pdf/2502.09352v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.05366v2","updated":"2025-02-13T14:16:56Z","published":"2024-06-08T06:06:20Z","title":"Regret Bounds for Episodic Risk-Sensitive Linear Quadratic Regulator","summary":"  Risk-sensitive linear quadratic regulator is one of the most fundamental\nproblems in risk-sensitive optimal control. In this paper, we study online\nadaptive control of risk-sensitive linear quadratic regulator in the finite\nhorizon episodic setting. We propose a simple least-squares greedy algorithm\nand show that it achieves $\\widetilde{\\mathcal{O}}(\\log N)$ regret under a\nspecific identifiability assumption, where $N$ is the total number of episodes.\nIf the identifiability assumption is not satisfied, we propose incorporating\nexploration noise into the least-squares-based algorithm, resulting in an\nalgorithm with $\\widetilde{\\mathcal{O}}(\\sqrt{N})$ regret. To our best\nknowledge, this is the first set of regret bounds for episodic risk-sensitive\nlinear quadratic regulator. Our proof relies on perturbation analysis of\nless-standard Riccati equations for risk-sensitive linear quadratic control,\nand a delicate analysis of the loss in the risk-sensitive performance criterion\ndue to applying the suboptimal controller in the online learning process.\n","authors":["Wenhao Xu","Xuefeng Gao","Xuedong He"],"pdf_url":"https://arxiv.org/pdf/2406.05366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09346v1","updated":"2025-02-13T14:11:33Z","published":"2025-02-13T14:11:33Z","title":"Machine learning for modelling unstructured grid data in computational\n  physics: a review","summary":"  Unstructured grid data are essential for modelling complex geometries and\ndynamics in computational physics. Yet, their inherent irregularity presents\nsignificant challenges for conventional machine learning (ML) techniques. This\npaper provides a comprehensive review of advanced ML methodologies designed to\nhandle unstructured grid data in high-dimensional dynamical systems. Key\napproaches discussed include graph neural networks, transformer models with\nspatial attention mechanisms, interpolation-integrated ML methods, and meshless\ntechniques such as physics-informed neural networks. These methodologies have\nproven effective across diverse fields, including fluid dynamics and\nenvironmental simulations. This review is intended as a guidebook for\ncomputational scientists seeking to apply ML approaches to unstructured grid\ndata in their domains, as well as for ML researchers looking to address\nchallenges in computational physics. It places special focus on how ML methods\ncan overcome the inherent limitations of traditional numerical techniques and,\nconversely, how insights from computational physics can inform ML development.\nTo support benchmarking, this review also provides a summary of open-access\ndatasets of unstructured grid data in computational physics. Finally, emerging\ndirections such as generative models with unstructured data, reinforcement\nlearning for mesh generation, and hybrid physics-data-driven paradigms are\ndiscussed to inspire future advancements in this evolving field.\n","authors":["Sibo Cheng","Marc Bocquet","Weiping Ding","Tobias Sebastian Finn","Rui Fu","Jinlong Fu","Yike Guo","Eleda Johnson","Siyi Li","Che Liu","Eric Newton Moro","Jie Pan","Matthew Piggott","Cesar Quilodran","Prakhar Sharma","Kun Wang","Dunhui Xiao","Xiao Xue","Yong Zeng","Mingrui Zhang","Hao Zhou","Kewei Zhu","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2502.09346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04708v2","updated":"2025-02-13T14:06:51Z","published":"2024-11-07T13:45:26Z","title":"Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs","summary":"  Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the *multi-level nature* of\nthe graph modality, even though different chemistry tasks may benefit from\ndifferent feature levels. In this work, we first study the effect of feature\ngranularity and reveal that even reducing all GNN-generated feature tokens to a\nsingle one does not significantly impact model performance. We then investigate\nthe effect of various graph feature levels and demonstrate that both the\nquality of LLM-generated molecules and model performance across different tasks\ndepend on different graph feature levels. Therefore, we conclude with two key\ninsights: (1) current molecular-related multimodal LLMs lack a comprehensive\nunderstanding of graph features, and (2) static processing is not sufficient\nfor hierarchical graph feature. We share our findings in detail, with the hope\nof paving the way for the community to develop more advanced multimodal LLMs\nfor incorporating molecular graphs.\n","authors":["Chengxin Hu","Hao Li","Yihe Yuan","Jing Li","Ivor Tsang"],"pdf_url":"https://arxiv.org/pdf/2411.04708v2.pdf","comment":"9 pages, 4 tables, 1 figure, paper under review"},{"id":"http://arxiv.org/abs/2502.09341v1","updated":"2025-02-13T14:01:15Z","published":"2025-02-13T14:01:15Z","title":"Neural Spatiotemporal Point Processes: Trends and Challenges","summary":"  Spatiotemporal point processes (STPPs) are probabilistic models for events\noccurring in continuous space and time. Real-world event data often exhibit\nintricate dependencies and heterogeneous dynamics. By incorporating modern deep\nlearning techniques, STPPs can model these complexities more effectively than\ntraditional approaches. Consequently, the fusion of neural methods with STPPs\nhas become an active and rapidly evolving research area. In this review, we\ncategorize existing approaches, unify key design choices, and explain the\nchallenges of working with this data modality. We further highlight emerging\ntrends and diverse application domains. Finally, we identify open challenges\nand gaps in the literature.\n","authors":["Sumantrak Mukherjee","Mouad Elhamdi","George Mohler","David A. Selby","Yao Xie","Sebastian Vollmer","Gerrit Grossmann"],"pdf_url":"https://arxiv.org/pdf/2502.09341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09340v1","updated":"2025-02-13T14:00:55Z","published":"2025-02-13T14:00:55Z","title":"This looks like what? Challenges and Future Research Directions for\n  Part-Prototype Models","summary":"  The growing interest in eXplainable Artificial Intelligence (XAI) has\nprompted research into models with built-in interpretability, the most\nprominent of which are part-prototype models. Part-Prototype Models (PPMs) make\ndecisions by comparing an input image to a set of learned prototypes, providing\nhuman-understandable explanations in the form of ``this looks like that''.\nDespite their inherent interpretability, PPMS are not yet considered a valuable\nalternative to post-hoc models. In this survey, we investigate the reasons for\nthis and provide directions for future research. We analyze papers from 2019 to\n2024, and derive a taxonomy of the challenges that current PPMS face. Our\nanalysis shows that the open challenges are quite diverse. The main concern is\nthe quality and quantity of prototypes. Other concerns are the lack of\ngeneralization to a variety of tasks and contexts, and general methodological\nissues, including non-standardized evaluation. We provide ideas for future\nresearch in five broad directions: improving predictive performance, developing\nnovel architectures grounded in theory, establishing frameworks for human-AI\ncollaboration, aligning models with humans, and establishing metrics and\nbenchmarks for evaluation. We hope that this survey will stimulate research and\npromote intrinsically interpretable models for application domains. Our list of\nsurveyed papers is available at https://github.com/aix-group/ppm-survey.\n","authors":["Khawla Elhadri","Tomasz Michalski","Adam Wróbel","Jörg Schlötterer","Bartosz Zieliński","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2502.09340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07532v2","updated":"2025-02-13T13:55:08Z","published":"2025-02-11T13:15:16Z","title":"Diffusion-LAM: Probabilistic Limited Area Weather Forecasting with\n  Diffusion","summary":"  Machine learning methods have been shown to be effective for weather\nforecasting, based on the speed and accuracy compared to traditional numerical\nmodels. While early efforts primarily concentrated on deterministic\npredictions, the field has increasingly shifted toward probabilistic\nforecasting to better capture the forecast uncertainty. Most machine\nlearning-based models have been designed for global-scale predictions, with\nonly limited work targeting regional or limited area forecasting, which allows\nmore specialized and flexible modeling for specific locations. This work\nintroduces Diffusion-LAM, a probabilistic limited area weather model leveraging\nconditional diffusion. By conditioning on boundary data from surrounding\nregions, our approach generates forecasts within a defined area. Experimental\nresults on the MEPS limited area dataset demonstrate the potential of\nDiffusion-LAM to deliver accurate probabilistic forecasts, highlighting its\npromise for limited-area weather prediction.\n","authors":["Erik Larsson","Joel Oskarsson","Tomas Landelius","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2502.07532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09335v1","updated":"2025-02-13T13:54:58Z","published":"2025-02-13T13:54:58Z","title":"Graph Diffusion Network for Drug-Gene Prediction","summary":"  Predicting drug-gene associations is crucial for drug development and disease\ntreatment. While graph neural networks (GNN) have shown effectiveness in this\ntask, they face challenges with data sparsity and efficient contrastive\nlearning implementation. We introduce a graph diffusion network for drug-gene\nprediction (GDNDGP), a framework that addresses these limitations through two\nkey innovations. First, it employs meta-path-based homogeneous graph learning\nto capture drug-drug and gene-gene relationships, ensuring similar entities\nshare embedding spaces. Second, it incorporates a parallel diffusion network\nthat generates hard negative samples during training, eliminating the need for\nexhaustive negative sample retrieval. Our model achieves superior performance\non the DGIdb 4.0 dataset and demonstrates strong generalization capability on\ntripartite drug-gene-disease networks. Results show significant improvements\nover existing methods in drug-gene prediction tasks, particularly in handling\ncomplex heterogeneous relationships. The source code is publicly available at\nhttps://github.com/csjywu1/GDNDGP.\n","authors":["Jiayang Wu","Wensheng Gan","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.09335v1.pdf","comment":"IEEE/ACM TCBB. 14 pages"},{"id":"http://arxiv.org/abs/2412.05000v2","updated":"2025-02-13T13:53:58Z","published":"2024-12-06T12:52:24Z","title":"Noise Matters: Diffusion Model-based Urban Mobility Generation with\n  Collaborative Noise Priors","summary":"  With global urbanization, the focus on sustainable cities has largely grown,\ndriving research into equity, resilience, and urban planning, which often\nrelies on mobility data. The rise of web-based apps and mobile devices has\nprovided valuable user data for mobility-related research. However, real-world\nmobility data is costly and raises privacy concerns. To protect privacy while\nretaining key features of real-world movement, the demand for synthetic data\nhas steadily increased. Recent advances in diffusion models have shown great\npotential for mobility trajectory generation due to their ability to model\nrandomness and uncertainty. However, existing approaches often directly apply\nidentically distributed (i.i.d.) noise sampling from image generation\ntechniques, which fail to account for the spatiotemporal correlations and\nsocial interactions that shape urban mobility patterns. In this paper, we\npropose CoDiffMob, a diffusion model for urban mobility generation with\ncollaborative noise priors, we emphasize the critical role of noise in\ndiffusion models for generating mobility data. By leveraging both individual\nmovement characteristics and population-wide dynamics, we construct novel\ncollaborative noise priors that provide richer and more informative guidance\nthroughout the generation process. Extensive experiments demonstrate the\nsuperiority of our method, with generated data accurately capturing both\nindividual preferences and collective patterns, achieving an improvement of\nover 32%. Furthermore, it can effectively replace web-derived mobility data to\nbetter support downstream applications, while safeguarding user privacy and\nfostering a more secure and ethical web. This highlights its tremendous\npotential for applications in sustainable city-related research. The code and\ndata are available at https://github.com/tsinghua-fib-lab/CoDiffMob.\n","authors":["Yuheng Zhang","Yuan Yuan","Jingtao Ding","Jian Yuan","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2412.05000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09332v1","updated":"2025-02-13T13:49:52Z","published":"2025-02-13T13:49:52Z","title":"Full Swap Regret and Discretized Calibration","summary":"  We study the problem of minimizing swap regret in structured normal-form\ngames. Players have a very large (potentially infinite) number of pure actions,\nbut each action has an embedding into $d$-dimensional space and payoffs are\ngiven by bilinear functions of these embeddings. We provide an efficient\nlearning algorithm for this setting that incurs at most\n$\\tilde{O}(T^{(d+1)/(d+3)})$ swap regret after $T$ rounds.\n  To achieve this, we introduce a new online learning problem we call\n\\emph{full swap regret minimization}. In this problem, a learner repeatedly\ntakes a (randomized) action in a bounded convex $d$-dimensional action set\n$\\mathcal{K}$ and then receives a loss from the adversary, with the goal of\nminimizing their regret with respect to the \\emph{worst-case} swap function\nmapping $\\mathcal{K}$ to $\\mathcal{K}$. For varied assumptions about the\nconvexity and smoothness of the loss functions, we design algorithms with full\nswap regret bounds ranging from $O(T^{d/(d+2)})$ to $O(T^{(d+1)/(d+2)})$.\n  Finally, we apply these tools to the problem of online forecasting to\nminimize calibration error, showing that several notions of calibration can be\nviewed as specific instances of full swap regret. In particular, we design\nefficient algorithms for online forecasting that guarantee at most $O(T^{1/3})$\n$\\ell_2$-calibration error and $O(\\max(\\sqrt{\\epsilon T}, T^{1/3}))$\n\\emph{discretized-calibration} error (when the forecaster is restricted to\npredicting multiples of $\\epsilon$).\n","authors":["Maxwell Fishelson","Robert Kleinberg","Princewill Okoroafor","Renato Paes Leme","Jon Schneider","Yifeng Teng"],"pdf_url":"https://arxiv.org/pdf/2502.09332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09329v1","updated":"2025-02-13T13:43:52Z","published":"2025-02-13T13:43:52Z","title":"Bayesian Optimization for Simultaneous Selection of Machine Learning\n  Algorithms and Hyperparameters on Shared Latent Space","summary":"  Selecting the optimal combination of a machine learning (ML) algorithm and\nits hyper-parameters is crucial for the development of high-performance ML\nsystems. However, since the combination of ML algorithms and hyper-parameters\nis enormous, the exhaustive validation requires a significant amount of time.\nMany existing studies use Bayesian optimization (BO) for accelerating the\nsearch. On the other hand, a significant difficulty is that, in general, there\nexists a different hyper-parameter space for each one of candidate ML\nalgorithms. BO-based approaches typically build a surrogate model independently\nfor each hyper-parameter space, by which sufficient observations are required\nfor all candidate ML algorithms. In this study, our proposed method embeds\ndifferent hyper-parameter spaces into a shared latent space, in which a\nsurrogate multi-task model for BO is estimated. This approach can share\ninformation of observations from different ML algorithms by which efficient\noptimization is expected with a smaller number of total observations. We\nfurther propose the pre-training of the latent space embedding with an\nadversarial regularization, and a ranking model for selecting an effective\npre-trained embedding for a given target dataset. Our empirical study\ndemonstrates effectiveness of the proposed method through datasets from OpenML.\n","authors":["Kazuki Ishikawa","Ryota Ozaki","Yohei Kanzaki","Ichiro Takeuchi","Masayuki Karasuyama"],"pdf_url":"https://arxiv.org/pdf/2502.09329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16839v3","updated":"2025-02-13T13:39:26Z","published":"2025-01-28T10:28:17Z","title":"Flow Matching: Markov Kernels, Stochastic Processes and Transport Plans","summary":"  Among generative neural models, flow matching techniques stand out for their\nsimple applicability and good scaling properties. Here, velocity fields of\ncurves connecting a simple latent and a target distribution are learned. Then\nthe corresponding ordinary differential equation can be used to sample from a\ntarget distribution, starting in samples from the latent one. This paper\nreviews from a mathematical point of view different techniques to learn the\nvelocity fields of absolutely continuous curves in the Wasserstein geometry. We\nshow how the velocity fields can be characterized and learned via i) transport\nplans (couplings) between latent and target distributions, ii) Markov kernels\nand iii) stochastic processes, where the latter two include the coupling\napproach, but are in general broader. Besides this main goal, we show how flow\nmatching can be used for solving Bayesian inverse problems, where the\ndefinition of conditional Wasserstein distances plays a central role. Finally,\nwe briefly address continuous normalizing flows and score matching techniques,\nwhich approach the learning of velocity fields of curves from other directions.\n","authors":["Christian Wald","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2501.16839v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09324v1","updated":"2025-02-13T13:37:52Z","published":"2025-02-13T13:37:52Z","title":"Depth-Bounds for Neural Networks via the Braid Arrangement","summary":"  We contribute towards resolving the open question of how many hidden layers\nare required in ReLU networks for exactly representing all continuous and\npiecewise linear functions on $\\mathbb{R}^d$. While the question has been\nresolved in special cases, the best known lower bound in general is still 2. We\nfocus on neural networks that are compatible with certain polyhedral complexes,\nmore precisely with the braid fan. For such neural networks, we prove a\nnon-constant lower bound of $\\Omega(\\log\\log d)$ hidden layers required to\nexactly represent the maximum of $d$ numbers. Additionally, under our\nassumption, we provide a combinatorial proof that 3 hidden layers are necessary\nto compute the maximum of 5 numbers; this had only been verified with an\nexcessive computation so far. Finally, we show that a natural generalization of\nthe best known upper bound to maxout networks is not tight, by demonstrating\nthat a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to\nrepresent the maximum of 7 numbers.\n","authors":["Moritz Grillo","Christoph Hertrich","Georg Loho"],"pdf_url":"https://arxiv.org/pdf/2502.09324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09319v1","updated":"2025-02-13T13:33:45Z","published":"2025-02-13T13:33:45Z","title":"Bridging Jensen Gap for Max-Min Group Fairness Optimization in\n  Recommendation","summary":"  Group max-min fairness (MMF) is commonly used in fairness-aware recommender\nsystems (RS) as an optimization objective, as it aims to protect marginalized\nitem groups and ensures a fair competition platform. However, our theoretical\nanalysis indicates that integrating MMF constraint violates the assumption of\nsample independence during optimization, causing the loss function to deviate\nfrom linear additivity. Such nonlinearity property introduces the Jensen gap\nbetween the model's convergence point and the optimal point if mini-batch\nsampling is applied. Both theoretical and empirical studies show that as the\nmini-batch size decreases and the group size increases, the Jensen gap will\nwiden accordingly. Some methods using heuristic re-weighting or debiasing\nstrategies have the potential to bridge the Jensen gap. However, they either\nlack theoretical guarantees or suffer from heavy computational costs. To\novercome these limitations, we first theoretically demonstrate that the\nMMF-constrained objective can be essentially reformulated as a group-weighted\noptimization objective. Then we present an efficient and effective algorithm\nnamed FairDual, which utilizes a dual optimization technique to minimize the\nJensen gap. Our theoretical analysis demonstrates that FairDual can achieve a\nsub-linear convergence rate to the globally optimal solution and the Jensen gap\ncan be well bounded under a mini-batch sampling strategy with random shuffle.\nExtensive experiments conducted using six large-scale RS backbone models on\nthree publicly available datasets demonstrate that FairDual outperforms all\nbaselines in terms of both accuracy and fairness. Our data and codes are shared\nat https://github.com/XuChen0427/FairDual.\n","authors":["Chen Xu","Yuxin Li","Wenjie Wang","Liang Pang","Jun Xu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.09319v1.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09318v1","updated":"2025-02-13T13:33:35Z","published":"2025-02-13T13:33:35Z","title":"SigGate: Enhancing Recurrent Neural Networks with Signature-Based Gating\n  Mechanisms","summary":"  In this paper, we propose a novel approach that enhances recurrent neural\nnetworks (RNNs) by incorporating path signatures into their gating mechanisms.\nOur method modifies both Long Short-Term Memory (LSTM) and Gated Recurrent Unit\n(GRU) architectures by replacing their forget and reset gates, respectively,\nwith learnable path signatures. These signatures, which capture the geometric\nfeatures of the entire path history, provide a richer context for controlling\ninformation flow through the network's memory. This modification allows the\nnetworks to make memory decisions based on the full historical context rather\nthan just the current input and state. Through experimental studies, we\ndemonstrate that our Signature-LSTM (SigLSTM) and Signature-GRU (SigGRU) models\noutperform their traditional counterparts across various sequential learning\ntasks. By leveraging path signatures in recurrent architectures, this method\noffers new opportunities to enhance performance in time series analysis and\nforecasting applications.\n","authors":["Rémi Genet","Hugo Inzirillo"],"pdf_url":"https://arxiv.org/pdf/2502.09318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04750v2","updated":"2025-02-13T13:33:19Z","published":"2025-02-07T08:33:28Z","title":"Tighter sparse variational Gaussian processes","summary":"  Sparse variational Gaussian process (GP) approximations based on inducing\npoints have become the de facto standard for scaling GPs to large datasets,\nowing to their theoretical elegance, computational efficiency, and ease of\nimplementation. This paper introduces a provably tighter variational\napproximation by relaxing the standard assumption that the conditional\napproximate posterior given the inducing points must match that in the prior.\nThe key innovation is to modify the conditional posterior to have smaller\nvariances than that of the prior at the training points. We derive the\ncollapsed bound for the regression case, describe how to use the proposed\napproximation in large data settings, and discuss its application to handle\northogonally structured inducing points and GP latent variable models.\nExtensive experiments on regression benchmarks, classification, and latent\nvariable models demonstrate that the proposed approximation consistently\nmatches or outperforms standard sparse variational GPs while maintaining the\nsame computational cost. An implementation will be made available in all\npopular GP packages.\n","authors":["Thang D. Bui","Matthew Ashman","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2502.04750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03345v3","updated":"2025-02-13T13:25:18Z","published":"2024-06-05T15:04:27Z","title":"Feature contamination: Neural networks learn uncorrelated features and\n  fail to generalize","summary":"  Learning representations that generalize under distribution shifts is\ncritical for building robust machine learning models. However, despite\nsignificant efforts in recent years, algorithmic advances in this direction\nhave been limited. In this work, we seek to understand the fundamental\ndifficulty of out-of-distribution generalization with deep neural networks. We\nfirst empirically show that perhaps surprisingly, even allowing a neural\nnetwork to explicitly fit the representations obtained from a teacher network\nthat can generalize out-of-distribution is insufficient for the generalization\nof the student network. Then, by a theoretical study of two-layer ReLU networks\noptimized by stochastic gradient descent (SGD) under a structured feature\nmodel, we identify a fundamental yet unexplored feature learning proclivity of\nneural networks, feature contamination: neural networks can learn uncorrelated\nfeatures together with predictive features, resulting in generalization failure\nunder distribution shifts. Notably, this mechanism essentially differs from the\nprevailing narrative in the literature that attributes the generalization\nfailure to spurious correlations. Overall, our results offer new insights into\nthe non-linear feature learning dynamics of neural networks and highlight the\nnecessity of considering inductive biases in out-of-distribution\ngeneralization.\n","authors":["Tianren Zhang","Chujie Zhao","Guanyu Chen","Yizhou Jiang","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.03345v3.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2502.09306v1","updated":"2025-02-13T13:18:30Z","published":"2025-02-13T13:18:30Z","title":"Non-asymptotic Analysis of Diffusion Annealed Langevin Monte Carlo for\n  Generative Modelling","summary":"  We investigate the theoretical properties of general diffusion\n(interpolation) paths and their Langevin Monte Carlo implementation, referred\nto as diffusion annealed Langevin Monte Carlo (DALMC), under weak conditions on\nthe data distribution. Specifically, we analyse and provide non-asymptotic\nerror bounds for the annealed Langevin dynamics where the path of distributions\nis defined as Gaussian convolutions of the data distribution as in diffusion\nmodels. We then extend our results to recently proposed heavy-tailed (Student's\nt) diffusion paths, demonstrating their theoretical properties for heavy-tailed\ndata distributions for the first time. Our analysis provides theoretical\nguarantees for a class of score-based generative models that interpolate\nbetween a simple distribution (Gaussian or Student's t) and the data\ndistribution in finite time. This approach offers a broader perspective\ncompared to standard score-based diffusion approaches, which are typically\nbased on a forward Ornstein-Uhlenbeck (OU) noising process.\n","authors":["Paula Cordero-Encinar","O. Deniz Akyildiz","Andrew B. Duncan"],"pdf_url":"https://arxiv.org/pdf/2502.09306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09303v1","updated":"2025-02-13T13:16:10Z","published":"2025-02-13T13:16:10Z","title":"Towards Seamless Hierarchical Federated Learning under Intermittent\n  Client Participation: A Stagewise Decision-Making Methodology","summary":"  Federated Learning (FL) offers a pioneering distributed learning paradigm\nthat enables devices/clients to build a shared global model. This global model\nis obtained through frequent model transmissions between clients and a central\nserver, which may cause high latency, energy consumption, and congestion over\nbackhaul links. To overcome these drawbacks, Hierarchical Federated Learning\n(HFL) has emerged, which organizes clients into multiple clusters and utilizes\nedge nodes (e.g., edge servers) for intermediate model aggregations between\nclients and the central server. Current research on HFL mainly focus on\nenhancing model accuracy, latency, and energy consumption in scenarios with a\nstable/fixed set of clients. However, addressing the dynamic availability of\nclients -- a critical aspect of real-world scenarios -- remains underexplored.\nThis study delves into optimizing client selection and client-to-edge\nassociations in HFL under intermittent client participation so as to minimize\noverall system costs (i.e., delay and energy), while achieving fast model\nconvergence. We unveil that achieving this goal involves solving a complex\nNP-hard problem. To tackle this, we propose a stagewise methodology that splits\nthe solution into two stages, referred to as Plan A and Plan B. Plan A focuses\non identifying long-term clients with high chance of participation in\nsubsequent model training rounds. Plan B serves as a backup, selecting\nalternative clients when long-term clients are unavailable during model\ntraining rounds. This stagewise methodology offers a fresh perspective on\nclient selection that can enhance both HFL and conventional FL via enabling\nlow-overhead decision-making processes. Through evaluations on MNIST and\nCIFAR-10 datasets, we show that our methodology outperforms existing benchmarks\nin terms of model accuracy and system costs.\n","authors":["Minghong Wu","Minghui Liwang","Yuhan Su","Li Li","Seyyedali Hosseinalipour","Xianbin Wang","Huaiyu Dai","Zhenzhen Jiao"],"pdf_url":"https://arxiv.org/pdf/2502.09303v1.pdf","comment":"20 pages, 8 figures,5 tables"},{"id":"http://arxiv.org/abs/2108.12113v3","updated":"2025-02-13T13:12:41Z","published":"2021-08-27T04:18:45Z","title":"A method of supervised learning from conflicting data with hidden\n  contexts","summary":"  Conventional supervised learning assumes a stable input-output relationship.\nHowever, this assumption fails in open-ended training settings where the\ninput-output relationship depends on hidden contexts. In this work, we\nformulate a more general supervised learning problem in which training data is\ndrawn from multiple unobservable domains, each potentially exhibiting distinct\ninput-output maps. This inherent conflict in data renders standard empirical\nrisk minimization training ineffective. To address this challenge, we propose a\nmethod LEAF that introduces an allocation function, which learns to assign\nconflicting data to different predictive models. We establish a connection\nbetween LEAF and a variant of the Expectation-Maximization algorithm, allowing\nus to derive an analytical expression for the allocation function. Finally, we\nprovide a theoretical analysis of LEAF and empirically validate its\neffectiveness on both synthetic and real-world tasks involving conflicting\ndata.\n","authors":["Tianren Zhang","Yizhou Jiang","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2108.12113v3.pdf","comment":"35 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09298v1","updated":"2025-02-13T13:12:16Z","published":"2025-02-13T13:12:16Z","title":"Convex Is Back: Solving Belief MDPs With Convexity-Informed Deep\n  Reinforcement Learning","summary":"  We present a novel method for Deep Reinforcement Learning (DRL),\nincorporating the convex property of the value function over the belief space\nin Partially Observable Markov Decision Processes (POMDPs). We introduce hard-\nand soft-enforced convexity as two different approaches, and compare their\nperformance against standard DRL on two well-known POMDP environments, namely\nthe Tiger and FieldVisionRockSample problems. Our findings show that including\nthe convexity feature can substantially increase performance of the agents, as\nwell as increase robustness over the hyperparameter space, especially when\ntesting on out-of-distribution domains. The source code for this work can be\nfound at https://github.com/Dakout/Convex_DRL.\n","authors":["Daniel Koutas","Daniel Hettegger","Kostas G. Papakonstantinou","Daniel Straub"],"pdf_url":"https://arxiv.org/pdf/2502.09298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09297v1","updated":"2025-02-13T13:11:54Z","published":"2025-02-13T13:11:54Z","title":"When do neural networks learn world models?","summary":"  Humans develop world models that capture the underlying generation process of\ndata. Whether neural networks can learn similar world models remains an open\nproblem. In this work, we provide the first theoretical results for this\nproblem, showing that in a multi-task setting, models with a low-degree bias\nprovably recover latent data-generating variables under mild assumptions --\neven if proxy tasks involve complex, non-linear functions of the latents.\nHowever, such recovery is also sensitive to model architecture. Our analysis\nleverages Boolean models of task solutions via the Fourier-Walsh transform and\nintroduces new techniques for analyzing invertible Boolean transforms, which\nmay be of independent interest. We illustrate the algorithmic implications of\nour results and connect them to related research areas, including\nself-supervised learning, out-of-distribution generalization, and the linear\nrepresentation hypothesis in large language models.\n","authors":["Tianren Zhang","Guanyu Chen","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.09297v1.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.16106v2","updated":"2025-02-13T13:11:46Z","published":"2024-10-21T15:34:44Z","title":"Statistical Inference for Temporal Difference Learning with Linear\n  Function Approximation","summary":"  Statistical inference with finite-sample validity for the value function of a\ngiven policy in Markov decision processes (MDPs) is crucial for ensuring the\nreliability of reinforcement learning. Temporal Difference (TD) learning,\narguably the most widely used algorithm for policy evaluation, serves as a\nnatural framework for this purpose. In this paper, we study the consistency\nproperties of TD learning with Polyak-Ruppert averaging and linear function\napproximation, and obtain three significant improvements over existing results.\nFirst, we derive a novel sharp high-dimensional probability convergence\nguarantee that depends explicitly on the asymptotic variance and holds under\nweak conditions. We further establish refined high-dimensional Berry-Esseen\nbounds over the class of convex sets that guarantee faster rates than those in\nthe literature. Finally, we propose a plug-in estimator for the asymptotic\ncovariance matrix, designed for efficient online computation. These results\nenable the construction of confidence regions and simultaneous confidence\nintervals for the linear parameters of the value function, with guaranteed\nfinite-sample coverage. We demonstrate the applicability of our theoretical\nfindings through numerical experiments.\n","authors":["Weichen Wu","Gen Li","Yuting Wei","Alessandro Rinaldo"],"pdf_url":"https://arxiv.org/pdf/2410.16106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09291v1","updated":"2025-02-13T13:08:11Z","published":"2025-02-13T13:08:11Z","title":"Joint Attention Mechanism Learning to Facilitate Opto-physiological\n  Monitoring during Physical Activity","summary":"  Opto-physiological monitoring is a non-contact technique for measuring\ncardiac signals, i.e., photoplethysmography (PPG). Quality PPG signals directly\nlead to reliable physiological readings. However, PPG signal acquisition\nprocedures are often accompanied by spurious motion artefacts (MAs), especially\nduring low-to-high-intensity physical activity. This study proposes a practical\nadversarial learning approach for opto-physiological monitoring by using a\ngenerative adversarial network with an attention mechanism (AM-GAN) to model\nmotion noise and to allow MA removal. The AM-GAN learns an MA-resistant mapping\nfrom raw and noisy signals to clear PPG signals in an adversarial manner,\nguided by an attention mechanism to directly translate the motion reference of\ntriaxial acceleration to the MAs appearing in the raw signal. The AM-GAN was\nexperimented with three various protocols engaged with 39 subjects in various\nphysical activities. The average absolute error for heart rate (HR) derived\nfrom the MA-free PPG signal via the AM-GAN, is 1.81 beats/min for the IEEE-SPC\ndataset and 3.86 beats/min for the PPGDalia dataset. The same procedure applied\nto an in-house LU dataset resulted in average absolute errors for HR and\nrespiratory rate (RR) of less than 1.37 beats/min and 2.49 breaths/min,\nrespectively. The study demonstrates the robustness and resilience of AM-GAN,\nparticularly during low-to-high-intensity physical activities.\n","authors":["Xiaoyu Zheng","Sijung Hu","Vincent Dwyer","Mahsa Derakhshani","Laura Barrett"],"pdf_url":"https://arxiv.org/pdf/2502.09291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09290v1","updated":"2025-02-13T13:06:56Z","published":"2025-02-13T13:06:56Z","title":"Dynamic Rolling Horizon Optimization for Network-Constrained V2X Value\n  Stacking of Electric Vehicles Under Uncertainties","summary":"  Electric vehicle (EV) coordination can provide significant benefits through\nvehicle-to-everything (V2X) by interacting with the grid, buildings, and other\nEVs. This work aims to develop a V2X value-stacking framework, including\nvehicle-to-building (V2B), vehicle-to-grid (V2G), and energy trading, to\nmaximize economic benefits for residential communities while maintaining\ndistribution voltage. This work also seeks to quantify the impact of prediction\nerrors related to building load, renewable energy, and EV arrivals. A dynamic\nrolling-horizon optimization (RHO) method is employed to leverage multiple\nrevenue streams and maximize the potential of EV coordination. To address\nenergy uncertainties, including hourly local building load, local photovoltaic\n(PV) generation, and EV arrivals, this work develops a Transformer-based\nforecasting model named Gated Recurrent Units-Encoder-Temporal Fusion Decoder\n(GRU-EN-TFD). The simulation results, using real data from Australia's National\nElectricity Market, and the Independent System Operators in New England and New\nYork in the US, reveal that V2X value stacking can significantly reduce energy\ncosts. The proposed GRU-EN-TFD model outperforms the benchmark forecast model.\nUncertainties in EV arrivals have a more substantial impact on value-stacking\nperformance, highlighting the significance of its accurate forecast. This work\nprovides new insights into the dynamic interactions among residential\ncommunities, unlocking the full potential of EV batteries.\n","authors":["Canchen Jiang","Ariel Liebman","Bo Jie","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09290v1.pdf","comment":"21 pages, accepted by Renewable Energy"},{"id":"http://arxiv.org/abs/2502.09287v1","updated":"2025-02-13T13:01:46Z","published":"2025-02-13T13:01:46Z","title":"An Uncertainty Principle for Linear Recurrent Neural Networks","summary":"  We consider linear recurrent neural networks, which have become a key\nbuilding block of sequence modeling due to their ability for stable and\neffective long-range modeling. In this paper, we aim at characterizing this\nability on a simple but core copy task, whose goal is to build a linear filter\nof order $S$ that approximates the filter that looks $K$ time steps in the past\n(which we refer to as the shift-$K$ filter), where $K$ is larger than $S$.\nUsing classical signal models and quadratic cost, we fully characterize the\nproblem by providing lower bounds of approximation, as well as explicit filters\nthat achieve this lower bound up to constants. The optimal performance\nhighlights an uncertainty principle: the optimal filter has to average values\naround the $K$-th time step in the past with a range~(width) that is\nproportional to $K/S$.\n","authors":["Alexandre François","Antonio Orvieto","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2502.09287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07115v2","updated":"2025-02-13T12:54:36Z","published":"2025-02-10T23:11:44Z","title":"Online Scheduling for LLM Inference with KV Cache Constraints","summary":"  Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.\n","authors":["Patrick Jaillet","Jiashuo Jiang","Chara Podimata","Zijie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09282v1","updated":"2025-02-13T12:54:13Z","published":"2025-02-13T12:54:13Z","title":"FE-LWS: Refined Image-Text Representations via Decoder Stacking and\n  Fused Encodings for Remote Sensing Image Captioning","summary":"  Remote sensing image captioning aims to generate descriptive text from remote\nsensing images, typically employing an encoder-decoder framework. In this\nsetup, a convolutional neural network (CNN) extracts feature representations\nfrom the input image, which then guide the decoder in a sequence-to-sequence\ncaption generation process. Although much research has focused on refining the\ndecoder, the quality of image representations from the encoder remains crucial\nfor accurate captioning. This paper introduces a novel approach that integrates\nfeatures from two distinct CNN based encoders, capturing complementary\ninformation to enhance caption generation. Additionally, we propose a weighted\naveraging technique to combine the outputs of all GRUs in the stacked decoder.\nFurthermore, a comparison-based beam search strategy is incorporated to refine\ncaption selection. The results demonstrate that our fusion-based approach,\nalong with the enhanced stacked decoder, significantly outperforms both the\ntransformer-based state-of-the-art model and other LSTM-based baselines.\n","authors":["Swadhin Das","Raksha Sharma"],"pdf_url":"https://arxiv.org/pdf/2502.09282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09795v4","updated":"2025-02-13T12:35:53Z","published":"2024-10-13T10:48:22Z","title":"WGFormer: An SE(3)-Transformer Driven by Wasserstein Gradient Flows for\n  Molecular Ground-State Conformation Prediction","summary":"  Predicting molecular ground-state conformation (i.e., energy-minimized\nconformation) is crucial for many chemical applications such as molecular\ndocking and property prediction. Classic energy-based simulation is\ntime-consuming when solving this problem while existing learning-based methods\nhave advantages in computational efficiency but sacrifice accuracy and\ninterpretability. In this work, we propose a novel and effective method to\nbridge the energy-based simulation and the learning-based strategy, which\ndesigns and learns a Wasserstein gradient flow-driven SE(3)-Transformer, called\nWGFormer, for molecular ground-state conformation prediction. Specifically, our\nmethod tackles this task within an auto-encoding framework, which encodes\nlow-quality conformations by the proposed WGFormer and decodes corresponding\nground-state conformations by an MLP. The architecture of WGFormer corresponds\nto Wasserstein gradient flows -- it optimizes molecular conformations by\nminimizing an energy function defined on the latent mixture models of atoms,\nthereby significantly improving performance and interpretability. Extensive\nexperiments show that our method consistently outperforms state-of-the-art\ncompetitors, providing a new and insightful paradigm to predict molecular\nground-state conformation.\n","authors":["Fanmeng Wang","Minjie Cheng","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2410.09795v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09271v1","updated":"2025-02-13T12:33:39Z","published":"2025-02-13T12:33:39Z","title":"LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via\n  Subgraph Injection","summary":"  Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in\nmodeling data with graph structures, yet recent research reveals their\nsusceptibility to adversarial attacks. Traditional attack methodologies, which\nrely on manipulating the original graph or adding links to artificially created\nnodes, often prove impractical in real-world settings. This paper introduces a\nnovel adversarial scenario involving the injection of an isolated subgraph to\ndeceive both the link recommender and the node classifier within a GNN system.\nSpecifically, the link recommender is mislead to propose links between targeted\nvictim nodes and the subgraph, encouraging users to unintentionally establish\nconnections and that would degrade the node classification accuracy, thereby\nfacilitating a successful attack. To address this, we present the LiSA\nframework, which employs a dual surrogate model and bi-level optimization to\nsimultaneously meet two adversarial objectives. Extensive experiments on\nreal-world datasets demonstrate the effectiveness of our method.\n","authors":["Wenlun Zhang","Enyan Dai","Kentaro Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2502.09271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17460v3","updated":"2025-02-13T12:32:45Z","published":"2024-01-30T21:46:09Z","title":"Rendering Wireless Environments Useful for Gradient Estimators: A\n  Zero-Order Stochastic Federated Learning Method","summary":"  Cross-device federated learning (FL) is a growing machine learning setting\nwhereby multiple edge devices collaborate to train a model without disclosing\ntheir raw data. With the great number of mobile devices participating in more\nFL applications via the wireless environment, the practical implementation of\nthese applications will be hindered due to the limited uplink capacity of\ndevices, causing critical bottlenecks. In this work, we propose a novel doubly\ncommunication-efficient zero-order (ZO) method with a one-point gradient\nestimator that replaces communicating long vectors with scalar values and that\nharnesses the nature of the wireless communication channel, overcoming the need\nto know the channel state coefficient. It is the first method that includes the\nwireless channel in the learning algorithm itself instead of wasting resources\nto analyze it and remove its impact. We then offer a thorough analysis of the\nproposed zero-order federated learning (ZOFL) framework and prove that our\nmethod converges \\textit{almost surely}, which is a novel result in nonconvex\nZO optimization. We further prove a convergence rate of\n$O(\\frac{1}{\\sqrt[3]{K}})$ in the nonconvex setting. We finally demonstrate the\npotential of our algorithm with experimental results.\n","authors":["Elissa Mhanna","Mohamad Assaad"],"pdf_url":"https://arxiv.org/pdf/2401.17460v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09268v1","updated":"2025-02-13T12:29:50Z","published":"2025-02-13T12:29:50Z","title":"GEVRM: Goal-Expressive Video Generation Model For Robust Visual\n  Manipulation","summary":"  With the rapid development of embodied artificial intelligence, significant\nprogress has been made in vision-language-action (VLA) models for general robot\ndecision-making. However, the majority of existing VLAs fail to account for the\ninevitable external perturbations encountered during deployment. These\nperturbations introduce unforeseen state information to the VLA, resulting in\ninaccurate actions and consequently, a significant decline in generalization\nperformance. The classic internal model control (IMC) principle demonstrates\nthat a closed-loop system with an internal model that includes external input\nsignals can accurately track the reference input and effectively offset the\ndisturbance. We propose a novel closed-loop VLA method GEVRM that integrates\nthe IMC principle to enhance the robustness of robot visual manipulation. The\ntext-guided video generation model in GEVRM can generate highly expressive\nfuture visual planning goals. Simultaneously, we evaluate perturbations by\nsimulating responses, which are called internal embeddings and optimized\nthrough prototype contrastive learning. This allows the model to implicitly\ninfer and distinguish perturbations from the external environment. The proposed\nGEVRM achieves state-of-the-art performance on both standard and perturbed\nCALVIN benchmarks and shows significant improvements in realistic robot tasks.\n","authors":["Hongyin Zhang","Pengxiang Ding","Shangke Lyu","Ying Peng","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09268v1.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09263v1","updated":"2025-02-13T12:24:23Z","published":"2025-02-13T12:24:23Z","title":"Unlocking the Potential of Classic GNNs for Graph-level Tasks: Simple\n  Architectures Meet Excellence","summary":"  Message-passing Graph Neural Networks (GNNs) are often criticized for their\nlimited expressiveness, issues like over-smoothing and over-squashing, and\nchallenges in capturing long-range dependencies, while Graph Transformers (GTs)\nare considered superior due to their global attention mechanisms. Literature\nfrequently suggests that GTs outperform GNNs, particularly in graph-level tasks\nsuch as graph classification and regression. In this study, we explore the\nuntapped potential of GNNs through an enhanced framework, GNN+, which\nintegrates six widely used techniques: edge feature integration, normalization,\ndropout, residual connections, feed-forward networks, and positional encoding,\nto effectively tackle graph-level tasks. We conduct a systematic evaluation of\nthree classic GNNs, namely GCN, GIN, and GatedGCN, enhanced by the GNN+\nframework across 14 well-known graph-level datasets. Our results show that,\ncontrary to the prevailing belief, classic GNNs excel in graph-level tasks,\nsecuring top three rankings across all datasets and achieving first place in\neight, while also demonstrating greater efficiency than GTs. This highlights\nthe potential of simple GNN architectures, challenging the belief that complex\nmechanisms in GTs are essential for superior graph-level performance.\n","authors":["Yuankai Luo","Lei Shi","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.09266v5","updated":"2025-02-13T12:15:30Z","published":"2021-05-19T17:22:47Z","title":"Copyright in Generative Deep Learning","summary":"  Machine-generated artworks are now part of the contemporary art scene: they\nare attracting significant investments and they are presented in exhibitions\ntogether with those created by human artists. These artworks are mainly based\non generative deep learning techniques, which have seen a formidable\ndevelopment and remarkable refinement in the very recent years. Given the\ninherent characteristics of these techniques, a series of novel legal problems\narise. In this article, we consider a set of key questions in the area of\ngenerative deep learning for the arts, including the following: is it possible\nto use copyrighted works as training set for generative models? How do we\nlegally store their copies in order to perform the training process? Who (if\nsomeone) will own the copyright on the generated data? We try to answer these\nquestions considering the law in force in both the United States of America and\nthe European Union, and potential future alternatives. We then extend our\nanalysis to code generation, which is an emerging area of generative deep\nlearning. Finally, we also formulate a set of practical guidelines for artists\nand developers working on deep learning generated art, as well as some policy\nsuggestions for policymakers.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2105.09266v5.pdf","comment":"Published in Data & Policy at\n  https://www.cambridge.org/core/journals/data-and-policy/article/copyright-in-generative-deep-learning/C401539FDF79A6AC6CEE8C5256508B5E"},{"id":"http://arxiv.org/abs/2502.09257v1","updated":"2025-02-13T12:13:25Z","published":"2025-02-13T12:13:25Z","title":"Bandit Multiclass List Classification","summary":"  We study the problem of multiclass list classification with (semi-)bandit\nfeedback, where input examples are mapped into subsets of size $m$ of a\ncollection of $K$ possible labels, and the feedback consists of the predicted\nlabels which lie in the set of true labels of the given example. Our main\nresult is for the $(\\varepsilon,\\delta)$-PAC variant of the problem for which\nwe design an algorithm that returns an $\\varepsilon$-optimal hypothesis with\nhigh probability using a sample complexity of $O \\big( (\\mathrm{poly}(K/m) + sm\n/ \\varepsilon^2) \\log (|H|/\\delta) \\big)$ where $H$ is the underlying (finite)\nhypothesis class and $s$ is an upper bound on the number of true labels for a\ngiven example. This bound improves upon known bounds for combinatorial\nsemi-bandits whenever $s \\ll K$. Moreover, in the regime where $s = O(1)$ the\nleading terms in our bound match the corresponding full-information rates,\nimplying that bandit feedback essentially comes at no cost. Our PAC learning\nalgorithm is also computationally efficient given access to an ERM oracle for\n$H$. Additionally, we consider the regret minimization setting where data can\nbe generated adversarially, and establish a regret bound of $\\widetilde O(|H| +\n\\sqrt{smT \\log |H|})$. Our results generalize and extend those of Erez et al.\n(2024) who consider the simpler single-label setting corresponding to $s=m=1$,\nand in fact hold for the more general contextual combinatorial semi-bandit\nproblem with $s$-sparse rewards.\n","authors":["Liad Erez","Tomer Koren"],"pdf_url":"https://arxiv.org/pdf/2502.09257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.02726v7","updated":"2025-02-13T12:12:28Z","published":"2021-04-06T18:00:06Z","title":"Creativity and Machine Learning: A Survey","summary":"  There is a growing interest in the area of machine learning and creativity.\nThis survey presents an overview of the history and the state of the art of\ncomputational creativity theories, key machine learning techniques (including\ngenerative deep learning), and corresponding automatic evaluation methods.\nAfter presenting a critical discussion of the key contributions in this area,\nwe outline the current research challenges and emerging opportunities in this\nfield.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2104.02726v7.pdf","comment":"Published in ACM Computing Surveys at\n  https://dl.acm.org/doi/10.1145/3664595"},{"id":"http://arxiv.org/abs/2502.07005v3","updated":"2025-02-13T12:11:58Z","published":"2025-02-10T20:10:25Z","title":"Geometry-aware RL for Manipulation of Varying Shapes and Deformable\n  Objects","summary":"  Manipulating objects with varying geometries and deformable objects is a\nmajor challenge in robotics. Tasks such as insertion with different objects or\ncloth hanging require precise control and effective modelling of complex\ndynamics. In this work, we frame this problem through the lens of a\nheterogeneous graph that comprises smaller sub-graphs, such as actuators and\nobjects, accompanied by different edge types describing their interactions.\nThis graph representation serves as a unified structure for both rigid and\ndeformable objects tasks, and can be extended further to tasks comprising\nmultiple actuators. To evaluate this setup, we present a novel and challenging\nreinforcement learning benchmark, including rigid insertion of diverse objects,\nas well as rope and cloth manipulation with multiple end-effectors. These tasks\npresent a large search space, as both the initial and target configurations are\nuniformly sampled in 3D space. To address this issue, we propose a novel\ngraph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi),\nutilizing $SE(3)$ equivariant message passing networks as the main backbone to\nexploit the geometric symmetry. In addition, by modeling explicit\nheterogeneity, HEPi can outperform Transformer-based and non-heterogeneous\nequivariant policies in terms of average returns, sample efficiency, and\ngeneralization to unseen objects.\n","authors":["Tai Hoang","Huy Le","Philipp Becker","Vien Anh Ngo","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2502.07005v3.pdf","comment":"Accepted at ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.09254v1","updated":"2025-02-13T12:10:05Z","published":"2025-02-13T12:10:05Z","title":"AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection","summary":"  Graph anomaly detection (GAD) aims to identify abnormal nodes that differ\nfrom the majority of the nodes in a graph, which has been attracting\nsignificant attention in recent years. Existing generalist graph models have\nachieved remarkable success in different graph tasks but struggle to generalize\nto the GAD task. This limitation arises from their difficulty in learning\ngeneralized knowledge for capturing the inherently infrequent, irregular and\nheterogeneous abnormality patterns in graphs from different domains. To address\nthis challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model\nthat supports zero-shot inference and few-shot prompt tuning for GAD in diverse\ngraph datasets. One key insight is that graph-agnostic representations for\nnormal and abnormal classes are required to support effective zero/few-shot GAD\nacross different graphs. Motivated by this, AnomalyGFM is pre-trained to align\ndata-independent, learnable normal and abnormal class prototypes with node\nrepresentation residuals (i.e., representation deviation of a node from its\nneighbors). The residual features essentially project the node information into\na unified feature space where we can effectively measure the abnormality of\nnodes from different graphs in a consistent way. This provides a driving force\nfor the learning of graph-agnostic, discriminative prototypes for the normal\nand abnormal classes, which can be used to enable zero-shot GAD on new graphs,\nincluding very large-scale graphs. If there are few-shot labeled normal nodes\navailable in the new graphs, AnomalyGFM can further support prompt tuning to\nleverage these nodes for better adaptation. Comprehensive experiments on 11\nwidely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM\nsignificantly outperforms state-of-the-art competing methods under both zero-\nand few-shot GAD settings.\n","authors":["Hezhe Qiao","Chaoxi Niu","Ling Chen","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2502.09254v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.09252v1","updated":"2025-02-13T12:09:17Z","published":"2025-02-13T12:09:17Z","title":"On the Importance of Embedding Norms in Self-Supervised Learning","summary":"  Self-supervised learning (SSL) allows training data representations without a\nsupervised signal and has become an important paradigm in machine learning.\nMost SSL methods employ the cosine similarity between embedding vectors and\nhence effectively embed data on a hypersphere. While this seemingly implies\nthat embedding norms cannot play any role in SSL, a few recent works have\nsuggested that embedding norms have properties related to network convergence\nand confidence. In this paper, we resolve this apparent contradiction and\nsystematically establish the embedding norm's role in SSL training. Using\ntheoretical analysis, simulations, and experiments, we show that embedding\nnorms (i) govern SSL convergence rates and (ii) encode network confidence, with\nsmaller norms corresponding to unexpected samples. Additionally, we show that\nmanipulating embedding norms can have large effects on convergence speed. Our\nfindings demonstrate that SSL embedding norms are integral to understanding and\noptimizing network behavior.\n","authors":["Andrew Draganov","Sharvaree Vadgama","Sebastian Damrich","Jan Niklas Böhm","Lucas Maes","Dmitry Kobak","Erik Bekkers"],"pdf_url":"https://arxiv.org/pdf/2502.09252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07477v2","updated":"2025-02-13T12:06:33Z","published":"2024-12-10T12:50:25Z","title":"Progressive-Resolution Policy Distillation: Leveraging Coarse-Resolution\n  Simulations for Time-Efficient Fine-Resolution Policy Learning","summary":"  In earthwork and construction, excavators often encounter large rocks mixed\nwith various soil conditions, requiring skilled operators. This paper presents\na framework for achieving autonomous excavation using reinforcement learning\n(RL) through a rock excavation simulator. In the simulation, resolution can be\ndefined by the particle size/number in the whole soil space. Fine-resolution\nsimulations closely mimic real-world behavior but demand significant\ncalculation time and challenging sample collection, while coarse-resolution\nsimulations enable faster sample collection but deviate from real-world\nbehavior. To combine the advantages of both resolutions, we explore using\npolicies developed in coarse-resolution simulations for pre-training in\nfine-resolution simulations. To this end, we propose a novel policy learning\nframework called Progressive-Resolution Policy Distillation (PRPD), which\nprogressively transfers policies through some middle-resolution simulations\nwith conservative policy transfer to avoid domain gaps that could lead to\npolicy transfer failure. Validation in a rock excavation simulator and nine\nreal-world rock environments demonstrated that PRPD reduced sampling time to\nless than 1/7 while maintaining task success rates comparable to those achieved\nthrough policy learning in a fine-resolution simulation.\n","authors":["Yuki Kadokawa","Hirotaka Tahara","Takamitsu Matsubara"],"pdf_url":"https://arxiv.org/pdf/2412.07477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09245v1","updated":"2025-02-13T12:00:50Z","published":"2025-02-13T12:00:50Z","title":"You Do Not Fully Utilize Transformer's Representation Capacity","summary":"  In contrast to RNNs, which compress previous tokens into a single hidden\nstate, Transformers can attend to all previous tokens directly. However,\nstandard Transformers only use representations from the immediately preceding\nlayer. In this paper, we show that this design choice causes representation\ncollapse and leads to suboptimal performance. To address this issue, we\nintroduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that\npreserves the model's overall memory footprint while expanding its\nrepresentational capacity by allowing access to hidden states from earlier\nlayers. Through extensive experiments across various architectures and\ndifferent lookup mechanisms, we demonstrate consistent performance improvements\non a wide range of tasks. Moreover, our analysis of the learned representation\ndynamics and our exploration of depthwise circuits reveal how LIMe integrates\ninformation across layers, pointing to promising directions for future\nresearch.\n","authors":["Gleb Gerasimov","Yaroslav Aksenov","Nikita Balagansky","Viacheslav Sinii","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2502.09245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18458v4","updated":"2025-02-13T11:59:20Z","published":"2024-05-28T17:27:20Z","title":"Asymmetrical estimator for training encapsulated deep photonic neural\n  networks","summary":"  Photonic neural networks (PNNs) are fast in-propagation and high bandwidth\nparadigms that aim to popularize reproducible NN acceleration with higher\nefficiency and lower cost. However, the training of PNN is known to be\nchallenging, where the device-to-device and system-to-system variations create\nimperfect knowledge of the PNN. Despite backpropagation (BP)-based training\nalgorithms being the industry standard for their robustness, generality, and\nfast gradient convergence for digital training, existing PNN-BP methods rely\nheavily on accurate intermediate state extraction or extensive computational\nresources for deep PNNs (DPNNs). The truncated photonic signal propagation and\nthe computation overhead bottleneck DPNN's operation efficiency and increase\nsystem construction cost. Here, we introduce the asymmetrical training (AsyT)\nmethod, tailored for encapsulated DPNNs, where the signal is preserved in the\nanalogue photonic domain for the entire structure. AsyT offers a lightweight\nsolution for DPNNs with minimum readouts, fast and energy-efficient operation,\nand minimum system footprint. AsyT's ease of operation, error tolerance, and\ngenerality aim to promote PNN acceleration in a widened operational scenario\ndespite the fabrication variations and imperfect controls. We demonstrated AsyT\nfor encapsulated DPNN with integrated photonic chips, repeatably enhancing the\nperformance from in-silico BP for different network structures and datasets.\n","authors":["Yizhi Wang","Minjia Chen","Chunhui Yao","Jie Ma","Ting Yan","Richard Penty","Qixiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.18458v4.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.09219v1","updated":"2025-02-13T11:50:04Z","published":"2025-02-13T11:50:04Z","title":"Abduction of Domain Relationships from Data for VQA","summary":"  In this paper, we study the problem of visual question answering (VQA) where\nthe image and query are represented by ASP programs that lack domain data. We\nprovide an approach that is orthogonal and complementary to existing knowledge\naugmentation techniques where we abduce domain relationships of image\nconstructs from past examples. After framing the abduction problem, we provide\na baseline approach, and an implementation that significantly improves the\naccuracy of query answering yet requires few examples.\n","authors":["Al Mehdi Saadat Chowdhury","Paulo Shakarian","Gerardo I. Simari"],"pdf_url":"https://arxiv.org/pdf/2502.09219v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09213v1","updated":"2025-02-13T11:48:46Z","published":"2025-02-13T11:48:46Z","title":"Neuro-Symbolic Contrastive Learning for Cross-domain Inference","summary":"  Pre-trained language models (PLMs) have made significant advances in natural\nlanguage inference (NLI) tasks, however their sensitivity to textual\nperturbations and dependence on large datasets indicate an over-reliance on\nshallow heuristics. In contrast, inductive logic programming (ILP) excels at\ninferring logical relationships across diverse, sparse and limited datasets,\nbut its discrete nature requires the inputs to be precisely specified, which\nlimits their application. This paper proposes a bridge between the two\napproaches: neuro-symbolic contrastive learning. This allows for smooth and\ndifferentiable optimisation that improves logical accuracy across an otherwise\ndiscrete, noisy, and sparse topological space of logical functions. We show\nthat abstract logical relationships can be effectively embedded within a\nneuro-symbolic paradigm, by representing data as logic programs and sets of\nlogic rules. The embedding space captures highly varied textual information\nwith similar semantic logical relations, but can also separate similar textual\nrelations that have dissimilar logical relations. Experimental results\ndemonstrate that our approach significantly improves the inference capabilities\nof the models in terms of generalisation and reasoning.\n","authors":["Mingyue Liu","Ryo Ueda","Zhen Wan","Katsumi Inoue","Chris G. Willcocks"],"pdf_url":"https://arxiv.org/pdf/2502.09213v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2410.13914v5","updated":"2025-02-13T11:47:51Z","published":"2024-10-17T03:08:28Z","title":"Exogenous Matching: Learning Good Proposals for Tractable Counterfactual\n  Estimation","summary":"  We propose an importance sampling method for tractable and efficient\nestimation of counterfactual expressions in general settings, named Exogenous\nMatching. By minimizing a common upper bound of counterfactual estimators, we\ntransform the variance minimization problem into a conditional distribution\nlearning problem, enabling its integration with existing conditional\ndistribution modeling approaches. We validate the theoretical results through\nexperiments under various types and settings of Structural Causal Models (SCMs)\nand demonstrate the outperformance on counterfactual estimation tasks compared\nto other existing importance sampling methods. We also explore the impact of\ninjecting structural prior knowledge (counterfactual Markov boundaries) on the\nresults. Finally, we apply this method to identifiable proxy SCMs and\ndemonstrate the unbiasedness of the estimates, empirically illustrating the\napplicability of the method to practical scenarios.\n","authors":["Yikang Chen","Dehui Du","Lili Tian"],"pdf_url":"https://arxiv.org/pdf/2410.13914v5.pdf","comment":"51 pages, 15 figures. Accepted at NeurIPS 2024, see\n  https://papers.nips.cc/paper_files/paper/2024/hash/ee94bf235482e4c1f689c04c81656dbf-Abstract-Conference.html"},{"id":"http://arxiv.org/abs/2407.06447v2","updated":"2025-02-13T11:46:41Z","published":"2024-07-08T23:11:47Z","title":"Geospatial Trajectory Generation via Efficient Abduction: Deployment for\n  Independent Testing","summary":"  The ability to generate artificial human movement patterns while meeting\nlocation and time constraints is an important problem in the security\ncommunity, particularly as it enables the study of the analog problem of\ndetecting such patterns while maintaining privacy. We frame this problem as an\ninstance of abduction guided by a novel parsimony function represented as an\naggregate truth value over an annotated logic program. This approach has the\nadded benefit of affording explainability to an analyst user. By showing that\nany subset of such a program can provide a lower bound on this parsimony\nrequirement, we are able to abduce movement trajectories efficiently through an\ninformed (i.e., A*) search. We describe how our implementation was enhanced\nwith the application of multiple techniques in order to be scaled and\nintegrated with a cloud-based software stack that included bottom-up rule\nlearning, geolocated knowledge graph retrieval/management, and interfaces with\ngovernment systems for independently conducted government-run tests for which\nwe provide results. We also report on our own experiments showing that we not\nonly provide exact results but also scale to very large scenarios and provide\nrealistic agent trajectories that can go undetected by machine learning anomaly\ndetectors.\n","authors":["Divyagna Bavikadi","Dyuman Aditya","Devendra Parkar","Paulo Shakarian","Graham Mueller","Chad Parvis","Gerardo I. Simari"],"pdf_url":"https://arxiv.org/pdf/2407.06447v2.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09203v1","updated":"2025-02-13T11:43:43Z","published":"2025-02-13T11:43:43Z","title":"Revisiting Euclidean Alignment for Transfer Learning in EEG-Based\n  Brain-Computer Interfaces","summary":"  Due to the non-stationarity and large individual differences of EEG signals,\nEEG-based brain-computer interfaces (BCIs) usually need subject-specific\ncalibration to tailor the decoding algorithm for each new subject, which is\ntime-consuming and user-unfriendly, hindering their real-world applications.\nTransfer learning (TL) has been extensively used to expedite the calibration,\nby making use of EEG data from other subjects/sessions. An important\nconsideration in TL for EEG-based BCIs is to reduce the data distribution\ndiscrepancies among different subjects/session, to avoid negative transfer.\nEuclidean alignment (EA) was proposed in 2020 to address this challenge.\nNumerous experiments from 10 different BCI paradigms demonstrated its\neffectiveness and efficiency. This paper revisits the EA, explaining its\nprocedure and correct usage, introducing its applications and extensions, and\npointing out potential new research directions. It should be very helpful to\nBCI researchers, especially those who are working on EEG signal decoding.\n","authors":["Dongrui Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06913v2","updated":"2025-02-13T11:42:53Z","published":"2025-02-10T09:26:57Z","title":"A Simple yet Effective DDG Predictor is An Unsupervised Antibody\n  Optimizer and Explainer","summary":"  The proteins that exist today have been optimized over billions of years of\nnatural evolution, during which nature creates random mutations and selects\nthem. The discovery of functionally promising mutations is challenged by the\nlimited evolutionary accessible regions, i.e., only a small region on the\nfitness landscape is beneficial. There have been numerous priors used to\nconstrain protein evolution to regions of landscapes with high-fitness\nvariants, among which the change in binding free energy (DDG) of protein\ncomplexes upon mutations is one of the most commonly used priors. However, the\nhuge mutation space poses two challenges: (1) how to improve the efficiency of\nDDG prediction for fast mutation screening; and (2) how to explain mutation\npreferences and efficiently explore accessible evolutionary regions. To address\nthese challenges, we propose a lightweight DDG predictor (Light-DDG), which\nadopts a structure-aware Transformer as the backbone and enhances it by\nknowledge distilled from existing powerful but computationally heavy DDG\npredictors. Additionally, we augmented, annotated, and released a large-scale\ndataset containing millions of mutation data for pre-training Light-DDG. We\nfind that such a simple yet effective Light-DDG can serve as a good\nunsupervised antibody optimizer and explainer. For the target antibody, we\npropose a novel Mutation Explainer to learn mutation preferences, which\naccounts for the marginal benefit of each mutation per residue. To further\nexplore accessible evolutionary regions, we conduct preference-guided antibody\noptimization and evaluate antibody candidates quickly using Light-DDG to\nidentify desirable mutations.\n","authors":["Lirong Wu","Yunfan Liu","Haitao Lin","Yufei Huang","Guojiang Zhao","Zhifeng Gao","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2502.06913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09198v1","updated":"2025-02-13T11:37:55Z","published":"2025-02-13T11:37:55Z","title":"Understanding High-Dimensional Bayesian Optimization","summary":"  Recent work reported that simple Bayesian optimization methods perform well\nfor high-dimensional real-world tasks, seemingly contradicting prior work and\ntribal knowledge. This paper investigates the 'why'. We identify fundamental\nchallenges that arise in high-dimensional Bayesian optimization and explain why\nrecent methods succeed. Our analysis shows that vanishing gradients caused by\nGaussian process initialization schemes play a major role in the failures of\nhigh-dimensional Bayesian optimization and that methods that promote local\nsearch behaviors are better suited for the task. We find that maximum\nlikelihood estimation of Gaussian process length scales suffices for\nstate-of-the-art performance. Based on this, we propose a simple variant of\nmaximum likelihood estimation called MSR that leverages these findings to\nachieve state-of-the-art performance on a comprehensive set of real-world\napplications. We also present targeted experiments to illustrate and confirm\nour findings.\n","authors":["Leonard Papenmeier","Matthias Poloczek","Luigi Nardi"],"pdf_url":"https://arxiv.org/pdf/2502.09198v1.pdf","comment":"19 pages, 20 figures"},{"id":"http://arxiv.org/abs/2502.09193v1","updated":"2025-02-13T11:33:17Z","published":"2025-02-13T11:33:17Z","title":"Generalizability through Explainability: Countering Overfitting with\n  Counterfactual Examples","summary":"  Overfitting is a well-known issue in machine learning that occurs when a\nmodel struggles to generalize its predictions to new, unseen data beyond the\nscope of its training set. Traditional techniques to mitigate overfitting\ninclude early stopping, data augmentation, and regularization. In this work, we\ndemonstrate that the degree of overfitting of a trained model is correlated\nwith the ability to generate counterfactual examples. The higher the\noverfitting, the easier it will be to find a valid counterfactual example for a\nrandomly chosen input data point. Therefore, we introduce CF-Reg, a novel\nregularization term in the training loss that controls overfitting by ensuring\nenough margin between each instance and its corresponding counterfactual.\nExperiments conducted across multiple datasets and models show that our\ncounterfactual regularizer generally outperforms existing regularization\ntechniques.\n","authors":["Flavio Giorgi","Fabiano Veglianti","Fabrizio Silvestri","Gabriele Tolomei"],"pdf_url":"https://arxiv.org/pdf/2502.09193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11619v3","updated":"2025-02-13T11:10:54Z","published":"2024-08-21T13:46:58Z","title":"Data-driven Modeling of Combined Sewer Systems for Urban Sustainability:\n  An Empirical Evaluation","summary":"  Climate change poses complex challenges, with extreme weather events becoming\nincreasingly frequent and difficult to model. Examples include the dynamics of\nCombined Sewer Systems (CSS). Overburdened CSS during heavy rainfall will\noverflow untreated wastewater into surface water bodies. Classical approaches\nto modeling the impact of extreme rainfall events rely on physical simulations,\nwhich are particularly challenging to create for large urban infrastructures.\nDeep Learning (DL) models offer a cost-effective alternative for modeling the\ncomplex dynamics of sewer systems. In this study, we present a comprehensive\nempirical evaluation of several state-of-the-art DL time series models for\npredicting sewer system dynamics in a large urban infrastructure, utilizing\nthree years of measurement data. We especially investigate the potential of DL\nmodels to maintain predictive precision during network outages by comparing\nglobal models, which have access to all variables within the sewer system, and\nlocal models, which are limited to data from a restricted set of local sensors.\nOur findings demonstrate that DL models can accurately predict the dynamics of\nsewer system load, even under network outage conditions. These results suggest\nthat DL models can effectively aid in balancing the load redistribution in CSS,\nthereby enhancing the sustainability and resilience of urban infrastructures.\n","authors":["Vipin Singh","Tianheng Ling","Teodor Chiaburu","Felix Biessmann"],"pdf_url":"https://arxiv.org/pdf/2408.11619v3.pdf","comment":"8 pages, 4 figures, accepted at 2nd Workshop on 'Public Interest AI'\n  co-located with 47th German Conference on Artificial Intelligence, Wuerzburg\n  23rd September 2024"},{"id":"http://arxiv.org/abs/2404.18573v2","updated":"2025-02-13T11:09:19Z","published":"2024-04-29T10:28:28Z","title":"Predicting Safety Misbehaviours in Autonomous Driving Systems using\n  Uncertainty Quantification","summary":"  The automated real-time recognition of unexpected situations plays a crucial\nrole in the safety of autonomous vehicles, especially in unsupported and\nunpredictable scenarios. This paper evaluates different Bayesian uncertainty\nquantification methods from the deep learning domain for the anticipatory\ntesting of safety-critical misbehaviours during system-level simulation-based\ntesting. Specifically, we compute uncertainty scores as the vehicle executes,\nfollowing the intuition that high uncertainty scores are indicative of\nunsupported runtime conditions that can be used to distinguish safe from\nfailure-inducing driving behaviors. In our study, we conducted an evaluation of\nthe effectiveness and computational overhead associated with two Bayesian\nuncertainty quantification methods, namely MC- Dropout and Deep Ensembles, for\nmisbehaviour avoidance. Overall, for three benchmarks from the Udacity\nsimulator comprising both out-of-distribution and unsafe conditions introduced\nvia mutation testing, both methods successfully detected a high number of\nout-of-bounds episodes providing early warnings several seconds in advance,\noutperforming two state-of-the-art misbehaviour prediction methods based on\nautoencoders and attention maps in terms of effectiveness and efficiency.\nNotably, Deep Ensembles detected most misbehaviours without any false alarms\nand did so even when employing a relatively small number of models, making them\ncomputationally feasible for real-time detection. Our findings suggest that\nincorporating uncertainty quantification methods is a viable approach for\nbuilding fail-safe mechanisms in deep neural network-based autonomous vehicles.\n","authors":["Ruben Grewal","Paolo Tonella","Andrea Stocco"],"pdf_url":"https://arxiv.org/pdf/2404.18573v2.pdf","comment":"In proceedings of the 17th IEEE International Conference on Software\n  Testing, Verification and Validation 2024 (ICST '24)"},{"id":"http://arxiv.org/abs/2502.09173v1","updated":"2025-02-13T10:57:25Z","published":"2025-02-13T10:57:25Z","title":"Two-Stage Representation Learning for Analyzing Movement Behavior\n  Dynamics in People Living with Dementia","summary":"  In remote healthcare monitoring, time series representation learning reveals\ncritical patient behavior patterns from high-frequency data. This study\nanalyzes home activity data from individuals living with dementia by proposing\na two-stage, self-supervised learning approach tailored to uncover low-rank\nstructures. The first stage converts time-series activities into text sequences\nencoded by a pre-trained language model, providing a rich, high-dimensional\nlatent state space using a PageRank-based method. This PageRank vector captures\nlatent state transitions, effectively compressing complex behaviour data into a\nsuccinct form that enhances interpretability. This low-rank representation not\nonly enhances model interpretability but also facilitates clustering and\ntransition analysis, revealing key behavioral patterns correlated with\nclinicalmetrics such as MMSE and ADAS-COG scores. Our findings demonstrate the\nframework's potential in supporting cognitive status prediction, personalized\ncare interventions, and large-scale health monitoring.\n","authors":["Jin Cui","Alexander Capstick","Payam Barnaghi","Gregory Scott"],"pdf_url":"https://arxiv.org/pdf/2502.09173v1.pdf","comment":"AAAI 2025 Workshop on Large Language Models and Generative AI for\n  Health"},{"id":"http://arxiv.org/abs/2502.09172v1","updated":"2025-02-13T10:56:58Z","published":"2025-02-13T10:56:58Z","title":"LOB-Bench: Benchmarking Generative AI for Finance - an Application to\n  Limit Order Book Data","summary":"  While financial data presents one of the most challenging and interesting\nsequence modelling tasks due to high noise, heavy tails, and strategic\ninteractions, progress in this area has been hindered by the lack of consensus\non quantitative evaluation paradigms. To address this, we present LOB-Bench, a\nbenchmark, implemented in python, designed to evaluate the quality and realism\nof generative message-by-order data for limit order books (LOB) in the LOBSTER\nformat. Our framework measures distributional differences in conditional and\nunconditional statistics between generated and real LOB data, supporting\nflexible multivariate statistical evaluation. The benchmark also includes\nfeatures commonly used LOB statistics such as spread, order book volumes, order\nimbalance, and message inter-arrival times, along with scores from a trained\ndiscriminator network. Lastly, LOB-Bench contains \"market impact metrics\", i.e.\nthe cross-correlations and price response functions for specific events in the\ndata. We benchmark generative autoregressive state-space models, a (C)GAN, as\nwell as a parametric LOB model and find that the autoregressive GenAI approach\nbeats traditional model classes.\n","authors":["Peer Nagy","Sascha Frey","Kang Li","Bidipta Sarkar","Svitlana Vyetrenko","Stefan Zohren","Ani Calinescu","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2502.09172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09164v1","updated":"2025-02-13T10:48:11Z","published":"2025-02-13T10:48:11Z","title":"E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot\n  Object Customization","summary":"  We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked\n$\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions\nand $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient\nframework for zero-shot object image customization. Unlike prior works reliant\non resource-intensive Unet architectures, our approach employs lightweight\nmasked diffusion transformers operating on latent patches, offering\nsignificantly improved computational efficiency. The framework integrates three\ncore components: (1) an efficient masked diffusion transformer for processing\nautoencoder latents, (2) a disentangled condition design that ensures\ncompactness while preserving background alignment and fine details, and (3) a\nlearnable Conditions Collector that consolidates multiple inputs into a compact\nrepresentation for efficient denoising and learning. E-MD3C outperforms the\nexisting approach on the VITON-HD dataset across metrics such as PSNR, FID,\nSSIM, and LPIPS, demonstrating clear advantages in parameters, memory\nefficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our\nTransformer-based 468M model delivers $2.5\\times$ faster inference and uses\n$\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent\ndiffusion model.\n","authors":["Trung X. Pham","Zhang Kang","Ji Woo Hong","Xuran Zheng","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2502.09164v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.14630v2","updated":"2025-02-13T10:43:50Z","published":"2024-10-18T17:30:20Z","title":"On the Regularization of Learnable Embeddings for Time Series\n  Forecasting","summary":"  In forecasting multiple time series, accounting for the individual features\nof each sequence can be challenging. To address this, modern deep learning\nmethods for time series analysis combine a shared (global) model with local\nlayers, specific to each time series, often implemented as learnable\nembeddings. Ideally, these local embeddings should encode meaningful\nrepresentations of the unique dynamics of each sequence. However, when these\nare learned end-to-end as parameters of a forecasting model, they may end up\nacting as mere sequence identifiers. Shared processing blocks may then become\nreliant on such identifiers, limiting their transferability to new contexts. In\nthis paper, we address this issue by investigating methods to regularize the\nlearning of local learnable embeddings for time series processing.\nSpecifically, we perform the first extensive empirical study on the subject and\nshow how such regularizations consistently improve performance in widely\nadopted architectures. Furthermore, we show that methods attempting to prevent\nthe co-adaptation of local and global parameters by means of embeddings\nperturbation are particularly effective in this context. In this regard, we\ninclude in the comparison several perturbation-based regularization methods,\ngoing as far as periodically resetting the embeddings during training. The\nobtained results provide an important contribution to understanding the\ninterplay between learnable local parameters and shared processing layers: a\nkey challenge in modern time series processing models and a step toward\ndeveloping effective foundation models for time series.\n","authors":["Luca Butera","Giovanni De Felice","Andrea Cini","Cesare Alippi"],"pdf_url":"https://arxiv.org/pdf/2410.14630v2.pdf","comment":"Accepted at TMLR"},{"id":"http://arxiv.org/abs/2502.09152v1","updated":"2025-02-13T10:29:31Z","published":"2025-02-13T10:29:31Z","title":"Vertical Federated Continual Learning via Evolving Prototype Knowledge","summary":"  Vertical Federated Learning (VFL) has garnered significant attention as a\nprivacy-preserving machine learning framework for sample-aligned feature\nfederation. However, traditional VFL approaches do not address the challenges\nof class and feature continual learning, resulting in catastrophic forgetting\nof knowledge from previous tasks. To address the above challenge, we propose a\nnovel vertical federated continual learning method, named Vertical Federated\nContinual Learning via Evolving Prototype Knowledge (V-LETO), which primarily\nfacilitates the transfer of knowledge from previous tasks through the evolution\nof prototypes. Specifically, we propose an evolving prototype knowledge method,\nenabling the global model to retain both previous and current task knowledge.\nFurthermore, we introduce a model optimization technique that mitigates the\nforgetting of previous task knowledge by restricting updates to specific\nparameters of the local model, thereby enhancing overall performance. Extensive\nexperiments conducted in both CIL and FIL settings demonstrate that our method,\nV-LETO, outperforms the other state-of-the-art methods. For example, our method\noutperforms the state-of-the-art method by 10.39% and 35.15% for CIL and FIL\ntasks, respectively. Our code is available at\nhttps://anonymous.4open.science/r/V-LETO-0108/README.md.\n","authors":["Shuo Wang","Keke Gai","Jing Yu","Liehuang Zhu","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05279v2","updated":"2025-02-13T10:28:42Z","published":"2025-01-09T14:43:08Z","title":"Learning convolution operators on compact Abelian groups","summary":"  We consider the problem of learning convolution operators associated to\ncompact Abelian groups. We study a regularization-based approach and provide\ncorresponding learning guarantees, discussing natural regularity condition on\nthe convolution kernel. More precisely, we assume the convolution kernel is a\nfunction in a translation invariant Hilbert space and analyze a natural ridge\nregression (RR) estimator. Building on existing results for RR, we characterize\nthe accuracy of the estimator in terms of finite sample bounds. Interestingly,\nregularity assumptions which are classical in the analysis of RR, have a novel\nand natural interpretation in terms of space/frequency localization.\nTheoretical results are illustrated by numerical simulations.\n","authors":["Emilia Magnani","Ernesto De Vito","Philipp Hennig","Lorenzo Rosasco"],"pdf_url":"https://arxiv.org/pdf/2501.05279v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09151v1","updated":"2025-02-13T10:27:30Z","published":"2025-02-13T10:27:30Z","title":"Regularization can make diffusion models more efficient","summary":"  Diffusion models are one of the key architectures of generative AI. Their\nmain drawback, however, is the computational costs. This study indicates that\nthe concept of sparsity, well known especially in statistics, can provide a\npathway to more efficient diffusion pipelines. Our mathematical guarantees\nprove that sparsity can reduce the input dimension's influence on the\ncomputational complexity to that of a much smaller intrinsic dimension of the\ndata. Our empirical findings confirm that inducing sparsity can indeed lead to\nbetter samples at a lower cost.\n","authors":["Mahsa Taheri","Johannes Lederer"],"pdf_url":"https://arxiv.org/pdf/2502.09151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15421v2","updated":"2025-02-13T10:26:45Z","published":"2024-05-24T10:36:23Z","title":"Model-free reinforcement learning with noisy actions for automated\n  experimental control in optics","summary":"  Setting up and controlling optical systems is often a challenging and tedious\ntask. The high number of degrees of freedom to control mirrors, lenses, or\nphases of light makes automatic control challenging, especially when the\ncomplexity of the system cannot be adequately modeled due to noise or\nnon-linearities. Here, we show that reinforcement learning (RL) can overcome\nthese challenges when coupling laser light into an optical fiber, using a\nmodel-free RL approach that trains directly on the experiment without\npre-training. By utilizing the sample-efficient algorithms Soft Actor-Critic\n(SAC) or Truncated Quantile Critics (TQC), our agent learns to couple with 90%\nefficiency, comparable to the human expert. We demonstrate that direct training\non an experiment can replace extensive system modeling. Our result exemplifies\nRL's potential to tackle problems in optics, paving the way for more complex\napplications where full noise modeling is not feasible.\n","authors":["Lea Richtmann","Viktoria-S. Schmiesing","Dennis Wilken","Jan Heine","Aaron Tranter","Avishek Anand","Tobias J. Osborne","Michèle Heurs"],"pdf_url":"https://arxiv.org/pdf/2405.15421v2.pdf","comment":"10 pages + 12 pages appendices, 2 + 12 figures"},{"id":"http://arxiv.org/abs/2502.09150v1","updated":"2025-02-13T10:25:52Z","published":"2025-02-13T10:25:52Z","title":"Shortcut Learning Susceptibility in Vision Classifiers","summary":"  Shortcut learning, where machine learning models exploit spurious\ncorrelations in data instead of capturing meaningful features, poses a\nsignificant challenge to building robust and generalizable models. This\nphenomenon is prevalent across various machine learning applications, including\nvision, natural language processing, and speech recognition, where models may\nfind unintended cues that minimize training loss but fail to capture the\nunderlying structure of the data. Vision classifiers such as Convolutional\nNeural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers\n(ViTs) leverage distinct architectural principles to process spatial and\nstructural information, making them differently susceptible to shortcut\nlearning. In this study, we systematically evaluate these architectures by\nintroducing deliberate shortcuts into the dataset that are positionally\ncorrelated with class labels, creating a controlled setup to assess whether\nmodels rely on these artificial cues or learn actual distinguishing features.\nWe perform both quantitative evaluation by training on the shortcut-modified\ndataset and testing them on two different test sets -- one containing the same\nshortcuts and another without them -- to determine the extent of reliance on\nshortcuts. Additionally, qualitative evaluation is performed by using network\ninversion-based reconstruction techniques to analyze what the models\ninternalize in their weights, aiming to reconstruct the training data as\nperceived by the classifiers. We evaluate shortcut learning behavior across\nmultiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and\nCIFAR-10, to compare the susceptibility of different vision classifier\narchitectures to shortcut reliance and assess their varying degrees of\nsensitivity to spurious correlations.\n","authors":["Pirzada Suhail","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2502.09150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09143v1","updated":"2025-02-13T10:18:44Z","published":"2025-02-13T10:18:44Z","title":"Feature-based Graph Attention Networks Improve Online Continual Learning","summary":"  Online continual learning for image classification is crucial for models to\nadapt to new data while retaining knowledge of previously learned tasks. This\ncapability is essential to address real-world challenges involving dynamic\nenvironments and evolving data distributions. Traditional approaches\npredominantly employ Convolutional Neural Networks, which are limited to\nprocessing images as grids and primarily capture local patterns rather than\nrelational information. Although the emergence of transformer architectures has\nimproved the ability to capture relationships, these models often require\nsignificantly larger resources. In this paper, we present a novel online\ncontinual learning framework based on Graph Attention Networks (GATs), which\neffectively capture contextual relationships and dynamically update the\ntask-specific representation via learned attention weights. Our approach\nutilizes a pre-trained feature extractor to convert images into graphs using\nhierarchical feature maps, representing information at varying levels of\ngranularity. These graphs are then processed by a GAT and incorporate an\nenhanced global pooling strategy to improve classification performance for\ncontinual learning. In addition, we propose the rehearsal memory duplication\ntechnique that improves the representation of the previous tasks while\nmaintaining the memory budget. Comprehensive evaluations on benchmark datasets,\nincluding SVHN, CIFAR10, CIFAR100, and MiniImageNet, demonstrate the\nsuperiority of our method compared to the state-of-the-art methods.\n","authors":["Adjovi Sim","Zhengkui Wang","Aik Beng Ng","Shalini De Mello","Simon See","Wonmin Byeon"],"pdf_url":"https://arxiv.org/pdf/2502.09143v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.09140v1","updated":"2025-02-13T10:15:16Z","published":"2025-02-13T10:15:16Z","title":"Replay-free Online Continual Learning with Self-Supervised MultiPatches","summary":"  Online Continual Learning (OCL) methods train a model on a non-stationary\ndata stream where only a few examples are available at a time, often leveraging\nreplay strategies. However, usage of replay is sometimes forbidden, especially\nin applications with strict privacy regulations. Therefore, we propose\nContinual MultiPatches (CMP), an effective plug-in for existing OCL\nself-supervised learning strategies that avoids the use of replay samples. CMP\ngenerates multiple patches from a single example and projects them into a\nshared feature space, where patches coming from the same example are pushed\ntogether without collapsing into a single point. CMP surpasses replay and other\nSSL-based strategies on OCL streams, challenging the role of replay as a go-to\nsolution for self-supervised OCL.\n","authors":["Giacomo Cignoni","Andrea Cossu","Alex Gomez-Villa","Joost van de Weijer","Antonio Carta"],"pdf_url":"https://arxiv.org/pdf/2502.09140v1.pdf","comment":"Accepted at ESANN 2025"},{"id":"http://arxiv.org/abs/2502.09137v1","updated":"2025-02-13T10:13:20Z","published":"2025-02-13T10:13:20Z","title":"Trust Me, I Know the Way: Predictive Uncertainty in the Presence of\n  Shortcut Learning","summary":"  The correct way to quantify predictive uncertainty in neural networks remains\na topic of active discussion. In particular, it is unclear whether the\nstate-of-the art entropy decomposition leads to a meaningful representation of\nmodel, or epistemic, uncertainty (EU) in the light of a debate that pits\nignorance against disagreement perspectives. We aim to reconcile the\nconflicting viewpoints by arguing that both are valid but arise from different\nlearning situations. Notably, we show that the presence of shortcuts is\ndecisive for EU manifesting as disagreement.\n","authors":["Lisa Wimmer","Bernd Bischl","Ludwig Bothmann"],"pdf_url":"https://arxiv.org/pdf/2502.09137v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2502.09135v1","updated":"2025-02-13T10:11:36Z","published":"2025-02-13T10:11:36Z","title":"Interpreting and Steering Protein Language Models through Sparse\n  Autoencoders","summary":"  The rapid advancements in transformer-based language models have\nrevolutionized natural language processing, yet understanding the internal\nmechanisms of these models remains a significant challenge. This paper explores\nthe application of sparse autoencoders (SAE) to interpret the internal\nrepresentations of protein language models, specifically focusing on the ESM-2\n8M parameter model. By performing a statistical analysis on each latent\ncomponent's relevance to distinct protein annotations, we identify potential\ninterpretations linked to various protein characteristics, including\ntransmembrane regions, binding sites, and specialized motifs.\n  We then leverage these insights to guide sequence generation, shortlisting\nthe relevant latent components that can steer the model towards desired targets\nsuch as zinc finger domains. This work contributes to the emerging field of\nmechanistic interpretability in biological sequence models, offering new\nperspectives on model steering for sequence design.\n","authors":["Edith Natalia Villegas Garcia","Alessio Ansuini"],"pdf_url":"https://arxiv.org/pdf/2502.09135v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.09130v1","updated":"2025-02-13T10:07:35Z","published":"2025-02-13T10:07:35Z","title":"Finite-Time Analysis of Discrete-Time Stochastic Interpolants","summary":"  The stochastic interpolant framework offers a powerful approach for\nconstructing generative models based on ordinary differential equations (ODEs)\nor stochastic differential equations (SDEs) to transform arbitrary data\ndistributions. However, prior analyses of this framework have primarily focused\non the continuous-time setting, assuming a perfect solution of the underlying\nequations. In this work, we present the first discrete-time analysis of the\nstochastic interpolant framework, where we introduce an innovative\ndiscrete-time sampler and derive a finite-time upper bound on its distribution\nestimation error. Our result provides a novel quantification of how different\nfactors, including the distance between source and target distributions and\nestimation accuracy, affect the convergence rate and also offers a new\nprincipled way to design efficient schedules for convergence acceleration.\nFinally, numerical experiments are conducted on the discrete-time sampler to\ncorroborate our theoretical findings.\n","authors":["Yuhao Liu","Yu Chen","Rui Hu","Longbo Huang"],"pdf_url":"https://arxiv.org/pdf/2502.09130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09128v1","updated":"2025-02-13T10:05:44Z","published":"2025-02-13T10:05:44Z","title":"A Novel Dialect-Aware Framework for the Classification of Arabic\n  Dialects and Emotions","summary":"  Arabic is one of the oldest languages still in use today. As a result,\nseveral Arabic-speaking regions have developed dialects that are unique to\nthem. Dialect and emotion recognition have various uses in Arabic text\nanalysis, such as determining an online customer's origin based on their\ncomments. Furthermore, intelligent chatbots that are aware of a user's emotions\ncan respond appropriately to the user. Current research in emotion detection in\nthe Arabic language lacks awareness of how emotions are exhibited in different\ndialects, which motivates the work found in this study. This research addresses\nthe problems of dialect and emotion classification in Arabic. Specifically,\nthis is achieved by building a novel framework that can identify and predict\nArabic dialects and emotions from a given text. The framework consists of three\nmodules: A text-preprocessing module, a classification module, and a clustering\nmodule with the novel capability of building new dialect-aware emotion\nlexicons. The proposed framework generated a new emotional lexicon for\ndifferent dialects. It achieved an accuracy of 88.9% in classifying Arabic\ndialects, which outperforms the state-of-the-art results by 6.45 percentage\npoints. Furthermore, the framework achieved 89.1-79% accuracy in detecting\nemotions in the Egyptian and Gulf dialects, respectively.\n","authors":["Nasser A Alsadhan"],"pdf_url":"https://arxiv.org/pdf/2502.09128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08925v3","updated":"2025-02-13T10:00:58Z","published":"2024-10-11T15:50:31Z","title":"An Overview of Prototype Formulations for Interpretable Deep Learning","summary":"  Prototypical part networks offer interpretable alternatives to black-box deep\nlearning models. However, many of these networks rely on Euclidean prototypes,\nwhich may limit their flexibility. This work provides a comprehensive overview\nof various prototype formulations. Experiments conducted on the CUB-200-2011,\nStanford Cars, and Oxford Flowers datasets demonstrate the effectiveness and\nversatility of these different formulations.\n","authors":["Maximilian Xiling Li","Korbinian Franz Rudolf","Nils Blank","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.08925v3.pdf","comment":"Equal Contribution of M.X.Li and K.F.Rudolf"},{"id":"http://arxiv.org/abs/2303.03388v3","updated":"2025-02-13T10:00:15Z","published":"2023-03-03T07:09:17Z","title":"Multi-modal Multi-kernel Graph Learning for Autism Prediction and\n  Biomarker Discovery","summary":"  Due to its complexity, graph learning-based multi-modal integration and\nclassification is one of the most challenging obstacles for disease prediction.\nTo effectively offset the negative impact between modalities in the process of\nmulti-modal integration and extract heterogeneous information from graphs, we\npropose a novel method called MMKGL (Multi-modal Multi-Kernel Graph Learning).\nFor the problem of negative impact between modalities, we propose a multi-modal\ngraph embedding module to construct a multi-modal graph. Different from\nconventional methods that manually construct static graphs for all modalities,\neach modality generates a separate graph by adaptive learning, where a function\ngraph and a supervision graph are introduced for optimization during the\nmulti-graph fusion embedding process. We then propose a multi-kernel graph\nlearning module to extract heterogeneous information from the multi-modal\ngraph. The information in the multi-modal graph at different levels is\naggregated by convolutional kernels with different receptive field sizes,\nfollowed by generating a cross-kernel discovery tensor for disease prediction.\nOur method is evaluated on the benchmark Autism Brain Imaging Data Exchange\n(ABIDE) dataset and outperforms the state-of-the-art methods. In addition,\ndiscriminative brain regions associated with autism are identified by our\nmodel, providing guidance for the study of autism pathology.\n","authors":["Jin Liu","Junbin Mao","Hanhe Lin","Hulin Kuang","Shirui Pan","Xusheng Wu","Shan Xie","Fei Liu","Yi Pan"],"pdf_url":"https://arxiv.org/pdf/2303.03388v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09624v2","updated":"2025-02-13T09:57:50Z","published":"2024-06-13T23:19:48Z","title":"DrivAerNet++: A Large-Scale Multimodal Car Dataset with Computational\n  Fluid Dynamics Simulations and Deep Learning Benchmarks","summary":"  We present DrivAerNet++, the largest and most comprehensive multimodal\ndataset for aerodynamic car design. DrivAerNet++ comprises 8,000 diverse car\ndesigns modeled with high-fidelity computational fluid dynamics (CFD)\nsimulations. The dataset includes diverse car configurations such as fastback,\nnotchback, and estateback, with different underbody and wheel designs to\nrepresent both internal combustion engines and electric vehicles. Each entry in\nthe dataset features detailed 3D meshes, parametric models, aerodynamic\ncoefficients, and extensive flow and surface field data, along with segmented\nparts for car classification and point cloud data. This dataset supports a wide\narray of machine learning applications including data-driven design\noptimization, generative modeling, surrogate model training, CFD simulation\nacceleration, and geometric classification. With more than 39 TB of publicly\navailable engineering data, DrivAerNet++ fills a significant gap in available\nresources, providing high-quality, diverse data to enhance model training,\npromote generalization, and accelerate automotive design processes. Along with\nrigorous dataset validation, we also provide ML benchmarking results on the\ntask of aerodynamic drag prediction, showcasing the breadth of applications\nsupported by our dataset. This dataset is set to significantly impact\nautomotive design and broader engineering disciplines by fostering innovation\nand improving the fidelity of aerodynamic evaluations. Dataset and code\navailable at: https://github.com/Mohamedelrefaie/DrivAerNet.\n","authors":["Mohamed Elrefaie","Florin Morar","Angela Dai","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2406.09624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09122v1","updated":"2025-02-13T09:57:25Z","published":"2025-02-13T09:57:25Z","title":"Improving Deep Regression with Tightness","summary":"  For deep regression, preserving the ordinality of the targets with respect to\nthe feature representation improves performance across various tasks. However,\na theoretical explanation for the benefits of ordinality is still lacking. This\nwork reveals that preserving ordinality reduces the conditional entropy\n$H(Z|Y)$ of representation $Z$ conditional on the target $Y$. However, our\nfindings reveal that typical regression losses do little to reduce $H(Z|Y)$,\neven though it is vital for generalization performance. With this motivation,\nwe introduce an optimal transport-based regularizer to preserve the similarity\nrelationships of targets in the feature space to reduce $H(Z|Y)$. Additionally,\nwe introduce a simple yet efficient strategy of duplicating the regressor\ntargets, also with the aim of reducing $H(Z|Y)$. Experiments on three\nreal-world regression tasks verify the effectiveness of our strategies to\nimprove deep regression. Code:\nhttps://github.com/needylove/Regression_tightness.\n","authors":["Shihao Zhang","Yuguang Yan","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2502.09122v1.pdf","comment":"ICLR 2025, Code: https://github.com/needylove/Regression_tightness"},{"id":"http://arxiv.org/abs/2404.03713v2","updated":"2025-02-13T09:48:12Z","published":"2024-04-04T17:46:20Z","title":"Explaining Explainability: Recommendations for Effective Use of Concept\n  Activation Vectors","summary":"  Concept-based explanations translate the internal representations of deep\nlearning models into a language that humans are familiar with: concepts. One\npopular method for finding concepts is Concept Activation Vectors (CAVs), which\nare learnt using a probe dataset of concept exemplars. In this work, we\ninvestigate three properties of CAVs: (1) inconsistency across layers, (2)\nentanglement with other concepts, and (3) spatial dependency. Each property\nprovides both challenges and opportunities in interpreting models. We introduce\ntools designed to detect the presence of these properties, provide insight into\nhow each property can lead to misleading explanations, and provide\nrecommendations to mitigate their impact. To demonstrate practical\napplications, we apply our recommendations to a melanoma classification task,\nshowing how entanglement can lead to uninterpretable results and that the\nchoice of negative probe set can have a substantial impact on the meaning of a\nCAV. Further, we show that understanding these properties can be used to our\nadvantage. For example, we introduce spatially dependent CAVs to test if a\nmodel is translation invariant with respect to a specific concept and class.\nOur experiments are performed on natural images (ImageNet), skin lesions (ISIC\n2019), and a new synthetic dataset, Elements. Elements is designed to capture a\nknown ground truth relationship between concepts and classes. We release this\ndataset to facilitate further research in understanding and evaluating\ninterpretability methods.\n","authors":["Angus Nicolson","Lisa Schut","J. Alison Noble","Yarin Gal"],"pdf_url":"https://arxiv.org/pdf/2404.03713v2.pdf","comment":"Accepted by Transactions on Machine Learning Research (02/2025)"},{"id":"http://arxiv.org/abs/2502.08644v2","updated":"2025-02-13T09:48:02Z","published":"2025-02-12T18:58:34Z","title":"Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and\n  learning in neural networks","summary":"  The brain can rapidly adapt to new contexts and learn from limited data, a\ncoveted characteristic that artificial intelligence algorithms have struggled\nto mimic. Inspired by oscillatory rhythms of the mechanical structures of\nneural cells, we developed a learning paradigm that is based on oscillations in\nlink strengths and associates learning with the coordination of these\noscillations. We find that this paradigm yields rapid adaptation and learning\nin artificial neural networks. Link oscillations can rapidly change\ncoordination, endowing the network with the ability to sense subtle context\nchanges in an unsupervised manner. In other words, the network generates the\nmissing contextual tokens required to perform as a generalist AI architecture\ncapable of predicting dynamics in multiple contexts. Oscillations also allow\nthe network to extrapolate dynamics to never-seen-before contexts. These\ncapabilities make our learning paradigm a powerful starting point for novel\nmodels of learning and cognition. Furthermore, learning through link\ncoordination is agnostic to the specifics of the neural network architecture,\nhence our study opens the door for introducing rapid adaptation and learning\ncapabilities into leading AI models.\n","authors":["Hoony Kang","Wolfgang Losert"],"pdf_url":"https://arxiv.org/pdf/2502.08644v2.pdf","comment":"13 pages, 3 figures v.2 comments: Updated email, updated typo on\n  p.11: h -> h^2 for RMSE"},{"id":"http://arxiv.org/abs/2501.19334v2","updated":"2025-02-13T09:47:38Z","published":"2025-01-31T17:34:53Z","title":"The Value of Prediction in Identifying the Worst-Off","summary":"  Machine learning is increasingly used in government programs to identify and\nsupport the most vulnerable individuals, prioritizing assistance for those at\ngreatest risk over optimizing aggregate outcomes. This paper examines the\nwelfare impacts of prediction in equity-driven contexts, and how they compare\nto other policy levers, such as expanding bureaucratic capacity. Through\nmathematical models and a real-world case study on long-term unemployment\namongst German residents, we develop a comprehensive understanding of the\nrelative effectiveness of prediction in surfacing the worst-off. Our findings\nprovide clear analytical frameworks and practical, data-driven tools that\nempower policymakers to make principled decisions when designing these systems.\n","authors":["Unai Fischer-Abaigar","Christoph Kern","Juan Carlos Perdomo"],"pdf_url":"https://arxiv.org/pdf/2501.19334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02308v2","updated":"2025-02-13T09:38:00Z","published":"2025-02-04T13:24:28Z","title":"Real-Time Operator Takeover for Visuomotor Diffusion Policy Training","summary":"  We present a Real-Time Operator Takeover (RTOT) paradigm enabling operators\nto seamlessly take control of a live visuomotor diffusion policy, guiding the\nsystem back into desirable states or reinforcing specific demonstrations. We\npresent new insights in using the Mahalonobis distance to automatically\nidentify undesirable states. Once the operator has intervened and redirected\nthe system, the control is seamlessly returned to the policy, which resumes\ngenerating actions until further intervention is required. We demonstrate that\nincorporating the targeted takeover demonstrations significantly improves\npolicy performance compared to training solely with an equivalent number of,\nbut longer, initial demonstrations. We provide an in-depth analysis of using\nthe Mahalanobis distance to detect out-of-distribution states, illustrating its\nutility for identifying critical failure points during execution. Supporting\nmaterials, including videos of initial and takeover demonstrations and all rice\nscooping experiments, are available on the project website:\nhttps://operator-takeover.github.io/\n","authors":["Nils Ingelhag","Jesper Munkeby","Michael C. Welle","Marco Moletta","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2502.02308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06099v2","updated":"2025-02-13T09:35:44Z","published":"2024-06-10T08:34:13Z","title":"Sequential Binary Classification for Intrusion Detection","summary":"  Network Intrusion Detection Systems (IDS) have become increasingly important\nas networks become more vulnerable to new and sophisticated attacks. Machine\nLearning (ML)-based IDS are increasingly seen as the most effective approach to\nhandle this issue. However, IDS datasets suffer from high class imbalance,\nwhich impacts the performance of standard ML models. Different from existing\ndata-driven techniques to handling class imbalance, this paper explores a\nstructural approach to handling class imbalance in multi-class classification\n(MCC) problems. The proposed approach - Sequential Binary Classification (SBC),\nis a hierarchical cascade of (regular) binary classifiers. Experiments on\nbenchmark IDS datasets demonstrate that the structural approach to handling\nclass-imbalance, as exemplified by SBC, is a viable approach to handling the\nissue.\n","authors":["Shrihari Vasudevan","Ishan Chokshi","Raaghul Ranganathan","Nachiappan Sundaram"],"pdf_url":"https://arxiv.org/pdf/2406.06099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09106v1","updated":"2025-02-13T09:29:04Z","published":"2025-02-13T09:29:04Z","title":"Scaling Law for Stochastic Gradient Descent in Quadratically\n  Parameterized Linear Regression","summary":"  In machine learning, the scaling law describes how the model performance\nimproves with the model and data size scaling up. From a learning theory\nperspective, this class of results establishes upper and lower generalization\nbounds for a specific learning algorithm. Here, the exact algorithm running\nusing a specific model parameterization often offers a crucial implicit\nregularization effect, leading to good generalization. To characterize the\nscaling law, previous theoretical studies mainly focus on linear models,\nwhereas, feature learning, a notable process that contributes to the remarkable\nempirical success of neural networks, is regretfully vacant. This paper studies\nthe scaling law over a linear regression with the model being quadratically\nparameterized. We consider infinitely dimensional data and slope ground truth,\nboth signals exhibiting certain power-law decay rates. We study convergence\nrates for Stochastic Gradient Descent and demonstrate the learning rates for\nvariables will automatically adapt to the ground truth. As a result, in the\ncanonical linear regression, we provide explicit separations for generalization\ncurves between SGD with and without feature learning, and the\ninformation-theoretical lower bound that is agnostic to parametrization method\nand the algorithm. Our analysis for decaying ground truth provides a new\ncharacterization for the learning dynamic of the model.\n","authors":["Shihong Ding","Haihan Zhang","Hanzhen Zhao","Cong Fang"],"pdf_url":"https://arxiv.org/pdf/2502.09106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1912.05737v6","updated":"2025-02-13T13:23:14Z","published":"2019-12-12T02:28:13Z","title":"Finite sample properties of parametric MMD estimation: robustness to\n  misspecification and dependence","summary":"  Many works in statistics aim at designing a universal estimation procedure,\nthat is, an estimator that would converge to the best approximation of the\n(unknown) data generating distribution in a model, without any assumption on\nthis distribution. This question is of major interest, in particular because\nthe universality property leads to the robustness of the estimator. In this\npaper, we tackle the problem of universal estimation using a minimum distance\nestimator presented in Briol et al. (2019) based on the Maximum Mean\nDiscrepancy. We show that the estimator is robust to both dependence and to the\npresence of outliers in the dataset. Finally, we provide a theoretical study of\nthe stochastic gradient descent algorithm used to compute the estimator, and we\nsupport our findings with numerical simulations.\n  ** The proof of Proposition 4.4 in the published version contains a mistake.\nThe mistake is fixed here (and the bound is actually improved by a factor 2).\n**\n","authors":["Badr-Eddine Chérief-Abdellatif","Pierre Alquier"],"pdf_url":"https://arxiv.org/pdf/1912.05737v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05797v4","updated":"2025-02-13T10:47:50Z","published":"2023-02-11T22:18:26Z","title":"Global Convergence Rate of Deep Equilibrium Models with General\n  Activations","summary":"  In a recent paper, Ling et al. investigated the over-parametrized Deep\nEquilibrium Model (DEQ) with ReLU activation. They proved that the gradient\ndescent converges to a globally optimal solution at a linear convergence rate\nfor the quadratic loss function. This paper shows that this fact still holds\nfor DEQs with any general activation that has bounded first and second\nderivatives. Since the new activation function is generally non-homogeneous,\nbounding the least eigenvalue of the Gram matrix of the equilibrium point is\nparticularly challenging. To accomplish this task, we need to create a novel\npopulation Gram matrix and develop a new form of dual activation with Hermite\npolynomial expansion.\n","authors":["Lan V. Truong"],"pdf_url":"https://arxiv.org/pdf/2302.05797v4.pdf","comment":"Accepted to TMLR"},{"id":"http://arxiv.org/abs/2208.04284v4","updated":"2025-02-13T10:11:21Z","published":"2022-08-08T17:24:04Z","title":"On Rademacher Complexity-based Generalization Bounds for Deep Learning","summary":"  We show that the Rademacher complexity-based approach can generate\nnon-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) for\nclassifying a small number of classes of images. The development of new\ncontraction lemmas for high-dimensional mappings between vector spaces for\ngeneral Lipschitz activation functions is a key technical contribution. These\nlemmas extend and improve the Talagrand contraction lemma in a variety of\ncases. Our generalisation bound can improve Golowich et al. for ReLU DNNs.\nFurthermore, while prior works that use the Rademacher complexity-based\napproach primarily focus on ReLU DNNs, our results extend to a broader class of\nactivation functions.\n","authors":["Lan V. Truong"],"pdf_url":"https://arxiv.org/pdf/2208.04284v4.pdf","comment":"43 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.07538v2","updated":"2025-02-13T14:21:12Z","published":"2025-02-11T13:24:38Z","title":"Visual-based spatial audio generation system for multi-speaker\n  environments","summary":"  In multimedia applications such as films and video games, spatial audio\ntechniques are widely employed to enhance user experiences by simulating 3D\nsound: transforming mono audio into binaural formats. However, this process is\noften complex and labor-intensive for sound designers, requiring precise\nsynchronization of audio with the spatial positions of visual components. To\naddress these challenges, we propose a visual-based spatial audio generation\nsystem - an automated system that integrates face detection YOLOv8 for object\ndetection, monocular depth estimation, and spatial audio techniques. Notably,\nthe system operates without requiring additional binaural dataset training. The\nproposed system is evaluated against existing Spatial Audio generation system\nusing objective metrics. Experimental results demonstrate that our method\nsignificantly improves spatial consistency between audio and video, enhances\nspeech quality, and performs robustly in multi-speaker scenarios. By\nstreamlining the audio-visual alignment process, the proposed system enables\nsound engineers to achieve high-quality results efficiently, making it a\nvaluable tool for professionals in multimedia production.\n","authors":["Xiaojing Liu","Ogulcan Gurelli","Yan Wang","Joshua Reiss"],"pdf_url":"https://arxiv.org/pdf/2502.07538v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14755v2","updated":"2025-02-13T03:15:41Z","published":"2024-04-23T05:36:33Z","title":"SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework\n  with Interactive Vision-Language Models","summary":"  With the continuous advancement of vision language models (VLMs) technology,\nremarkable research achievements have emerged in the dermatology field, the\nfourth most prevalent human disease category. However, despite these\nadvancements, VLM still faces explainable problems to user in diagnosis due to\nthe inherent complexity of dermatological conditions, existing tools offer\nrelatively limited support for user comprehension. We propose SkinGEN, a\ndiagnosis-to-generation framework that leverages the stable diffusion(SD) model\nto generate reference demonstrations from diagnosis results provided by VLM,\nthereby enhancing the visual explainability for users. Through extensive\nexperiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for\nskin condition image generation. We conduct a user study with 32 participants\nevaluating both the system performance and explainability. Results demonstrate\nthat SkinGEN significantly improves users' comprehension of VLM predictions and\nfosters increased trust in the diagnostic process. This work paves the way for\nmore transparent and user-centric VLM applications in dermatology and beyond.\n","authors":["Bo Lin","Yingjing Xu","Xuanwen Bao","Zhou Zhao","Zhouyang Wang","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2404.14755v2.pdf","comment":null}]}}